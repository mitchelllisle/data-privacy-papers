
<h2>2024-08</h2>

<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.00327v1">Demo: FedCampus: A Real-world Privacy-preserving Mobile Application for
  Smart Campus via Federated Learning & Analytics</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2024-08-31T01:58:36Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jiaxiang Geng, Beilong Tang, Boyan Zhang, Jiaqi Shao, Bing Luo</p>
    <p><b>Summary:</b> In this demo, we introduce FedCampus, a privacy-preserving mobile application
for smart \underline{campus} with \underline{fed}erated learning (FL) and
federated analytics (FA). FedCampus enables cross-platform on-device FL/FA for
both iOS and Android, supporting continuously models and algorithms deployment
(MLOps). Our app integrates privacy-preserving processed data via differential
privacy (DP) from smartwatches, where the processed parameters are used for
FL/FA through the FedCampus backend platform. We distributed 100 smartwatches
to volunteers at Duke Kunshan University and have successfully completed a
series of smart campus tasks featuring capabilities such as sleep tracking,
physical activity monitoring, personalized recommendations, and heavy hitters.
Our project is opensourced at https://github.com/FedCampus/FedCampus_Flutter.
See the FedCampus video at https://youtu.be/k5iu46IjA38.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.17378v1">Empowering Open Data Sharing for Social Good: A Privacy-Aware Approach</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB">
  <p><b>Published on:</b> 2024-08-30T16:14:32Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tânia Carvalho, Luís Antunes, Cristina Costa, Nuno Moniz</p>
    <p><b>Summary:</b> The Covid-19 pandemic has affected the world at multiple levels. Data sharing
was pivotal for advancing research to understand the underlying causes and
implement effective containment strategies. In response, many countries have
promoted the availability of daily cases to support research initiatives,
fostering collaboration between organisations and making such data available to
the public through open data platforms. Despite the several advantages of data
sharing, one of the major concerns before releasing health data is its impact
on individuals' privacy. Such a sharing process should be based on
state-of-the-art methods in Data Protection by Design and by Default. In this
paper, we use a data set related to Covid-19 cases in the second largest
hospital in Portugal to show how it is feasible to ensure data privacy while
improving the quality and maintaining the utility of the data. Our goal is to
demonstrate how knowledge exchange in multidisciplinary teams of healthcare
practitioners, data privacy, and data science experts is crucial to
co-developing strategies that ensure high utility of de-identified data.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.17354v1">Forget to Flourish: Leveraging Machine-Unlearning on Pretrained Language
  Models for Privacy Leakage</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-30T15:35:09Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Md Rafi Ur Rashid, Jing Liu, Toshiaki Koike-Akino, Shagufta Mehnaz, Ye Wang</p>
    <p><b>Summary:</b> Fine-tuning large language models on private data for downstream applications
poses significant privacy risks in potentially exposing sensitive information.
Several popular community platforms now offer convenient distribution of a
large variety of pre-trained models, allowing anyone to publish without
rigorous verification. This scenario creates a privacy threat, as pre-trained
models can be intentionally crafted to compromise the privacy of fine-tuning
datasets. In this study, we introduce a novel poisoning technique that uses
model-unlearning as an attack tool. This approach manipulates a pre-trained
language model to increase the leakage of private data during the fine-tuning
process. Our method enhances both membership inference and data extraction
attacks while preserving model utility. Experimental results across different
models, datasets, and fine-tuning setups demonstrate that our attacks
significantly surpass baseline performance. This work serves as a cautionary
note for users who download pre-trained models from unverified sources,
highlighting the potential risks involved.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.17263v1">Privacy-Preserving Set-Based Estimation Using Differential Privacy and
  Zonotopes</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-30T13:05:38Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mohammed M. Dawoud, Changxin Liu, Karl H. Johansson, Amr Alanwar</p>
    <p><b>Summary:</b> For large-scale cyber-physical systems, the collaboration of spatially
distributed sensors is often needed to perform the state estimation process.
Privacy concerns arise from disclosing sensitive measurements to a cloud
estimator. To solve this issue, we propose a differentially private set-based
estimation protocol that guarantees true state containment in the estimated set
and differential privacy for the sensitive measurements throughout the
set-based state estimation process within the central and local differential
privacy models. Zonotopes are employed in the proposed differentially private
set-based estimator, offering computational advantages in set operations. We
consider a plant of a non-linear discrete-time dynamical system with bounded
modeling uncertainties, sensors that provide sensitive measurements with
bounded measurement uncertainties, and a cloud estimator that predicts the
system's state. The privacy-preserving noise perturbs the centers of
measurement zonotopes, thereby concealing the precise position of these
zonotopes, i.e., ensuring privacy preservation for the sets containing
sensitive measurements. Compared to existing research, our approach achieves
less privacy loss and utility loss through the central and local differential
privacy models by leveraging a numerically optimized truncated noise
distribution. The proposed estimator is perturbed by weaker noise than the
analytical approaches in the literature to guarantee the same level of privacy,
therefore improving the estimation utility. Numerical and comparison
experiments with truncated Laplace noise are presented to support our approach.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.17151v1">Investigating Privacy Leakage in Dimensionality Reduction Methods via
  Reconstruction Attack</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-08-30T09:40:52Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chayadon Lumbut, Donlapark Ponnoprat</p>
    <p><b>Summary:</b> This study investigates privacy leakage in dimensionality reduction methods
through a novel machine learning-based reconstruction attack. Employing an
\emph{informed adversary} threat model, we develop a neural network capable of
reconstructing high-dimensional data from low-dimensional embeddings.
  We evaluate six popular dimensionality reduction techniques: PCA, sparse
random projection (SRP), multidimensional scaling (MDS), Isomap, $t$-SNE, and
UMAP. Using both MNIST and NIH Chest X-ray datasets, we perform a qualitative
analysis to identify key factors affecting reconstruction quality. Furthermore,
we assess the effectiveness of an additive noise mechanism in mitigating these
reconstruction attacks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.17049v1">SPOQchain: Platform for Secure, Scalable, and Privacy-Preserving Supply
  Chain Tracing and Counterfeit Protection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-30T07:15:43Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Moritz Finke, Alexandra Dmitrienko, Jasper Stang</p>
    <p><b>Summary:</b> Product lifecycle tracing is increasingly in the focus of regulators and
producers, as shown with the initiative of the Digital Product Pass. Likewise,
new methods of counterfeit detection are developed that are, e.g., based on
Physical Unclonable Functions (PUFs). In order to ensure trust and integrity of
product lifecycle data, multiple existing supply chain tracing systems are
built on blockchain technology. However, only few solutions employ secure
identifiers such as PUFs. Furthermore, existing systems that publish the data
of individual products, in part fully transparently, have a detrimental impact
on scalability and the privacy of users. This work proposes SPOQchain, a novel
blockchain-based platform that provides comprehensive lifecycle traceability
and originality verification while ensuring high efficiency and user privacy.
The improved efficiency is achieved by a sophisticated batching mechanism that
removes lifecycle redundancies. In addition to the successful evaluation of
SPOQchain's scalability, this work provides a comprehensive analysis of privacy
and security aspects, demonstrating the need and qualification of SPOQchain for
the future of supply chain tracing.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.16913v1">Analyzing Inference Privacy Risks Through Gradients in Machine Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2024-08-29T21:21:53Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhuohang Li, Andrew Lowy, Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, Bradley Malin, Ye Wang</p>
    <p><b>Summary:</b> In distributed learning settings, models are iteratively updated with shared
gradients computed from potentially sensitive user data. While previous work
has studied various privacy risks of sharing gradients, our paper aims to
provide a systematic approach to analyze private information leakage from
gradients. We present a unified game-based framework that encompasses a broad
range of attacks including attribute, property, distributional, and user
disclosures. We investigate how different uncertainties of the adversary affect
their inferential power via extensive experiments on five datasets across
various data modalities. Our results demonstrate the inefficacy of solely
relying on data aggregation to achieve privacy against inference attacks in
distributed learning. We further evaluate five types of defenses, namely,
gradient pruning, signed gradient descent, adversarial perturbations,
variational information bottleneck, and differential privacy, under both static
and adaptive adversary settings. We provide an information-theoretic view for
analyzing the effectiveness of these defenses against inference from gradients.
Finally, we introduce a method for auditing attribute inference privacy,
improving the empirical estimation of worst-case privacy through crafting
adversarial canary records.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.00138v1">PrivacyLens: Evaluating Privacy Norm Awareness of Language Models in
  Action</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-29T17:58:38Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yijia Shao, Tianshi Li, Weiyan Shi, Yanchen Liu, Diyi Yang</p>
    <p><b>Summary:</b> As language models (LMs) are widely utilized in personalized communication
scenarios (e.g., sending emails, writing social media posts) and endowed with a
certain level of agency, ensuring they act in accordance with the contextual
privacy norms becomes increasingly critical. However, quantifying the privacy
norm awareness of LMs and the emerging privacy risk in LM-mediated
communication is challenging due to (1) the contextual and long-tailed nature
of privacy-sensitive cases, and (2) the lack of evaluation approaches that
capture realistic application scenarios. To address these challenges, we
propose PrivacyLens, a novel framework designed to extend privacy-sensitive
seeds into expressive vignettes and further into agent trajectories, enabling
multi-level evaluation of privacy leakage in LM agents' actions. We instantiate
PrivacyLens with a collection of privacy norms grounded in privacy literature
and crowdsourced seeds. Using this dataset, we reveal a discrepancy between LM
performance in answering probing questions and their actual behavior when
executing user instructions in an agent setup. State-of-the-art LMs, like GPT-4
and Llama-3-70B, leak sensitive information in 25.68% and 38.69% of cases, even
when prompted with privacy-enhancing instructions. We also demonstrate the
dynamic nature of PrivacyLens by extending each seed into multiple trajectories
to red-team LM privacy leakage risk. Dataset and code are available at
https://github.com/SALT-NLP/PrivacyLens.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.16304v1">Understanding Privacy Norms through Web Forms</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-29T07:11:09Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Hao Cui, Rahmadi Trimananda, Athina Markopoulou</p>
    <p><b>Summary:</b> Web forms are one of the primary ways to collect personal information online,
yet they are relatively under-studied. Unlike web tracking, data collection
through web forms is explicit and contextualized. Users (i) are asked to input
specific personal information types, and (ii) know the specific context (i.e.,
on which website and for what purpose). For web forms to be trusted by users,
they must meet the common sense standards of appropriate data collection
practices within a particular context (i.e., privacy norms). In this paper, we
extract the privacy norms embedded within web forms through a measurement
study. First, we build a specialized crawler to discover web forms on websites.
We run it on 11,500 popular websites, and we create a dataset of 293K web
forms. Second, to process data of this scale, we develop a cost-efficient way
to annotate web forms with form types and personal information types, using
text classifiers trained with assistance of large language models (LLMs).
Third, by analyzing the annotated dataset, we reveal common patterns of data
collection practices. We find that (i) these patterns are explained by
functional necessities and legal obligations, thus reflecting privacy norms,
and that (ii) deviations from the observed norms often signal unnecessary data
collection. In addition, we analyze the privacy policies that accompany web
forms. We show that, despite their wide adoption and use, there is a disconnect
between privacy policy disclosures and the observed privacy norms.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.15694v1">Protecting Privacy in Federated Time Series Analysis: A Pragmatic
  Technology Review for Application Developers</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-28T10:41:31Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Daniel Bachlechner, Ruben Hetfleisch, Stephan Krenn, Thomas Lorünser, Michael Rader</p>
    <p><b>Summary:</b> The federated analysis of sensitive time series has huge potential in various
domains, such as healthcare or manufacturing. Yet, to fully unlock this
potential, requirements imposed by various stakeholders must be fulfilled,
regarding, e.g., efficiency or trust assumptions. While many of these
requirements can be addressed by deploying advanced secure computation
paradigms such as fully homomorphic encryption, certain aspects require an
integration with additional privacy-preserving technologies.
  In this work, we perform a qualitative requirements elicitation based on
selected real-world use cases. We match the derived requirements categories
against the features and guarantees provided by available technologies. For
each technology, we additionally perform a maturity assessment, including the
state of standardization and availability on the market. Furthermore, we
provide a decision tree supporting application developers in identifying the
most promising technologies available matching their needs. Finally, existing
gaps are identified, highlighting research potential to advance the field.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.15688v1">PDSR: A Privacy-Preserving Diversified Service Recommendation Method on
  Distributed Data</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Retrieval-5BC0EB">
  <p><b>Published on:</b> 2024-08-28T10:25:36Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Lina Wang, Huan Yang, Yiran Shen, Chao Liu, Lianyong Qi, Xiuzhen Cheng, Feng Li</p>
    <p><b>Summary:</b> The last decade has witnessed a tremendous growth of service computing, while
efficient service recommendation methods are desired to recommend high-quality
services to users. It is well known that collaborative filtering is one of the
most popular methods for service recommendation based on QoS, and many existing
proposals focus on improving recommendation accuracy, i.e., recommending
high-quality redundant services. Nevertheless, users may have different
requirements on QoS, and hence diversified recommendation has been attracting
increasing attention in recent years to fulfill users' diverse demands and to
explore potential services. Unfortunately, the recommendation performances
relies on a large volume of data (e.g., QoS data), whereas the data may be
distributed across multiple platforms. Therefore, to enable data sharing across
the different platforms for diversified service recommendation, we propose a
Privacy-preserving Diversified Service Recommendation (PDSR) method.
Specifically, we innovate in leveraging the Locality-Sensitive Hashing (LSH)
mechanism such that privacy-preserved data sharing across different platforms
is enabled to construct a service similarity graph. Based on the similarity
graph, we propose a novel accuracy-diversity metric and design a
$2$-approximation algorithm to select $K$ services to recommend by maximizing
the accuracy-diversity measure. Extensive experiments on real datasets are
conducted to verify the efficacy of our PDSR method.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.15621v1">Convergent Differential Privacy Analysis for General Federated Learning:
  the f-DP Perspective</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-28T08:22:21Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yan Sun, Li Shen, Dacheng Tao</p>
    <p><b>Summary:</b> Federated learning (FL) is an efficient collaborative training paradigm
extensively developed with a focus on local privacy protection, and
differential privacy (DP) is a classical approach to capture and ensure the
reliability of local privacy. The powerful cooperation of FL and DP provides a
promising learning framework for large-scale private clients, juggling both
privacy securing and trustworthy learning. As the predominant algorithm of DP,
the noisy perturbation has been widely studied and incorporated into various
federated algorithms, theoretically proven to offer significant privacy
protections. However, existing analyses in noisy FL-DP mostly rely on the
composition theorem and cannot tightly quantify the privacy leakage challenges,
which is nearly tight for small numbers of communication rounds but yields an
arbitrarily loose and divergent bound under the large communication rounds.
This implies a counterintuitive judgment, suggesting that FL may not provide
adequate privacy protection during long-term training. To further investigate
the convergent privacy and reliability of the FL-DP framework, in this paper,
we comprehensively evaluate the worst privacy of two classical methods under
the non-convex and smooth objectives based on the f-DP analysis, i.e.
Noisy-FedAvg and Noisy-FedProx methods. With the aid of the
shifted-interpolation technique, we successfully prove that the worst privacy
of the Noisy-FedAvg method achieves a tight convergent lower bound. Moreover,
in the Noisy-FedProx method, with the regularization of the proxy term, the
worst privacy has a stable constant lower bound. Our analysis further provides
a solid theoretical foundation for the reliability of privacy protection in
FL-DP. Meanwhile, our conclusions can also be losslessly converted to other
classical DP analytical frameworks, e.g. $(\epsilon,\delta)$-DP and
R$\acute{\text{e}}$nyi-DP (RDP).</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.15391v2">Examining the Interplay Between Privacy and Fairness for Speech
  Processing: A Review and Perspective</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Sound-D91E36">
  <p><b>Published on:</b> 2024-08-27T20:32:01Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Anna Leschanowsky, Sneha Das</p>
    <p><b>Summary:</b> Speech technology has been increasingly deployed in various areas of daily
life including sensitive domains such as healthcare and law enforcement. For
these technologies to be effective, they must work reliably for all users while
preserving individual privacy. Although tradeoffs between privacy and utility,
as well as fairness and utility, have been extensively researched, the specific
interplay between privacy and fairness in speech processing remains
underexplored. This review and position paper offers an overview of emerging
privacy-fairness tradeoffs throughout the entire machine learning lifecycle for
speech processing. By drawing on well-established frameworks on fairness and
privacy, we examine existing biases and sources of privacy harm that coexist
during the development of speech processing models. We then highlight how
corresponding privacy-enhancing technologies have the potential to
inadvertently increase these biases and how bias mitigation strategies may
conversely reduce privacy. By raising open questions, we advocate for a
comprehensive evaluation of privacy-fairness tradeoffs for speech technology
and the development of privacy-enhancing and fairness-aware algorithms in this
domain.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.15077v2">MMASD+: A Novel Dataset for Privacy-Preserving Behavior Analysis of
  Children with Autism Spectrum Disorder</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-08-27T14:05:48Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Pavan Uttej Ravva, Behdokht Kiafar, Pinar Kullu, Jicheng Li, Anjana Bhat, Roghayeh Leila Barmaki</p>
    <p><b>Summary:</b> Autism spectrum disorder (ASD) is characterized by significant challenges in
social interaction and comprehending communication signals. Recently,
therapeutic interventions for ASD have increasingly utilized Deep learning
powered-computer vision techniques to monitor individual progress over time.
These models are trained on private, non-public datasets from the autism
community, creating challenges in comparing results across different models due
to privacy-preserving data-sharing issues. This work introduces MMASD+, an
enhanced version of the novel open-source dataset called Multimodal ASD
(MMASD). MMASD+ consists of diverse data modalities, including 3D-Skeleton, 3D
Body Mesh, and Optical Flow data. It integrates the capabilities of Yolov8 and
Deep SORT algorithms to distinguish between the therapist and children,
addressing a significant barrier in the original dataset. Additionally, a
Multimodal Transformer framework is proposed to predict 11 action types and the
presence of ASD. This framework achieves an accuracy of 95.03% for predicting
action types and 96.42% for predicting ASD presence, demonstrating over a 10%
improvement compared to models trained on single data modalities. These
findings highlight the advantages of integrating multiple data modalities
within the Multimodal Transformer framework.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.14830v1">PolicyLR: A Logic Representation For Privacy Policies</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2024-08-27T07:27:16Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ashish Hooda, Rishabh Khandelwal, Prasad Chalasani, Kassem Fawaz, Somesh Jha</p>
    <p><b>Summary:</b> Privacy policies are crucial in the online ecosystem, defining how services
handle user data and adhere to regulations such as GDPR and CCPA. However,
their complexity and frequent updates often make them difficult for
stakeholders to understand and analyze. Current automated analysis methods,
which utilize natural language processing, have limitations. They typically
focus on individual tasks and fail to capture the full context of the policies.
We propose PolicyLR, a new paradigm that offers a comprehensive
machine-readable representation of privacy policies, serving as an all-in-one
solution for multiple downstream tasks. PolicyLR converts privacy policies into
a machine-readable format using valuations of atomic formulae, allowing for
formal definitions of tasks like compliance and consistency. We have developed
a compiler that transforms unstructured policy text into this format using
off-the-shelf Large Language Models (LLMs). This compiler breaks down the
transformation task into a two-stage translation and entailment procedure. This
procedure considers the full context of the privacy policy to infer a complex
formula, where each formula consists of simpler atomic formulae. The advantage
of this model is that PolicyLR is interpretable by design and grounded in
segments of the privacy policy. We evaluated the compiler using ToS;DR, a
community-annotated privacy policy entailment dataset. Utilizing open-source
LLMs, our compiler achieves precision and recall values of 0.91 and 0.88,
respectively. Finally, we demonstrate the utility of PolicyLR in three privacy
tasks: Policy Compliance, Inconsistency Detection, and Privacy Comparison
Shopping.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.14753v1">CoopASD: Cooperative Machine Anomalous Sound Detection with Privacy
  Concerns</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Sound-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB"> 
  <p><b>Published on:</b> 2024-08-27T03:07:03Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Anbai Jiang, Yuchen Shi, Pingyi Fan, Wei-Qiang Zhang, Jia Liu</p>
    <p><b>Summary:</b> Machine anomalous sound detection (ASD) has emerged as one of the most
promising applications in the Industrial Internet of Things (IIoT) due to its
unprecedented efficacy in mitigating risks of malfunctions and promoting
production efficiency. Previous works mainly investigated the machine ASD task
under centralized settings. However, developing the ASD system under
decentralized settings is crucial in practice, since the machine data are
dispersed in various factories and the data should not be explicitly shared due
to privacy concerns. To enable these factories to cooperatively develop a
scalable ASD model while preserving their privacy, we propose a novel framework
named CoopASD, where each factory trains an ASD model on its local dataset, and
a central server aggregates these local models periodically. We employ a
pre-trained model as the backbone of the ASD model to improve its robustness
and develop specialized techniques to stabilize the model under a completely
non-iid and domain shift setting. Compared with previous state-of-the-art
(SOTA) models trained in centralized settings, CoopASD showcases competitive
results with negligible degradation of 0.08%. We also conduct extensive
ablation studies to demonstrate the effectiveness of CoopASD.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.14735v1">PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework
  with Correlated Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Multimedia-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2024-08-27T02:03:36Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xianzhi Zhang, Yipeng Zhou, Di Wu, Quan Z. Sheng, Miao Hu, Linchang Xiao</p>
    <p><b>Summary:</b> Online video streaming has evolved into an integral component of the
contemporary Internet landscape. Yet, the disclosure of user requests presents
formidable privacy challenges. As users stream their preferred online videos,
their requests are automatically seized by video content providers, potentially
leaking users' privacy.
  Unfortunately, current protection methods are not well-suited to preserving
user request privacy from content providers while maintaining high-quality
online video services. To tackle this challenge, we introduce a novel
Privacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge
devices to pre-fetch and cache videos, ensuring the privacy of users' requests
while optimizing the efficiency of edge caching. More specifically, we design
PPVF with three core components: (1) \textit{Online privacy budget scheduler},
which employs a theoretically guaranteed online algorithm to select
non-requested videos as candidates with assigned privacy budgets. Alternative
videos are chosen by an online algorithm that is theoretically guaranteed to
consider both video utilities and available privacy budgets. (2) \textit{Noisy
video request generator}, which generates redundant video requests (in addition
to original ones) utilizing correlated differential privacy to obfuscate
request privacy. (3) \textit{Online video utility predictor}, which leverages
federated learning to collaboratively evaluate video utility in an online
fashion, aiding in video selection in (1) and noise generation in (2). Finally,
we conduct extensive experiments using real-world video request traces from
Tencent Video. The results demonstrate that PPVF effectively safeguards user
request privacy while upholding high video caching performance.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.14689v1">Federated User Preference Modeling for Privacy-Preserving Cross-Domain
  Recommendation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Retrieval-5BC0EB">
  <p><b>Published on:</b> 2024-08-26T23:29:03Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Li Wang, Shoujin Wang, Quangui Zhang, Qiang Wu, Min Xu</p>
    <p><b>Summary:</b> Cross-domain recommendation (CDR) aims to address the data-sparsity problem
by transferring knowledge across domains. Existing CDR methods generally assume
that the user-item interaction data is shareable between domains, which leads
to privacy leakage. Recently, some privacy-preserving CDR (PPCDR) models have
been proposed to solve this problem. However, they primarily transfer simple
representations learned only from user-item interaction histories, overlooking
other useful side information, leading to inaccurate user preferences.
Additionally, they transfer differentially private user-item interaction
matrices or embeddings across domains to protect privacy. However, these
methods offer limited privacy protection, as attackers may exploit external
information to infer the original data. To address these challenges, we propose
a novel Federated User Preference Modeling (FUPM) framework. In FUPM, first, a
novel comprehensive preference exploration module is proposed to learn users'
comprehensive preferences from both interaction data and additional data
including review texts and potentially positive items. Next, a private
preference transfer module is designed to first learn differentially private
local and global prototypes, and then privately transfer the global prototypes
using a federated learning strategy. These prototypes are generalized
representations of user groups, making it difficult for attackers to infer
individual information. Extensive experiments on four CDR tasks conducted on
the Amazon and Douban datasets validate the superiority of FUPM over SOTA
baselines. Code is available at https://github.com/Lili1013/FUPM.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.14329v1">PHEVA: A Privacy-preserving Human-centric Video Anomaly Detection
  Dataset</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-08-26T14:55:23Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ghazal Alinezhad Noghre, Shanle Yao, Armin Danesh Pazho, Babak Rahimi Ardabili, Vinit Katariya, Hamed Tabkhi</p>
    <p><b>Summary:</b> PHEVA, a Privacy-preserving Human-centric Ethical Video Anomaly detection
dataset. By removing pixel information and providing only de-identified human
annotations, PHEVA safeguards personally identifiable information. The dataset
includes seven indoor/outdoor scenes, featuring one novel, context-specific
camera, and offers over 5x the pose-annotated frames compared to the largest
previous dataset. This study benchmarks state-of-the-art methods on PHEVA using
a comprehensive set of metrics, including the 10% Error Rate (10ER), a metric
used for anomaly detection for the first time providing insights relevant to
real-world deployment. As the first of its kind, PHEVA bridges the gap between
conventional training and real-world deployment by introducing continual
learning benchmarks, with models outperforming traditional methods in 82.14% of
cases. The dataset is publicly available at
https://github.com/TeCSAR-UNCC/PHEVA.git.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.13460v1">DOPPLER: Differentially Private Optimizers with Low-pass Filter for
  Privacy Noise Reduction</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2024-08-24T04:27:07Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xinwei Zhang, Zhiqi Bu, Mingyi Hong, Meisam Razaviyayn</p>
    <p><b>Summary:</b> Privacy is a growing concern in modern deep-learning systems and
applications. Differentially private (DP) training prevents the leakage of
sensitive information in the collected training data from the trained machine
learning models. DP optimizers, including DP stochastic gradient descent
(DPSGD) and its variants, privatize the training procedure by gradient clipping
and DP noise injection. However, in practice, DP models trained using DPSGD and
its variants often suffer from significant model performance degradation. Such
degradation prevents the application of DP optimization in many key tasks, such
as foundation model pretraining. In this paper, we provide a novel signal
processing perspective to the design and analysis of DP optimizers. We show
that a ``frequency domain'' operation called low-pass filtering can be used to
effectively reduce the impact of DP noise. More specifically, by defining the
``frequency domain'' for both the gradient and differential privacy (DP) noise,
we have developed a new component, called DOPPLER. This component is designed
for DP algorithms and works by effectively amplifying the gradient while
suppressing DP noise within this frequency domain. As a result, it maintains
privacy guarantees and enhances the quality of the DP-protected model. Our
experiments show that the proposed DP optimizers with a low-pass filter
outperform their counterparts without the filter by 3%-10% in test accuracy on
various models and datasets. Both theoretical and practical evidence suggest
that the DOPPLER is effective in closing the gap between DP and non-DP
training.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.13424v1">Enabling Humanitarian Applications with Targeted Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-24T01:34:37Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Nitin Kohli, Joshua Blumenstock</p>
    <p><b>Summary:</b> The proliferation of mobile phones in low- and middle-income countries has
suddenly and dramatically increased the extent to which the world's poorest and
most vulnerable populations can be observed and tracked by governments and
corporations. Millions of historically "off the grid" individuals are now
passively generating digital data; these data, in turn, are being used to make
life-altering decisions about those individuals -- including whether or not
they receive government benefits, and whether they qualify for a consumer loan.
  This paper develops an approach to implementing algorithmic decisions based
on personal data, while also providing formal privacy guarantees to data
subjects. The approach adapts differential privacy to applications that require
decisions about individuals, and gives decision makers granular control over
the level of privacy guaranteed to data subjects. We show that stronger privacy
guarantees typically come at some cost, and use data from two real-world
applications -- an anti-poverty program in Togo and a consumer lending platform
in Nigeria -- to illustrate those costs. Our empirical results quantify the
tradeoff between privacy and predictive accuracy, and characterize how
different privacy guarantees impact overall program effectiveness. More
broadly, our results demonstrate a way for humanitarian programs to responsibly
use personal data, and better equip program designers to make informed
decisions about data privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.13038v1">Improving the Classification Effect of Clinical Images of Diseases for
  Multi-Source Privacy Protection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-08-23T12:52:24Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tian Bowen, Xu Zhengyang, Yin Zhihao, Wang Jingying, Yue Yutao</p>
    <p><b>Summary:</b> Privacy data protection in the medical field poses challenges to data
sharing, limiting the ability to integrate data across hospitals for training
high-precision auxiliary diagnostic models. Traditional centralized training
methods are difficult to apply due to violations of privacy protection
principles. Federated learning, as a distributed machine learning framework,
helps address this issue, but it requires multiple hospitals to participate in
training simultaneously, which is hard to achieve in practice. To address these
challenges, we propose a medical privacy data training framework based on data
vectors. This framework allows each hospital to fine-tune pre-trained models on
private data, calculate data vectors (representing the optimization direction
of model parameters in the solution space), and sum them up to generate
synthetic weights that integrate model information from multiple hospitals.
This approach enhances model performance without exchanging private data or
requiring synchronous training. Experimental results demonstrate that this
method effectively utilizes dispersed private data resources while protecting
patient privacy. The auxiliary diagnostic model trained using this approach
significantly outperforms models trained independently by a single hospital,
providing a new perspective for resolving the conflict between medical data
privacy protection and model training and advancing the development of medical
intelligence.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.12787v2">LLM-PBE: Assessing Data Privacy in Large Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-08-23T01:37:29Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Qinbin Li, Junyuan Hong, Chulin Xie, Jeffrey Tan, Rachel Xin, Junyi Hou, Xavier Yin, Zhun Wang, Dan Hendrycks, Zhangyang Wang, Bo Li, Bingsheng He, Dawn Song</p>
    <p><b>Summary:</b> Large Language Models (LLMs) have become integral to numerous domains,
significantly advancing applications in data management, mining, and analysis.
Their profound capabilities in processing and interpreting complex language
data, however, bring to light pressing concerns regarding data privacy,
especially the risk of unintentional training data leakage. Despite the
critical nature of this issue, there has been no existing literature to offer a
comprehensive assessment of data privacy risks in LLMs. Addressing this gap,
our paper introduces LLM-PBE, a toolkit crafted specifically for the systematic
evaluation of data privacy risks in LLMs. LLM-PBE is designed to analyze
privacy across the entire lifecycle of LLMs, incorporating diverse attack and
defense strategies, and handling various data types and metrics. Through
detailed experimentation with multiple LLMs, LLM-PBE facilitates an in-depth
exploration of data privacy concerns, shedding light on influential factors
such as model size, data characteristics, and evolving temporal dimensions.
This study not only enriches the understanding of privacy issues in LLMs but
also serves as a vital resource for future research in the field. Aimed at
enhancing the breadth of knowledge in this area, the findings, resources, and
our full technical report are made available at https://llm-pbe.github.io/,
providing an open platform for academic and practical advancements in LLM
privacy assessment.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.12385v1">Sharper Bounds for Chebyshev Moment Matching with Applications to
  Differential Privacy and Beyond</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-08-22T13:26:41Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Cameron Musco, Christopher Musco, Lucas Rosenblatt, Apoorv Vikram Singh</p>
    <p><b>Summary:</b> We study the problem of approximately recovering a probability distribution
given noisy measurements of its Chebyshev polynomial moments. We sharpen prior
work, proving that accurate recovery in the Wasserstein distance is possible
with more noise than previously known.
  As a main application, our result yields a simple "linear query" algorithm
for constructing a differentially private synthetic data distribution with
Wasserstein-1 error $\tilde{O}(1/n)$ based on a dataset of $n$ points in
$[-1,1]$. This bound is optimal up to log factors and matches a recent
breakthrough of Boedihardjo, Strohmer, and Vershynin [Probab. Theory. Rel.,
2024], which uses a more complex "superregular random walk" method to beat an
$O(1/\sqrt{n})$ accuracy barrier inherent to earlier approaches.
  We illustrate a second application of our new moment-based recovery bound in
numerical linear algebra: by improving an approach of Braverman, Krishnan, and
Musco [STOC 2022], our result yields a faster algorithm for estimating the
spectral density of a symmetric matrix up to small error in the Wasserstein
distance.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.12353v1">Distributed quasi-Newton robust estimation under differential privacy</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Statistics Theory-D91E36"> 
  <p><b>Published on:</b> 2024-08-22T12:51:28Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chuhan Wang, Lixing Zhu, Xuehu Zhu</p>
    <p><b>Summary:</b> For distributed computing with Byzantine machines under Privacy Protection
(PP) constraints, this paper develops a robust PP distributed quasi-Newton
estimation, which only requires the node machines to transmit five vectors to
the central processor with high asymptotic relative efficiency. Compared with
the gradient descent strategy which requires more rounds of transmission and
the Newton iteration strategy which requires the entire Hessian matrix to be
transmitted, the novel quasi-Newton iteration has advantages in reducing
privacy budgeting and transmission cost. Moreover, our PP algorithm does not
depend on the boundedness of gradients and second-order derivatives. When
gradients and second-order derivatives follow sub-exponential distributions, we
offer a mechanism that can ensure PP with a sufficiently high probability.
Furthermore, this novel estimator can achieve the optimal convergence rate and
the asymptotic normality. The numerical studies on synthetic and real data sets
evaluate the performance of the proposed algorithm.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.12010v1">Confounding Privacy and Inverse Composition</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-21T21:45:13Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tao Zhang, Bradley A. Malin, Netanel Raviv, Yevgeniy Vorobeychik</p>
    <p><b>Summary:</b> We introduce a novel privacy notion of ($\epsilon, \delta$)-confounding
privacy that generalizes both differential privacy and Pufferfish privacy. In
differential privacy, sensitive information is contained in the dataset while
in Pufferfish privacy, sensitive information determines data distribution.
Consequently, both assume a chain-rule relationship between the sensitive
information and the output of privacy mechanisms. Confounding privacy, in
contrast, considers general causal relationships between the dataset and
sensitive information. One of the key properties of differential privacy is
that it can be easily composed over multiple interactions with the mechanism
that maps private data to publicly shared information. In contrast, we show
that the quantification of the privacy loss under the composition of
independent ($\epsilon, \delta$)-confounding private mechanisms using the
optimal composition of differential privacy \emph{underestimates} true privacy
loss. To address this, we characterize an inverse composition framework to
tightly implement a target global ($\epsilon_{g}, \delta_{g}$)-confounding
privacy under composition while keeping individual mechanisms independent and
private. In particular, we propose a novel copula-perturbation method which
ensures that (1) each individual mechanism $i$ satisfies a target local
($\epsilon_{i}, \delta_{i}$)-confounding privacy and (2) the target global
($\epsilon_{g}, \delta_{g}$)-confounding privacy is tightly implemented by
solving an optimization problem. Finally, we study inverse composition
empirically on real datasets.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.11649v1">Video-to-Text Pedestrian Monitoring (VTPM): Leveraging Computer Vision
  and Large Language Models for Privacy-Preserve Pedestrian Activity Monitoring
  at Intersections</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-08-21T14:21:53Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ahmed S. Abdelrahman, Mohamed Abdel-Aty, Dongdong Wang</p>
    <p><b>Summary:</b> Computer vision has advanced research methodologies, enhancing system
services across various fields. It is a core component in traffic monitoring
systems for improving road safety; however, these monitoring systems don't
preserve the privacy of pedestrians who appear in the videos, potentially
revealing their identities. Addressing this issue, our paper introduces
Video-to-Text Pedestrian Monitoring (VTPM), which monitors pedestrian movements
at intersections and generates real-time textual reports, including traffic
signal and weather information. VTPM uses computer vision models for pedestrian
detection and tracking, achieving a latency of 0.05 seconds per video frame.
Additionally, it detects crossing violations with 90.2% accuracy by
incorporating traffic signal data. The proposed framework is equipped with
Phi-3 mini-4k to generate real-time textual reports of pedestrian activity
while stating safety concerns like crossing violations, conflicts, and the
impact of weather on their behavior with latency of 0.33 seconds. To enhance
comprehensive analysis of the generated textual reports, Phi-3 medium is
fine-tuned for historical analysis of these generated textual reports. This
fine-tuning enables more reliable analysis about the pedestrian safety at
intersections, effectively detecting patterns and safety critical events. The
proposed VTPM offers a more efficient alternative to video footage by using
textual reports reducing memory usage, saving up to 253 million percent,
eliminating privacy issues, and enabling comprehensive interactive historical
analysis.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.11290v1">Privacy Preservation in Delay-Based Localization Systems: Artificial
  Noise or Artificial Multipath?</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2024-08-21T02:38:44Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yuchen Zhang, Hui Chen, Henk Wymeersch</p>
    <p><b>Summary:</b> Localization plays an increasingly pivotal role in 5G/6G systems, enabling
various applications. This paper focuses on the privacy concerns associated
with delay-based localization, where unauthorized base stations attempt to
infer the location of the end user. We propose a method to disrupt localization
at unauthorized nodes by injecting artificial components into the pilot signal,
exploiting model mismatches inherent in these nodes. Specifically, we
investigate the effectiveness of two techniques, namely artificial multipath
(AM) and artificial noise (AN), in mitigating location leakage. By leveraging
the misspecified Cram\'er-Rao bound framework, we evaluate the impact of these
techniques on unauthorized localization performance. Our results demonstrate
that pilot manipulation significantly degrades the accuracy of unauthorized
localization while minimally affecting legitimate localization. Moreover, we
find that the superiority of AM over AN varies depending on the specific
scenario.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.11263v1">Privacy-Preserving Data Management using Blockchains</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB">
  <p><b>Published on:</b> 2024-08-21T01:10:39Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Michael Mireku Kwakye</p>
    <p><b>Summary:</b> Privacy-preservation policies are guidelines formulated to protect data
providers private data. Previous privacy-preservation methodologies have
addressed privacy in which data are permanently stored in repositories and
disconnected from changing data provider privacy preferences. This occurrence
becomes evident as data moves to another data repository. Hence, the need for
data providers to control and flexibly update their existing privacy
preferences due to changing data usage continues to remain a problem. This
paper proposes a blockchain-based methodology for preserving data providers
private and sensitive data. The research proposes to tightly couple data
providers private attribute data element to privacy preferences and data
accessor data element into a privacy tuple. The implementation presents a
framework of tightly-coupled relational database and blockchains. This delivers
secure, tamper-resistant, and query-efficient platform for data management and
query processing. The evaluation analysis from the implementation validates
efficient query processing of privacy-aware queries on the privacy
infrastructure.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.12387v1">Makeup-Guided Facial Privacy Protection via Untrained Neural Network
  Priors</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-08-20T17:59:39Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Fahad Shamshad, Muzammal Naseer, Karthik Nandakumar</p>
    <p><b>Summary:</b> Deep learning-based face recognition (FR) systems pose significant privacy
risks by tracking users without their consent. While adversarial attacks can
protect privacy, they often produce visible artifacts compromising user
experience. To mitigate this issue, recent facial privacy protection approaches
advocate embedding adversarial noise into the natural looking makeup styles.
However, these methods require training on large-scale makeup datasets that are
not always readily available. In addition, these approaches also suffer from
dataset bias. For instance, training on makeup data that predominantly contains
female faces could compromise protection efficacy for male faces. To handle
these issues, we propose a test-time optimization approach that solely
optimizes an untrained neural network to transfer makeup style from a reference
to a source image in an adversarial manner. We introduce two key modules: a
correspondence module that aligns regions between reference and source images
in latent space, and a decoder with conditional makeup layers. The untrained
decoder, optimized via carefully designed structural and makeup consistency
losses, generates a protected image that resembles the source but incorporates
adversarial makeup to deceive FR models. As our approach does not rely on
training with makeup face datasets, it avoids potential male/female dataset
biases while providing effective protection. We further extend the proposed
approach to videos by leveraging on temporal correlations. Experiments on
benchmark datasets demonstrate superior performance in face verification and
identification tasks and effectiveness against commercial FR systems. Our code
and models will be available at
https://github.com/fahadshamshad/deep-facial-privacy-prior</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.10715v1">Fine-Tuning a Local LLaMA-3 Large Language Model for Automated
  Privacy-Preserving Physician Letter Generation in Radiation Oncology</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-08-20T10:31:36Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yihao Hou, Christoph Bert, Ahmed Gomaa, Godehard Lahmer, Daniel Hoefler, Thomas Weissmann, Raphaela Voigt, Philipp Schubert, Charlotte Schmitter, Alina Depardon, Sabine Semrau, Andreas Maier, Rainer Fietkau, Yixing Huang, Florian Putz</p>
    <p><b>Summary:</b> Generating physician letters is a time-consuming task in daily clinical
practice. This study investigates local fine-tuning of large language models
(LLMs), specifically LLaMA models, for physician letter generation in a
privacy-preserving manner within the field of radiation oncology. Our findings
demonstrate that base LLaMA models, without fine-tuning, are inadequate for
effectively generating physician letters. The QLoRA algorithm provides an
efficient method for local intra-institutional fine-tuning of LLMs with limited
computational resources (i.e., a single 48 GB GPU workstation within the
hospital). The fine-tuned LLM successfully learns radiation oncology-specific
information and generates physician letters in an institution-specific style.
ROUGE scores of the generated summary reports highlight the superiority of the
8B LLaMA-3 model over the 13B LLaMA-2 model. Further multidimensional physician
evaluations of 10 cases reveal that, although the fine-tuned LLaMA-3 model has
limited capacity to generate content beyond the provided input data, it
successfully generates salutations, diagnoses and treatment histories,
recommendations for further treatment, and planned schedules. Overall, clinical
benefit was rated highly by the clinical experts (average score of 3.44 on a
4-point scale). With careful physician review and correction, automated
LLM-based physician letter generation has significant practical value.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.10648v1">Smart Contract Coordinated Privacy Preserving Crowd-Sensing Campaigns</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Networking and Internet Architecture-04E762">
  <p><b>Published on:</b> 2024-08-20T08:41:57Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Luca Bedogni, Stefano Ferretti</p>
    <p><b>Summary:</b> Crowd-sensing has emerged as a powerful data retrieval model, enabling
diverse applications by leveraging active user participation. However, data
availability and privacy concerns pose significant challenges. Traditional
methods like data encryption and anonymization, while essential, may not fully
address these issues. For instance, in sparsely populated areas, anonymized
data can still be traced back to individual users. Additionally, the volume of
data generated by users can reveal their identities. To develop credible
crowd-sensing systems, data must be anonymized, aggregated and separated into
uniformly sized chunks. Furthermore, decentralizing the data management
process, rather than relying on a single server, can enhance security and
trust. This paper proposes a system utilizing smart contracts and blockchain
technologies to manage crowd-sensing campaigns. The smart contract handles user
subscriptions, data encryption, and decentralized storage, creating a secure
data marketplace. Incentive policies within the smart contract encourage user
participation and data diversity. Simulation results confirm the system's
viability, highlighting the importance of user participation for data
credibility and the impact of geographical data scarcity on rewards. This
approach aims to balance data origin and reduce cheating risks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.10647v1">Privacy-preserving Universal Adversarial Defense for Black-box Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2024-08-20T08:40:39Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Qiao Li, Cong Wu, Jing Chen, Zijun Zhang, Kun He, Ruiying Du, Xinxin Wang, Qingchuang Zhao, Yang Liu</p>
    <p><b>Summary:</b> Deep neural networks (DNNs) are increasingly used in critical applications
such as identity authentication and autonomous driving, where robustness
against adversarial attacks is crucial. These attacks can exploit minor
perturbations to cause significant prediction errors, making it essential to
enhance the resilience of DNNs. Traditional defense methods often rely on
access to detailed model information, which raises privacy concerns, as model
owners may be reluctant to share such data. In contrast, existing black-box
defense methods fail to offer a universal defense against various types of
adversarial attacks. To address these challenges, we introduce DUCD, a
universal black-box defense method that does not require access to the target
model's parameters or architecture. Our approach involves distilling the target
model by querying it with data, creating a white-box surrogate while preserving
data privacy. We further enhance this surrogate model using a certified defense
based on randomized smoothing and optimized noise selection, enabling robust
defense against a broad range of adversarial attacks. Comparative evaluations
between the certified defenses of the surrogate and target models demonstrate
the effectiveness of our approach. Experiments on multiple image classification
datasets show that DUCD not only outperforms existing black-box defenses but
also matches the accuracy of white-box defenses, all while enhancing data
privacy and reducing the success rate of membership inference attacks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.10468v4">Tracing Privacy Leakage of Language Models to Training Data via Adjusted
  Influence Functions</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-20T00:40:49Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jinxin Liu, Zao Yang</p>
    <p><b>Summary:</b> The responses generated by Large Language Models (LLMs) can include sensitive
information from individuals and organizations, leading to potential privacy
leakage. This work implements Influence Functions (IFs) to trace privacy
leakage back to the training data, thereby mitigating privacy concerns of
Language Models (LMs). However, we notice that current IFs struggle to
accurately estimate the influence of tokens with large gradient norms,
potentially overestimating their influence. When tracing the most influential
samples, this leads to frequently tracing back to samples with large gradient
norm tokens, overshadowing the actual most influential samples even if their
influences are well estimated. To address this issue, we propose Heuristically
Adjusted IF (HAIF), which reduces the weight of tokens with large gradient
norms, thereby significantly improving the accuracy of tracing the most
influential samples. To establish easily obtained groundtruth for tracing
privacy leakage, we construct two datasets, PII-E and PII-CR, representing two
distinct scenarios: one with identical text in the model outputs and
pre-training data, and the other where models leverage their reasoning
abilities to generate text divergent from pre-training data. HAIF significantly
improves tracing accuracy, enhancing it by 20.96% to 73.71% on the PII-E
dataset and 3.21% to 45.93% on the PII-CR dataset, compared to the best SOTA
IFs against various GPT-2 and QWen-1.5 models. HAIF also outperforms SOTA IFs
on real-world pretraining data CLUECorpus2020, demonstrating strong robustness
regardless prompt and response lengths.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.10442v1">Feasibility of assessing cognitive impairment via distributed camera
  network and privacy-preserving edge computing</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-08-19T22:34:43Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chaitra Hegde, Yashar Kiarashi, Allan I Levey, Amy D Rodriguez, Hyeokhyen Kwon, Gari D Clifford</p>
    <p><b>Summary:</b> INTRODUCTION: Mild cognitive impairment (MCI) is characterized by a decline
in cognitive functions beyond typical age and education-related expectations.
Since, MCI has been linked to reduced social interactions and increased aimless
movements, we aimed to automate the capture of these behaviors to enhance
longitudinal monitoring.
  METHODS: Using a privacy-preserving distributed camera network, we collected
movement and social interaction data from groups of individuals with MCI
undergoing therapy within a 1700$m^2$ space. We developed movement and social
interaction features, which were then used to train a series of machine
learning algorithms to distinguish between higher and lower cognitive
functioning MCI groups.
  RESULTS: A Wilcoxon rank-sum test revealed statistically significant
differences between high and low-functioning cohorts in features such as linear
path length, walking speed, change in direction while walking, entropy of
velocity and direction change, and number of group formations in the indoor
space. Despite lacking individual identifiers to associate with specific levels
of MCI, a machine learning approach using the most significant features
provided a 71% accuracy.
  DISCUSSION: We provide evidence to show that a privacy-preserving low-cost
camera network using edge computing framework has the potential to distinguish
between different levels of cognitive impairment from the movements and social
interactions captured during group activities.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.10053v1">Privacy Checklist: Privacy Violation Detection Grounding on Contextual
  Integrity Theory</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-19T14:48:04Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Haoran Li, Wei Fan, Yulin Chen, Jiayang Cheng, Tianshu Chu, Xuebing Zhou, Peizhao Hu, Yangqiu Song</p>
    <p><b>Summary:</b> Privacy research has attracted wide attention as individuals worry that their
private data can be easily leaked during interactions with smart devices,
social platforms, and AI applications. Computer science researchers, on the
other hand, commonly study privacy issues through privacy attacks and defenses
on segmented fields. Privacy research is conducted on various sub-fields,
including Computer Vision (CV), Natural Language Processing (NLP), and Computer
Networks. Within each field, privacy has its own formulation. Though pioneering
works on attacks and defenses reveal sensitive privacy issues, they are
narrowly trapped and cannot fully cover people's actual privacy concerns.
Consequently, the research on general and human-centric privacy research
remains rather unexplored. In this paper, we formulate the privacy issue as a
reasoning problem rather than simple pattern matching. We ground on the
Contextual Integrity (CI) theory which posits that people's perceptions of
privacy are highly correlated with the corresponding social context. Based on
such an assumption, we develop the first comprehensive checklist that covers
social identities, private attributes, and existing privacy regulations. Unlike
prior works on CI that either cover limited expert annotated norms or model
incomplete social context, our proposed privacy checklist uses the whole Health
Insurance Portability and Accountability Act of 1996 (HIPAA) as an example, to
show that we can resort to large language models (LLMs) to completely cover the
HIPAA's regulations. Additionally, our checklist also gathers expert
annotations across multiple ontologies to determine private information
including but not limited to personally identifiable information (PII). We use
our preliminary results on the HIPAA to shed light on future context-centric
privacy research to cover more privacy regulations, social norms and standards.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.09943v2">Calibrating Noise for Group Privacy in Subsampled Mechanisms</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-19T12:32:50Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yangfan Jiang, Xinjian Luo, Yin Yang, Xiaokui Xiao</p>
    <p><b>Summary:</b> Given a group size m and a sensitive dataset D, group privacy (GP) releases
information about D with the guarantee that the adversary cannot infer with
high confidence whether the underlying data is D or a neighboring dataset D'
that differs from D by m records. GP generalizes the well-established notion of
differential privacy (DP) for protecting individuals' privacy; in particular,
when m=1, GP reduces to DP. Compared to DP, GP is capable of protecting the
sensitive aggregate information of a group of up to m individuals, e.g., the
average annual income among members of a yacht club. Despite its longstanding
presence in the research literature and its promising applications, GP is often
treated as an afterthought, with most approaches first developing a DP
mechanism and then using a generic conversion to adapt it for GP, treating the
DP solution as a black box. As we point out in the paper, this methodology is
suboptimal when the underlying DP solution involves subsampling, e.g., in the
classic DP-SGD method for training deep learning models. In this case, the
DP-to-GP conversion is overly pessimistic in its analysis, leading to low
utility in the published results under GP.
  Motivated by this, we propose a novel analysis framework that provides tight
privacy accounting for subsampled GP mechanisms. Instead of converting a
black-box DP mechanism to GP, our solution carefully analyzes and utilizes the
inherent randomness in subsampled mechanisms, leading to a substantially
improved bound on the privacy loss with respect to GP. The proposed solution
applies to a wide variety of foundational mechanisms with subsampling.
Extensive experiments with real datasets demonstrate that compared to the
baseline convert-from-blackbox-DP approach, our GP mechanisms achieve noise
reductions of over an order of magnitude in several practical settings,
including deep neural network training.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.09935v1">Privacy Technologies for Financial Intelligence</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-19T12:13:53Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yang Li, Thilina Ranbaduge, Kee Siong Ng</p>
    <p><b>Summary:</b> Financial crimes like terrorism financing and money laundering can have real
impacts on society, including the abuse and mismanagement of public funds,
increase in societal problems such as drug trafficking and illicit gambling
with attendant economic costs, and loss of innocent lives in the case of
terrorism activities. Complex financial crimes can be hard to detect primarily
because data related to different pieces of the overall puzzle is usually
distributed across a network of financial institutions, regulators, and
law-enforcement agencies and they cannot be easily shared due to privacy
constraints. Recent advances in Privacy-Preserving Data Matching and Machine
Learning provide an opportunity for regulators and the financial industry to
come together to solve the risk-discovery problem with technology. This paper
provides a survey of the financial intelligence landscape and where
opportunities lie for privacy technologies to improve the state-of-the-art in
financial-crime detection.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.09659v1">An Algorithm for Enhancing Privacy-Utility Tradeoff in the Privacy
  Funnel and Other Lift-based Measures</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2024-08-19T02:43:18Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mohammad Amin Zarrabian, Parastoo Sadeghi</p>
    <p><b>Summary:</b> This paper investigates the privacy funnel, a privacy-utility tradeoff
problem in which mutual information quantifies both privacy and utility. The
objective is to maximize utility while adhering to a specified privacy budget.
However, the privacy funnel represents a non-convex optimization problem,
making it challenging to achieve an optimal solution. An existing proposed
approach to this problem involves substituting the mutual information with the
lift (the exponent of information density) and then solving the optimization.
Since mutual information is the expectation of the information density, this
substitution overestimates the privacy loss and results in a final smaller
bound on the privacy of mutual information than what is allowed in the budget.
This significantly compromises the utility. To overcome this limitation, we
propose using a privacy measure that is more relaxed than the lift but stricter
than mutual information while still allowing the optimization to be efficiently
solved. Instead of directly using information density, our proposed measure is
the average of information density over the sensitive data distribution for
each observed data realization. We then introduce a heuristic algorithm capable
of achieving solutions that produce extreme privacy values, which enhances
utility. The numerical results confirm improved utility at the same privacy
budget compared to existing solutions in the literature. Additionally, we
explore two other privacy measures, $\ell_{1}$-norm and strong
$\chi^2$-divergence, demonstrating the applicability of our algorithm to these
lift-based measures. We evaluate the performance of our method by comparing its
output with previous works. Finally, we validate our heuristic approach with a
theoretical framework that estimates the optimal utility for strong
$\chi^2$-divergence, numerically showing a perfect match.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.08722v1">A Novel Buffered Federated Learning Framework for Privacy-Driven Anomaly
  Detection in IIoT</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-16T13:01:59Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Samira Kamali Poorazad, Chafika Benzaid, Tarik Taleb</p>
    <p><b>Summary:</b> Industrial Internet of Things (IIoT) is highly sensitive to data privacy and
cybersecurity threats. Federated Learning (FL) has emerged as a solution for
preserving privacy, enabling private data to remain on local IIoT clients while
cooperatively training models to detect network anomalies. However, both
synchronous and asynchronous FL architectures exhibit limitations, particularly
when dealing with clients with varying speeds due to data heterogeneity and
resource constraints. Synchronous architecture suffers from straggler effects,
while asynchronous methods encounter communication bottlenecks. Additionally,
FL models are prone to adversarial inference attacks aimed at disclosing
private training data. To address these challenges, we propose a Buffered FL
(BFL) framework empowered by homomorphic encryption for anomaly detection in
heterogeneous IIoT environments. BFL utilizes a novel weighted average time
approach to mitigate both straggler effects and communication bottlenecks,
ensuring fairness between clients with varying processing speeds through
collaboration with a buffer-based server. The performance results, derived from
two datasets, show the superiority of BFL compared to state-of-the-art FL
methods, demonstrating improved accuracy and convergence speed while enhancing
privacy preservation.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.08666v1">A Multivocal Literature Review on Privacy and Fairness in Federated
  Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-08-16T11:15:52Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Beatrice Balbierer, Lukas Heinlein, Domenique Zipperling, Niklas Kühl</p>
    <p><b>Summary:</b> Federated Learning presents a way to revolutionize AI applications by
eliminating the necessity for data sharing. Yet, research has shown that
information can still be extracted during training, making additional
privacy-preserving measures such as differential privacy imperative. To
implement real-world federated learning applications, fairness, ranging from a
fair distribution of performance to non-discriminative behaviour, must be
considered. Particularly in high-risk applications (e.g. healthcare), avoiding
the repetition of past discriminatory errors is paramount. As recent research
has demonstrated an inherent tension between privacy and fairness, we conduct a
multivocal literature review to examine the current methods to integrate
privacy and fairness in federated learning. Our analyses illustrate that the
relationship between privacy and fairness has been neglected, posing a critical
risk for real-world applications. We highlight the need to explore the
relationship between privacy, fairness, and performance, advocating for the
creation of integrated federated learning frameworks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.08642v1">The Power of Bias: Optimizing Client Selection in Federated Learning
  with Heterogeneous Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-08-16T10:19:27Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jiating Ma, Yipeng Zhou, Qi Li, Quan Z. Sheng, Laizhong Cui, Jiangchuan Liu</p>
    <p><b>Summary:</b> To preserve the data privacy, the federated learning (FL) paradigm emerges in
which clients only expose model gradients rather than original data for
conducting model training. To enhance the protection of model gradients in FL,
differentially private federated learning (DPFL) is proposed which incorporates
differentially private (DP) noises to obfuscate gradients before they are
exposed. Yet, an essential but largely overlooked problem in DPFL is the
heterogeneity of clients' privacy requirement, which can vary significantly
between clients and extremely complicates the client selection problem in DPFL.
In other words, both the data quality and the influence of DP noises should be
taken into account when selecting clients. To address this problem, we conduct
convergence analysis of DPFL under heterogeneous privacy, a generic client
selection strategy, popular DP mechanisms and convex loss. Based on convergence
analysis, we formulate the client selection problem to minimize the value of
loss function in DPFL with heterogeneous privacy, which is a convex
optimization problem and can be solved efficiently. Accordingly, we propose the
DPFL-BCS (biased client selection) algorithm. The extensive experiment results
with real datasets under both convex and non-convex loss functions indicate
that DPFL-BCS can remarkably improve model utility compared with the SOTA
baselines.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.08529v1">Privacy-Preserving Vision Transformer Using Images Encrypted with
  Restricted Random Permutation Matrices</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-08-16T04:57:21Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kouki Horio, Kiyoshi Nishikawa, Hitoshi Kiya</p>
    <p><b>Summary:</b> We propose a novel method for privacy-preserving fine-tuning vision
transformers (ViTs) with encrypted images. Conventional methods using encrypted
images degrade model performance compared with that of using plain images due
to the influence of image encryption. In contrast, the proposed encryption
method using restricted random permutation matrices can provide a higher
performance than the conventional ones.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.08475v2">Models Matter: Setting Accurate Privacy Expectations for Local and
  Central Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2024-08-16T01:21:57Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mary Anne Smart, Priyanka Nanayakkara, Rachel Cummings, Gabriel Kaptchuk, Elissa Redmiles</p>
    <p><b>Summary:</b> Differential privacy is a popular privacy-enhancing technology that has been
deployed both in industry and government agencies. Unfortunately, existing
explanations of differential privacy fail to set accurate privacy expectations
for data subjects, which depend on the choice of deployment model. We design
and evaluate new explanations of differential privacy for the local and central
models, drawing inspiration from prior work explaining other privacy-enhancing
technologies. We find that consequences-focused explanations in the style of
privacy nutrition labels that lay out the implications of differential privacy
are a promising approach for setting accurate privacy expectations. Further, we
find that while process-focused explanations are not enough to set accurate
privacy expectations, combining consequences-focused explanations with a brief
description of how differential privacy works leads to greater trust.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.08107v1">Communication-robust and Privacy-safe Distributed Estimation for
  Heterogeneous Community-level Behind-the-meter Solar Power Generation</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36">
  <p><b>Published on:</b> 2024-08-15T12:11:03Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jinglei Feng, Zhengshuo Li</p>
    <p><b>Summary:</b> The rapid growth of behind-the-meter (BTM) solar power generation systems
presents challenges for distribution system planning and scheduling due to
invisible solar power generation. To address the data leakage problem of
centralized machine-learning methods in BTM solar power generation estimation,
the federated learning (FL) method has been investigated for its distributed
learning capability. However, the conventional FL method has encountered
various challenges, including heterogeneity, communication failures, and
malicious privacy attacks. To overcome these challenges, this study proposes a
communication-robust and privacy-safe distributed estimation method for
heterogeneous community-level BTM solar power generation. Specifically, this
study adopts multi-task FL as the main structure and learns the common and
unique features of all communities. Simultaneously, it embeds an updated
parameters estimation method into the multi-task FL, automatically identifies
similarities between any two clients, and estimates the updated parameters for
unavailable clients to mitigate the negative effects of communication failures.
Finally, this study adopts a differential privacy mechanism under the dynamic
privacy budget allocation strategy to combat malicious privacy attacks and
improve model training efficiency. Case studies show that in the presence of
heterogeneity and communication failures, the proposed method exhibits better
estimation accuracy and convergence performance as compared with traditional FL
and localized learning methods, while providing stronger privacy protection.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.08002v1">Practical Privacy-Preserving Identity Verification using Third-Party
  Cloud Services and FHE (Role of Data Encoding in Circuit Depth Management)</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-15T08:12:07Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Deep Inder Mohan, Srinivas Vivek</p>
    <p><b>Summary:</b> National digital identity verification systems have played a critical role in
the effective distribution of goods and services, particularly, in developing
countries. Due to the cost involved in deploying and maintaining such systems,
combined with a lack of in-house technical expertise, governments seek to
outsource this service to third-party cloud service providers to the extent
possible. This leads to increased concerns regarding the privacy of users'
personal data. In this work, we propose a practical privacy-preserving digital
identity (ID) verification protocol where the third-party cloud services
process the identity data encrypted using a (single-key) Fully Homomorphic
Encryption (FHE) scheme such as BFV. Though the role of a trusted entity such
as government is not completely eliminated, our protocol does significantly
reduces the computation load on such parties.
  A challenge in implementing a privacy-preserving ID verification protocol
using FHE is to support various types of queries such as exact and/or fuzzy
demographic and biometric matches including secure age comparisons. From a
cryptographic engineering perspective, our main technical contribution is a
user data encoding scheme that encodes demographic and biometric user data in
only two BFV ciphertexts and yet facilitates us to outsource various types of
ID verification queries to a third-party cloud. Our encoding scheme also
ensures that the only computation done by the trusted entity is a
query-agnostic "extended" decryption. This is in stark contrast with recent
works that outsource all the non-arithmetic operations to a trusted server. We
implement our protocol using the Microsoft SEAL FHE library and demonstrate its
practicality.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.07892v3">Personhood credentials: Artificial intelligence and the value of
  privacy-preserving tools to distinguish who is real online</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2024-08-15T02:41:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Steven Adler, Zoë Hitzig, Shrey Jain, Catherine Brewer, Wayne Chang, Renée DiResta, Eddy Lazzarin, Sean McGregor, Wendy Seltzer, Divya Siddarth, Nouran Soliman, Tobin South, Connor Spelliscy, Manu Sporny, Varya Srivastava, John Bailey, Brian Christian, Andrew Critch, Ronnie Falcon, Heather Flanagan, Kim Hamilton Duffy, Eric Ho, Claire R. Leibowicz, Srikanth Nadhamuni, Alan Z. Rozenshtein, David Schnurr, Evan Shapiro, Lacey Strahm, Andrew Trask, Zoe Weinberg, Cedric Whitney, Tom Zick</p>
    <p><b>Summary:</b> Anonymity is an important principle online. However, malicious actors have
long used misleading identities to conduct fraud, spread disinformation, and
carry out other deceptive schemes. With the advent of increasingly capable AI,
bad actors can amplify the potential scale and effectiveness of their
operations, intensifying the challenge of balancing anonymity and
trustworthiness online. In this paper, we analyze the value of a new tool to
address this challenge: "personhood credentials" (PHCs), digital credentials
that empower users to demonstrate that they are real people -- not AIs -- to
online services, without disclosing any personal information. Such credentials
can be issued by a range of trusted institutions -- governments or otherwise. A
PHC system, according to our definition, could be local or global, and does not
need to be biometrics-based. Two trends in AI contribute to the urgency of the
challenge: AI's increasing indistinguishability from people online (i.e.,
lifelike content and avatars, agentic activity), and AI's increasing
scalability (i.e., cost-effectiveness, accessibility). Drawing on a long
history of research into anonymous credentials and "proof-of-personhood"
systems, personhood credentials give people a way to signal their
trustworthiness on online platforms, and offer service providers new tools for
reducing misuse by bad actors. In contrast, existing countermeasures to
automated deception -- such as CAPTCHAs -- are inadequate against sophisticated
AI, while stringent identity verification solutions are insufficiently private
for many use-cases. After surveying the benefits of personhood credentials, we
also examine deployment risks and design challenges. We conclude with
actionable next steps for policymakers, technologists, and standards bodies to
consider in consultation with the public.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.07614v1">Practical Considerations for Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-14T15:28:28Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kareem Amin, Alex Kulesza, Sergei Vassilvitskii</p>
    <p><b>Summary:</b> Differential privacy is the gold standard for statistical data release. Used
by governments, companies, and academics, its mathematically rigorous
guarantees and worst-case assumptions on the strength and knowledge of
attackers make it a robust and compelling framework for reasoning about
privacy. However, even with landmark successes, differential privacy has not
achieved widespread adoption in everyday data use and data protection. In this
work we examine some of the practical obstacles that stand in the way.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.07021v1">Improved Counting under Continual Observation with Pure Differential
  Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B">
  <p><b>Published on:</b> 2024-08-13T16:36:33Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Joel Daniel Andersson, Rasmus Pagh, Sahel Torkamani</p>
    <p><b>Summary:</b> Counting under continual observation is a well-studied problem in the area of
differential privacy. Given a stream of updates $x_1,x_2,\dots,x_T \in \{0,1\}$
the problem is to continuously release estimates of the prefix sums
$\sum_{i=1}^t x_i$ for $t=1,\dots,T$ while protecting each input $x_i$ in the
stream with differential privacy. Recently, significant leaps have been made in
our understanding of this problem under $\textit{approximate}$ differential
privacy, aka. $(\varepsilon,\delta)$$\textit{-differential privacy}$. However,
for the classical case of $\varepsilon$-differential privacy, we are not aware
of any improvement in mean squared error since the work of Honaker (TPDP 2015).
In this paper we present such an improvement, reducing the mean squared error
by a factor of about 4, asymptotically. The key technique is a new
generalization of the binary tree mechanism that uses a $k$-ary number system
with $\textit{negative digits}$ to improve the privacy-accuracy trade-off. Our
mechanism improves the mean squared error over all 'optimal'
$(\varepsilon,\delta)$-differentially private factorization mechanisms based on
Gaussian noise whenever $\delta$ is sufficiently small. Specifically, using
$k=19$ we get an asymptotic improvement over the bound given in the work by
Henzinger, Upadhyay and Upadhyay (SODA 2023) when $\delta = O(T^{-0.92})$.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.07006v1">The Complexities of Differential Privacy for Survey Data</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-13T16:15:42Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jörg Drechsler, James Bailie</p>
    <p><b>Summary:</b> The concept of differential privacy (DP) has gained substantial attention in
recent years, most notably since the U.S. Census Bureau announced the adoption
of the concept for its 2020 Decennial Census. However, despite its attractive
theoretical properties, implementing DP in practice remains challenging,
especially when it comes to survey data. In this paper we present some results
from an ongoing project funded by the U.S. Census Bureau that is exploring the
possibilities and limitations of DP for survey data. Specifically, we identify
five aspects that need to be considered when adopting DP in the survey context:
the multi-staged nature of data production; the limited privacy amplification
from complex sampling designs; the implications of survey-weighted estimates;
the weighting adjustments for nonresponse and other data deficiencies, and the
imputation of missing values. We summarize the project's key findings with
respect to each of these aspects and also discuss some of the challenges that
still need to be addressed before DP could become the new data protection
standard at statistical agencies.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.07004v1">Casper: Prompt Sanitization for Protecting User Privacy in Web-Based
  Large Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-08-13T16:08:37Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chun Jie Chong, Chenxi Hou, Zhihao Yao, Seyed Mohammadjavad Seyed Talebi</p>
    <p><b>Summary:</b> Web-based Large Language Model (LLM) services have been widely adopted and
have become an integral part of our Internet experience. Third-party plugins
enhance the functionalities of LLM by enabling access to real-world data and
services. However, the privacy consequences associated with these services and
their third-party plugins are not well understood. Sensitive prompt data are
stored, processed, and shared by cloud-based LLM providers and third-party
plugins. In this paper, we propose Casper, a prompt sanitization technique that
aims to protect user privacy by detecting and removing sensitive information
from user inputs before sending them to LLM services. Casper runs entirely on
the user's device as a browser extension and does not require any changes to
the online LLM services. At the core of Casper is a three-layered sanitization
mechanism consisting of a rule-based filter, a Machine Learning (ML)-based
named entity recognizer, and a browser-based local LLM topic identifier. We
evaluate Casper on a dataset of 4000 synthesized prompts and show that it can
effectively filter out Personal Identifiable Information (PII) and
privacy-sensitive topics with high accuracy, at 98.5% and 89.9%, respectively.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.08909v1">An Adaptive Differential Privacy Method Based on Federated Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2024-08-13T13:08:11Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhiqiang Wang, Xinyue Yu, Qianli Huang, Yongguang Gong</p>
    <p><b>Summary:</b> Differential privacy is one of the methods to solve the problem of privacy
protection in federated learning. Setting the same privacy budget for each
round will result in reduced accuracy in training. The existing methods of the
adjustment of privacy budget consider fewer influencing factors and tend to
ignore the boundaries, resulting in unreasonable privacy budgets. Therefore, we
proposed an adaptive differential privacy method based on federated learning.
The method sets the adjustment coefficient and scoring function according to
accuracy, loss, training rounds, and the number of datasets and clients. And
the privacy budget is adjusted based on them. Then the local model update is
processed according to the scaling factor and the noise. Fi-nally, the server
aggregates the noised local model update and distributes the noised global
model. The range of parameters and the privacy of the method are analyzed.
Through the experimental evaluation, it can reduce the privacy budget by about
16%, while the accuracy remains roughly the same.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.06460v1">Evaluating Privacy Measures for Load Hiding</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36">   
  <p><b>Published on:</b> 2024-08-12T19:21:34Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Vadim Arzamasov, Klemens Böhm</p>
    <p><b>Summary:</b> In smart grids, the use of smart meters to measure electricity consumption at
a household level raises privacy concerns. To address them, researchers have
designed various load hiding algorithms that manipulate the electricity
consumption measured. To compare how well these algorithms preserve privacy,
various privacy measures have been proposed. However, there currently is no
consensus on which privacy measure is most appropriate to use. In this study,
we aim to identify the most effective privacy measure(s) for load hiding
algorithms. We have crafted a series of experiments to assess the effectiveness
of these measures. found 20 of the 25 measures studied to be ineffective. Next,
focused on the well-known "appliance usage" secret, we have designed synthetic
data to find the measure that best deals with this secret. We observe that such
a measure, a variant of mutual information, actually exists.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.08904v1">Privacy in Federated Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-12T18:41:58Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jaydip Sen, Hetvi Waghela, Sneha Rakshit</p>
    <p><b>Summary:</b> Federated Learning (FL) represents a significant advancement in distributed
machine learning, enabling multiple participants to collaboratively train
models without sharing raw data. This decentralized approach enhances privacy
by keeping data on local devices. However, FL introduces new privacy
challenges, as model updates shared during training can inadvertently leak
sensitive information. This chapter delves into the core privacy concerns
within FL, including the risks of data reconstruction, model inversion attacks,
and membership inference. It explores various privacy-preserving techniques,
such as Differential Privacy (DP) and Secure Multi-Party Computation (SMPC),
which are designed to mitigate these risks. The chapter also examines the
trade-offs between model accuracy and privacy, emphasizing the importance of
balancing these factors in practical implementations. Furthermore, it discusses
the role of regulatory frameworks, such as GDPR, in shaping the privacy
standards for FL. By providing a comprehensive overview of the current state of
privacy in FL, this chapter aims to equip researchers and practitioners with
the knowledge necessary to navigate the complexities of secure federated
learning environments. The discussion highlights both the potential and
limitations of existing privacy-enhancing techniques, offering insights into
future research directions and the development of more robust solutions.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.06197v1">Lancelot: Towards Efficient and Privacy-Preserving Byzantine-Robust
  Federated Learning within Fully Homomorphic Encryption</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2024-08-12T14:48:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Siyang Jiang, Hao Yang, Qipeng Xie, Chuan Ma, Sen Wang, Guoliang Xing</p>
    <p><b>Summary:</b> In sectors such as finance and healthcare, where data governance is subject
to rigorous regulatory requirements, the exchange and utilization of data are
particularly challenging. Federated Learning (FL) has risen as a pioneering
distributed machine learning paradigm that enables collaborative model training
across multiple institutions while maintaining data decentralization. Despite
its advantages, FL is vulnerable to adversarial threats, particularly poisoning
attacks during model aggregation, a process typically managed by a central
server. However, in these systems, neural network models still possess the
capacity to inadvertently memorize and potentially expose individual training
instances. This presents a significant privacy risk, as attackers could
reconstruct private data by leveraging the information contained in the model
itself. Existing solutions fall short of providing a viable, privacy-preserving
BRFL system that is both completely secure against information leakage and
computationally efficient. To address these concerns, we propose Lancelot, an
innovative and computationally efficient BRFL framework that employs fully
homomorphic encryption (FHE) to safeguard against malicious client activities
while preserving data privacy. Our extensive testing, which includes medical
imaging diagnostics and widely-used public image datasets, demonstrates that
Lancelot significantly outperforms existing methods, offering more than a
twenty-fold increase in processing speed, all while maintaining data privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.06167v1">Blind-Match: Efficient Homomorphic Encryption-Based 1:N Matching for
  Privacy-Preserving Biometric Identification</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-12T14:13:08Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Hyunmin Choi, Jiwon Kim, Chiyoung Song, Simon S. Woo, Hyoungshick Kim</p>
    <p><b>Summary:</b> We present Blind-Match, a novel biometric identification system that
leverages homomorphic encryption (HE) for efficient and privacy-preserving 1:N
matching. Blind-Match introduces a HE-optimized cosine similarity computation
method, where the key idea is to divide the feature vector into smaller parts
for processing rather than computing the entire vector at once. By optimizing
the number of these parts, Blind-Match minimizes execution time while ensuring
data privacy through HE. Blind-Match achieves superior performance compared to
state-of-the-art methods across various biometric datasets. On the LFW face
dataset, Blind-Match attains a 99.63% Rank-1 accuracy with a 128-dimensional
feature vector, demonstrating its robustness in face recognition tasks. For
fingerprint identification, Blind-Match achieves a remarkable 99.55% Rank-1
accuracy on the PolyU dataset, even with a compact 16-dimensional feature
vector, significantly outperforming the state-of-the-art method, Blind-Touch,
which achieves only 59.17%. Furthermore, Blind-Match showcases practical
efficiency in large-scale biometric identification scenarios, such as Naver
Cloud's FaceSign, by processing 6,144 biometric samples in 0.74 seconds using a
128-dimensional feature vector.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.06395v1">Fast John Ellipsoid Computation with Differential Privacy Optimization</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-08-12T03:47:55Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jiuxiang Gu, Xiaoyu Li, Yingyu Liang, Zhenmei Shi, Zhao Song, Junwei Yu</p>
    <p><b>Summary:</b> Determining the John ellipsoid - the largest volume ellipsoid contained
within a convex polytope - is a fundamental problem with applications in
machine learning, optimization, and data analytics. Recent work has developed
fast algorithms for approximating the John ellipsoid using sketching and
leverage score sampling techniques. However, these algorithms do not provide
privacy guarantees for sensitive input data. In this paper, we present the
first differentially private algorithm for fast John ellipsoid computation. Our
method integrates noise perturbation with sketching and leverage score sampling
to achieve both efficiency and privacy. We prove that (1) our algorithm
provides $(\epsilon,\delta)$-differential privacy, and the privacy guarantee
holds for neighboring datasets that are $\epsilon_0$-close, allowing
flexibility in the privacy definition; (2) our algorithm still converges to a
$(1+\xi)$-approximation of the optimal John ellipsoid in
$O(\xi^{-2}(\log(n/\delta_0) + (L\epsilon_0)^{-2}))$ iterations where $n$ is
the number of data point, $L$ is the Lipschitz constant, $\delta_0$ is the
failure probability, and $\epsilon_0$ is the closeness of neighboring input
datasets. Our theoretical analysis demonstrates the algorithm's convergence and
privacy properties, providing a robust approach for balancing utility and
privacy in John ellipsoid computation. This is the first differentially private
algorithm for fast John ellipsoid computation, opening avenues for future
research in privacy-preserving optimization techniques.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.05723v1">Deep Learning with Data Privacy via Residual Perturbation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-08-11T08:26:43Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Wenqi Tao, Huaming Ling, Zuoqiang Shi, Bao Wang</p>
    <p><b>Summary:</b> Protecting data privacy in deep learning (DL) is of crucial importance.
Several celebrated privacy notions have been established and used for
privacy-preserving DL. However, many existing mechanisms achieve privacy at the
cost of significant utility degradation and computational overhead. In this
paper, we propose a stochastic differential equation-based residual
perturbation for privacy-preserving DL, which injects Gaussian noise into each
residual mapping of ResNets. Theoretically, we prove that residual perturbation
guarantees differential privacy (DP) and reduces the generalization gap of DL.
Empirically, we show that residual perturbation is computationally efficient
and outperforms the state-of-the-art differentially private stochastic gradient
descent (DPSGD) in utility maintenance without sacrificing membership privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.05543v1">PixelFade: Privacy-preserving Person Re-identification with Noise-guided
  Progressive Replacement</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-08-10T12:52:54Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Delong Zhang, Yi-Xing Peng, Xiao-Ming Wu, Ancong Wu, Wei-Shi Zheng</p>
    <p><b>Summary:</b> Online person re-identification services face privacy breaches from potential
data leakage and recovery attacks, exposing cloud-stored images to malicious
attackers and triggering public concern. The privacy protection of pedestrian
images is crucial. Previous privacy-preserving person re-identification methods
are unable to resist recovery attacks and compromise accuracy. In this paper,
we propose an iterative method (PixelFade) to optimize pedestrian images into
noise-like images to resist recovery attacks. We first give an in-depth study
of protected images from previous privacy methods, which reveal that the chaos
of protected images can disrupt the learning of recovery models. Accordingly,
Specifically, we propose Noise-guided Objective Function with the feature
constraints of a specific authorization model, optimizing pedestrian images to
normal-distributed noise images while preserving their original identity
information as per the authorization model. To solve the above non-convex
optimization problem, we propose a heuristic optimization algorithm that
alternately performs the Constraint Operation and the Partial Replacement
Operation. This strategy not only safeguards that original pixels are replaced
with noises to protect privacy, but also guides the images towards an improved
optimization direction to effectively preserve discriminative features.
Extensive experiments demonstrate that our PixelFade outperforms previous
methods in resisting recovery attacks and Re-ID performance. The code is
available at https://github.com/iSEE-Laboratory/PixelFade.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.05212v1">Preserving Privacy in Large Language Models: A Survey on Current Threats
  and Solutions</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-08-10T05:41:19Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Michele Miranda, Elena Sofia Ruzzetti, Andrea Santilli, Fabio Massimo Zanzotto, Sébastien Bratières, Emanuele Rodolà</p>
    <p><b>Summary:</b> Large Language Models (LLMs) represent a significant advancement in
artificial intelligence, finding applications across various domains. However,
their reliance on massive internet-sourced datasets for training brings notable
privacy issues, which are exacerbated in critical domains (e.g., healthcare).
Moreover, certain application-specific scenarios may require fine-tuning these
models on private data. This survey critically examines the privacy threats
associated with LLMs, emphasizing the potential for these models to memorize
and inadvertently reveal sensitive information. We explore current threats by
reviewing privacy attacks on LLMs and propose comprehensive solutions for
integrating privacy mechanisms throughout the entire learning pipeline. These
solutions range from anonymizing training datasets to implementing differential
privacy during training or inference and machine unlearning after training. Our
comprehensive review of existing literature highlights ongoing challenges,
available tools, and future directions for preserving privacy in LLMs. This
work aims to guide the development of more secure and trustworthy AI systems by
providing a thorough understanding of privacy preservation methods and their
effectiveness in mitigating risks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.05092v1">PriPHiT: Privacy-Preserving Hierarchical Training of Deep Neural
  Networks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">  
  <p><b>Published on:</b> 2024-08-09T14:33:34Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yamin Sepehri, Pedram Pad, Pascal Frossard, L. Andrea Dunbar</p>
    <p><b>Summary:</b> The training phase of deep neural networks requires substantial resources and
as such is often performed on cloud servers. However, this raises privacy
concerns when the training dataset contains sensitive content, e.g., face
images. In this work, we propose a method to perform the training phase of a
deep learning model on both an edge device and a cloud server that prevents
sensitive content being transmitted to the cloud while retaining the desired
information. The proposed privacy-preserving method uses adversarial early
exits to suppress the sensitive content at the edge and transmits the
task-relevant information to the cloud. This approach incorporates noise
addition during the training phase to provide a differential privacy guarantee.
We extensively test our method on different facial datasets with diverse face
attributes using various deep learning architectures, showcasing its
outstanding performance. We also demonstrate the effectiveness of privacy
preservation through successful defenses against different white-box and deep
reconstruction attacks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.04931v1">Privacy-Preserved Taxi Demand Prediction System Utilizing Distributed
  Data</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2024-08-09T08:24:47Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ren Ozeki, Haruki Yonekura, Hamada Rizk, Hirozumi Yamaguchi</p>
    <p><b>Summary:</b> Accurate taxi-demand prediction is essential for optimizing taxi operations
and enhancing urban transportation services. However, using customers' data in
these systems raises significant privacy and security concerns. Traditional
federated learning addresses some privacy issues by enabling model training
without direct data exchange but often struggles with accuracy due to varying
data distributions across different regions or service providers. In this
paper, we propose CC-Net: a novel approach using collaborative learning
enhanced with contrastive learning for taxi-demand prediction. Our method
ensures high performance by enabling multiple parties to collaboratively train
a demand-prediction model through hierarchical federated learning. In this
approach, similar parties are clustered together, and federated learning is
applied within each cluster. The similarity is defined without data exchange,
ensuring privacy and security. We evaluated our approach using real-world data
from five taxi service providers in Japan over fourteen months. The results
demonstrate that CC-Net maintains the privacy of customers' data while
improving prediction accuracy by at least 2.2% compared to existing techniques.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.04888v2">Locally Private Histograms in All Privacy Regimes</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Discrete Mathematics-04E762">
  <p><b>Published on:</b> 2024-08-09T06:22:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Clément L. Canonne, Abigail Gentle</p>
    <p><b>Summary:</b> Frequency estimation, a.k.a. histograms, is a workhorse of data analysis, and
as such has been thoroughly studied under differentially privacy. In
particular, computing histograms in the \emph{local} model of privacy has been
the focus of a fruitful recent line of work, and various algorithms have been
proposed, achieving the order-optimal $\ell_\infty$ error in the high-privacy
(small $\varepsilon$) regime while balancing other considerations such as time-
and communication-efficiency. However, to the best of our knowledge, the
picture is much less clear when it comes to the medium- or low-privacy regime
(large $\varepsilon$), despite its increased relevance in practice. In this
paper, we investigate locally private histograms, and the very related
distribution learning task, in this medium-to-low privacy regime, and establish
near-tight (and somewhat unexpected) bounds on the $\ell_\infty$ error
achievable. As a direct corollary of our results, we obtain a protocol for
histograms in the \emph{shuffle} model of differential privacy, with accuracy
matching previous algorithms but significantly better message and communication
complexity.
  Our theoretical findings emerge from a novel analysis, which appears to
improve bounds across the board for the locally private histogram problem. We
back our theoretical findings by an empirical comparison of existing algorithms
in all privacy regimes, to assess their typical performance and behaviour
beyond the worst-case setting.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.04684v1">Moving beyond privacy and airspace safety: Guidelines for just drones in
  policing</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2024-08-08T09:04:01Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mateusz Dolata, Gerhard Schwabe</p>
    <p><b>Summary:</b> The use of drones offers police forces potential gains in efficiency and
safety. However, their use may also harm public perception of the police if
drones are refused. Therefore, police forces should consider the perception of
bystanders and broader society to maximize drones' potential. This article
examines the concerns expressed by members of the public during a field trial
involving 52 test participants. Analysis of the group interviews suggests that
their worries go beyond airspace safety and privacy, broadly discussed in
existing literature and regulations. The interpretation of the results
indicates that the perceived justice of drone use is a significant factor in
acceptance. Leveraging the concept of organizational justice and data
collected, we propose a catalogue of guidelines for just operation of drones to
supplement the existing policy. We present the organizational justice
perspective as a framework to integrate the concerns of the public and
bystanders into legal work. Finally, we discuss the relevance of justice for
the legitimacy of the police's actions and provide implications for research
and practice.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.04315v1">Federated Cubic Regularized Newton Learning with
  Sparsification-amplified Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36"> 
  <p><b>Published on:</b> 2024-08-08T08:48:54Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Wei Huo, Changxin Liu, Kemi Ding, Karl Henrik Johansson, Ling Shi</p>
    <p><b>Summary:</b> This paper investigates the use of the cubic-regularized Newton method within
a federated learning framework while addressing two major concerns that
commonly arise in federated learning: privacy leakage and communication
bottleneck. We introduce a federated learning algorithm called Differentially
Private Federated Cubic Regularized Newton (DP-FCRN). By leveraging
second-order techniques, our algorithm achieves lower iteration complexity
compared to first-order methods. We also incorporate noise perturbation during
local computations to ensure privacy. Furthermore, we employ sparsification in
uplink transmission, which not only reduces the communication costs but also
amplifies the privacy guarantee. Specifically, this approach reduces the
necessary noise intensity without compromising privacy protection. We analyze
the convergence properties of our algorithm and establish the privacy
guarantee. Finally, we validate the effectiveness of the proposed algorithm
through experiments on a benchmark dataset.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.04188v1">Trustworthy Semantic-Enabled 6G Communication: A Task-oriented and
  Privacy-preserving Perspective</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36"> 
  <p><b>Published on:</b> 2024-08-08T03:16:42Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shuaishuai Guo, Anbang Zhang, Yanhu Wang, Chenyuan Feng, Tony Q. S. Quek</p>
    <p><b>Summary:</b> Trustworthy task-oriented semantic communication (ToSC) emerges as an
innovative approach in the 6G landscape, characterized by the transmission of
only vital information that is directly pertinent to a specific task. While
ToSC offers an efficient mode of communication, it concurrently raises concerns
regarding privacy, as sophisticated adversaries might possess the capability to
reconstruct the original data from the transmitted features. This article
provides an in-depth analysis of privacy-preserving strategies specifically
designed for ToSC relying on deep neural network-based joint source and channel
coding (DeepJSCC). The study encompasses a detailed comparative assessment of
trustworthy feature perturbation methods such as differential privacy and
encryption, alongside intrinsic security incorporation approaches like
adversarial learning to train the JSCC and learning-based vector quantization
(LBVQ). This comparative analysis underscores the integration of advanced
explainable learning algorithms into communication systems, positing a new
benchmark for privacy standards in the forthcoming 6G era.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.03897v1">Speech privacy-preserving methods using secret key for convolutional
  neural network models and their robustness evaluation</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Sound-D91E36">
  <p><b>Published on:</b> 2024-08-07T16:51:39Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shoko Niwa, Sayaka Shiota, Hitoshi Kiya</p>
    <p><b>Summary:</b> In this paper, we propose privacy-preserving methods with a secret key for
convolutional neural network (CNN)-based models in speech processing tasks. In
environments where untrusted third parties, like cloud servers, provide
CNN-based systems, ensuring the privacy of speech queries becomes essential.
This paper proposes encryption methods for speech queries using secret keys and
a model structure that allows for encrypted queries to be accepted without
decryption. Our approach introduces three types of secret keys: Shuffling,
Flipping, and random orthogonal matrix (ROM). In experiments, we demonstrate
that when the proposed methods are used with the correct key, identification
performance did not degrade. Conversely, when an incorrect key is used, the
performance significantly decreased. Particularly, with the use of ROM, we show
that even with a relatively small key space, high privacy-preserving
performance can be maintained many speech processing tasks. Furthermore, we
also demonstrate the difficulty of recovering original speech from encrypted
queries in various robustness evaluations.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.03578v1">Unraveling Privacy Threat Modeling Complexity: Conceptual Privacy
  Analysis Layers</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-07T06:30:20Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kim Wuyts, Avi Douglen</p>
    <p><b>Summary:</b> Analyzing privacy threats in software products is an essential part of
software development to ensure systems are privacy-respecting; yet it is still
a far from trivial activity. While there have been many advancements in the
past decade, they tend to focus on describing 'what' the threats are. What
isn't entirely clear yet is 'how' to actually find these threats. Privacy is a
complex domain. We propose to use four conceptual layers (feature, ecosystem,
business context, and environment) to capture this privacy complexity. These
layers can be used as a frame to structure and specify the privacy analysis
support in a more tangible and actionable way, thereby improving applicability
of the analysis process.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.03185v1">MaskAnyone Toolkit: Offering Strategies for Minimizing Privacy Risks and
  Maximizing Utility in Audio-Visual Data Archiving</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Multimedia-5BC0EB">
  <p><b>Published on:</b> 2024-08-06T13:35:27Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Babajide Alamu Owoyele, Martin Schilling, Rohan Sawahn, Niklas Kaemer, Pavel Zherebenkov, Bhuvanesh Verma, Wim Pouw, Gerard de Melo</p>
    <p><b>Summary:</b> This paper introduces MaskAnyone, a novel toolkit designed to navigate some
privacy and ethical concerns of sharing audio-visual data in research.
MaskAnyone offers a scalable, user-friendly solution for de-identifying
individuals in video and audio content through face-swapping and voice
alteration, supporting multi-person masking and real-time bulk processing. By
integrating this tool within research practices, we aim to enhance data
reproducibility and utility in social science research. Our approach draws on
Design Science Research, proposing that MaskAnyone can facilitate safer data
sharing and potentially reduce the storage of fully identifiable data. We
discuss the development and capabilities of MaskAnyone, explore its integration
into ethical research practices, and consider the broader implications of
audio-visual data masking, including issues of consent and the risk of misuse.
The paper concludes with a preliminary evaluation framework for assessing the
effectiveness and ethical integration of masking tools in such research
settings.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.02927v1">HARMONIC: Harnessing LLMs for Tabular Data Synthesis and Privacy
  Protection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-06T03:21:13Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yuxin Wang, Duanyu Feng, Yongfu Dai, Zhengyu Chen, Jimin Huang, Sophia Ananiadou, Qianqian Xie, Hao Wang</p>
    <p><b>Summary:</b> Data serves as the fundamental foundation for advancing deep learning,
particularly tabular data presented in a structured format, which is highly
conducive to modeling. However, even in the era of LLM, obtaining tabular data
from sensitive domains remains a challenge due to privacy or copyright
concerns. Hence, exploring how to effectively use models like LLMs to generate
realistic and privacy-preserving synthetic tabular data is urgent. In this
paper, we take a step forward to explore LLMs for tabular data synthesis and
privacy protection, by introducing a new framework HARMONIC for tabular data
generation and evaluation. In the tabular data generation of our framework,
unlike previous small-scale LLM-based methods that rely on continued
pre-training, we explore the larger-scale LLMs with fine-tuning to generate
tabular data and enhance privacy. Based on idea of the k-nearest neighbors
algorithm, an instruction fine-tuning dataset is constructed to inspire LLMs to
discover inter-row relationships. Then, with fine-tuning, LLMs are trained to
remember the format and connections of the data rather than the data itself,
which reduces the risk of privacy leakage. In the evaluation part of our
framework, we develop specific privacy risk metrics DLT for LLM synthetic data
generation, as well as performance evaluation metrics LLE for downstream LLM
tasks. Our experiments find that this tabular data generation framework
achieves equivalent performance to existing methods with better privacy, which
also demonstrates our evaluation framework for the effectiveness of synthetic
data and privacy risks in LLM scenarios.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.02750v1">Privacy-Safe Iris Presentation Attack Detection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> 
  <p><b>Published on:</b> 2024-08-05T18:09:02Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mahsa Mitcheff, Patrick Tinsley, Adam Czajka</p>
    <p><b>Summary:</b> This paper proposes a framework for a privacy-safe iris presentation attack
detection (PAD) method, designed solely with synthetically-generated,
identity-leakage-free iris images. Once trained, the method is evaluated in a
classical way using state-of-the-art iris PAD benchmarks. We designed two
generative models for the synthesis of ISO/IEC 19794-6-compliant iris images.
The first model synthesizes bona fide-looking samples. To avoid ``identity
leakage,'' the generated samples that accidentally matched those used in the
model's training were excluded. The second model synthesizes images of irises
with textured contact lenses and is conditioned by a given contact lens brand
to have better control over textured contact lens appearance when forming the
training set. Our experiments demonstrate that models trained solely on
synthetic data achieve a lower but still reasonable performance when compared
to solutions trained with iris images collected from human subjects. This is
the first-of-its-kind attempt to use solely synthetic data to train a
fully-functional iris PAD solution, and despite the performance gap between
regular and the proposed methods, this study demonstrates that with the
increasing fidelity of generative models, creating such privacy-safe iris PAD
methods may be possible. The source codes and generative models trained for
this work are offered along with the paper.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.02373v1">Operationalizing Contextual Integrity in Privacy-Conscious Assistants</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-08-05T10:53:51Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sahra Ghalebikesabi, Eugene Bagdasaryan, Ren Yi, Itay Yona, Ilia Shumailov, Aneesh Pappu, Chongyang Shi, Laura Weidinger, Robert Stanforth, Leonard Berrada, Pushmeet Kohli, Po-Sen Huang, Borja Balle</p>
    <p><b>Summary:</b> Advanced AI assistants combine frontier LLMs and tool access to autonomously
perform complex tasks on behalf of users. While the helpfulness of such
assistants can increase dramatically with access to user information including
emails and documents, this raises privacy concerns about assistants sharing
inappropriate information with third parties without user supervision. To steer
information-sharing assistants to behave in accordance with privacy
expectations, we propose to operationalize $\textit{contextual integrity}$
(CI), a framework that equates privacy with the appropriate flow of information
in a given context. In particular, we design and evaluate a number of
strategies to steer assistants' information-sharing actions to be CI compliant.
Our evaluation is based on a novel form filling benchmark composed of synthetic
data and human annotations, and it reveals that prompting frontier LLMs to
perform CI-based reasoning yields strong results.</p>
  </details>
</div>



<h2>2024-09</h2>

<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.04366v1">Deanonymizing Ethereum Validators: The P2P Network Has a Privacy Issue</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-09-06T15:57:43Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Lioba Heimbach, Yann Vonlanthen, Juan Villacis, Lucianna Kiffer, Roger Wattenhofer</p>
    <p><b>Summary:</b> Many blockchain networks aim to preserve the anonymity of validators in the
peer-to-peer (P2P) network, ensuring that no adversary can link a validator's
identifier to the IP address of a peer due to associated privacy and security
concerns. This work demonstrates that the Ethereum P2P network does not offer
this anonymity. We present a methodology that enables any node in the network
to identify validators hosted on connected peers and empirically verify the
feasibility of our proposed method. Using data collected from four nodes over
three days, we locate more than 15% of Ethereum validators in the P2P network.
The insights gained from our deanonymization technique provide valuable
information on the distribution of validators across peers, their geographic
locations, and hosting organizations. We further discuss the implications and
risks associated with the lack of anonymity in the P2P network and propose
methods to help validators protect their privacy. The Ethereum Foundation has
awarded us a bug bounty, acknowledging the impact of our results.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.04257v1">Privacy risk from synthetic data: practical proposals</a></h3>
  
  <p><b>Published on:</b> 2024-09-06T13:10:40Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Gillian M Raab</p>
    <p><b>Summary:</b> This paper proposes and compares measures of identity and attribute
disclosure risk for synthetic data. Data custodians can use the methods
proposed here to inform the decision as to whether to release synthetic
versions of confidential data. Different measures are evaluated on two data
sets. Insight into the measures is obtained by examining the details of the
records identified as posing a disclosure risk. This leads to methods to
identify, and possibly exclude, apparently risky records where the
identification or attribution would be expected by someone with background
knowledge of the data. The methods described are available as part of the
\textbf{synthpop} package for \textbf{R}.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.04194v1">Towards Privacy-Preserving Relational Data Synthesis via Probabilistic
  Relational Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-09-06T11:24:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Malte Luttermann, Ralf Möller, Mattis Hartwig</p>
    <p><b>Summary:</b> Probabilistic relational models provide a well-established formalism to
combine first-order logic and probabilistic models, thereby allowing to
represent relationships between objects in a relational domain. At the same
time, the field of artificial intelligence requires increasingly large amounts
of relational training data for various machine learning tasks. Collecting
real-world data, however, is often challenging due to privacy concerns, data
protection regulations, high costs, and so on. To mitigate these challenges,
the generation of synthetic data is a promising approach. In this paper, we
solve the problem of generating synthetic relational data via probabilistic
relational models. In particular, we propose a fully-fledged pipeline to go
from relational database to probabilistic relational model, which can then be
used to sample new synthetic relational data points from its underlying
probability distribution. As part of our proposed pipeline, we introduce a
learning algorithm to construct a probabilistic relational model from a given
relational database.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.04173v1">NPU-NTU System for Voice Privacy 2024 Challenge</a></h3>
  
  <p><b>Published on:</b> 2024-09-06T10:32:42Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jixun Yao, Nikita Kuzmin, Qing Wang, Pengcheng Guo, Ziqian Ning, Dake Guo, Kong Aik Lee, Eng-Siong Chng, Lei Xie</p>
    <p><b>Summary:</b> Speaker anonymization is an effective privacy protection solution that
conceals the speaker's identity while preserving the linguistic content and
paralinguistic information of the original speech. To establish a fair
benchmark and facilitate comparison of speaker anonymization systems, the
VoicePrivacy Challenge (VPC) was held in 2020 and 2022, with a new edition
planned for 2024. In this paper, we describe our proposed speaker anonymization
system for VPC 2024. Our system employs a disentangled neural codec
architecture and a serial disentanglement strategy to gradually disentangle the
global speaker identity and time-variant linguistic content and paralinguistic
information. We introduce multiple distillation methods to disentangle
linguistic content, speaker identity, and emotion. These methods include
semantic distillation, supervised speaker distillation, and frame-level emotion
distillation. Based on these distillations, we anonymize the original speaker
identity using a weighted sum of a set of candidate speaker identities and a
randomly generated speaker identity. Our system achieves the best trade-off of
privacy protection and emotion preservation in VPC 2024.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.04167v1">Do Android App Developers Accurately Report Collection of
  Privacy-Related Data?</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-09-06T10:05:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mugdha Khedkar, Ambuj Kumar Mondal, Eric Bodden</p>
    <p><b>Summary:</b> Many Android applications collect data from users. The European Union's
General Data Protection Regulation (GDPR) requires vendors to faithfully
disclose which data their apps collect. This task is complicated because many
apps use third-party code for which the same information is not readily
available. Hence we ask: how accurately do current Android apps fulfill these
requirements?
  In this work, we first expose a multi-layered definition of privacy-related
data to correctly report data collection in Android apps. We further create a
dataset of privacy-sensitive data classes that may be used as input by an
Android app. This dataset takes into account data collected both through the
user interface and system APIs.
  We manually examine the data safety sections of 70 Android apps to observe
how data collection is reported, identifying instances of over- and
under-reporting. Additionally, we develop a prototype to statically extract and
label privacy-related data collected via app source code, user interfaces, and
permissions. Comparing the prototype's results with the data safety sections of
20 apps reveals reporting discrepancies. Using the results from two Messaging
and Social Media apps (Signal and Instagram), we discuss how app developers
under-report and over-report data collection, respectively, and identify
inaccurately reported data categories.
  Our results show that app developers struggle to accurately report data
collection, either due to Google's abstract definition of collected data or
insufficient existing tool support.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.04048v1">Exploring User Privacy Awareness on GitHub: An Empirical Study</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Software Engineering-D91E36">
  <p><b>Published on:</b> 2024-09-06T06:41:46Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Costanza Alfieri, Juri Di Rocco, Phuong T. Nguyen, Paola Inverardi</p>
    <p><b>Summary:</b> GitHub provides developers with a practical way to distribute source code and
collaboratively work on common projects. To enhance account security and
privacy, GitHub allows its users to manage access permissions, review audit
logs, and enable two-factor authentication. However, despite the endless
effort, the platform still faces various issues related to the privacy of its
users. This paper presents an empirical study delving into the GitHub
ecosystem. Our focus is on investigating the utilization of privacy settings on
the platform and identifying various types of sensitive information disclosed
by users. Leveraging a dataset comprising 6,132 developers, we report and
analyze their activities by means of comments on pull requests. Our findings
indicate an active engagement by users with the available privacy settings on
GitHub. Notably, we observe the disclosure of different forms of private
information within pull request comments. This observation has prompted our
exploration into sensitivity detection using a large language model and BERT,
to pave the way for a personalized privacy assistant. Our work provides
insights into the utilization of existing privacy protection tools, such as
privacy settings, along with their inherent limitations. Essentially, we aim to
advance research in this field by providing both the motivation for creating
such privacy protection tools and a proposed methodology for personalizing
them.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.04026v1">Efficient Fault-Tolerant Quantum Protocol for Differential Privacy in
  the Shuffle Model</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-09-06T04:53:19Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Hassan Jameel Asghar, Arghya Mukherjee, Gavin K. Brennen</p>
    <p><b>Summary:</b> We present a quantum protocol which securely and implicitly implements a
random shuffle to realize differential privacy in the shuffle model. The
shuffle model of differential privacy amplifies privacy achievable via local
differential privacy by randomly permuting the tuple of outcomes from data
contributors. In practice, one needs to address how this shuffle is
implemented. Examples include implementing the shuffle via mix-networks, or
shuffling via a trusted third-party. These implementation specific issues raise
non-trivial computational and trust requirements in a classical system. We
propose a quantum version of the protocol using entanglement of quantum states
and show that the shuffle can be implemented without these extra requirements.
Our protocol implements k-ary randomized response, for any value of k > 2, and
furthermore, can be efficiently implemented using fault-tolerant computation.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.03707v1">A Different Level Text Protection Mechanism With Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-09-05T17:13:38Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Qingwen Fu</p>
    <p><b>Summary:</b> The article introduces a method for extracting words of different degrees of
importance based on the BERT pre-training model and proves the effectiveness of
this method. The article also discusses the impact of maintaining the same
perturbation results for words of different importance on the overall text
utility. This method can be applied to long text protection.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.03655v1">Privacy versus Emotion Preservation Trade-offs in Emotion-Preserving
  Speaker Anonymization</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-09-05T16:10:31Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zexin Cai, Henry Li Xinyuan, Ashi Garg, Leibny Paola García-Perera, Kevin Duh, Sanjeev Khudanpur, Nicholas Andrews, Matthew Wiesner</p>
    <p><b>Summary:</b> Advances in speech technology now allow unprecedented access to personally
identifiable information through speech. To protect such information, the
differential privacy field has explored ways to anonymize speech while
preserving its utility, including linguistic and paralinguistic aspects.
However, anonymizing speech while maintaining emotional state remains
challenging. We explore this problem in the context of the VoicePrivacy 2024
challenge. Specifically, we developed various speaker anonymization pipelines
and find that approaches either excel at anonymization or preserving emotion
state, but not both simultaneously. Achieving both would require an in-domain
emotion recognizer. Additionally, we found that it is feasible to train a
semi-effective speaker verification system using only emotion representations,
demonstrating the challenge of separating these two modalities.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.03568v1">Enabling Practical and Privacy-Preserving Image Processing</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2024-09-05T14:22:02Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chao Wang, Shubing Yang, Xiaoyan Sun, Jun Dai, Dongfang Zhao</p>
    <p><b>Summary:</b> Fully Homomorphic Encryption (FHE) enables computations on encrypted data,
preserving confidentiality without the need for decryption. However, FHE is
often hindered by significant performance overhead, particularly for
high-precision and complex data like images. Due to serious efficiency issues,
traditional FHE methods often encrypt images by monolithic data blocks (such as
pixel rows), instead of pixels. However, this strategy compromises the
advantages of homomorphic operations and disables pixel-level image processing.
In this study, we address these challenges by proposing and implementing a
pixel-level homomorphic encryption approach, iCHEETAH, based on the CKKS
scheme. To enhance computational efficiency, we introduce three novel caching
mechanisms to pre-encrypt radix values or frequently occurring pixel values,
substantially reducing redundant encryption operations. Extensive experiments
demonstrate that our approach achieves up to a 19-fold improvement in
encryption speed compared to the original CKKS, while maintaining high image
quality. Additionally, real-world image applications such as mean filtering,
brightness enhancement, image matching and watermarking are tested based on
FHE, showcasing up to a 91.53% speed improvement. We also proved that our
method is IND-CPA (Indistinguishability under Chosen Plaintext Attack) secure,
providing strong encryption security. These results underscore the practicality
and efficiency of iCHEETAH, marking a significant advancement in
privacy-preserving image processing at scale.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.03344v1">Rethinking Improved Privacy-Utility Trade-off with Pre-existing
  Knowledge for DP Training</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-09-05T08:40:54Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yu Zheng, Wenchao Zhang, Yonggang Zhang, Wei Song, Kai Zhou, Bo Han</p>
    <p><b>Summary:</b> Differential privacy (DP) provides a provable framework for protecting
individuals by customizing a random mechanism over a privacy-sensitive dataset.
Deep learning models have demonstrated privacy risks in model exposure as an
established learning model unintentionally records membership-level privacy
leakage. Differentially private stochastic gradient descent (DP- SGD) has been
proposed to safeguard training individuals by adding random Gaussian noise to
gradient updates in the backpropagation. Researchers identify that DP-SGD
typically causes utility loss since the injected homogeneous noise alters the
gradient updates calculated at each iteration. Namely, all elements in the
gradient are contaminated regardless of their importance in updating model
parameters. In this work, we argue that the utility loss mainly results from
the homogeneity of injected noise. Consequently, we propose a generic
differential privacy framework with heterogeneous noise (DP-Hero) by defining a
heterogeneous random mechanism to abstract its property. The insight of DP-Hero
is to leverage the knowledge encoded in the previously trained model to guide
the subsequent allocation of noise heterogeneity, thereby leveraging the
statistical perturbation and achieving enhanced utility. Atop DP-Hero, we
instantiate a heterogeneous version of DP-SGD, where the noise injected into
gradients is heterogeneous and guided by prior-established model parameters. We
conduct comprehensive experiments to verify and explain the effectiveness of
the proposed DP-Hero, showing improved training accuracy compared with
state-of-the-art works. Broadly, we shed light on improving the privacy-utility
space by learning the noise guidance from the pre-existing leaked knowledge
encoded in the previously trained model, showing a different perspective of
understanding the utility-improved DP training.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.03326v1">Enhancing User-Centric Privacy Protection: An Interactive Framework
  through Diffusion Models and Machine Unlearning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-09-05T07:55:55Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Huaxi Huang, Xin Yuan, Qiyu Liao, Dadong Wang, Tongliang Liu</p>
    <p><b>Summary:</b> In the realm of multimedia data analysis, the extensive use of image datasets
has escalated concerns over privacy protection within such data. Current
research predominantly focuses on privacy protection either in data sharing or
upon the release of trained machine learning models. Our study pioneers a
comprehensive privacy protection framework that safeguards image data privacy
concurrently during data sharing and model publication. We propose an
interactive image privacy protection framework that utilizes generative machine
learning models to modify image information at the attribute level and employs
machine unlearning algorithms for the privacy preservation of model parameters.
This user-interactive framework allows for adjustments in privacy protection
intensity based on user feedback on generated images, striking a balance
between maximal privacy safeguarding and maintaining model performance. Within
this framework, we instantiate two modules: a differential privacy diffusion
model for protecting attribute information in images and a feature unlearning
algorithm for efficient updates of the trained model on the revised image
dataset. Our approach demonstrated superiority over existing methods on facial
datasets across various attribute classifications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.03294v1">Federated Prototype-based Contrastive Learning for Privacy-Preserving
  Cross-domain Recommendation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Retrieval-5BC0EB">
  <p><b>Published on:</b> 2024-09-05T06:59:56Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Li Wang, Quangui Zhang, Lei Sang, Qiang Wu, Min Xu</p>
    <p><b>Summary:</b> Cross-domain recommendation (CDR) aims to improve recommendation accuracy in
sparse domains by transferring knowledge from data-rich domains. However,
existing CDR methods often assume the availability of user-item interaction
data across domains, overlooking user privacy concerns. Furthermore, these
methods suffer from performance degradation in scenarios with sparse
overlapping users, as they typically depend on a large number of fully shared
users for effective knowledge transfer. To address these challenges, we propose
a Federated Prototype-based Contrastive Learning (CL) method for
Privacy-Preserving CDR, named FedPCL-CDR. This approach utilizes
non-overlapping user information and prototypes to improve multi-domain
performance while protecting user privacy. FedPCL-CDR comprises two modules:
local domain (client) learning and global server aggregation. In the local
domain, FedPCL-CDR clusters all user data to learn representative prototypes,
effectively utilizing non-overlapping user information and addressing the
sparse overlapping user issue. It then facilitates knowledge transfer by
employing both local and global prototypes returned from the server in a CL
manner. Simultaneously, the global server aggregates representative prototypes
from local domains to learn both local and global prototypes. The combination
of prototypes and federated learning (FL) ensures that sensitive user data
remains decentralized, with only prototypes being shared across domains,
thereby protecting user privacy. Extensive experiments on four CDR tasks using
two real-world datasets demonstrate that FedPCL-CDR outperforms the
state-of-the-art baselines.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.03796v1">Protecting Activity Sensing Data Privacy Using Hierarchical Information
  Dissociation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-09-04T15:38:00Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Guangjing Wang, Hanqing Guo, Yuanda Wang, Bocheng Chen, Ce Zhou, Qiben Yan</p>
    <p><b>Summary:</b> Smartphones and wearable devices have been integrated into our daily lives,
offering personalized services. However, many apps become overprivileged as
their collected sensing data contains unnecessary sensitive information. For
example, mobile sensing data could reveal private attributes (e.g., gender and
age) and unintended sensitive features (e.g., hand gestures when entering
passwords). To prevent sensitive information leakage, existing methods must
obtain private labels and users need to specify privacy policies. However, they
only achieve limited control over information disclosure. In this work, we
present Hippo to dissociate hierarchical information including private metadata
and multi-grained activity information from the sensing data. Hippo achieves
fine-grained control over the disclosure of sensitive information without
requiring private labels. Specifically, we design a latent guidance-based
diffusion model, which generates multi-grained versions of raw sensor data
conditioned on hierarchical latent activity features. Hippo enables users to
control the disclosure of sensitive information in sensing data, ensuring their
privacy while preserving the necessary features to meet the utility
requirements of applications. Hippo is the first unified model that achieves
two goals: perturbing the sensitive attributes and controlling the disclosure
of sensitive information in mobile sensing data. Extensive experiments show
that Hippo can anonymize personal attributes and transform activity information
at various resolutions across different types of sensing data.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.02614v1">Evaluating the Effects of Digital Privacy Regulations on User Trust</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2024-09-04T11:11:41Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mehmet Berk Cetin</p>
    <p><b>Summary:</b> In today's digital society, issues related to digital privacy have become
increasingly important. Issues such as data breaches result in misuse of data,
financial loss, and cyberbullying, which leads to less user trust in digital
services. This research investigates the impact of digital privacy laws on user
trust by comparing the regulations in the Netherlands, Ghana, and Malaysia. The
study employs a comparative case study method, involving interviews with
digital privacy law experts, IT educators, and consumers from each country. The
main findings reveal that while the General Data Protection Regulation (GDPR)
in the Netherlands is strict, its practical impact is limited by enforcement
challenges. In Ghana, the Data Protection Act is underutilized due to low
public awareness and insufficient enforcement, leading to reliance on personal
protective measures. In Malaysia, trust in digital services is largely
dependent on the security practices of individual platforms rather than the
Personal Data Protection Act. The study highlights the importance of public
awareness, effective enforcement, and cultural considerations in shaping the
effectiveness of digital privacy laws. Based on these insights, a
recommendation framework is proposed to enhance digital privacy practices, also
aiming to provide valuable guidance for policymakers, businesses, and citizens
in navigating the challenges of digitalization.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.02404v1">Learning Privacy-Preserving Student Networks via
  Discriminative-Generative Distillation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-09-04T03:06:13Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shiming Ge, Bochao Liu, Pengju Wang, Yong Li, Dan Zeng</p>
    <p><b>Summary:</b> While deep models have proved successful in learning rich knowledge from
massive well-annotated data, they may pose a privacy leakage risk in practical
deployment. It is necessary to find an effective trade-off between high utility
and strong privacy. In this work, we propose a discriminative-generative
distillation approach to learn privacy-preserving deep models. Our key idea is
taking models as bridge to distill knowledge from private data and then
transfer it to learn a student network via two streams. First, discriminative
stream trains a baseline classifier on private data and an ensemble of teachers
on multiple disjoint private subsets, respectively. Then, generative stream
takes the classifier as a fixed discriminator and trains a generator in a
data-free manner. After that, the generator is used to generate massive
synthetic data which are further applied to train a variational autoencoder
(VAE). Among these synthetic data, a few of them are fed into the teacher
ensemble to query labels via differentially private aggregation, while most of
them are embedded to the trained VAE for reconstructing synthetic data.
Finally, a semi-supervised student learning is performed to simultaneously
handle two tasks: knowledge transfer from the teachers with distillation on few
privately labeled synthetic data, and knowledge enhancement with tangent-normal
adversarial regularization on many triples of reconstructed synthetic data. In
this way, our approach can control query cost over private data and mitigate
accuracy degradation in a unified manner, leading to a privacy-preserving
student model. Extensive experiments and analysis clearly show the
effectiveness of the proposed approach.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.02375v1">How Privacy-Savvy Are Large Language Models? A Case Study on Compliance
  and Privacy Technical Review</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2024-09-04T01:51:37Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xichou Zhu, Yang Liu, Zhou Shen, Yi Liu, Min Li, Yujun Chen, Benzi John, Zhenzhen Ma, Tao Hu, Bolong Yang, Manman Wang, Zongxing Xie, Peng Liu, Dan Cai, Junhui Wang</p>
    <p><b>Summary:</b> The recent advances in large language models (LLMs) have significantly
expanded their applications across various fields such as language generation,
summarization, and complex question answering. However, their application to
privacy compliance and technical privacy reviews remains under-explored,
raising critical concerns about their ability to adhere to global privacy
standards and protect sensitive user data. This paper seeks to address this gap
by providing a comprehensive case study evaluating LLMs' performance in
privacy-related tasks such as privacy information extraction (PIE), legal and
regulatory key point detection (KPD), and question answering (QA) with respect
to privacy policies and data protection regulations. We introduce a Privacy
Technical Review (PTR) framework, highlighting its role in mitigating privacy
risks during the software development life-cycle. Through an empirical
assessment, we investigate the capacity of several prominent LLMs, including
BERT, GPT-3.5, GPT-4, and custom models, in executing privacy compliance checks
and technical privacy reviews. Our experiments benchmark the models across
multiple dimensions, focusing on their precision, recall, and F1-scores in
extracting privacy-sensitive information and detecting key regulatory
compliance points. While LLMs show promise in automating privacy reviews and
identifying regulatory discrepancies, significant gaps persist in their ability
to fully comply with evolving legal standards. We provide actionable
recommendations for enhancing LLMs' capabilities in privacy compliance,
emphasizing the need for robust model improvements and better integration with
legal and regulatory requirements. This study underscores the growing
importance of developing privacy-aware LLMs that can both support businesses in
compliance efforts and safeguard user privacy rights.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.02364v1">Examining Caregiving Roles to Differentiate the Effects of Using a
  Mobile App for Community Oversight for Privacy and Security</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2024-09-04T01:21:56Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mamtaj Akter, Jess Kropczynski, Heather Lipford, Pamela Wisniewski</p>
    <p><b>Summary:</b> We conducted a 4-week field study with 101 smartphone users who
self-organized into 22 small groups of family, friends, and neighbors to use
``CO-oPS,'' a mobile app for co-managing mobile privacy and security. We
differentiated between those who provided oversight (i.e., caregivers) and
those who did not (i.e., caregivees) to examine differential effects on their
experiences and behaviors while using CO-oPS. Caregivers reported higher power
use, community trust, belonging, collective efficacy, and self-efficacy than
caregivees. Both groups' self-efficacy and collective efficacy for mobile
privacy and security increased after using CO-oPS. However, this increase was
significantly stronger for caregivees. Our research demonstrates how
community-based approaches can benefit people who need additional help managing
their digital privacy and security. We provide recommendations to support
community-based oversight for managing privacy and security within communities
of different roles and skills.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.02044v1">FedMinds: Privacy-Preserving Personalized Brain Visual Decoding</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB"> 
  <p><b>Published on:</b> 2024-09-03T16:46:29Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Guangyin Bao, Duoqian Miao</p>
    <p><b>Summary:</b> Exploring the mysteries of the human brain is a long-term research topic in
neuroscience. With the help of deep learning, decoding visual information from
human brain activity fMRI has achieved promising performance. However, these
decoding models require centralized storage of fMRI data to conduct training,
leading to potential privacy security issues. In this paper, we focus on
privacy preservation in multi-individual brain visual decoding. To this end, we
introduce a novel framework called FedMinds, which utilizes federated learning
to protect individuals' privacy during model training. In addition, we deploy
individual adapters for each subject, thus allowing personalized visual
decoding. We conduct experiments on the authoritative NSD datasets to evaluate
the performance of the proposed framework. The results demonstrate that our
framework achieves high-precision visual decoding along with privacy
protection.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.01924v1">Privacy-Preserving and Post-Quantum Counter Denial of Service Framework
  for Wireless Networks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-09-03T14:14:41Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Saleh Darzi, Attila Altay Yavuz</p>
    <p><b>Summary:</b> As network services progress and mobile and IoT environments expand, numerous
security concerns have surfaced for spectrum access systems. The omnipresent
risk of Denial-of-Service (DoS) attacks and raising concerns about user privacy
(e.g., location privacy, anonymity) are among such cyber threats. These
security and privacy risks increase due to the threat of quantum computers that
can compromise long-term security by circumventing conventional cryptosystems
and increasing the cost of countermeasures. While some defense mechanisms exist
against these threats in isolation, there is a significant gap in the state of
the art on a holistic solution against DoS attacks with privacy and anonymity
for spectrum management systems, especially when post-quantum (PQ) security is
in mind. In this paper, we propose a new cybersecurity framework PACDoSQ, which
is (to the best of our knowledge) the first to offer location privacy and
anonymity for spectrum management with counter DoS and PQ security
simultaneously. Our solution introduces the private spectrum bastion (database)
concept to exploit existing architectural features of spectrum management
systems and then synergizes them with multi-server private information
retrieval and PQ-secure Tor to guarantee a location-private and anonymous
acquisition of spectrum information together with hash-based client-server
puzzles for counter DoS. We prove that PACDoSQ achieves its security
objectives, and show its feasibility via a comprehensive performance
evaluation.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.01710v1">Privacy-Preserving Multimedia Mobile Cloud Computing Using Protective
  Perturbation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Multimedia-5BC0EB">
  <p><b>Published on:</b> 2024-09-03T08:47:17Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhongze Tang, Mengmei Ye, Yao Liu, Sheng Wei</p>
    <p><b>Summary:</b> Mobile cloud computing has been adopted in many multimedia applications,
where the resource-constrained mobile device sends multimedia data (e.g.,
images) to remote cloud servers to request computation-intensive multimedia
services (e.g., image recognition). While significantly improving the
performance of the mobile applications, the cloud-based mechanism often causes
privacy concerns as the multimedia data and services are offloaded from the
trusted user device to untrusted cloud servers. Several recent studies have
proposed perturbation-based privacy preserving mechanisms, which obfuscate the
offloaded multimedia data to eliminate privacy exposures without affecting the
functionality of the remote multimedia services. However, the existing privacy
protection approaches require the deployment of computation-intensive
perturbation generation on the resource-constrained mobile devices. Also, the
obfuscated images are typically not compliant with the standard image
compression algorithms and suffer from significant bandwidth consumption. In
this paper, we develop a novel privacy-preserving multimedia mobile cloud
computing framework, namely $PMC^2$, to address the resource and bandwidth
challenges. $PMC^2$ employs secure confidential computing in the cloud to
deploy the perturbation generator, which addresses the resource challenge while
maintaining the privacy. Furthermore, we develop a neural compressor
specifically trained to compress the perturbed images in order to address the
bandwidth challenge. We implement $PMC^2$ in an end-to-end mobile cloud
computing system, based on which our evaluations demonstrate superior latency,
power efficiency, and bandwidth consumption achieved by $PMC^2$ while
maintaining high accuracy in the target multimedia service.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.01661v1">$S^2$NeRF: Privacy-preserving Training Framework for NeRF</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-09-03T07:08:30Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Bokang Zhang, Yanglin Zhang, Zhikun Zhang, Jinglan Yang, Lingying Huang, Junfeng Wu</p>
    <p><b>Summary:</b> Neural Radiance Fields (NeRF) have revolutionized 3D computer vision and
graphics, facilitating novel view synthesis and influencing sectors like
extended reality and e-commerce. However, NeRF's dependence on extensive data
collection, including sensitive scene image data, introduces significant
privacy risks when users upload this data for model training. To address this
concern, we first propose SplitNeRF, a training framework that incorporates
split learning (SL) techniques to enable privacy-preserving collaborative model
training between clients and servers without sharing local data. Despite its
benefits, we identify vulnerabilities in SplitNeRF by developing two attack
methods, Surrogate Model Attack and Scene-aided Surrogate Model Attack, which
exploit the shared gradient data and a few leaked scene images to reconstruct
private scene information. To counter these threats, we introduce $S^2$NeRF,
secure SplitNeRF that integrates effective defense mechanisms. By introducing
decaying noise related to the gradient norm into the shared gradient
information, $S^2$NeRF preserves privacy while maintaining a high utility of
the NeRF model. Our extensive evaluations across multiple datasets demonstrate
the effectiveness of $S^2$NeRF against privacy breaches, confirming its
viability for secure NeRF training in sensitive applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.01329v1">Assessing the Impact of Image Dataset Features on Privacy-Preserving
  Machine Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-09-02T15:30:27Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Lucas Lange, Maurice-Maximilian Heykeroth, Erhard Rahm</p>
    <p><b>Summary:</b> Machine Learning (ML) is crucial in many sectors, including computer vision.
However, ML models trained on sensitive data face security challenges, as they
can be attacked and leak information. Privacy-Preserving Machine Learning
(PPML) addresses this by using Differential Privacy (DP) to balance utility and
privacy. This study identifies image dataset characteristics that affect the
utility and vulnerability of private and non-private Convolutional Neural
Network (CNN) models. Through analyzing multiple datasets and privacy budgets,
we find that imbalanced datasets increase vulnerability in minority classes,
but DP mitigates this issue. Datasets with fewer classes improve both model
utility and privacy, while high entropy or low Fisher Discriminant Ratio (FDR)
datasets deteriorate the utility-privacy trade-off. These insights offer
valuable guidance for practitioners and researchers in estimating and
optimizing the utility-privacy trade-off in image datasets, helping to inform
data and privacy modifications for better outcomes based on dataset
characteristics.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.01088v1">Towards Split Learning-based Privacy-Preserving Record Linkage</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-09-02T09:17:05Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Michail Zervas, Alexandros Karakasidis</p>
    <p><b>Summary:</b> Split Learning has been recently introduced to facilitate applications where
user data privacy is a requirement. However, it has not been thoroughly studied
in the context of Privacy-Preserving Record Linkage, a problem in which the
same real-world entity should be identified among databases from different
dataholders, but without disclosing any additional information. In this paper,
we investigate the potentials of Split Learning for Privacy-Preserving Record
Matching, by introducing a novel training method through the utilization of
Reference Sets, which are publicly available data corpora, showcasing minimal
matching impact against a traditional centralized SVM-based technique.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.00974v1">Enhancing Privacy in Federated Learning: Secure Aggregation for
  Real-World Healthcare Applications</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-09-02T06:43:22Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Riccardo Taiello, Sergen Cansiz, Marc Vesin, Francesco Cremonesi, Lucia Innocenti, Melek Önen, Marco Lorenzi</p>
    <p><b>Summary:</b> Deploying federated learning (FL) in real-world scenarios, particularly in
healthcare, poses challenges in communication and security. In particular, with
respect to the federated aggregation procedure, researchers have been focusing
on the study of secure aggregation (SA) schemes to provide privacy guarantees
over the model's parameters transmitted by the clients. Nevertheless, the
practical availability of SA in currently available FL frameworks is currently
limited, due to computational and communication bottlenecks. To fill this gap,
this study explores the implementation of SA within the open-source Fed-BioMed
framework. We implement and compare two SA protocols, Joye-Libert (JL) and Low
Overhead Masking (LOM), by providing extensive benchmarks in a panel of
healthcare data analysis problems. Our theoretical and experimental evaluations
on four datasets demonstrate that SA protocols effectively protect privacy
while maintaining task accuracy. Computational overhead during training is less
than 1% on a CPU and less than 50% on a GPU for large models, with protection
phases taking less than 10 seconds. Incorporating SA into Fed-BioMed impacts
task accuracy by no more than 2% compared to non-SA scenarios. Overall this
study demonstrates the feasibility of SA in real-world healthcare applications
and contributes in reducing the gap towards the adoption of privacy-preserving
technologies in sensitive applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.00740v1">VPVet: Vetting Privacy Policies of Virtual Reality Apps</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-09-01T15:07:11Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yuxia Zhan, Yan Meng, Lu Zhou, Yichang Xiong, Xiaokuan Zhang, Lichuan Ma, Guoxing Chen, Qingqi Pei, Haojin Zhu</p>
    <p><b>Summary:</b> Virtual reality (VR) apps can harvest a wider range of user data than
web/mobile apps running on personal computers or smartphones. Existing law and
privacy regulations emphasize that VR developers should inform users of what
data are collected/used/shared (CUS) through privacy policies. However, privacy
policies in the VR ecosystem are still in their early stages, and many
developers fail to write appropriate privacy policies that comply with
regulations and meet user expectations. In this paper, we propose VPVet to
automatically vet privacy policy compliance issues for VR apps. VPVet first
analyzes the availability and completeness of a VR privacy policy and then
refines its analysis based on three key criteria: granularity, minimization,
and consistency of CUS statements. Our study establishes the first and
currently largest VR privacy policy dataset named VRPP, consisting of privacy
policies of 11,923 different VR apps from 10 mainstream platforms. Our vetting
results reveal severe privacy issues within the VR ecosystem, including the
limited availability and poor quality of privacy policies, along with their
coarse granularity, lack of adaptation to VR traits and the inconsistency
between CUS statements in privacy policies and their actual behaviors. We
open-source VPVet system along with our findings at repository
https://github.com/kalamoo/PPAudit, aiming to raise awareness within the VR
community and pave the way for further research in this field.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.00739v1">Designing and Evaluating Scalable Privacy Awareness and Control User
  Interfaces for Mixed Reality</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2024-09-01T15:06:40Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Marvin Strauss, Viktorija Paneva, Florian Alt, Stefan Schneegass</p>
    <p><b>Summary:</b> As Mixed Reality (MR) devices become increasingly popular across industries,
they raise significant privacy and ethical concerns due to their capacity to
collect extensive data on users and their environments. This paper highlights
the urgent need for privacy-aware user interfaces that educate and empower both
users and bystanders, enabling them to understand, control, and manage data
collection and sharing. Key research questions include improving user awareness
of privacy implications, developing usable privacy controls, and evaluating the
effectiveness of these measures in real-world settings. The proposed research
roadmap aims to embed privacy considerations into the design and development of
MR technologies, promoting responsible innovation that safeguards user privacy
while preserving the functionality and appeal of these emerging technologies.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.02715v1">Recoverable Anonymization for Pose Estimation: A Privacy-Enhancing
  Approach</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-09-01T05:58:00Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Wenjun Huang, Yang Ni, Arghavan Rezvani, SungHeon Jeong, Hanning Chen, Yezi Liu, Fei Wen, Mohsen Imani</p>
    <p><b>Summary:</b> Human pose estimation (HPE) is crucial for various applications. However,
deploying HPE algorithms in surveillance contexts raises significant privacy
concerns due to the potential leakage of sensitive personal information (SPI)
such as facial features, and ethnicity. Existing privacy-enhancing methods
often compromise either privacy or performance, or they require costly
additional modalities. We propose a novel privacy-enhancing system that
generates privacy-enhanced portraits while maintaining high HPE performance.
Our key innovations include the reversible recovery of SPI for authorized
personnel and the preservation of contextual information. By jointly optimizing
a privacy-enhancing module, a privacy recovery module, and a pose estimator,
our system ensures robust privacy protection, efficient SPI recovery, and
high-performance HPE. Experimental results demonstrate the system's robust
performance in privacy enhancement, SPI recovery, and HPE.</p>
  </details>
</div>

