
<h2>2024-01</h2>

<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.00205v1">Decentralised, Collaborative, and Privacy-preserving Machine Learning
  for Multi-Hospital Data</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-01-31T22:06:10Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Congyu Fang, Adam Dziedzic, Lin Zhang, Laura Oliva, Amol Verma, Fahad Razak, Nicolas Papernot, Bo Wang</p>
    <p><b>Summary:</b> Machine Learning (ML) has demonstrated its great potential on medical data
analysis. Large datasets collected from diverse sources and settings are
essential for ML models in healthcare to achieve better accuracy and
generalizability. Sharing data across different healthcare institutions is
challenging because of complex and varying privacy and regulatory requirements.
Hence, it is hard but crucial to allow multiple parties to collaboratively
train an ML model leveraging the private datasets available at each party
without the need for direct sharing of those datasets or compromising the
privacy of the datasets through collaboration. In this paper, we address this
challenge by proposing Decentralized, Collaborative, and Privacy-preserving ML
for Multi-Hospital Data (DeCaPH). It offers the following key benefits: (1) it
allows different parties to collaboratively train an ML model without
transferring their private datasets; (2) it safeguards patient privacy by
limiting the potential privacy leakage arising from any contents shared across
the parties during the training process; and (3) it facilitates the ML model
training without relying on a centralized server. We demonstrate the
generalizability and power of DeCaPH on three distinct tasks using real-world
distributed medical datasets: patient mortality prediction using electronic
health records, cell-type classification using single-cell human genomes, and
pathology identification using chest radiology images. We demonstrate that the
ML models trained with DeCaPH framework have an improved utility-privacy
trade-off, showing it enables the models to have good performance while
preserving the privacy of the training data points. In addition, the ML models
trained with DeCaPH framework in general outperform those trained solely with
the private datasets from individual parties, showing that DeCaPH enhances the
model generalizability.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.17829v1">Evolving privacy: drift parameter estimation for discretely observed
  i.i.d. diffusion processes under LDP</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Statistics Theory-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36">   
  <p><b>Published on:</b> 2024-01-31T13:41:23Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chiara Amorino, Arnaud Gloter, Hélène Halconruy</p>
    <p><b>Summary:</b> The problem of estimating a parameter in the drift coefficient is addressed
for $N$ discretely observed independent and identically distributed stochastic
differential equations (SDEs). This is done considering additional constraints,
wherein only public data can be published and used for inference. The concept
of local differential privacy (LDP) is formally introduced for a system of
stochastic differential equations. The objective is to estimate the drift
parameter by proposing a contrast function based on a pseudo-likelihood
approach. A suitably scaled Laplace noise is incorporated to meet the privacy
requirements. Our key findings encompass the derivation of explicit conditions
tied to the privacy level. Under these conditions, we establish the consistency
and asymptotic normality of the associated estimator. Notably, the convergence
rate is intricately linked to the privacy level, and is some situations may be
completely different from the case where privacy constraints are ignored. Our
results hold true as the discretization step approaches zero and the number of
processes $N$ tends to infinity.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.00896v1">Privacy and Security Implications of Cloud-Based AI Services : A Survey</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-01-31T13:30:20Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Alka Luqman, Riya Mahesh, Anupam Chattopadhyay</p>
    <p><b>Summary:</b> This paper details the privacy and security landscape in today's cloud
ecosystem and identifies that there is a gap in addressing the risks introduced
by machine learning models. As machine learning algorithms continue to evolve
and find applications across diverse domains, the need to categorize and
quantify privacy and security risks becomes increasingly critical. With the
emerging trend of AI-as-a-Service (AIaaS), machine learned AI models (or ML
models) are deployed on the cloud by model providers and used by model
consumers. We first survey the AIaaS landscape to document the various kinds of
liabilities that ML models, especially Deep Neural Networks pose and then
introduce a taxonomy to bridge this gap by holistically examining the risks
that creators and consumers of ML models are exposed to and their known
defences till date. Such a structured approach will be beneficial for ML model
providers to create robust solutions. Likewise, ML model consumers will find it
valuable to evaluate such solutions and understand the implications of their
engagement with such services. The proposed taxonomies provide a foundational
basis for solutions in private, secure and robust ML, paving the way for more
transparent and resilient AI systems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.17823v1">Privacy-preserving data release leveraging optimal transport and
  particle gradient descent</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-01-31T13:28:07Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Konstantin Donhauser, Javier Abad, Neha Hulkund, Fanny Yang</p>
    <p><b>Summary:</b> We present a novel approach for differentially private data synthesis of
protected tabular datasets, a relevant task in highly sensitive domains such as
healthcare and government. Current state-of-the-art methods predominantly use
marginal-based approaches, where a dataset is generated from private estimates
of the marginals. In this paper, we introduce PrivPGD, a new generation method
for marginal-based private data synthesis, leveraging tools from optimal
transport and particle gradient descent. Our algorithm outperforms existing
methods on a large range of datasets while being highly scalable and offering
the flexibility to incorporate additional domain-specific constraints.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.17630v1">Towards Personalized Privacy: User-Governed Data Contribution for
  Federated Recommendation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Retrieval-5BC0EB">
  <p><b>Published on:</b> 2024-01-31T07:20:56Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Liang Qu, Wei Yuan, Ruiqi Zheng, Lizhen Cui, Yuhui Shi, Hongzhi Yin</p>
    <p><b>Summary:</b> Federated recommender systems (FedRecs) have gained significant attention for
their potential to protect user's privacy by keeping user privacy data locally
and only communicating model parameters/gradients to the server. Nevertheless,
the currently existing architecture of FedRecs assumes that all users have the
same 0-privacy budget, i.e., they do not upload any data to the server, thus
overlooking those users who are less concerned about privacy and are willing to
upload data to get a better recommendation service. To bridge this gap, this
paper explores a user-governed data contribution federated recommendation
architecture where users are free to take control of whether they share data
and the proportion of data they share to the server. To this end, this paper
presents a cloud-device collaborative graph neural network federated
recommendation model, named CDCGNNFed. It trains user-centric ego graphs
locally, and high-order graphs based on user-shared data in the server in a
collaborative manner via contrastive learning. Furthermore, a graph mending
strategy is utilized to predict missing links in the graph on the server, thus
leveraging the capabilities of graph neural networks over high-order graphs.
Extensive experiments were conducted on two public datasets, and the results
demonstrate the effectiveness of the proposed method.</p>
  </details>
</div>



<h2>2024-02</h2>

<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.06137v1">On the Privacy of Selection Mechanisms with Gaussian Noise</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-02-09T02:11:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jonathan Lebensold, Doina Precup, Borja Balle</p>
    <p><b>Summary:</b> Report Noisy Max and Above Threshold are two classical differentially private
(DP) selection mechanisms. Their output is obtained by adding noise to a
sequence of low-sensitivity queries and reporting the identity of the query
whose (noisy) answer satisfies a certain condition. Pure DP guarantees for
these mechanisms are easy to obtain when Laplace noise is added to the queries.
On the other hand, when instantiated using Gaussian noise, standard analyses
only yield approximate DP guarantees despite the fact that the outputs of these
mechanisms lie in a discrete space. In this work, we revisit the analysis of
Report Noisy Max and Above Threshold with Gaussian noise and show that, under
the additional assumption that the underlying queries are bounded, it is
possible to provide pure ex-ante DP bounds for Report Noisy Max and pure
ex-post DP bounds for Above Threshold. The resulting bounds are tight and
depend on closed-form expressions that can be numerically evaluated using
standard methods. Empirically we find these lead to tighter privacy accounting
in the high privacy, low data regime. Further, we propose a simple privacy
filter for composing pure ex-post DP guarantees, and use it to derive a fully
adaptive Gaussian Sparse Vector Technique mechanism. Finally, we provide
experiments on mobility and energy consumption datasets demonstrating that our
Sparse Vector Technique is practically competitive with previous approaches and
requires less hyper-parameter tuning.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.05860v1">Privacy-Preserving Synthetic Continual Semantic Segmentation for Robotic
  Surgery</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-02-08T17:44:06Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mengya Xu, Mobarakol Islam, Long Bai, Hongliang Ren</p>
    <p><b>Summary:</b> Deep Neural Networks (DNNs) based semantic segmentation of the robotic
instruments and tissues can enhance the precision of surgical activities in
robot-assisted surgery. However, in biological learning, DNNs cannot learn
incremental tasks over time and exhibit catastrophic forgetting, which refers
to the sharp decline in performance on previously learned tasks after learning
a new one. Specifically, when data scarcity is the issue, the model shows a
rapid drop in performance on previously learned instruments after learning new
data with new instruments. The problem becomes worse when it limits releasing
the dataset of the old instruments for the old model due to privacy concerns
and the unavailability of the data for the new or updated version of the
instruments for the continual learning model. For this purpose, we develop a
privacy-preserving synthetic continual semantic segmentation framework by
blending and harmonizing (i) open-source old instruments foreground to the
synthesized background without revealing real patient data in public and (ii)
new instruments foreground to extensively augmented real background. To boost
the balanced logit distillation from the old model to the continual learning
model, we design overlapping class-aware temperature normalization (CAT) by
controlling model learning utility. We also introduce multi-scale
shifted-feature distillation (SD) to maintain long and short-range spatial
relationships among the semantic objects where conventional short-range spatial
features with limited information reduce the power of feature distillation. We
demonstrate the effectiveness of our framework on the EndoVis 2017 and 2018
instrument segmentation dataset with a generalized continual learning setting.
Code is available at~\url{https://github.com/XuMengyaAmy/Synthetic_CAT_SD}.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.05690v1">Overcoming Noise Limitations in QKD with Quantum Privacy Amplification</a></h3>
  
  <p><b>Published on:</b> 2024-02-08T14:07:36Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Philipp Sohr, Sebastian Ecker, Lukas Bulla, Martin Bohmann, Rupert Ursin</p>
    <p><b>Summary:</b> High-quality, distributed quantum entanglement is the distinctive resource
for quantum communication and forms the foundation for the unequalled level of
security that can be assured in quantum key distribution. While the
entanglement provider does not need to be trusted, the secure key rate drops to
zero if the entanglement used is too noisy. In this paper, we show
experimentally that QPA is able to increase the secure key rate achievable with
QKD by improving the quality of distributed entanglement, thus increasing the
quantum advantage in QKD. Beyond that, we show that QPA enables key generation
at noise levels that previously prevented key generation. These remarkable
results were only made possible by the efficient implementation exploiting
hyperentanglement in the polarisation and energy-time degrees of freedom. We
provide a detailed characterisation of the gain in secure key rate achieved in
our proof-of-principle experiment at different noise levels. The results are
paramount for the implementation of a global quantum network linking quantum
processors and ensuring future-proof data security.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.05453v1">Mitigating Privacy Risk in Membership Inference by Convex-Concave Loss</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-02-08T07:14:17Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhenlong Liu, Lei Feng, Huiping Zhuang, Xiaofeng Cao, Hongxin Wei</p>
    <p><b>Summary:</b> Machine learning models are susceptible to membership inference attacks
(MIAs), which aim to infer whether a sample is in the training set. Existing
work utilizes gradient ascent to enlarge the loss variance of training data,
alleviating the privacy risk. However, optimizing toward a reverse direction
may cause the model parameters to oscillate near local minima, leading to
instability and suboptimal performance. In this work, we propose a novel method
-- Convex-Concave Loss, which enables a high variance of training loss
distribution by gradient descent. Our method is motivated by the theoretical
analysis that convex losses tend to decrease the loss variance during training.
Thus, our key idea behind CCL is to reduce the convexity of loss functions with
a concave term. Trained with CCL, neural networks produce losses with high
variance for training data, reinforcing the defense against MIAs. Extensive
experiments demonstrate the superiority of CCL, achieving state-of-the-art
balance in the privacy-utility trade-off.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.04840v1">Efficient Estimation of a Gaussian Mean with Local Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Statistics Theory-D91E36"> 
  <p><b>Published on:</b> 2024-02-07T13:41:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Nikita Kalinin, Lukas Steinberger</p>
    <p><b>Summary:</b> In this paper we study the problem of estimating the unknown mean $\theta$ of
a unit variance Gaussian distribution in a locally differentially private (LDP)
way. In the high-privacy regime ($\epsilon\le 0.67$), we identify the exact
optimal privacy mechanism that minimizes the variance of the estimator
asymptotically. It turns out to be the extraordinarily simple sign mechanism
that applies randomized response to the sign of $X_i-\theta$. However, since
this optimal mechanism depends on the unknown mean $\theta$, we employ a
two-stage LDP parameter estimation procedure which requires splitting agents
into two groups. The first $n_1$ observations are used to consistently but not
necessarily efficiently estimate the parameter $\theta$ by
$\tilde{\theta}_{n_1}$. Then this estimate is updated by applying the sign
mechanism with $\tilde{\theta}_{n_1}$ instead of $\theta$ to the remaining
$n-n_1$ observations, to obtain an LDP and efficient estimator of the unknown
mean.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.04489v1">De-amplifying Bias from Differential Privacy in Language Model
  Fine-tuning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> 
  <p><b>Published on:</b> 2024-02-07T00:30:58Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sanjari Srivastava, Piotr Mardziel, Zhikhun Zhang, Archana Ahlawat, Anupam Datta, John C Mitchell</p>
    <p><b>Summary:</b> Fairness and privacy are two important values machine learning (ML)
practitioners often seek to operationalize in models. Fairness aims to reduce
model bias for social/demographic sub-groups. Privacy via differential privacy
(DP) mechanisms, on the other hand, limits the impact of any individual's
training data on the resulting model. The trade-offs between privacy and
fairness goals of trustworthy ML pose a challenge to those wishing to address
both. We show that DP amplifies gender, racial, and religious bias when
fine-tuning large language models (LLMs), producing models more biased than
ones fine-tuned without DP. We find the cause of the amplification to be a
disparity in convergence of gradients across sub-groups. Through the case of
binary gender bias, we demonstrate that Counterfactual Data Augmentation (CDA),
a known method for addressing bias, also mitigates bias amplification by DP. As
a consequence, DP and CDA together can be used to fine-tune models while
maintaining both fairness and privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.04033v1">On provable privacy vulnerabilities of graph representations</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-02-06T14:26:22Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ruofan Wu, Guanhua Fang, Qiying Pan, Mingyang Zhang, Tengfei Liu, Weiqiang Wang, Wenbiao Zhao</p>
    <p><b>Summary:</b> Graph representation learning (GRL) is critical for extracting insights from
complex network structures, but it also raises security concerns due to
potential privacy vulnerabilities in these representations. This paper
investigates the structural vulnerabilities in graph neural models where
sensitive topological information can be inferred through edge reconstruction
attacks. Our research primarily addresses the theoretical underpinnings of
cosine-similarity-based edge reconstruction attacks (COSERA), providing
theoretical and empirical evidence that such attacks can perfectly reconstruct
sparse Erdos Renyi graphs with independent random features as graph size
increases. Conversely, we establish that sparsity is a critical factor for
COSERA's effectiveness, as demonstrated through analysis and experiments on
stochastic block models. Finally, we explore the resilience of (provably)
private graph representations produced via noisy aggregation (NAG) mechanism
against COSERA. We empirically delineate instances wherein COSERA demonstrates
both efficacy and deficiency in its capacity to function as an instrument for
elucidating the trade-off between privacy and utility.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.04013v1">Privacy Leakage on DNNs: A Survey of Model Inversion Attacks and
  Defenses</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-02-06T14:06:23Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Hao Fang, Yixiang Qiu, Hongyao Yu, Wenbo Yu, Jiawei Kong, Baoli Chong, Bin Chen, Xuan Wang, Shu-Tao Xia</p>
    <p><b>Summary:</b> Model Inversion (MI) attacks aim to disclose private information about the
training data by abusing access to the pre-trained models. These attacks enable
adversaries to reconstruct high-fidelity data that closely aligns with the
private training data, which has raised significant privacy concerns. Despite
the rapid advances in the field, we lack a comprehensive overview of existing
MI attacks and defenses. To fill this gap, this paper thoroughly investigates
this field and presents a holistic survey. Firstly, our work briefly reviews
the traditional MI on machine learning scenarios. We then elaborately analyze
and compare numerous recent attacks and defenses on \textbf{D}eep
\textbf{N}eural \textbf{N}etworks (DNNs) across multiple modalities and
learning tasks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.03907v1">Embedding Large Language Models into Extended Reality: Opportunities and
  Challenges for Inclusion, Engagement, and Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-02-06T11:19:40Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Efe Bozkir, Süleyman Özdel, Ka Hei Carrie Lau, Mengdi Wang, Hong Gao, Enkelejda Kasneci</p>
    <p><b>Summary:</b> Recent developments in computer graphics, hardware, artificial intelligence
(AI), and human-computer interaction likely lead to extended reality (XR)
devices and setups being more pervasive. While these devices and setups provide
users with interactive, engaging, and immersive experiences with different
sensing modalities, such as eye and hand trackers, many non-player characters
are utilized in a pre-scripted way or by conventional AI techniques. In this
paper, we argue for using large language models (LLMs) in XR by embedding them
in virtual avatars or as narratives to facilitate more inclusive experiences
through prompt engineering according to user profiles and fine-tuning the LLMs
for particular purposes. We argue that such inclusion will facilitate diversity
for XR use. In addition, we believe that with the versatile conversational
capabilities of LLMs, users will engage more with XR environments, which might
help XR be more used in everyday life. Lastly, we speculate that combining the
information provided to LLM-powered environments by the users and the biometric
data obtained through the sensors might lead to novel privacy invasions. While
studying such possible privacy invasions, user privacy concerns and preferences
should also be investigated. In summary, despite some challenges, embedding
LLMs into XR is a promising and novel research area with several opportunities.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.03702v1">On Learning Spatial Provenance in Privacy-Constrained Wireless Networks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Networking and Internet Architecture-04E762"> 
  <p><b>Published on:</b> 2024-02-06T04:44:36Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Manish Bansal, Pramsu Srivastava, J. Harshan</p>
    <p><b>Summary:</b> In Vehicle-to-Everything networks that involve multi-hop communication, the
Road Side Units (RSUs) typically aim to collect location information from the
participating vehicles to provide security and network diagnostics features.
While the vehicles commonly use the Global Positioning System (GPS) for
navigation, they may refrain from sharing their precise GPS coordinates with
the RSUs due to privacy concerns. Therefore, to jointly address the high
localization requirements by the RSUs as well as the vehicles' privacy, we
present a novel spatial-provenance framework wherein each vehicle uses Bloom
filters to embed their partial location information when forwarding the
packets. In this framework, the RSUs and the vehicles agree upon fragmenting
the coverage area into several smaller regions so that the vehicles can embed
the identity of their regions through Bloom filters. Given the probabilistic
nature of Bloom filters, we derive an analytical expression on the error-rates
in provenance recovery and then pose an optimization problem to choose the
underlying parameters. With the help of extensive simulation results, we show
that our method offers near-optimal Bloom filter parameters in learning spatial
provenance. Some interesting trade-offs between the communication-overhead,
spatial privacy of the vehicles and the error rates in provenance recovery are
also discussed.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.03688v1">A Survey of Privacy Threats and Defense in Vertical Federated Learning:
  From Model Life Cycle Perspective</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-02-06T04:22:44Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Lei Yu, Meng Han, Yiming Li, Changting Lin, Yao Zhang, Mingyang Zhang, Yan Liu, Haiqin Weng, Yuseok Jeon, Ka-Ho Chow, Stacy Patterson</p>
    <p><b>Summary:</b> Vertical Federated Learning (VFL) is a federated learning paradigm where
multiple participants, who share the same set of samples but hold different
features, jointly train machine learning models. Although VFL enables
collaborative machine learning without sharing raw data, it is still
susceptible to various privacy threats. In this paper, we conduct the first
comprehensive survey of the state-of-the-art in privacy attacks and defenses in
VFL. We provide taxonomies for both attacks and defenses, based on their
characterizations, and discuss open challenges and future research directions.
Specifically, our discussion is structured around the model's life cycle, by
delving into the privacy threats encountered during different stages of machine
learning and their corresponding countermeasures. This survey not only serves
as a resource for the research community but also offers clear guidance and
actionable insights for practitioners to safeguard data privacy throughout the
model's life cycle.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.03612v1">Privacy risk in GeoData: A survey</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-02-06T00:55:06Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mahrokh Abdollahi Lorestani, Thilina Ranbaduge, Thierry Rakotoarivelo</p>
    <p><b>Summary:</b> With the ubiquitous use of location-based services, large-scale
individual-level location data has been widely collected through
location-awareness devices. The exposure of location data constitutes a
significant privacy risk to users as it can lead to de-anonymisation, the
inference of sensitive information, and even physical threats. Geoprivacy
concerns arise on the issues of user identity de-anonymisation and location
exposure. In this survey, we analyse different geomasking techniques that have
been proposed to protect the privacy of individuals in geodata. We present a
taxonomy to characterise these techniques along different dimensions, and
conduct a survey of geomasking techniques. We then highlight shortcomings of
current techniques and discuss avenues for future research.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.03582v1">Matcha: An IDE Plugin for Creating Accurate Privacy Nutrition Labels</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-02-05T23:17:08Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tianshi Li, Lorrie Faith Cranor, Yuvraj Agarwal, Jason I. Hong</p>
    <p><b>Summary:</b> Apple and Google introduced their versions of privacy nutrition labels to the
mobile app stores to better inform users of the apps' data practices. However,
these labels are self-reported by developers and have been found to contain
many inaccuracies due to misunderstandings of the label taxonomy. In this work,
we present Matcha, an IDE plugin that uses automated code analysis to help
developers create accurate Google Play data safety labels. Developers can
benefit from Matcha's ability to detect user data accesses and transmissions
while staying in control of the generated label by adding custom Java
annotations and modifying an auto-generated XML specification. Our evaluation
with 12 developers showed that Matcha helped our participants improved the
accuracy of a label they created with Google's official tool for a real-world
app they developed. We found that participants preferred Matcha for its
accuracy benefits. Drawing on Matcha, we discuss general design recommendations
for developer tools used to create accurate standardized privacy notices.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.03531v1">Fairness and Privacy Guarantees in Federated Contextual Bandits</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-02-05T21:38:23Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sambhav Solanki, Shweta Jain, Sujit Gujar</p>
    <p><b>Summary:</b> This paper considers the contextual multi-armed bandit (CMAB) problem with
fairness and privacy guarantees in a federated environment. We consider
merit-based exposure as the desired fair outcome, which provides exposure to
each action in proportion to the reward associated. We model the algorithm's
effectiveness using fairness regret, which captures the difference between fair
optimal policy and the policy output by the algorithm. Applying fair CMAB
algorithm to each agent individually leads to fairness regret linear in the
number of agents. We propose that collaborative -- federated learning can be
more effective and provide the algorithm Fed-FairX-LinUCB that also ensures
differential privacy. The primary challenge in extending the existing privacy
framework is designing the communication protocol for communicating required
information across agents. A naive protocol can either lead to weaker privacy
guarantees or higher regret. We design a novel communication protocol that
allows for (i) Sub-linear theoretical bounds on fairness regret for
Fed-FairX-LinUCB and comparable bounds for the private counterpart,
Priv-FairX-LinUCB (relative to single-agent learning), (ii) Effective use of
privacy budget in Priv-FairX-LinUCB. We demonstrate the efficacy of our
proposed algorithm with extensive simulations-based experiments. We show that
both Fed-FairX-LinUCB and Priv-FairX-LinUCB achieve near-optimal fairness
regret.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.03435v1">Psychological Assessments with Large Language Models: A Privacy-Focused
  and Cost-Effective Approach</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2024-02-05T19:00:02Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sergi Blanco-Cuaresma</p>
    <p><b>Summary:</b> This study explores the use of Large Language Models (LLMs) to analyze text
comments from Reddit users, aiming to achieve two primary objectives: firstly,
to pinpoint critical excerpts that support a predefined psychological
assessment of suicidal risk; and secondly, to summarize the material to
substantiate the preassigned suicidal risk level. The work is circumscribed to
the use of "open-source" LLMs that can be run locally, thereby enhancing data
privacy. Furthermore, it prioritizes models with low computational
requirements, making it accessible to both individuals and institutions
operating on limited computing budgets. The implemented strategy only relies on
a carefully crafted prompt and a grammar to guide the LLM's text completion.
Despite its simplicity, the evaluation metrics show outstanding results, making
it a valuable privacy-focused and cost-effective approach. This work is part of
the Computational Linguistics and Clinical Psychology (CLPsych) 2024 shared
task.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.03114v1">Augmenting Security and Privacy in the Virtual Realm: An Analysis of
  Extended Reality Devices</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-02-05T15:45:11Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Derin Cayir, Abbas Acar, Riccardo Lazzeretti, Marco Angelini, Mauro Conti, Selcuk Uluagac</p>
    <p><b>Summary:</b> In this work, we present a device-centric analysis of security and privacy
attacks and defenses on Extended Reality (XR) devices, highlighting the need
for robust and privacy-aware security mechanisms. Based on our analysis, we
present future research directions and propose design considerations to help
ensure the security and privacy of XR devices.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.02672v1">Estimation of conditional average treatment effects on distributed data:
  A privacy-preserving approach</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-02-05T02:17:21Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yuji Kawamata, Ryoki Motai, Yukihiko Okada, Akira Imakura, Tetsuya Sakurai</p>
    <p><b>Summary:</b> Estimation of conditional average treatment effects (CATEs) is an important
topic in various fields such as medical and social sciences. CATEs can be
estimated with high accuracy if distributed data across multiple parties can be
centralized. However, it is difficult to aggregate such data if they contain
privacy information. To address this issue, we proposed data collaboration
double machine learning (DC-DML), a method that can estimate CATE models with
privacy preservation of distributed data, and evaluated the method through
numerical experiments. Our contributions are summarized in the following three
points. First, our method enables estimation and testing of semi-parametric
CATE models without iterative communication on distributed data.
Semi-parametric or non-parametric CATE models enable estimation and testing
that is more robust to model mis-specification than parametric models. However,
to our knowledge, no communication-efficient method has been proposed for
estimating and testing semi-parametric or non-parametric CATE models on
distributed data. Second, our method enables collaborative estimation between
different parties as well as multiple time points because the
dimensionality-reduced intermediate representations can be accumulated. Third,
our method performed as well or better than other methods in evaluation
experiments using synthetic, semi-synthetic and real-world datasets.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.02230v1">Federated Learning with Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB"> 
  <p><b>Published on:</b> 2024-02-03T18:21:38Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Adrien Banse, Jan Kreischer, Xavier Oliva i Jürgens</p>
    <p><b>Summary:</b> Federated learning (FL), as a type of distributed machine learning, is
capable of significantly preserving client's private data from being shared
among different parties. Nevertheless, private information can still be
divulged by analyzing uploaded parameter weights from clients. In this report,
we showcase our empirical benchmark of the effect of the number of clients and
the addition of differential privacy (DP) mechanisms on the performance of the
model on different types of data. Our results show that non-i.i.d and small
datasets have the highest decrease in performance in a distributed and
differentially private setting.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.01994v1">Human-Centered Privacy Research in the Age of Large Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-02-03T02:32:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tianshi Li, Sauvik Das, Hao-Ping Lee, Dakuo Wang, Bingsheng Yao, Zhiping Zhang</p>
    <p><b>Summary:</b> The emergence of large language models (LLMs), and their increased use in
user-facing systems, has led to substantial privacy concerns. To date, research
on these privacy concerns has been model-centered: exploring how LLMs lead to
privacy risks like memorization, or can be used to infer personal
characteristics about people from their content. We argue that there is a need
for more research focusing on the human aspect of these privacy issues: e.g.,
research on how design paradigms for LLMs affect users' disclosure behaviors,
users' mental models and preferences for privacy controls, and the design of
tools, systems, and artifacts that empower end-users to reclaim ownership over
their personal data. To build usable, efficient, and privacy-friendly systems
powered by these models with imperfect privacy properties, our goal is to
initiate discussions to outline an agenda for conducting human-centered
research on privacy issues in LLM-powered systems. This Special Interest Group
(SIG) aims to bring together researchers with backgrounds in usable security
and privacy, human-AI collaboration, NLP, or any other related domains to share
their perspectives and experiences on this problem, to help our community
establish a collective understanding of the challenges, research opportunities,
research methods, and strategies to collaborate with researchers outside of
HCI.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.01857v1">Position Paper: Assessing Robustness, Privacy, and Fairness in Federated
  Learning Integrated with Foundation Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2024-02-02T19:26:00Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xi Li, Jiaqi Wang</p>
    <p><b>Summary:</b> Federated Learning (FL), while a breakthrough in decentralized machine
learning, contends with significant challenges such as limited data
availability and the variability of computational resources, which can stifle
the performance and scalability of the models. The integration of Foundation
Models (FMs) into FL presents a compelling solution to these issues, with the
potential to enhance data richness and reduce computational demands through
pre-training and data augmentation. However, this incorporation introduces
novel issues in terms of robustness, privacy, and fairness, which have not been
sufficiently addressed in the existing research. We make a preliminary
investigation into this field by systematically evaluating the implications of
FM-FL integration across these dimensions. We analyze the trade-offs involved,
uncover the threats and issues introduced by this integration, and propose a
set of criteria and strategies for navigating these challenges. Furthermore, we
identify potential research directions for advancing this field, laying a
foundation for future development in creating reliable, secure, and equitable
FL systems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.01546v1">Privacy-Preserving Distributed Learning for Residential Short-Term Load
  Forecasting</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Multiagent Systems-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36"> 
  <p><b>Published on:</b> 2024-02-02T16:39:08Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yi Dong, Yingjie Wang, Mariana Gama, Mustafa A. Mustafa, Geert Deconinck, Xiaowei Huang</p>
    <p><b>Summary:</b> In the realm of power systems, the increasing involvement of residential
users in load forecasting applications has heightened concerns about data
privacy. Specifically, the load data can inadvertently reveal the daily
routines of residential users, thereby posing a risk to their property
security. While federated learning (FL) has been employed to safeguard user
privacy by enabling model training without the exchange of raw data, these FL
models have shown vulnerabilities to emerging attack techniques, such as Deep
Leakage from Gradients and poisoning attacks. To counteract these, we initially
employ a Secure-Aggregation (SecAgg) algorithm that leverages multiparty
computation cryptographic techniques to mitigate the risk of gradient leakage.
However, the introduction of SecAgg necessitates the deployment of additional
sub-center servers for executing the multiparty computation protocol, thereby
escalating computational complexity and reducing system robustness, especially
in scenarios where one or more sub-centers are unavailable. To address these
challenges, we introduce a Markovian Switching-based distributed training
framework, the convergence of which is substantiated through rigorous
theoretical analysis. The Distributed Markovian Switching (DMS) topology shows
strong robustness towards the poisoning attacks as well. Case studies employing
real-world power system load data validate the efficacy of our proposed
algorithm. It not only significantly minimizes communication complexity but
also maintains accuracy levels comparable to traditional FL methods, thereby
enhancing the scalability of our load forecasting algorithm.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.01296v1">Bi-CryptoNets: Leveraging Different-Level Privacy for Encrypted
  Inference</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-02-02T10:35:05Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Man-Jie Yuan, Zheng Zou, Wei Gao</p>
    <p><b>Summary:</b> Privacy-preserving neural networks have attracted increasing attention in
recent years, and various algorithms have been developed to keep the balance
between accuracy, computational complexity and information security from the
cryptographic view. This work takes a different view from the input data and
structure of neural networks. We decompose the input data (e.g., some images)
into sensitive and insensitive segments according to importance and privacy.
The sensitive segment includes some important and private information such as
human faces and we take strong homomorphic encryption to keep security, whereas
the insensitive one contains some background and we add perturbations. We
propose the bi-CryptoNets, i.e., plaintext and ciphertext branches, to deal
with two segments, respectively, and ciphertext branch could utilize the
information from plaintext branch by unidirectional connections. We adopt
knowledge distillation for our bi-CryptoNets by transferring representations
from a well-trained teacher neural network. Empirical studies show the
effectiveness and decrease of inference latency for our bi-CryptoNets.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.01226v1">HW-SW Optimization of DNNs for Privacy-preserving People Counting on
  Low-resolution Infrared Arrays</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Hardware Architecture-04E762">
  <p><b>Published on:</b> 2024-02-02T08:45:38Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Matteo Risso, Chen Xie, Francesco Daghero, Alessio Burrello, Seyedmorteza Mollaei, Marco Castellano, Enrico Macii, Massimo Poncino, Daniele Jahier Pagliari</p>
    <p><b>Summary:</b> Low-resolution infrared (IR) array sensors enable people counting
applications such as monitoring the occupancy of spaces and people flows while
preserving privacy and minimizing energy consumption. Deep Neural Networks
(DNNs) have been shown to be well-suited to process these sensor data in an
accurate and efficient manner. Nevertheless, the space of DNNs' architectures
is huge and its manual exploration is burdensome and often leads to sub-optimal
solutions. To overcome this problem, in this work, we propose a highly
automated full-stack optimization flow for DNNs that goes from neural
architecture search, mixed-precision quantization, and post-processing, down to
the realization of a new smart sensor prototype, including a Microcontroller
with a customized instruction set. Integrating these cross-layer optimizations,
we obtain a large set of Pareto-optimal solutions in the 3D-space of energy,
memory, and accuracy. Deploying such solutions on our hardware platform, we
improve the state-of-the-art achieving up to 4.2x model size reduction, 23.8x
code size reduction, and 15.38x energy reduction at iso-accuracy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.01198v1">Physical Layer Location Privacy in SIMO Communication Using Fake Paths
  Injection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36">  
  <p><b>Published on:</b> 2024-02-02T07:52:32Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Trong Duy Tran, Maxime Ferreira Da Costa, Linh Trung Nguyen</p>
    <p><b>Summary:</b> Fake path injection is an emerging paradigm for inducing privacy over
wireless networks. In this paper, fake paths are injected by the transmitter
into a SIMO multipath communication channel to preserve her physical location
from an eavesdropper. A novel statistical privacy metric is defined as the
ratio between the largest (resp. smallest) eigenvalues of Bob's (resp. Eve's)
Cram\'er-Rao lower bound on the SIMO multipath channel parameters to assess the
privacy enhancements. Leveraging the spectral properties of generalized
Vandermonde matrices, bounds on the privacy margin of the proposed scheme are
derived. Specifically, it is shown that the privacy margin increases
quadratically in the inverse of the separation between the true and the fake
paths under Eve's perspective. Numerical simulations further showcase the
approach's benefit.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.01096v1">Trustworthy Distributed AI Systems: Robustness, Privacy, and Governance</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2024-02-02T01:58:58Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Wenqi Wei, Ling Liu</p>
    <p><b>Summary:</b> Emerging Distributed AI systems are revolutionizing big data computing and
data processing capabilities with growing economic and societal impact.
However, recent studies have identified new attack surfaces and risks caused by
security, privacy, and fairness issues in AI systems. In this paper, we review
representative techniques, algorithms, and theoretical foundations for
trustworthy distributed AI through robustness guarantee, privacy protection,
and fairness awareness in distributed learning. We first provide a brief
overview of alternative architectures for distributed learning, discuss
inherent vulnerabilities for security, privacy, and fairness of AI algorithms
in distributed learning, and analyze why these problems are present in
distributed learning regardless of specific architectures. Then we provide a
unique taxonomy of countermeasures for trustworthy distributed AI, covering (1)
robustness to evasion attacks and irregular queries at inference, and
robustness to poisoning attacks, Byzantine attacks, and irregular data
distribution during training; (2) privacy protection during distributed
learning and model inference at deployment; and (3) AI fairness and governance
with respect to both data and models. We conclude with a discussion on open
challenges and future research directions toward trustworthy distributed AI,
such as the need for trustworthy AI policy guidelines, the AI
responsibility-utility co-design, and incentives and compliance.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.01001v1">Ensuring Data Privacy in AC Optimal Power Flow with a Distributed
  Co-Simulation Framework</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2024-02-01T20:31:08Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xinliang Dai, Alexander Kocher, Jovana Kovačević, Burak Dindar, Yuning Jiang, Colin N. Jones, Hüseyin Çakmak, Veit Hagenmeyer</p>
    <p><b>Summary:</b> During the energy transition, the significance of collaborative management
among institutions is rising, confronting challenges posed by data privacy
concerns. Prevailing research on distributed approaches, as an alternative to
centralized management, often lacks numerical convergence guarantees or is
limited to single-machine numerical simulation. To address this, we present a
distributed approach for solving AC Optimal Power Flow (OPF) problems within a
geographically distributed environment. This involves integrating the energy
system Co-Simulation (eCoSim) module in the eASiMOV framework with the
convergence-guaranteed distributed optimization algorithm, i.e., the Augmented
Lagrangian based Alternating Direction Inexact Newton method (ALADIN).
Comprehensive evaluations across multiple system scenarios reveal a marginal
performance slowdown compared to the centralized approach and the distributed
approach executed on single machines -- a justified trade-off for enhanced data
privacy. This investigation serves as empirical validation of the successful
execution of distributed AC OPF within a geographically distributed
environment, highlighting potential directions for future research.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.00342v1">Survey of Privacy Threats and Countermeasures in Federated Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-02-01T05:13:14Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Masahiro Hayashitani, Junki Mori, Isamu Teranishi</p>
    <p><b>Summary:</b> Federated learning is widely considered to be as a privacy-aware learning
method because no training data is exchanged directly between clients.
Nevertheless, there are threats to privacy in federated learning, and privacy
countermeasures have been studied. However, we note that common and unique
privacy threats among typical types of federated learning have not been
categorized and described in a comprehensive and specific way. In this paper,
we describe privacy threats and countermeasures for the typical types of
federated learning; horizontal federated learning, vertical federated learning,
and transfer federated learning.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.00906v1">BrainLeaks: On the Privacy-Preserving Properties of Neuromorphic
  Architectures against Model Inversion Attacks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Neural and Evolutionary Computing-5BC0EB">
  <p><b>Published on:</b> 2024-02-01T03:16:40Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Hamed Poursiami, Ihsen Alouani, Maryam Parsa</p>
    <p><b>Summary:</b> With the mainstream integration of machine learning into security-sensitive
domains such as healthcare and finance, concerns about data privacy have
intensified. Conventional artificial neural networks (ANNs) have been found
vulnerable to several attacks that can leak sensitive data. Particularly, model
inversion (MI) attacks enable the reconstruction of data samples that have been
used to train the model. Neuromorphic architectures have emerged as a paradigm
shift in neural computing, enabling asynchronous and energy-efficient
computation. However, little to no existing work has investigated the privacy
of neuromorphic architectures against model inversion. Our study is motivated
by the intuition that the non-differentiable aspect of spiking neural networks
(SNNs) might result in inherent privacy-preserving properties, especially
against gradient-based attacks. To investigate this hypothesis, we propose a
thorough exploration of SNNs' privacy-preserving capabilities. Specifically, we
develop novel inversion attack strategies that are comprehensively designed to
target SNNs, offering a comparative analysis with their conventional ANN
counterparts. Our experiments, conducted on diverse event-based and static
datasets, demonstrate the effectiveness of the proposed attack strategies and
therefore questions the assumption of inherent privacy-preserving in
neuromorphic architectures.</p>
  </details>
</div>

