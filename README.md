
<h2>2025-03</h2>

<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.19872v1">NickPay, an Auditable, Privacy-Preserving, Nickname-Based Payment System</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-25T17:36:54Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Guillaume Quispe, Pierre Jouvelot, Gerard Memmi</p>
    <p><b>Summary:</b> In this paper, we describe the motivation, design, security properties, and a
prototype implementation of NickPay, a new privacy-preserving yet auditable
payment system built on top of the Ethereum blockchain platform. NickPay offers
a strong level of privacy to participants and prevents successive payment
transfers from being linked to their actual owners.
  It is providing the transparency that blockchains ensure and at the same
time, preserving the possibility for a trusted authority to access sensitive
information, e.g., for audit purposes or compliance with financial regulations.
  NickPay builds upon the Nicknames for Group Signatures (NGS) scheme, a new
signing system based on dynamic ``nicknames'' for signers that extends the
schemes of group signatures and signatures with flexible public keys.
  NGS enables identified group members to expose their flexible public keys,
thus allowing direct and natural applications such as auditable private payment
systems, NickPay being a blockchain-based prototype of these.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.19819v1">Domain-incremental White Blood Cell Classification with Privacy-aware
  Continual Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-03-25T16:30:58Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Pratibha Kumari, Afshin Bozorgpour, Daniel Reisenbüchler, Edgar Jost, Martina Crysandt, Christian Matek, Dorit Merhof</p>
    <p><b>Summary:</b> White blood cell (WBC) classification plays a vital role in hematology for
diagnosing various medical conditions. However, it faces significant challenges
due to domain shifts caused by variations in sample sources (e.g., blood or
bone marrow) and differing imaging conditions across hospitals. Traditional
deep learning models often suffer from catastrophic forgetting in such dynamic
environments, while foundation models, though generally robust, experience
performance degradation when the distribution of inference data differs from
that of the training data. To address these challenges, we propose a generative
replay-based Continual Learning (CL) strategy designed to prevent forgetting in
foundation models for WBC classification. Our method employs lightweight
generators to mimic past data with a synthetic latent representation to enable
privacy-preserving replay. To showcase the effectiveness, we carry out
extensive experiments with a total of four datasets with different task
ordering and four backbone models including ResNet50, RetCCL, CTransPath, and
UNI. Experimental results demonstrate that conventional fine-tuning methods
degrade performance on previously learned tasks and struggle with domain
shifts. In contrast, our continual learning strategy effectively mitigates
catastrophic forgetting, preserving model performance across varying domains.
This work presents a practical solution for maintaining reliable WBC
classification in real-world clinical settings, where data distributions
frequently evolve.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.19453v1">Average consensus with resilience and privacy guarantees without losing
  accuracy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Optimization and Control-F9C80E">
  <p><b>Published on:</b> 2025-03-25T08:41:12Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Guilherme Ramos, Daniel Silvestre, André M. H. Teixeira, Sérgio Pequito</p>
    <p><b>Summary:</b> This paper addresses the challenge of achieving private and resilient average
consensus among a group of discrete-time networked agents without compromising
accuracy. State-of-the-art solutions to attain privacy and resilient consensus
entail an explicit trade-off between the two with an implicit compromise on
accuracy. In contrast, in the present work, we propose a methodology that
avoids trade-offs between privacy, resilience, and accuracy. We design a
methodology that, under certain conditions, enables non-faulty agents, i.e.,
agents complying with the established protocol, to reach average consensus in
the presence of faulty agents, while keeping the non-faulty agents' initial
states private. For privacy, agents strategically add noise to obscure their
original state, while later withdrawing a function of it to ensure accuracy.
Besides, and unlikely many consensus methods, our approach does not require
each agent to compute the left-eigenvector of the dynamics matrix associated
with the eigenvalue one. Moreover, the proposed framework has a polynomial time
complexity relative to the number of agents and the maximum quantity of faulty
agents. Finally, we illustrate our method with examples covering diverse faulty
agents scenarios.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.18729v1">Two Types of Data Privacy Controls</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2025-03-24T14:37:57Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Eman Alashwali</p>
    <p><b>Summary:</b> Users share a vast amount of data while using web and mobile applications.
Most service providers such as email and social media providers provide users
with privacy controls, which aim to give users the means to control what, how,
when, and with whom, users share data. Nevertheless, it is not uncommon to hear
users say that they feel they have lost control over their data on the web.
  This article aims to shed light on the often overlooked difference between
two main types of privacy from a control perspective: privacy between a user
and other users, and privacy between a user and institutions. We argue why this
difference is important and what we need to do from here.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.18008v1">Personalized Language Models via Privacy-Preserving Evolutionary Model
  Merging</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Neural and Evolutionary Computing-5BC0EB">
  <p><b>Published on:</b> 2025-03-23T09:46:07Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kyuyoung Kim, Jinwoo Shin, Jaehyung Kim</p>
    <p><b>Summary:</b> Personalization in large language models (LLMs) seeks to tailor models to
individual user or user group preferences. Prompt-based methods augment queries
with user preference information, whereas training-based methods directly
encode preferences into model parameters for more effective personalization.
Despite achieving some success in personalizing LLMs, prior methods often fail
to directly optimize task-specific metrics and lack explicit
privacy-preservation mechanisms. To address these limitations, we propose
Privacy-Preserving Model Merging via Evolutionary Algorithms (PriME), a novel
approach to personalization that employs gradient-free methods to directly
optimize task-specific metrics while preserving user privacy. By incorporating
privacy preservation into optimization, PriME produces a personalized module
that effectively captures the target user's preferences while minimizing the
privacy risks for the users sharing their private information. Experiments on
the LaMP benchmark show that PriME outperforms both prompt-based and
training-based methods, achieving up to a 45% performance improvement over the
prior art. Further analysis shows that PriME achieves a significantly better
privacy-utility trade-off, highlighting the potential of evolutionary
approaches for privacy-preserving LLM personalization.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.17844v1">Privacy-Preserving Hamming Distance Computation with Property-Preserving
  Hashing</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computational Complexity-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-22T19:35:59Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Dongfang Zhao</p>
    <p><b>Summary:</b> We study the problem of approximating Hamming distance in sublinear time
under property-preserving hashing (PPH), where only hashed representations of
inputs are available. Building on the threshold evaluation framework of
Fleischhacker, Larsen, and Simkin (EUROCRYPT 2022), we present a sequence of
constructions with progressively improved complexity: a baseline binary search
algorithm, a refined variant with constant repetition per query, and a novel
hash design that enables constant-time approximation without oracle access. Our
results demonstrate that approximate distance recovery is possible under strong
cryptographic guarantees, bridging efficiency and security in similarity
estimation.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.17553v1">Autonomous Radiotherapy Treatment Planning Using DOLA: A
  Privacy-Preserving, LLM-Based Optimization Agent</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Emerging Technologies-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-03-21T22:01:19Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Humza Nusrat, Bing Luo, Ryan Hall, Joshua Kim, Hassan Bagher-Ebadian, Anthony Doemer, Benjamin Movsas, Kundan Thind</p>
    <p><b>Summary:</b> Radiotherapy treatment planning is a complex and time-intensive process,
often impacted by inter-planner variability and subjective decision-making. To
address these challenges, we introduce Dose Optimization Language Agent (DOLA),
an autonomous large language model (LLM)-based agent designed for optimizing
radiotherapy treatment plans while rigorously protecting patient privacy. DOLA
integrates the LLaMa3.1 LLM directly with a commercial treatment planning
system, utilizing chain-of-thought prompting, retrieval-augmented generation
(RAG), and reinforcement learning (RL). Operating entirely within secure local
infrastructure, this agent eliminates external data sharing. We evaluated DOLA
using a retrospective cohort of 18 prostate cancer patients prescribed 60 Gy in
20 fractions, comparing model sizes (8 billion vs. 70 billion parameters) and
optimization strategies (No-RAG, RAG, and RAG+RL) over 10 planning iterations.
The 70B model demonstrated significantly improved performance, achieving
approximately 16.4% higher final scores than the 8B model. The RAG approach
outperformed the No-RAG baseline by 19.8%, and incorporating RL accelerated
convergence, highlighting the synergy of retrieval-based memory and
reinforcement learning. Optimal temperature hyperparameter analysis identified
0.4 as providing the best balance between exploration and exploitation. This
proof of concept study represents the first successful deployment of locally
hosted LLM agents for autonomous optimization of treatment plans within a
commercial radiotherapy planning system. By extending human-machine interaction
through interpretable natural language reasoning, DOLA offers a scalable and
privacy-conscious framework, with significant potential for clinical
implementation and workflow improvement.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.17428v1">Would you mind being watched by machines? Privacy concerns in data
  mining</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2025-03-21T12:01:42Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Vincent C. Müller</p>
    <p><b>Summary:</b> Data mining is not an invasion of privacy because access to data is only by
machines, not by people: this is the argument that is investigated here. The
current importance of this problem is developed in a case study of data mining
in the USA for counterterrorism and other surveillance purposes. After a
clarification of the relevant nature of privacy, it is argued that access by
machines cannot warrant the access to further information, since the analysis
will have to be made either by humans or by machines that understand. It
concludes that the current data mining violates the right to privacy and should
be subject to the standard legal constraints for access to private information
by people.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.17011v1">Privacy Enhanced QKD Networks: Zero Trust Relay Architecture based on
  Homomorphic Encryption</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-21T10:20:06Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Aitor Brazaola-Vicario, Oscar Lage, Julen Bernabé-Rodríguez, Eduardo Jacob, Jasone Astorga</p>
    <p><b>Summary:</b> Quantum key distribution (QKD) enables unconditionally secure symmetric key
exchange between parties. However, terrestrial fibre-optic links face inherent
distance constraints due to quantum signal degradation. Traditional solutions
to overcome these limits rely on trusted relay nodes, which perform
intermediate re-encryption of keys using one-time pad (OTP) encryption. This
approach, however, exposes keys as plaintext at each relay, requiring
significant trust and stringent security controls at every intermediate node.
These "trusted" relays become a security liability if compromised.
  To address this issue, we propose a zero-trust relay design that applies
fully homomorphic encryption (FHE) to perform intermediate OTP re-encryption
without exposing plaintext keys, effectively mitigating the risks associated
with potentially compromised or malicious relay nodes. Additionally, the
architecture enhances crypto-agility by incorporating external quantum random
number generators, thus decoupling key generation from specific QKD hardware
and reducing vulnerabilities tied to embedded key-generation modules.
  The solution is designed with the existing European Telecommunication
Standards Institute (ETSI) QKD standards in mind, enabling straightforward
integration into current infrastructures. Its feasibility has been successfully
demonstrated through a hybrid network setup combining simulated and
commercially available QKD equipment. The proposed zero-trust architecture thus
significantly advances the scalability and practical security of large-scale
QKD networks, greatly reducing reliance on fully trusted infrastructure.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.16640v1">Visualizing Privacy-Relevant Data Flows in Android Applications</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Software Engineering-D91E36">
  <p><b>Published on:</b> 2025-03-20T18:47:02Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mugdha Khedkar, Michael Schlichtig, Santhosh Mohan, Eric Bodden</p>
    <p><b>Summary:</b> Android applications collecting data from users must protect it according to
the current legal frameworks. Such data protection has become even more
important since in 2018 the European Union rolled out the General Data
Protection Regulation (GDPR). Since app developers are not legal experts, they
find it difficult to integrate privacy-aware practices into source code
development. Despite these legal obligations, developers have limited tool
support to reason about data protection throughout their app development
process.
  This paper explores the use of static program slicing and software
visualization to analyze privacy-relevant data flows in Android apps. We
introduce SliceViz, a web tool that analyzes an Android app by slicing all
privacy-relevant data sources detected in the source code on the back-end. It
then helps developers by visualizing these privacy-relevant program slices.
  We conducted a user study with 12 participants demonstrating that SliceViz
effectively aids developers in identifying privacy-relevant properties in
Android apps.
  Our findings indicate that program slicing can be employed to identify and
reason about privacy-relevant data flows in Android applications. With further
usability improvements, developers can be better equipped to handle
privacy-sensitive information.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.16251v1">RESFL: An Uncertainty-Aware Framework for Responsible Federated Learning
  by Balancing Privacy, Fairness and Utility in Autonomous Vehicles</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Emerging Technologies-F9C80E">
  <p><b>Published on:</b> 2025-03-20T15:46:03Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Dawood Wasif, Terrence J. Moore, Jin-Hee Cho</p>
    <p><b>Summary:</b> Autonomous vehicles (AVs) increasingly rely on Federated Learning (FL) to
enhance perception models while preserving privacy. However, existing FL
frameworks struggle to balance privacy, fairness, and robustness, leading to
performance disparities across demographic groups. Privacy-preserving
techniques like differential privacy mitigate data leakage risks but worsen
fairness by restricting access to sensitive attributes needed for bias
correction. This work explores the trade-off between privacy and fairness in
FL-based object detection for AVs and introduces RESFL, an integrated solution
optimizing both. RESFL incorporates adversarial privacy disentanglement and
uncertainty-guided fairness-aware aggregation. The adversarial component uses a
gradient reversal layer to remove sensitive attributes, reducing privacy risks
while maintaining fairness. The uncertainty-aware aggregation employs an
evidential neural network to weight client updates adaptively, prioritizing
contributions with lower fairness disparities and higher confidence. This
ensures robust and equitable FL model updates. We evaluate RESFL on the FACET
dataset and CARLA simulator, assessing accuracy, fairness, privacy resilience,
and robustness under varying conditions. RESFL improves detection accuracy,
reduces fairness disparities, and lowers privacy attack success rates while
demonstrating superior robustness to adversarial conditions compared to other
approaches.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.16233v1">Empirical Analysis of Privacy-Fairness-Accuracy Trade-offs in Federated
  Learning: A Step Towards Responsible AI</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Emerging Technologies-F9C80E">
  <p><b>Published on:</b> 2025-03-20T15:31:01Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Dawood Wasif, Dian Chen, Sindhuja Madabushi, Nithin Alluru, Terrence J. Moore, Jin-Hee Cho</p>
    <p><b>Summary:</b> Federated Learning (FL) enables collaborative machine learning while
preserving data privacy but struggles to balance privacy preservation (PP) and
fairness. Techniques like Differential Privacy (DP), Homomorphic Encryption
(HE), and Secure Multi-Party Computation (SMC) protect sensitive data but
introduce trade-offs. DP enhances privacy but can disproportionately impact
underrepresented groups, while HE and SMC mitigate fairness concerns at the
cost of computational overhead. This work explores the privacy-fairness
trade-offs in FL under IID (Independent and Identically Distributed) and
non-IID data distributions, benchmarking q-FedAvg, q-MAML, and Ditto on diverse
datasets. Our findings highlight context-dependent trade-offs and offer
guidelines for designing FL systems that uphold responsible AI principles,
ensuring fairness, privacy, and equitable real-world applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.15972v1">TVineSynth: A Truncated C-Vine Copula Generator of Synthetic Tabular
  Data to Balance Privacy and Utility</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> 
  <p><b>Published on:</b> 2025-03-20T09:16:02Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Elisabeth Griesbauer, Claudia Czado, Arnoldo Frigessi, Ingrid Hobæk Haff</p>
    <p><b>Summary:</b> We propose TVineSynth, a vine copula based synthetic tabular data generator,
which is designed to balance privacy and utility, using the vine tree structure
and its truncation to do the trade-off. Contrary to synthetic data generators
that achieve DP by globally adding noise, TVineSynth performs a controlled
approximation of the estimated data generating distribution, so that it does
not suffer from poor utility of the resulting synthetic data for downstream
prediction tasks. TVineSynth introduces a targeted bias into the vine copula
model that, combined with the specific tree structure of the vine, causes the
model to zero out privacy-leaking dependencies while relying on those that are
beneficial for utility. Privacy is here measured with membership (MIA) and
attribute inference attacks (AIA). Further, we theoretically justify how the
construction of TVineSynth ensures AIA privacy under a natural privacy measure
for continuous sensitive attributes. When compared to competitor models, with
and without DP, on simulated and on real-world data, TVineSynth achieves a
superior privacy-utility balance.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.15966v1">Privacy-Preserving Utilization of Distribution System Flexibility for
  Enhanced TSO-DSO Interoperability: A Novel Machine Learning-Based Optimal
  Power Flow Approach</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36">
  <p><b>Published on:</b> 2025-03-20T09:08:54Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Burak Dindar, Can Berk Saner, Hüseyin K. Çakmak, Veit Hagenmeyer</p>
    <p><b>Summary:</b> Due to the transformation of the power system, the effective use of
flexibility from the distribution system (DS) is becoming crucial for efficient
network management. Leveraging this flexibility requires interoperability among
stakeholders, including Transmission System Operators (TSOs) and Distribution
System Operators (DSOs). However, data privacy concerns among stakeholders
present significant challenges for utilizing this flexibility effectively. To
address these challenges, we propose a machine learning (ML)-based method in
which the technical constraints of the DSs are represented by ML models trained
exclusively on non-sensitive data. Using these models, the TSO can solve the
optimal power flow (OPF) problem and directly determine the dispatch of
flexibility-providing units (FPUs), in our case, distributed generators (DGs),
in a single round of communication. To achieve this, we introduce a novel
neural network (NN) architecture specifically designed to efficiently represent
the feasible region of the DSs, ensuring computational effectiveness.
Furthermore, we incorporate various PQ charts rather than idealized ones,
demonstrating that the proposed method is adaptable to a wide range of FPU
characteristics. To assess the effectiveness of the proposed method, we
benchmark it against the standard AC-OPF on multiple DSs with meshed
connections and multiple points of common coupling (PCCs) with varying voltage
magnitudes. The numerical results indicate that the proposed method achieves
performant results while prioritizing data privacy. Additionally, since this
method directly determines the dispatch of FPUs, it eliminates the need for an
additional disaggregation step. By representing the DSs technical constraints
through ML models trained exclusively on non-sensitive data, the transfer of
sensitive information between stakeholders is prevented.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.15870v1">FedSAF: A Federated Learning Framework for Enhanced Gastric Cancer
  Detection and Privacy Preservation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-03-20T05:48:48Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yuxin Miao, Xinyuan Yang, Hongda Fan, Yichun Li, Yishu Hong, Xiechen Guo, Ali Braytee, Weidong Huang, Ali Anaissi</p>
    <p><b>Summary:</b> Gastric cancer is one of the most commonly diagnosed cancers and has a high
mortality rate. Due to limited medical resources, developing machine learning
models for gastric cancer recognition provides an efficient solution for
medical institutions. However, such models typically require large sample sizes
for training and testing, which can challenge patient privacy. Federated
learning offers an effective alternative by enabling model training across
multiple institutions without sharing sensitive patient data. This paper
addresses the limited sample size of publicly available gastric cancer data
with a modified data processing method. This paper introduces FedSAF, a novel
federated learning algorithm designed to improve the performance of existing
methods, particularly in non-independent and identically distributed (non-IID)
data scenarios. FedSAF incorporates attention-based message passing and the
Fisher Information Matrix to enhance model accuracy, while a model splitting
function reduces computation and transmission costs. Hyperparameter tuning and
ablation studies demonstrate the effectiveness of this new algorithm, showing
improvements in test accuracy on gastric cancer datasets, with FedSAF
outperforming existing federated learning methods like FedAMP, FedAvg, and
FedProx. The framework's robustness and generalization ability were further
validated across additional datasets (SEED, BOT, FashionMNIST, and CIFAR-10),
achieving high performance in diverse environments.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.15818v2">Computation-Efficient and Recognition-Friendly 3D Point Cloud Privacy
  Protection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-03-20T03:09:44Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Haotian Ma, Lin Gu, Siyi Wu, Yingying Zhu</p>
    <p><b>Summary:</b> 3D point cloud has been widely used in applications such as self-driving
cars, robotics, CAD models, etc. To the best of our knowledge, these
applications raised the issue of privacy leakage in 3D point clouds, which has
not been studied well. Different from the 2D image privacy, which is related to
texture and 2D geometric structure, the 3D point cloud is texture-less and only
relevant to 3D geometric structure. In this work, we defined the 3D point cloud
privacy problem and proposed an efficient privacy-preserving framework named
PointFlowGMM that can support downstream classification and segmentation tasks
without seeing the original data. Using a flow-based generative model, the
point cloud is projected into a latent Gaussian mixture distributed subspace.
We further designed a novel angular similarity loss to obfuscate the original
geometric structure and reduce the model size from 767MB to 120MB without a
decrease in recognition performance. The projected point cloud in the latent
space is orthogonally rotated randomly to further protect the original
geometric structure, the class-to-class relationship is preserved after
rotation, thus, the protected point cloud can support the recognition task. We
evaluated our model on multiple datasets and achieved comparable recognition
results on encrypted point clouds compared to the original point clouds.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.15287v1">Distributed Generalized Linear Models: A Privacy-Preserving Approach</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">  
  <p><b>Published on:</b> 2025-03-19T15:07:41Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Daniel Tinoco, Raquel Menezes, Carlos Baquero</p>
    <p><b>Summary:</b> This paper presents a novel approach to classical linear regression, enabling
model computation from data streams or in a distributed setting while
preserving data privacy in federated environments. We extend this framework to
generalized linear models (GLMs), ensuring scalability and adaptability to
diverse data distributions while maintaining privacy-preserving properties. To
assess the effectiveness of our approach, we conduct numerical studies on both
simulated and real datasets, comparing our method with conventional maximum
likelihood estimation for GLMs using iteratively reweighted least squares. Our
results demonstrate the advantages of the proposed method in distributed and
federated settings.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.15238v1">Your Signal, Their Data: An Empirical Privacy Analysis of
  Wireless-scanning SDKs in Android</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-19T14:15:02Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Aniketh Girish, Joel Reardon, Juan Tapiador, Srdjan Matic, Narseo Vallina-Rodriguez</p>
    <p><b>Summary:</b> Mobile apps frequently use Bluetooth Low Energy (BLE) and WiFi scanning
permissions to discover nearby devices like peripherals and connect to WiFi
Access Points (APs). However, wireless interfaces also serve as a covert proxy
for geolocation data, enabling continuous user tracking and profiling. This
includes technologies like BLE beacons, which are BLE devices broadcasting
unique identifiers to determine devices' indoor physical locations; such
beacons are easily found in shopping centres. Despite the widespread use of
wireless scanning APIs and their potential for privacy abuse, the interplay
between commercial mobile SDKs with wireless sensing and beaconing technologies
remains largely unexplored. In this work, we conduct the first systematic
analysis of 52 wireless-scanning SDKs, revealing their data collection
practices and privacy risks. We develop a comprehensive analysis pipeline that
enables us to detect beacon scanning capabilities, inject wireless events to
trigger app behaviors, and monitor runtime execution on instrumented devices.
Our findings show that 86% of apps integrating these SDKs collect at least one
sensitive data type, including device and user identifiers such as AAID, email,
along with GPS coordinates, WiFi and Bluetooth scan results. We uncover
widespread SDK-to-SDK data sharing and evidence of ID bridging, where
persistent and resettable identifiers are shared and synchronized within SDKs
embedded in applications to potentially construct detailed mobility profiles,
compromising user anonymity and enabling long-term tracking. We provide
evidence of key actors engaging in these practices and conclude by proposing
mitigation strategies such as stronger SDK sandboxing, stricter enforcement of
platform policies, and improved transparency mechanisms to limit unauthorized
tracking.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.15015v1">OFL: Opportunistic Federated Learning for Resource-Heterogeneous and
  Privacy-Aware Devices</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-19T09:12:47Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yunlong Mao, Mingyang Niu, Ziqin Dang, Chengxi Li, Hanning Xia, Yuejuan Zhu, Haoyu Bian, Yuan Zhang, Jingyu Hua, Sheng Zhong</p>
    <p><b>Summary:</b> Efficient and secure federated learning (FL) is a critical challenge for
resource-limited devices, especially mobile devices. Existing secure FL
solutions commonly incur significant overhead, leading to a contradiction
between efficiency and security. As a result, these two concerns are typically
addressed separately. This paper proposes Opportunistic Federated Learning
(OFL), a novel FL framework designed explicitly for resource-heterogenous and
privacy-aware FL devices, solving efficiency and security problems jointly. OFL
optimizes resource utilization and adaptability across diverse devices by
adopting a novel hierarchical and asynchronous aggregation strategy. OFL
provides strong security by introducing a differentially private and
opportunistic model updating mechanism for intra-cluster model aggregation and
an advanced threshold homomorphic encryption scheme for inter-cluster
aggregation. Moreover, OFL secures global model aggregation by implementing
poisoning attack detection using frequency analysis while keeping models
encrypted. We have implemented OFL in a real-world testbed and evaluated OFL
comprehensively. The evaluation results demonstrate that OFL achieves
satisfying model performance and improves efficiency and security,
outperforming existing solutions.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.14877v1">Synthesizing Grid Data with Cyber Resilience and Privacy Guarantees</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36">
  <p><b>Published on:</b> 2025-03-19T04:11:33Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shengyang Wu, Vladimir Dvorkin</p>
    <p><b>Summary:</b> Differential privacy (DP) provides a principled approach to synthesizing data
(e.g., loads) from real-world power systems while limiting the exposure of
sensitive information. However, adversaries may exploit synthetic data to
calibrate cyberattacks on the source grids. To control these risks, we propose
new DP algorithms for synthesizing data that provide the source grids with both
cyber resilience and privacy guarantees. The algorithms incorporate both normal
operation and attack optimization models to balance the fidelity of synthesized
data and cyber resilience. The resulting post-processing optimization is
reformulated as a robust optimization problem, which is compatible with the
exponential mechanism of DP to moderate its computational burden.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.15550v2">Zero-Knowledge Federated Learning: A New Trustworthy and
  Privacy-Preserving Distributed Learning Paradigm</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-03-18T06:21:08Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yuxin Jin, Taotao Wang, Qing Yang, Long Shi, Shengli Zhang</p>
    <p><b>Summary:</b> Federated Learning (FL) has emerged as a promising paradigm in distributed
machine learning, enabling collaborative model training while preserving data
privacy. However, despite its many advantages, FL still contends with
significant challenges -- most notably regarding security and trust.
Zero-Knowledge Proofs (ZKPs) offer a potential solution by establishing trust
and enhancing system integrity throughout the FL process. Although several
studies have explored ZKP-based FL (ZK-FL), a systematic framework and
comprehensive analysis are still lacking. This article makes two key
contributions. First, we propose a structured ZK-FL framework that categorizes
and analyzes the technical roles of ZKPs across various FL stages and tasks.
Second, we introduce a novel algorithm, Verifiable Client Selection FL
(Veri-CS-FL), which employs ZKPs to refine the client selection process. In
Veri-CS-FL, participating clients generate verifiable proofs for the
performance metrics of their local models and submit these concise proofs to
the server for efficient verification. The server then selects clients with
high-quality local models for uploading, subsequently aggregating the
contributions from these selected clients. By integrating ZKPs, Veri-CS-FL not
only ensures the accuracy of performance metrics but also fortifies trust among
participants while enhancing the overall efficiency and security of FL systems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.13872v1">Empirical Calibration and Metric Differential Privacy in Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-18T03:52:12Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Pedro Faustini, Natasha Fernandes, Annabelle McIver, Mark Dras</p>
    <p><b>Summary:</b> NLP models trained with differential privacy (DP) usually adopt the DP-SGD
framework, and privacy guarantees are often reported in terms of the privacy
budget $\epsilon$. However, $\epsilon$ does not have any intrinsic meaning, and
it is generally not possible to compare across variants of the framework. Work
in image processing has therefore explored how to empirically calibrate noise
across frameworks using Membership Inference Attacks (MIAs). However, this kind
of calibration has not been established for NLP. In this paper, we show that
MIAs offer little help in calibrating privacy, whereas reconstruction attacks
are more useful. As a use case, we define a novel kind of directional privacy
based on the von Mises-Fisher (VMF) distribution, a metric DP mechanism that
perturbs angular distance rather than adding (isotropic) Gaussian noise, and
apply this to NLP architectures. We show that, even though formal guarantees
are incomparable, empirical privacy calibration reveals that each mechanism has
different areas of strength with respect to utility-privacy trade-offs.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.13816v2">MOSAIC: Generating Consistent, Privacy-Preserving Scenes from Multiple
  Depth Views in Multi-Room Environments</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-03-18T01:50:57Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhixuan Liu, Haokun Zhu, Rui Chen, Jonathan Francis, Soonmin Hwang, Ji Zhang, Jean Oh</p>
    <p><b>Summary:</b> We introduce a novel diffusion-based approach for generating
privacy-preserving digital twins of multi-room indoor environments from depth
images only. Central to our approach is a novel Multi-view Overlapped Scene
Alignment with Implicit Consistency (MOSAIC) model that explicitly considers
cross-view dependencies within the same scene in the probabilistic sense.
MOSAIC operates through a novel inference-time optimization that avoids error
accumulation common in sequential or single-room constraint in panorama-based
approaches. MOSAIC scales to complex scenes with zero extra training and
provably reduces the variance during denoising processes when more overlapping
views are added, leading to improved generation quality. Experiments show that
MOSAIC outperforms state-of-the-art baselines on image fidelity metrics in
reconstructing complex multi-room environments. Project page is available at:
https://mosaic-cmubig.github.io</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.14539v1">Ethical Implications of AI in Data Collection: Balancing Innovation with
  Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-03-17T14:15:59Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shahmar Mirishli</p>
    <p><b>Summary:</b> This article examines the ethical and legal implications of artificial
intelligence (AI) driven data collection, focusing on developments from 2023 to
2024. It analyzes recent advancements in AI technologies and their impact on
data collection practices across various sectors. The study compares regulatory
approaches in the European Union, the United States, and China, highlighting
the challenges in creating a globally harmonized framework for AI governance.
Key ethical issues, including informed consent, algorithmic bias, and privacy
protection, are critically assessed in the context of increasingly
sophisticated AI systems. The research explores case studies in healthcare,
finance, and smart cities to illustrate the practical challenges of AI
implementation. It evaluates the effectiveness of current legal frameworks and
proposes solutions encompassing legal and policy recommendations, technical
safeguards, and ethical frameworks. The article emphasizes the need for
adaptive governance and international cooperation to address the global nature
of AI development while balancing innovation with the protection of individual
rights and societal values.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.13173v1">PAUSE: Low-Latency and Privacy-Aware Active User Selection for Federated
  Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> 
  <p><b>Published on:</b> 2025-03-17T13:50:35Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ori Peleg, Natalie Lang, Stefano Rini, Nir Shlezinger, Kobi Cohen</p>
    <p><b>Summary:</b> Federated learning (FL) enables multiple edge devices to collaboratively
train a machine learning model without the need to share potentially private
data. Federated learning proceeds through iterative exchanges of model updates,
which pose two key challenges: First, the accumulation of privacy leakage over
time, and second, communication latency. These two limitations are typically
addressed separately: The former via perturbed updates to enhance privacy and
the latter using user selection to mitigate latency - both at the expense of
accuracy. In this work, we propose a method that jointly addresses the
accumulation of privacy leakage and communication latency via active user
selection, aiming to improve the trade-off among privacy, latency, and model
performance. To achieve this, we construct a reward function that accounts for
these three objectives. Building on this reward, we propose a multi-armed
bandit (MAB)-based algorithm, termed Privacy-aware Active User SElection
(PAUSE) which dynamically selects a subset of users each round while ensuring
bounded overall privacy leakage. We establish a theoretical analysis,
systematically showing that the reward growth rate of PAUSE follows that of the
best-known rate in MAB literature. To address the complexity overhead of active
user selection, we propose a simulated annealing-based relaxation of PAUSE and
analyze its ability to approximate the reward-maximizing policy under reduced
complexity. We numerically validate the privacy leakage, associated improved
latency, and accuracy gains of our methods for the federated training in
various scenarios.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.12958v1">FedSDP: Explainable Differential Privacy in Federated Learning via
  Shapley Values</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-17T09:14:19Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yunbo Li, Jiaping Gui, Yue Wu</p>
    <p><b>Summary:</b> Federated learning (FL) enables participants to store data locally while
collaborating in training, yet it remains vulnerable to privacy attacks, such
as data reconstruction. Existing differential privacy (DP) technologies inject
noise dynamically into the training process to mitigate the impact of excessive
noise. However, this dynamic scheduling is often grounded in factors indirectly
related to privacy, making it difficult to clearly explain the intricate
relationship between dynamic noise adjustments and privacy requirements. To
address this issue, we propose FedSDP, a novel and explainable DP-based privacy
protection mechanism that guides noise injection based on privacy contribution.
Specifically, FedSDP leverages Shapley values to assess the contribution of
private attributes to local model training and dynamically adjusts the amount
of noise injected accordingly. By providing theoretical insights into the
injection of varying scales of noise into local training, FedSDP enhances
interpretability. Extensive experiments demonstrate that FedSDP can achieve a
superior balance between privacy preservation and model performance, surpassing
state-of-the-art (SOTA) solutions.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.15548v1">Privacy-Aware RAG: Secure and Isolated Knowledge Retrieval</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-03-17T07:45:05Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Pengcheng Zhou, Yinglun Feng, Zhongliang Yang</p>
    <p><b>Summary:</b> The widespread adoption of Retrieval-Augmented Generation (RAG) systems in
real-world applications has heightened concerns about the confidentiality and
integrity of their proprietary knowledge bases. These knowledge bases, which
play a critical role in enhancing the generative capabilities of Large Language
Models (LLMs), are increasingly vulnerable to breaches that could compromise
sensitive information. To address these challenges, this paper proposes an
advanced encryption methodology designed to protect RAG systems from
unauthorized access and data leakage. Our approach encrypts both textual
content and its corresponding embeddings prior to storage, ensuring that all
data remains securely encrypted. This mechanism restricts access to authorized
entities with the appropriate decryption keys, thereby significantly reducing
the risk of unintended data exposure. Furthermore, we demonstrate that our
encryption strategy preserves the performance and functionality of RAG
pipelines, ensuring compatibility across diverse domains and applications. To
validate the robustness of our method, we provide comprehensive security proofs
that highlight its resilience against potential threats and vulnerabilities.
These proofs also reveal limitations in existing approaches, which often lack
robustness, adaptability, or reliance on open-source models. Our findings
suggest that integrating advanced encryption techniques into the design and
deployment of RAG systems can effectively enhance privacy safeguards. This
research contributes to the ongoing discourse on improving security measures
for AI-driven services and advocates for stricter data protection standards
within RAG architectures.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.12786v1">Privacy-Preserving Biometric Verification with Handwritten Random Digit
  String</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-03-17T03:47:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Peirong Zhang, Yuliang Liu, Songxuan Lai, Hongliang Li, Lianwen Jin</p>
    <p><b>Summary:</b> Handwriting verification has stood as a steadfast identity authentication
method for decades. However, this technique risks potential privacy breaches
due to the inclusion of personal information in handwritten biometrics such as
signatures. To address this concern, we propose using the Random Digit String
(RDS) for privacy-preserving handwriting verification. This approach allows
users to authenticate themselves by writing an arbitrary digit sequence,
effectively ensuring privacy protection. To evaluate the effectiveness of RDS,
we construct a new HRDS4BV dataset composed of online naturally handwritten
RDS. Unlike conventional handwriting, RDS encompasses unconstrained and
variable content, posing significant challenges for modeling consistent
personal writing style. To surmount this, we propose the Pattern Attentive
VErification Network (PAVENet), along with a Discriminative Pattern Mining
(DPM) module. DPM adaptively enhances the recognition of consistent and
discriminative writing patterns, thus refining handwriting style
representation. Through comprehensive evaluations, we scrutinize the
applicability of online RDS verification and showcase a pronounced
outperformance of our model over existing methods. Furthermore, we discover a
noteworthy forgery phenomenon that deviates from prior findings and discuss its
positive impact in countering malicious impostor attacks. Substantially, our
work underscores the feasibility of privacy-preserving biometric verification
and propels the prospects of its broader acceptance and application.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.13550v1">Towards Privacy-Preserving Data-Driven Education: The Potential of
  Federated Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-16T14:37:32Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mohammad Khalil, Ronas Shakya, Qinyi Liu</p>
    <p><b>Summary:</b> The increasing adoption of data-driven applications in education such as in
learning analytics and AI in education has raised significant privacy and data
protection concerns. While these challenges have been widely discussed in
previous works, there are still limited practical solutions. Federated learning
has recently been discoursed as a promising privacy-preserving technique, yet
its application in education remains scarce. This paper presents an
experimental evaluation of federated learning for educational data prediction,
comparing its performance to traditional non-federated approaches. Our findings
indicate that federated learning achieves comparable predictive accuracy.
Furthermore, under adversarial attacks, federated learning demonstrates greater
resilience compared to non-federated settings. We summarise that our results
reinforce the value of federated learning as a potential approach for balancing
predictive performance and privacy in educational contexts.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.12464v1">Learning Privacy from Visual Entities</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-03-16T11:39:08Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Alessio Xompero, Andrea Cavallaro</p>
    <p><b>Summary:</b> Subjective interpretation and content diversity make predicting whether an
image is private or public a challenging task. Graph neural networks combined
with convolutional neural networks (CNNs), which consist of 14,000 to 500
millions parameters, generate features for visual entities (e.g., scene and
object types) and identify the entities that contribute to the decision. In
this paper, we show that using a simpler combination of transfer learning and a
CNN to relate privacy with scene types optimises only 732 parameters while
achieving comparable performance to that of graph-based methods. On the
contrary, end-to-end training of graph-based methods can mask the contribution
of individual components to the classification performance. Furthermore, we
show that a high-dimensional feature vector, extracted with CNNs for each
visual entity, is unnecessary and complexifies the model. The graph component
has also negligible impact on performance, which is driven by fine-tuning the
CNN to optimise image features for privacy nodes.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.16516v1">Using LLMs for Automated Privacy Policy Analysis: Prompt Engineering,
  Fine-Tuning and Explainability</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-03-16T10:50:31Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yuxin Chen, Peng Tang, Weidong Qiu, Shujun Li</p>
    <p><b>Summary:</b> Privacy policies are widely used by digital services and often required for
legal purposes. Many machine learning based classifiers have been developed to
automate detection of different concepts in a given privacy policy, which can
help facilitate other automated tasks such as producing a more reader-friendly
summary and detecting legal compliance issues. Despite the successful
applications of large language models (LLMs) to many NLP tasks in various
domains, there is very little work studying the use of LLMs for automated
privacy policy analysis, therefore, if and how LLMs can help automate privacy
policy analysis remains under-explored. To fill this research gap, we conducted
a comprehensive evaluation of LLM-based privacy policy concept classifiers,
employing both prompt engineering and LoRA (low-rank adaptation) fine-tuning,
on four state-of-the-art (SOTA) privacy policy corpora and taxonomies. Our
experimental results demonstrated that combining prompt engineering and
fine-tuning can make LLM-based classifiers outperform other SOTA methods,
\emph{significantly} and \emph{consistently} across privacy policy
corpora/taxonomies and concepts. Furthermore, we evaluated the explainability
of the LLM-based classifiers using three metrics: completeness, logicality, and
comprehensibility. For all three metrics, a score exceeding 91.1\% was observed
in our evaluation, indicating that LLMs are not only useful to improve the
classification performance, but also to enhance the explainability of detection
results.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.12347v1">Synthesizing Privacy-Preserving Text Data via Finetuning without
  Finetuning Billion-Scale LLMs</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-03-16T04:00:32Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Bowen Tan, Zheng Xu, Eric Xing, Zhiting Hu, Shanshan Wu</p>
    <p><b>Summary:</b> Synthetic data offers a promising path to train models while preserving data
privacy. Differentially private (DP) finetuning of large language models (LLMs)
as data generator is effective, but is impractical when computation resources
are limited. Meanwhile, prompt-based methods such as private evolution, depend
heavily on the manual prompts, and ineffectively use private information in
their iterative data selection process. To overcome these limitations, we
propose CTCL (Data Synthesis with ConTrollability and CLustering), a novel
framework for generating privacy-preserving synthetic data without extensive
prompt engineering or billion-scale LLM finetuning. CTCL pretrains a
lightweight 140M conditional generator and a clustering-based topic model on
large-scale public data. To further adapt to the private domain, the generator
is DP finetuned on private data for fine-grained textual information, while the
topic model extracts a DP histogram representing distributional information.
The DP generator then samples according to the DP histogram to synthesize a
desired number of data examples. Evaluation across five diverse domains
demonstrates the effectiveness of our framework, particularly in the strong
privacy regime. Systematic ablation validates the design of each framework
component and highlights the scalability of our approach.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.12314v1">Empirical Privacy Variance</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-16T01:43:49Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yuzheng Hu, Fan Wu, Ruicheng Xian, Yuhang Liu, Lydia Zakynthinou, Pritish Kamath, Chiyuan Zhang, David Forsyth</p>
    <p><b>Summary:</b> We propose the notion of empirical privacy variance and study it in the
context of differentially private fine-tuning of language models. Specifically,
we show that models calibrated to the same $(\varepsilon, \delta)$-DP guarantee
using DP-SGD with different hyperparameter configurations can exhibit
significant variations in empirical privacy, which we quantify through the lens
of memorization. We investigate the generality of this phenomenon across
multiple dimensions and discuss why it is surprising and relevant. Through
regression analysis, we examine how individual and composite hyperparameters
influence empirical privacy. The results reveal a no-free-lunch trade-off:
existing practices of hyperparameter tuning in DP-SGD, which focus on
optimizing utility under a fixed privacy budget, often come at the expense of
empirical privacy. To address this, we propose refined heuristics for
hyperparameter selection that explicitly account for empirical privacy, showing
that they are both precise and practically useful. Finally, we take preliminary
steps to understand empirical privacy variance. We propose two hypotheses,
identify limitations in existing techniques like privacy auditing, and outline
open questions for future research.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.12232v1">From Laboratory to Real World: A New Benchmark Towards Privacy-Preserved
  Visible-Infrared Person Re-Identification</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-03-15T18:56:29Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yan Jiang, Hao Yu, Xu Cheng, Haoyu Chen, Zhaodong Sun, Guoying Zhao</p>
    <p><b>Summary:</b> Aiming to match pedestrian images captured under varying lighting conditions,
visible-infrared person re-identification (VI-ReID) has drawn intensive
research attention and achieved promising results. However, in real-world
surveillance contexts, data is distributed across multiple devices/entities,
raising privacy and ownership concerns that make existing centralized training
impractical for VI-ReID. To tackle these challenges, we propose L2RW, a
benchmark that brings VI-ReID closer to real-world applications. The rationale
of L2RW is that integrating decentralized training into VI-ReID can address
privacy concerns in scenarios with limited data-sharing regulation.
Specifically, we design protocols and corresponding algorithms for different
privacy sensitivity levels. In our new benchmark, we ensure the model training
is done in the conditions that: 1) data from each camera remains completely
isolated, or 2) different data entities (e.g., data controllers of a certain
region) can selectively share the data. In this way, we simulate scenarios with
strict privacy constraints which is closer to real-world conditions. Intensive
experiments with various server-side federated algorithms are conducted,
showing the feasibility of decentralized VI-ReID training. Notably, when
evaluated in unseen domains (i.e., new data entities), our L2RW, trained with
isolated data (privacy-preserved), achieves performance comparable to SOTAs
trained with shared data (privacy-unrestricted). We hope this work offers a
novel research entry for deploying VI-ReID that fits real-world scenarios and
can benefit the community.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.12226v1">Research on Large Language Model Cross-Cloud Privacy Protection and
  Collaborative Training based on Federated Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-03-15T18:44:50Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ze Yang, Yihong Jin, Yihan Zhang, Juntian Liu, Xinhe Xu</p>
    <p><b>Summary:</b> The fast development of large language models (LLMs) and popularization of
cloud computing have led to increasing concerns on privacy safeguarding and
data security of cross-cloud model deployment and training as the key
challenges. We present a new framework for addressing these issues along with
enabling privacy preserving collaboration on training between distributed
clouds based on federated learning. Our mechanism encompasses cutting-edge
cryptographic primitives, dynamic model aggregation techniques, and cross-cloud
data harmonization solutions to enhance security, efficiency, and scalability
to the traditional federated learning paradigm. Furthermore, we proposed a
hybrid aggregation scheme to mitigate the threat of Data Leakage and to
optimize the aggregation of model updates, thus achieving substantial
enhancement on the model effectiveness and stability. Experimental results
demonstrate that the training efficiency, privacy protection, and model
accuracy of the proposed model compare favorably to those of the traditional
federated learning method.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.12225v1">Interpretation Gaps in LLM-Assisted Comprehension of Privacy Documents</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Retrieval-5BC0EB">
  <p><b>Published on:</b> 2025-03-15T18:43:13Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Rinku Dewri</p>
    <p><b>Summary:</b> This article explores the gaps that can manifest when using a large language
model (LLM) to obtain simplified interpretations of data practices from a
complex privacy policy. We exemplify these gaps to showcase issues in accuracy,
completeness, clarity and representation, while advocating for continued
research to realize an LLM's true potential in revolutionizing privacy
management through personal assistants and automated compliance checking.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.12220v2">PA-CFL: Privacy-Adaptive Clustered Federated Learning for
  Transformer-Based Sales Forecasting on Heterogeneous Retail Data</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-15T18:07:54Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yunbo Long, Liming Xu, Ge Zheng, Alexandra Brintrup</p>
    <p><b>Summary:</b> Federated learning (FL) enables retailers to share model parameters for
demand forecasting while maintaining privacy. However, heterogeneous data
across diverse regions, driven by factors such as varying consumer behavior,
poses challenges to the effectiveness of federated learning. To tackle this
challenge, we propose Privacy-Adaptive Clustered Federated Learning (PA-CFL)
tailored for demand forecasting on heterogeneous retail data. By leveraging
differential privacy and feature importance distribution, PA-CFL groups
retailers into distinct ``bubbles'', each forming its own federated learning
system to effectively isolate data heterogeneity. Within each bubble,
Transformer models are designed to predict local sales for each client. Our
experiments demonstrate that PA-CFL significantly surpasses FedAvg and
outperforms local learning in demand forecasting performance across all
participating clients. Compared to local learning, PA-CFL achieves a 5.4%
improvement in R^2, a 69% reduction in RMSE, and a 45% decrease in MAE. Our
approach enables effective FL through adaptive adjustments to diverse noise
levels and the range of clients participating in each bubble. By grouping
participants and proactively filtering out high-risk clients, PA-CFL mitigates
potential threats to the FL system. The findings demonstrate PA-CFL's ability
to enhance federated learning in time series prediction tasks with
heterogeneous data, achieving a balance between forecasting accuracy and
privacy preservation in retail applications. Additionally, PA-CFL's capability
to detect and neutralize poisoned data from clients enhances the system's
robustness and reliability.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.12156v1">Efficient and Privacy-Preserved Link Prediction via Condensed Graphs</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Social and Information Networks-662E9B">
  <p><b>Published on:</b> 2025-03-15T14:54:04Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yunbo Long, Liming Xu, Alexandra Brintrup</p>
    <p><b>Summary:</b> Link prediction is crucial for uncovering hidden connections within complex
networks, enabling applications such as identifying potential customers and
products. However, this research faces significant challenges, including
concerns about data privacy, as well as high computational and storage costs,
especially when dealing with large-scale networks. Condensed graphs, which are
much smaller than the original graphs while retaining essential information,
has become an effective solution to both maintain data utility and preserve
privacy. Existing methods, however, initialize synthetic graphs through random
node selection without considering node connectivity, and are mainly designed
for node classification tasks. As a result, their potential for
privacy-preserving link prediction remains largely unexplored. We introduce
HyDRO\textsuperscript{+}, a graph condensation method guided by algebraic
Jaccard similarity, which leverages local connectivity information to optimize
condensed graph structures. Extensive experiments on four real-world networks
show that our method outperforms state-of-the-art methods and even the original
networks in balancing link prediction accuracy and privacy preservation.
Moreover, our method achieves nearly 20* faster training and reduces storage
requirements by 452*, as demonstrated on the Computers dataset, compared to
link prediction on the original networks. This work represents the first
attempt to leverage condensed graphs for privacy-preserving link prediction
information sharing in real-world complex networks. It offers a promising
pathway for preserving link prediction information while safeguarding privacy,
advancing the use of graph condensation in large-scale networks with privacy
concerns.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.12045v1">Auditing Differential Privacy in the Black-Box Setting</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-03-15T08:34:40Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kaining Shi, Cong Ma</p>
    <p><b>Summary:</b> This paper introduces a novel theoretical framework for auditing differential
privacy (DP) in a black-box setting. Leveraging the concept of $f$-differential
privacy, we explicitly define type I and type II errors and propose an auditing
mechanism based on conformal inference. Our approach robustly controls the type
I error rate under minimal assumptions. Furthermore, we establish a fundamental
impossibility result, demonstrating the inherent difficulty of simultaneously
controlling both type I and type II errors without additional assumptions.
Nevertheless, under a monotone likelihood ratio (MLR) assumption, our auditing
mechanism effectively controls both errors. We also extend our method to
construct valid confidence bands for the trade-off function in the
finite-sample regime.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.11963v1">Effective and Efficient Cross-City Traffic Knowledge Transfer A
  Privacy-Preserving Perspective</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-15T02:26:24Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhihao Zeng, Ziquan Fang, Yuting Huang, Lu Chen, Yunjun Gao</p>
    <p><b>Summary:</b> Traffic prediction targets forecasting future traffic conditions using
historical traffic data, serving a critical role in urban computing and
transportation management. To mitigate the scarcity of traffic data while
maintaining data privacy, numerous Federated Traffic Knowledge Transfer (FTT)
approaches have been developed, which use transfer learning and federated
learning to transfer traffic knowledge from data-rich cities to data-scarce
cities, enhancing traffic prediction capabilities for the latter. However,
current FTT approaches face challenges such as privacy leakage, cross-city data
distribution discrepancies, low data quality, and inefficient knowledge
transfer, limiting their privacy protection, effectiveness, robustness, and
efficiency in real-world applications.
  To this end, we propose FedTT, an effective, efficient, and privacy-aware
cross-city traffic knowledge transfer framework that transforms the traffic
data domain from the data-rich cities and trains traffic models using the
transformed data for the data-scarce cities. First, to safeguard data privacy,
we propose a traffic secret transmission method that securely transmits and
aggregates traffic domain-transformed data from source cities using a
lightweight secret aggregation approach. Second, to mitigate the impact of
traffic data distribution discrepancies on model performance, we introduce a
traffic domain adapter to uniformly transform traffic data from the source
cities' domains to that of the target city. Third, to improve traffic data
quality, we design a traffic view imputation method to fill in and predict
missing traffic data. Finally, to enhance transfer efficiency, FedTT is
equipped with a federated parallel training method that enables the
simultaneous training of multiple modules. Extensive experiments using 4
real-life datasets demonstrate that FedTT outperforms the 14 state-of-the-art
baselines.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.11950v2">Privacy Ethics Alignment in AI: A Stakeholder-Centric Based Framework
  for Ethical AI</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-03-15T01:42:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ankur Barthwal, Molly Campbell, Ajay Kumar Shrestha</p>
    <p><b>Summary:</b> The increasing integration of Artificial Intelligence (AI) in digital
ecosystems has reshaped privacy dynamics, particularly for young digital
citizens navigating data-driven environments. This study explores evolving
privacy concerns across three key stakeholder groups, digital citizens (ages
16-19), parents/educators, and AI professionals, and assesses differences in
data ownership, trust, transparency, parental mediation, education, and
risk-benefit perceptions. Employing a grounded theory methodology, this
research synthesizes insights from 482 participants through structured surveys,
qualitative interviews, and focus groups. The findings reveal distinct privacy
expectations: Young users emphasize autonomy and digital freedom, while parents
and educators advocate for regulatory oversight and AI literacy programs. AI
professionals, in contrast, prioritize the balance between ethical system
design and technological efficiency. The data further highlights gaps in AI
literacy and transparency, emphasizing the need for comprehensive,
stakeholder-driven privacy frameworks that accommodate diverse user needs.
Using comparative thematic analysis, this study identifies key tensions in
privacy governance and develops the novel Privacy-Ethics Alignment in AI
(PEA-AI) model, which structures privacy decision-making as a dynamic
negotiation between stakeholders. By systematically analyzing themes such as
transparency, user control, risk perception, and parental mediation, this
research provides a scalable, adaptive foundation for AI governance, ensuring
that privacy protections evolve alongside emerging AI technologies and
youth-centric digital interactions.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.11947v1">Ethical AI for Young Digital Citizens: A Call to Action on Privacy
  Governance</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-03-15T01:35:56Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Austin Shouli, Ankur Barthwal, Molly Campbell, Ajay Kumar Shrestha</p>
    <p><b>Summary:</b> The rapid expansion of Artificial Intelligence (AI) in digital platforms used
by youth has created significant challenges related to privacy, autonomy, and
data protection. While AI-driven personalization offers enhanced user
experiences, it often operates without clear ethical boundaries, leaving young
users vulnerable to data exploitation and algorithmic biases. This paper
presents a call to action for ethical AI governance, advocating for a
structured framework that ensures youth-centred privacy protections,
transparent data practices, and regulatory oversight. We outline key areas
requiring urgent intervention, including algorithmic transparency, privacy
education, parental data-sharing ethics, and accountability measures. Through
this approach, we seek to empower youth with greater control over their digital
identities and propose actionable strategies for policymakers, AI developers,
and educators to build a fairer and more accountable AI ecosystem.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.11920v1">Practical Implications of Implementing Local Differential Privacy for
  Smart grids</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-14T23:11:46Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Khadija Hafeez, Mubashir Husain Rehmani, Sumita Mishra, Donna OShea</p>
    <p><b>Summary:</b> Recent smart grid advancements enable near-realtime reporting of electricity
consumption, raising concerns about consumer privacy. Differential privacy (DP)
has emerged as a viable privacy solution, where a calculated amount of noise is
added to the data by a trusted third party, or individual users perturb their
information locally, and only send the randomized data to an aggregator for
analysis safeguarding users and aggregators privacy. However, the practical
implementation of a Local DP-based (LDP) privacy model for smart grids has its
own challenges. In this paper, we discuss the challenges of implementing an
LDP-based model for smart grids. We compare existing LDP mechanisms in smart
grids for privacy preservation of numerical data and discuss different methods
for selecting privacy parameters in the existing literature, their limitations
and the non-existence of an optimal method for selecting the privacy
parameters. We also discuss the challenges of translating theoretical models of
LDP into a practical setting for smart grids for different utility functions,
the impact of the size of data set on privacy and accuracy, and vulnerability
of LDP-based smart grids to manipulation attacks. Finally, we discuss future
directions in research for better practical applications in LDP based models
for smart grids.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.11850v1">Local Pan-Privacy for Federated Analytics</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-03-14T20:18:33Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Vitaly Feldman, Audra McMillan, Guy N. Rothblum, Kunal Talwar</p>
    <p><b>Summary:</b> Pan-privacy was proposed by Dwork et al. as an approach to designing a
private analytics system that retains its privacy properties in the face of
intrusions that expose the system's internal state. Motivated by federated
telemetry applications, we study local pan-privacy, where privacy should be
retained under repeated unannounced intrusions on the local state. We consider
the problem of monitoring the count of an event in a federated system, where
event occurrences on a local device should be hidden even from an intruder on
that device. We show that under reasonable constraints, the goal of providing
information-theoretic differential privacy under intrusion is incompatible with
collecting telemetry information. We then show that this problem can be solved
in a scalable way using standard cryptographic primitives.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.11232v1">PrivacyScalpel: Enhancing LLM Privacy via Interpretable Feature
  Intervention with Sparse Autoencoders</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-03-14T09:31:01Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ahmed Frikha, Muhammad Reza Ar Razi, Krishna Kanth Nakka, Ricardo Mendes, Xue Jiang, Xuebing Zhou</p>
    <p><b>Summary:</b> Large Language Models (LLMs) have demonstrated remarkable capabilities in
natural language processing but also pose significant privacy risks by
memorizing and leaking Personally Identifiable Information (PII). Existing
mitigation strategies, such as differential privacy and neuron-level
interventions, often degrade model utility or fail to effectively prevent
leakage. To address this challenge, we introduce PrivacyScalpel, a novel
privacy-preserving framework that leverages LLM interpretability techniques to
identify and mitigate PII leakage while maintaining performance. PrivacyScalpel
comprises three key steps: (1) Feature Probing, which identifies layers in the
model that encode PII-rich representations, (2) Sparse Autoencoding, where a
k-Sparse Autoencoder (k-SAE) disentangles and isolates privacy-sensitive
features,
  and (3) Feature-Level Interventions, which employ targeted ablation and
vector steering to suppress PII leakage.
  Our empirical evaluation on Gemma2-2b and Llama2-7b, fine-tuned on the Enron
dataset, shows that PrivacyScalpel significantly reduces email leakage from
5.15\% to as low as 0.0\%, while maintaining over 99.4\% of the original
model's utility. Notably, our method outperforms neuron-level interventions in
privacy-utility trade-offs, demonstrating that acting on sparse, monosemantic
features is more effective than manipulating polysemantic neurons. Beyond
improving LLM privacy, our approach offers insights into the mechanisms
underlying PII memorization, contributing to the broader field of model
interpretability and secure AI deployment.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.11169v1">Security and Privacy: Key Requirements for Molecular Communication in
  Medicine and Healthcare</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2025-03-14T08:14:14Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Vida Gholamiyan, Yaning Zhao, Wafa Labidi, Holger Boche, Christian Deppe</p>
    <p><b>Summary:</b> Molecular communication (MC) is an emerging paradigm that enables data
transmission through biochemical signals rather than traditional
electromagnetic waves. This approach is particularly promising for environments
where conventional wireless communication is impractical, such as within the
human body. However, security and privacy pose significant challenges that must
be addressed to ensure reliable communication. Moreover, MC is often
event-triggered, making it logical to adopt goal-oriented communication
strategies, similar to those used in message identification. This work explores
secure identification strategies for MC, with a focus on the
information-theoretic security of message identification over Poisson wiretap
channels (DT-PWC).</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.11051v1">Towards Privacy-preserved Pre-training of Remote Sensing Foundation
  Models with Federated Mutual-guidance Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-03-14T03:38:49Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jieyi Tan, Chengwei Zhang, Bo Dang, Yansheng Li</p>
    <p><b>Summary:</b> Traditional Remote Sensing Foundation models (RSFMs) are pre-trained with a
data-centralized paradigm, through self-supervision on large-scale curated
remote sensing data. For each institution, however, pre-training RSFMs with
limited data in a standalone manner may lead to suboptimal performance, while
aggregating remote sensing data from multiple institutions for centralized
pre-training raises privacy concerns. Seeking for collaboration is a promising
solution to resolve this dilemma, where multiple institutions can
collaboratively train RSFMs without sharing private data. In this paper, we
propose a novel privacy-preserved pre-training framework (FedSense), which
enables multiple institutions to collaboratively train RSFMs without sharing
private data. However, it is a non-trivial task hindered by a vicious cycle,
which results from model drift by remote sensing data heterogeneity and high
communication overhead. To break this vicious cycle, we introduce Federated
Mutual-guidance Learning. Specifically, we propose a Server-to-Clients Guidance
(SCG) mechanism to guide clients updates towards global-flatness optimal
solutions. Additionally, we propose a Clients-to-Server Guidance (CSG)
mechanism to inject local knowledge into the server by low-bit communication.
Extensive experiments on four downstream tasks demonstrate the effectiveness of
our FedSense in both full-precision and communication-reduced scenarios,
showcasing remarkable communication efficiency and performance gains.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.10945v1">$(\varepsilon, δ)$ Considered Harmful: Best Practices for Reporting
  Differential Privacy Guarantees</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2025-03-13T23:06:30Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Juan Felipe Gomez, Bogdan Kulynych, Georgios Kaissis, Jamie Hayes, Borja Balle, Antti Honkela</p>
    <p><b>Summary:</b> Current practices for reporting the level of differential privacy (DP)
guarantees for machine learning (ML) algorithms provide an incomplete and
potentially misleading picture of the guarantees and make it difficult to
compare privacy levels across different settings. We argue for using Gaussian
differential privacy (GDP) as the primary means of communicating DP guarantees
in ML, with the full privacy profile as a secondary option in case GDP is too
inaccurate. Unlike other widely used alternatives, GDP has only one parameter,
which ensures easy comparability of guarantees, and it can accurately capture
the full privacy profile of many important ML applications. To support our
claims, we investigate the privacy profiles of state-of-the-art DP large-scale
image classification, and the TopDown algorithm for the U.S. Decennial Census,
observing that GDP fits the profiles remarkably well in all three cases.
Although GDP is ideal for reporting the final guarantees, other formalisms
(e.g., privacy loss random variables) are needed for accurate privacy
accounting. We show that such intermediate representations can be efficiently
converted to GDP with minimal loss in tightness.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.10915v1">Usable Privacy in Virtual Worlds: Design Implications for Data
  Collection Awareness and Control Interfaces in Virtual Reality</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-03-13T22:02:54Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Viktorija Paneva, Verena Winterhalter, Naga Sai Surya Vamsy Malladi, Marvin Strauss, Stefan Schneegass, Florian Alt</p>
    <p><b>Summary:</b> Extended reality (XR) devices have become ubiquitous. They are equipped with
arrays of sensors, collecting extensive user and environmental data, allowing
inferences about sensitive user information users may not realize they are
sharing. Current VR privacy notices largely replicate mechanisms from 2D
interfaces, failing to leverage the unique affordances of virtual 3D
environments. To address this, we conducted brainstorming and sketching
sessions with novice game developers and designers, followed by privacy expert
evaluations, to explore and refine privacy interfaces tailored for VR. Key
challenges include balancing user engagement with privacy awareness, managing
complex privacy information with user comprehension, and maintaining compliance
and trust. We identify design implications such as thoughtful gamification,
explicit and purpose-tied consent mechanisms, and granular, modifiable privacy
control options. Our findings provide actionable guidance to researchers and
practitioners for developing privacy-aware and user-friendly VR experiences.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.10350v1">Enhancing Facial Privacy Protection via Weakening Diffusion Purification</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-03-13T13:27:53Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ali Salar, Qing Liu, Yingli Tian, Guoying Zhao</p>
    <p><b>Summary:</b> The rapid growth of social media has led to the widespread sharing of
individual portrait images, which pose serious privacy risks due to the
capabilities of automatic face recognition (AFR) systems for mass surveillance.
Hence, protecting facial privacy against unauthorized AFR systems is essential.
Inspired by the generation capability of the emerging diffusion models, recent
methods employ diffusion models to generate adversarial face images for privacy
protection. However, they suffer from the diffusion purification effect,
leading to a low protection success rate (PSR). In this paper, we first propose
learning unconditional embeddings to increase the learning capacity for
adversarial modifications and then use them to guide the modification of the
adversarial latent code to weaken the diffusion purification effect. Moreover,
we integrate an identity-preserving structure to maintain structural
consistency between the original and generated images, allowing human observers
to recognize the generated image as having the same identity as the original.
Extensive experiments conducted on two public datasets, i.e., CelebA-HQ and
LADN, demonstrate the superiority of our approach. The protected faces
generated by our method outperform those produced by existing facial privacy
protection approaches in terms of transferability and natural appearance.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.10727v1">Word-level Annotation of GDPR Transparency Compliance in Privacy
  Policies using Large Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-03-13T11:41:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Thomas Cory, Wolf Rieder, Julia Krämer, Philip Raschke, Patrick Herbke, Axel Küpper</p>
    <p><b>Summary:</b> Ensuring transparency of data practices related to personal information is a
fundamental requirement under the General Data Protection Regulation (GDPR),
particularly as mandated by Articles 13 and 14. However, assessing compliance
at scale remains a challenge due to the complexity and variability of privacy
policy language. Manual audits are resource-intensive and inconsistent, while
existing automated approaches lack the granularity needed to capture nuanced
transparency disclosures.
  In this paper, we introduce a large language model (LLM)-based framework for
word-level GDPR transparency compliance annotation. Our approach comprises a
two-stage annotation pipeline that combines initial LLM-based annotation with a
self-correction mechanism for iterative refinement. This annotation pipeline
enables the systematic identification and fine-grained annotation of
transparency-related content in privacy policies, aligning with 21 GDPR-derived
transparency requirements. To enable large-scale analysis, we compile a dataset
of 703,791 English-language policies, from which we generate a sample of 200
manually annotated privacy policies.
  To evaluate our approach, we introduce a two-tiered methodology assessing
both label- and span-level annotation performance. We conduct a comparative
analysis of eight high-profile LLMs, providing insights into their
effectiveness in identifying GDPR transparency disclosures. Our findings
contribute to advancing the automation of GDPR compliance assessments and
provide valuable resources for future research in privacy policy analysis.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.10239v1">I Can Tell Your Secrets: Inferring Privacy Attributes from Mini-app
  Interaction History in Super-apps</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-13T10:29:40Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yifeng Cai, Ziqi Zhang, Mengyu Yao, Junlin Liu, Xiaoke Zhao, Xinyi Fu, Ruoyu Li, Zhe Li, Xiangqun Chen, Yao Guo, Ding Li</p>
    <p><b>Summary:</b> Super-apps have emerged as comprehensive platforms integrating various
mini-apps to provide diverse services. While super-apps offer convenience and
enriched functionality, they can introduce new privacy risks. This paper
reveals a new privacy leakage source in super-apps: mini-app interaction
history, including mini-app usage history (Mini-H) and operation history
(Op-H). Mini-H refers to the history of mini-apps accessed by users, such as
their frequency and categories. Op-H captures user interactions within
mini-apps, including button clicks, bar drags, and image views. Super-apps can
naturally collect these data without instrumentation due to the web-based
feature of mini-apps. We identify these data types as novel and unexplored
privacy risks through a literature review of 30 papers and an empirical
analysis of 31 super-apps. We design a mini-app interaction history-oriented
inference attack (THEFT), to exploit this new vulnerability. Using THEFT, the
insider threats within the low-privilege business department of the super-app
vendor acting as the adversary can achieve more than 95.5% accuracy in
inferring privacy attributes of over 16.1% of users. THEFT only requires a
small training dataset of 200 users from public breached databases on the
Internet. We also engage with super-app vendors and a standards association to
increase industry awareness and commitment to protect this data. Our
contributions are significant in identifying overlooked privacy risks,
demonstrating the effectiveness of a new attack, and influencing industry
practices toward better privacy protection in the super-app ecosystem.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.10147v1">Optimal Privacy-Preserving Distributed Median Consensus</a></h3>
  
  <p><b>Published on:</b> 2025-03-13T08:19:12Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Wenrui Yu, Qiongxiu Li, Richard Heusdens, Sokol Kosta</p>
    <p><b>Summary:</b> Distributed median consensus has emerged as a critical paradigm in
multi-agent systems due to the inherent robustness of the median against
outliers and anomalies in measurement. Despite the sensitivity of the data
involved, the development of privacy-preserving mechanisms for median consensus
remains underexplored. In this work, we present the first rigorous analysis of
privacy in distributed median consensus, focusing on an $L_1$-norm minimization
framework. We establish necessary and sufficient conditions under which exact
consensus and perfect privacy-defined as zero information leakage-can be
achieved simultaneously. Our information-theoretic analysis provides provable
guarantees against passive and eavesdropping adversaries, ensuring that private
data remain concealed. Extensive numerical experiments validate our theoretical
results, demonstrating the practical feasibility of achieving both accuracy and
privacy in distributed median consensus.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.09823v2">Data Traceability for Privacy Alignment</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2025-03-12T20:42:23Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kevin Liao, Shreya Thipireddy, Daniel Weitzner</p>
    <p><b>Summary:</b> This paper offers a new privacy approach for the growing ecosystem of
services -- ranging from open banking to healthcare -- dependent on sensitive
personal data sharing between individuals and third parties. While these
services offer significant benefits, individuals want control over their data,
transparency regarding how their data is used, and accountability from third
parties for misuse. However, existing legal and technical mechanisms are
inadequate for supporting these needs. A comprehensive approach to the modern
privacy challenges of accountable third-party data sharing requires a closer
alignment of technical system architecture and legal institutional design. In
order to achieve this privacy alignment, we extend traditional security threat
modeling and analysis to encompass a broader range of privacy notions than has
been typically considered. In particular, we introduce the concept of
covert-accountability, which addresses the risk from adversaries that may act
dishonestly but nevertheless face potential identification and legal
consequences. As a concrete instance of this design approach, we present the
OTrace protocol, designed to provide traceable, accountable, consumer-control
in third-party data sharing ecosystems. OTrace empowers consumers with the
knowledge of who has their data, what it is being used for, what consent or
other legal terms apply, and whom it is being shared with. By applying our
alignment framework, we demonstrate that OTrace's technical affordances can
provide more confident, scalable regulatory oversight when combined with
complementary legal mechanisms.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.09780v1">AgentDAM: Privacy Leakage Evaluation for Autonomous Web Agents</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-03-12T19:30:31Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Arman Zharmagambetov, Chuan Guo, Ivan Evtimov, Maya Pavlova, Ruslan Salakhutdinov, Kamalika Chaudhuri</p>
    <p><b>Summary:</b> LLM-powered AI agents are an emerging frontier with tremendous potential to
increase human productivity. However, empowering AI agents to take action on
their user's behalf in day-to-day tasks involves giving them access to
potentially sensitive and private information, which leads to a possible risk
of inadvertent privacy leakage when the agent malfunctions. In this work, we
propose one way to address that potential risk, by training AI agents to better
satisfy the privacy principle of data minimization. For the purposes of this
benchmark, by "data minimization" we mean instances where private information
is shared only when it is necessary to fulfill a specific task-relevant
purpose. We develop a benchmark called AgentDAM to evaluate how well existing
and future AI agents can limit processing of potentially private information
that we designate "necessary" to fulfill the task. Our benchmark simulates
realistic web interaction scenarios and is adaptable to all existing web
navigation agents. We use AgentDAM to evaluate how well AI agents built on top
of GPT-4, Llama-3 and Claude can limit processing of potentially private
information when unnecessary, and show that these agents are often prone to
inadvertent use of unnecessary sensitive information. We finally propose a
prompting-based approach that reduces this.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.11711v1">Privacy-Preserved Automated Scoring using Federated Learning for
  Educational Research</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-03-12T19:06:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ehsan Latif, Xiaoming Zhai</p>
    <p><b>Summary:</b> Data privacy remains a critical concern in educational research,
necessitating Institutional Review Board (IRB) certification and stringent data
handling protocols to ensure compliance with ethical standards. Traditional
approaches rely on anonymization and controlled data-sharing mechanisms to
facilitate research while mitigating privacy risks. However, these methods
still involve direct access to raw student data, posing potential
vulnerabilities and being time-consuming. This study proposes a federated
learning (FL) framework for automatic scoring in educational assessments,
eliminating the need to share raw data. Our approach leverages client-side
model training, where student responses are processed locally on edge devices,
and only optimized model parameters are shared with a central aggregation
server. To effectively aggregate heterogeneous model updates, we introduce an
adaptive weighted averaging strategy, which dynamically adjusts weight
contributions based on client-specific learning characteristics. This method
ensures robust model convergence while preserving privacy. We evaluate our
framework using assessment data from nine middle schools, comparing the
accuracy of federated learning-based scoring models with traditionally trained
centralized models. A statistical significance test (paired t-test, $t(8) =
2.29, p = 0.051$) confirms that the accuracy difference between the two
approaches is not statistically significant, demonstrating that federated
learning achieves comparable performance while safeguarding student data.
Furthermore, our method significantly reduces data collection, processing, and
deployment overhead, accelerating the adoption of AI-driven educational
assessments in a privacy-compliant manner.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.09448v1">Optimizing QoE-Privacy Tradeoff for Proactive VR Streaming</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Multimedia-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Multiagent Systems-662E9B">
  <p><b>Published on:</b> 2025-03-12T14:50:06Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xing Wei, Shengqian Han, Chenyang Yang, Chengjian Sun</p>
    <p><b>Summary:</b> Proactive virtual reality (VR) streaming requires users to upload
viewpoint-related information, raising significant privacy concerns. Existing
strategies preserve privacy by introducing errors to viewpoints, which,
however, compromises the quality of experience (QoE) of users. In this paper,
we first delve into the analysis of the viewpoint leakage probability achieved
by existing privacy-preserving approaches. We determine the optimal
distribution of viewpoint errors that minimizes the viewpoint leakage
probability. Our analyses show that existing approaches cannot fully eliminate
viewpoint leakage. Then, we propose a novel privacy-preserving approach that
introduces noise to uploaded viewpoint prediction errors, which can ensure zero
viewpoint leakage probability. Given the proposed approach, the tradeoff
between privacy preservation and QoE is optimized to minimize the QoE loss
while satisfying the privacy requirement. Simulation results validate our
analysis results and demonstrate that the proposed approach offers a promising
solution for balancing privacy and QoE.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.09381v2">Faithful and Privacy-Preserving Implementation of Average Consensus</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36">
  <p><b>Published on:</b> 2025-03-12T13:28:22Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kaoru Teranishi, Kiminao Kogiso, Takashi Tanaka</p>
    <p><b>Summary:</b> We propose a protocol based on mechanism design theory and encrypted control
to solve average consensus problems among rational and strategic agents while
preserving their privacy. The proposed protocol provides a mechanism that
incentivizes the agents to faithfully implement the intended behavior specified
in the protocol. Furthermore, the protocol runs over encrypted data using
homomorphic encryption and secret sharing to protect the privacy of agents. We
also analyze the security of the proposed protocol using a simulation paradigm
in secure multi-party computation. The proposed protocol demonstrates that
mechanism design and encrypted control can complement each other to achieve
security under rational adversaries.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.09365v1">Membership Inference Attacks fueled by Few-Short Learning to detect
  privacy leakage tackling data integrity</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-03-12T13:09:43Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Daniel Jiménez-López, Nuria Rodríguez-Barroso, M. Victoria Luzón, Francisco Herrera</p>
    <p><b>Summary:</b> Deep learning models have an intrinsic privacy issue as they memorize parts
of their training data, creating a privacy leakage. Membership Inference
Attacks (MIA) exploit it to obtain confidential information about the data used
for training, aiming to steal information. They can be repurposed as a
measurement of data integrity by inferring whether it was used to train a
machine learning model. While state-of-the-art attacks achieve a significant
privacy leakage, their requirements are not feasible enough, hindering their
role as practical tools to assess the magnitude of the privacy risk. Moreover,
the most appropriate evaluation metric of MIA, the True Positive Rate at low
False Positive Rate lacks interpretability. We claim that the incorporation of
Few-Shot Learning techniques to the MIA field and a proper qualitative and
quantitative privacy evaluation measure should deal with these issues. In this
context, our proposal is twofold. We propose a Few-Shot learning based MIA,
coined as the FeS-MIA model, which eases the evaluation of the privacy breach
of a deep learning model by significantly reducing the number of resources
required for the purpose. Furthermore, we propose an interpretable quantitative
and qualitative measure of privacy, referred to as Log-MIA measure. Jointly,
these proposals provide new tools to assess the privacy leakage and to ease the
evaluation of the training data integrity of deep learning models, that is, to
analyze the privacy breach of a deep learning model. Experiments carried out
with MIA over image classification and language modeling tasks and its
comparison to the state-of-the-art show that our proposals excel at reporting
the privacy leakage of a deep learning model with little extra information.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.09331v1">Large-Scale FPGA-Based Privacy Amplification Exceeding $10^8$ Bits for
  Quantum Key Distribution</a></h3>
  
  <p><b>Published on:</b> 2025-03-12T12:25:13Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xi Cheng, Hao-kun Mao, Hong-wei Xu, Qiong Li</p>
    <p><b>Summary:</b> Privacy Amplification (PA) is indispensable in Quantum Key Distribution (QKD)
post-processing, as it eliminates information leakage to eavesdroppers.
Field-programmable gate arrays (FPGAs) are highly attractive for QKD systems
due to their flexibility and high integration. However, due to limited
resources, input and output sizes remain the primary bottleneck in FPGA-based
PA schemes for Discrete Variable (DV)-QKD systems. In this paper, we present a
large-scale FPGA-based PA scheme that supports both input block sizes and
output key sizes exceeding $10^8$ bits, effectively addressing the challenges
posed by the finite-size effect. To accommodate the large input and output
sizes, we propose a novel PA algorithm and prove its security. We implement and
evaluate this scheme on a Xilinx XCKU095 FPGA platform. Experimental results
demonstrate that our PA implementation can handle an input block size of $10^8$
bits with flexible output sizes up to the input size. For DV-QKD systems, our
PA scheme supports an input block size nearly two orders of magnitude larger
than current FPGA-based PA schemes, significantly mitigating the impact of the
finite-size effect on the final secure key rate.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.09317v1">RaceTEE: A Practical Privacy-Preserving Off-Chain Smart Contract
  Execution Architecture</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-12T12:10:02Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Keyu Zhang, Andrew Martin</p>
    <p><b>Summary:</b> Decentralized on-chain smart contracts enable trustless collaboration, yet
their inherent data transparency and execution overhead hinder widespread
adoption. Existing cryptographic approaches incur high computational costs and
lack generality. Meanwhile, prior TEE-based solutions suffer from practical
limitations, such as the inability to support inter-contract interactions,
reliance on unbreakable TEEs, and compromised usability. We introduce RaceTEE,
a practical and privacy-preserving off-chain execution architecture for smart
contracts that leverages Trusted Execution Environments (TEEs). RaceTEE
decouples transaction ordering (on-chain) from execution (off-chain), with
computations performed competitively in TEEs, ensuring confidentiality and
minimizing overhead. It further enhances practicality through three key
improvements: supporting secure inter-contract interactions, providing a key
rotation scheme that enforces forward and backward secrecy even in the event of
TEE breaches, and enabling full compatibility with existing blockchains without
altering the user interaction model. To validate its feasibility, we prototype
RaceTEE using Intel SGX and Ethereum, demonstrating its applicability across
various use cases and evaluating its performance.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.09192v1">Differential Privacy Personalized Federated Learning Based on
  Dynamically Sparsified Client Updates</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-12T09:34:05Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chuanyin Wang, Yifei Zhang, Neng Gao, Qiang Luo</p>
    <p><b>Summary:</b> Personalized federated learning is extensively utilized in scenarios
characterized by data heterogeneity, facilitating more efficient and automated
local training on data-owning terminals. This includes the automated selection
of high-performance model parameters for upload, thereby enhancing the overall
training process. However, it entails significant risks of privacy leakage.
Existing studies have attempted to mitigate these risks by utilizing
differential privacy. Nevertheless, these studies present two major
limitations: (1) The integration of differential privacy into personalized
federated learning lacks sufficient personalization, leading to the
introduction of excessive noise into the model. (2) It fails to adequately
control the spatial scope of model update information, resulting in a
suboptimal balance between data privacy and model effectiveness in differential
privacy federated learning. In this paper, we propose a differentially private
personalized federated learning approach that employs dynamically sparsified
client updates through reparameterization and adaptive norm(DP-pFedDSU).
Reparameterization training effectively selects personalized client update
information, thereby reducing the quantity of updates. This approach minimizes
the introduction of noise to the greatest extent possible. Additionally,
dynamic adaptive norm refers to controlling the norm space of model updates
during the training process, mitigating the negative impact of clipping on the
update information. These strategies substantially enhance the effective
integration of differential privacy and personalized federated learning.
Experimental results on EMNIST, CIFAR-10, and CIFAR-100 demonstrate that our
proposed scheme achieves superior performance and is well-suited for more
complex personalized federated learning scenarios.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.08568v1">Privacy Law Enforcement Under Centralized Governance: A Qualitative
  Analysis of Four Years' Special Privacy Rectification Campaigns</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-11T15:56:09Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tao Jing, Yao Li, Jingzhou Ye, Jie Wang, Xueqiang Wang</p>
    <p><b>Summary:</b> In recent years, major privacy laws like the GDPR have brought about positive
changes. However, challenges remain in enforcing the laws, particularly due to
under-resourced regulators facing a large number of potential privacy-violating
software applications (apps) and the high costs of investigating them. Since
2019, China has launched a series of privacy enforcement campaigns known as
Special Privacy Rectification Campaigns (SPRCs) to address widespread privacy
violations in its mobile application (app) ecosystem. Unlike the enforcement of
the GDPR, SPRCs are characterized by large-scale privacy reviews and strict
sanctions, under the strong control of central authorities. In SPRCs, central
government authorities issue administrative orders to mobilize various
resources for market-wide privacy reviews of mobile apps. They enforce strict
sanctions by requiring privacy-violating apps to rectify issues within a short
timeframe or face removal from app stores. While there are a few reports on
SPRCs, the effectiveness and potential problems of this campaign-style privacy
enforcement approach remain unclear to the community. In this study, we
conducted 18 semi-structured interviews with app-related engineers involved in
SPRCs to better understand the campaign-style privacy enforcement. Based on the
interviews, we reported our findings on a variety of aspects of SPRCs, such as
the processes that app engineers regularly follow to achieve privacy compliance
in SPRCs, the challenges they encounter, the solutions they adopt to address
these challenges, and the impacts of SPRCs, etc. We found that app engineers
face a series of challenges in achieving privacy compliance in their apps...</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.08297v1">Privacy for Free: Leveraging Local Differential Privacy Perturbed Data
  from Multiple Services</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-11T11:10:03Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Rong Du, Qingqing Ye, Yue Fu, Haibo Hu</p>
    <p><b>Summary:</b> Local Differential Privacy (LDP) has emerged as a widely adopted
privacy-preserving technique in modern data analytics, enabling users to share
statistical insights while maintaining robust privacy guarantees. However,
current LDP applications assume a single service gathering perturbed
information from users. In reality, multiple services may be interested in
collecting users' data, which poses privacy burdens to users as more such
services emerge. To address this issue, this paper proposes a framework for
collecting and aggregating data based on perturbed information from multiple
services, regardless of their estimated statistics (e.g., mean or distribution)
and perturbation mechanisms.
  Then for mean estimation, we introduce the Unbiased Averaging (UA) method and
its optimized version, User-level Weighted Averaging (UWA). The former utilizes
biased perturbed data, while the latter assigns weights to different perturbed
results based on perturbation information, thereby achieving minimal variance.
For distribution estimation, we propose the User-level Likelihood Estimation
(ULE), which treats all perturbed results from a user as a whole for maximum
likelihood estimation. Experimental results demonstrate that our framework and
constituting methods significantly improve the accuracy of both mean and
distribution estimation.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.08175v1">Privacy-Enhancing Paradigms within Federated Multi-Agent Systems</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-03-11T08:38:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zitong Shi, Guancheng Wan, Wenke Huang, Guibin Zhang, Jiawei Shao, Mang Ye, Carl Yang</p>
    <p><b>Summary:</b> LLM-based Multi-Agent Systems (MAS) have proven highly effective in solving
complex problems by integrating multiple agents, each performing different
roles. However, in sensitive domains, they face emerging privacy protection
challenges. In this paper, we introduce the concept of Federated MAS,
highlighting the fundamental differences between Federated MAS and traditional
FL. We then identify key challenges in developing Federated MAS, including: 1)
heterogeneous privacy protocols among agents, 2) structural differences in
multi-party conversations, and 3) dynamic conversational network structures. To
address these challenges, we propose Embedded Privacy-Enhancing Agents
(EPEAgent), an innovative solution that integrates seamlessly into the
Retrieval-Augmented Generation (RAG) phase and the context retrieval stage.
This solution minimizes data flows, ensuring that only task-relevant,
agent-specific information is shared. Additionally, we design and generate a
comprehensive dataset to evaluate the proposed paradigm. Extensive experiments
demonstrate that EPEAgent effectively enhances privacy protection while
maintaining strong system performance. The code will be availiable at
https://github.com/ZitongShi/EPEAgent</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.08085v3">PRISM: Privacy-Preserving Improved Stochastic Masking for Federated
  Generative Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-03-11T06:37:54Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kyeongkook Seo, Dong-Jun Han, Jaejun Yoo</p>
    <p><b>Summary:</b> Despite recent advancements in federated learning (FL), the integration of
generative models into FL has been limited due to challenges such as high
communication costs and unstable training in heterogeneous data environments.
To address these issues, we propose PRISM, a FL framework tailored for
generative models that ensures (i) stable performance in heterogeneous data
distributions and (ii) resource efficiency in terms of communication cost and
final model size. The key of our method is to search for an optimal stochastic
binary mask for a random network rather than updating the model weights,
identifying a sparse subnetwork with high generative performance; i.e., a
``strong lottery ticket''. By communicating binary masks in a stochastic
manner, PRISM minimizes communication overhead. This approach, combined with
the utilization of maximum mean discrepancy (MMD) loss and a mask-aware dynamic
moving average aggregation method (MADA) on the server side, facilitates stable
and strong generative capabilities by mitigating local divergence in FL
scenarios. Moreover, thanks to its sparsifying characteristic, PRISM yields a
lightweight model without extra pruning or quantization, making it ideal for
environments such as edge devices. Experiments on MNIST, FMNIST, CelebA, and
CIFAR10 demonstrate that PRISM outperforms existing methods, while maintaining
privacy with minimal communication costs. PRISM is the first to successfully
generate images under challenging non-IID and privacy-preserving FL
environments on complex datasets, where previous methods have struggled.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.07775v1">Sublinear Algorithms for Wasserstein and Total Variation Distances:
  Applications to Fairness and Privacy Auditing</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B"> 
  <p><b>Published on:</b> 2025-03-10T18:57:48Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Debabrota Basu, Debarshi Chanda</p>
    <p><b>Summary:</b> Resource-efficiently computing representations of probability distributions
and the distances between them while only having access to the samples is a
fundamental and useful problem across mathematical sciences. In this paper, we
propose a generic algorithmic framework to estimate the PDF and CDF of any
sub-Gaussian distribution while the samples from them arrive in a stream. We
compute mergeable summaries of distributions from the stream of samples that
require sublinear space w.r.t. the number of observed samples. This allows us
to estimate Wasserstein and Total Variation (TV) distances between any two
sub-Gaussian distributions while samples arrive in streams and from multiple
sources (e.g. federated learning). Our algorithms significantly improves on the
existing methods for distance estimation incurring super-linear time and linear
space complexities. In addition, we use the proposed estimators of Wasserstein
and TV distances to audit the fairness and privacy of the ML algorithms. We
empirically demonstrate the efficiency of the algorithms for estimating these
distances and auditing using both synthetic and real-world datasets.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.07570v1">Split-n-Chain: Privacy-Preserving Multi-Node Split Learning with
  Blockchain-Based Auditability</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-03-10T17:40:05Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mukesh Sahani, Binanda Sengupta</p>
    <p><b>Summary:</b> Deep learning, when integrated with a large amount of training data, has the
potential to outperform machine learning in terms of high accuracy. Recently,
privacy-preserving deep learning has drawn significant attention of the
research community. Different privacy notions in deep learning include privacy
of data provided by data-owners and privacy of parameters and/or
hyperparameters of the underlying neural network. Federated learning is a
popular privacy-preserving execution environment where data-owners participate
in learning the parameters collectively without leaking their respective data
to other participants. However, federated learning suffers from certain
security/privacy issues. In this paper, we propose Split-n-Chain, a variant of
split learning where the layers of the network are split among several
distributed nodes. Split-n-Chain achieves several privacy properties:
data-owners need not share their training data with other nodes, and no nodes
have access to the parameters and hyperparameters of the neural network (except
that of the respective layers they hold). Moreover, Split-n-Chain uses
blockchain to audit the computation done by different nodes. Our experimental
results show that: Split-n-Chain is efficient, in terms of time required to
execute different phases, and the training loss trend is similar to that for
the same neural network when implemented in a monolithic fashion.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.07505v1">From Centralized to Decentralized Federated Learning: Theoretical
  Insights, Privacy Preservation, and Robustness Challenges</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2025-03-10T16:27:40Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Qiongxiu Li, Wenrui Yu, Yufei Xia, Jun Pang</p>
    <p><b>Summary:</b> Federated Learning (FL) enables collaborative learning without directly
sharing individual's raw data. FL can be implemented in either a centralized
(server-based) or decentralized (peer-to-peer) manner. In this survey, we
present a novel perspective: the fundamental difference between centralized FL
(CFL) and decentralized FL (DFL) is not merely the network topology, but the
underlying training protocol: separate aggregation vs. joint optimization. We
argue that this distinction in protocol leads to significant differences in
model utility, privacy preservation, and robustness to attacks. We
systematically review and categorize existing works in both CFL and DFL
according to the type of protocol they employ. This taxonomy provides deeper
insights into prior research and clarifies how various approaches relate or
differ. Through our analysis, we identify key gaps in the literature. In
particular, we observe a surprising lack of exploration of DFL approaches based
on distributed optimization methods, despite their potential advantages. We
highlight this under-explored direction and call for more research on
leveraging distributed optimization for federated learning. Overall, this work
offers a comprehensive overview from centralized to decentralized FL, sheds new
light on the core distinctions between approaches, and outlines open challenges
and future directions for the field.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.07427v2">Creating and Evaluating Privacy and Security Micro-Lessons for
  Elementary School Children</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2025-03-10T15:12:11Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Lan Gao, Elana B Blinder, Abigail Barnes, Kevin Song, Tamara Clegg, Jessica Vitak, Marshini Chetty</p>
    <p><b>Summary:</b> The growing use of technology in K--8 classrooms highlights a parallel need
for formal learning opportunities aimed at helping children use technology
safely and protect their personal information. Even the youngest students are
now using tablets, laptops, and apps to support their learning; however, there
are limited curricular materials available for elementary and middle school
children on digital privacy and security topics. To bridge this gap, we
developed a series of micro-lessons to help K--8 children learn about digital
privacy and security at school. We first conducted a formative study by
interviewing elementary school teachers to identify the design needs for
digital privacy and security lessons. We then developed micro-lessons --
multiple 15-20 minute activities designed to be easily inserted into the
existing curriculum -- using a co-design approach with multiple rounds of
developing and revising the micro-lessons in collaboration with teachers.
Throughout the process, we conducted evaluation sessions where teachers
implemented or reviewed the micro-lessons. Our study identifies strengths,
challenges, and teachers' tailoring strategies when incorporating micro-lessons
for K--8 digital privacy and security topics, providing design implications for
facilitating learning about these topics in school classrooms.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.07216v2">FedRand: Enhancing Privacy in Federated Learning with Randomized LoRA
  Subparameter Updates</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-03-10T11:55:50Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sangwoo Park, Seanie Lee, Byungjoo Kim, Sung Ju Hwang</p>
    <p><b>Summary:</b> Federated Learning (FL) is a widely used framework for training models in a
decentralized manner, ensuring that the central server does not have direct
access to data from local clients. However, this approach may still fail to
fully preserve data privacy, as models from local clients are exposed to the
central server during the aggregation process. This issue becomes even more
critical when training vision-language models (VLMs) with FL, as VLMs can
easily memorize training data instances, making them vulnerable to membership
inference attacks (MIAs). To address this challenge, we propose the FedRand
framework, which avoids disclosing the full set of client parameters. In this
framework, each client randomly selects subparameters of Low-Rank Adaptation
(LoRA) from the server and keeps the remaining counterparts of the LoRA weights
as private parameters. After training both parameters on the client's private
dataset, only the non-private client parameters are sent back to the server for
aggregation. This approach mitigates the risk of exposing client-side VLM
parameters, thereby enhancing data privacy. We empirically validate that
FedRand improves robustness against MIAs compared to relevant baselines while
achieving accuracy comparable to methods that communicate full LoRA parameters
across several benchmark datasets.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.07199v1">How Well Can Differential Privacy Be Audited in One Run?</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-10T11:32:30Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Amit Keinan, Moshe Shenfeld, Katrina Ligett</p>
    <p><b>Summary:</b> Recent methods for auditing the privacy of machine learning algorithms have
improved computational efficiency by simultaneously intervening on multiple
training examples in a single training run. Steinke et al. (2024) prove that
one-run auditing indeed lower bounds the true privacy parameter of the audited
algorithm, and give impressive empirical results. Their work leaves open the
question of how precisely one-run auditing can uncover the true privacy
parameter of an algorithm, and how that precision depends on the audited
algorithm. In this work, we characterize the maximum achievable efficacy of
one-run auditing and show that one-run auditing can only perfectly uncover the
true privacy parameters of algorithms whose structure allows the effects of
individual data elements to be isolated. Our characterization helps reveal how
and when one-run auditing is still a promising technique for auditing real
machine learning algorithms, despite these fundamental gaps.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.07048v1">A Failure-Free and Efficient Discrete Laplace Distribution for
  Differential Privacy in MPC</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-10T08:35:16Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ivan Tjuawinata, Jiabo Wang, Mengmeng Yang, Shanxiang Lyu, Huaxiong Wang, Kwok-Yan Lam</p>
    <p><b>Summary:</b> In an MPC-protected distributed computation, although the use of MPC assures
data privacy during computation, sensitive information may still be inferred by
curious MPC participants from the computation output. This can be observed, for
instance, in the inference attacks on either federated learning or a more
standard statistical computation with distributed inputs. In this work, we
address this output privacy issue by proposing a discrete and bounded
Laplace-inspired perturbation mechanism along with a secure realization of this
mechanism using MPC. The proposed mechanism strictly adheres to a zero failure
probability, overcoming the limitation encountered on other existing bounded
and discrete variants of Laplace perturbation. We provide analyses of the
proposed differential privacy (DP) perturbation in terms of its privacy and
utility. Additionally, we designed MPC protocols to implement this mechanism
and presented performance benchmarks based on our experimental setup. The MPC
realization of the proposed mechanism exhibits a complexity similar to the
state-of-the-art discrete Gaussian mechanism, which can be considered an
alternative with comparable efficiency while providing stronger differential
privacy guarantee. Moreover, efficiency of the proposed scheme can be further
enhanced by performing the noise generation offline while leaving the
perturbation phase online.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.06808v1">Privacy Auditing of Large Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-03-09T23:32:15Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ashwinee Panda, Xinyu Tang, Milad Nasr, Christopher A. Choquette-Choo, Prateek Mittal</p>
    <p><b>Summary:</b> Current techniques for privacy auditing of large language models (LLMs) have
limited efficacy -- they rely on basic approaches to generate canaries which
leads to weak membership inference attacks that in turn give loose lower bounds
on the empirical privacy leakage. We develop canaries that are far more
effective than those used in prior work under threat models that cover a range
of realistic settings. We demonstrate through extensive experiments on multiple
families of fine-tuned LLMs that our approach sets a new standard for detection
of privacy leakage. For measuring the memorization rate of non-privately
trained LLMs, our designed canaries surpass prior approaches. For example, on
the Qwen2.5-0.5B model, our designed canaries achieve $49.6\%$ TPR at $1\%$
FPR, vastly surpassing the prior approach's $4.2\%$ TPR at $1\%$ FPR. Our
method can be used to provide a privacy audit of $\varepsilon \approx 1$ for a
model trained with theoretical $\varepsilon$ of 4. To the best of our
knowledge, this is the first time that a privacy audit of LLM training has
achieved nontrivial auditing success in the setting where the attacker cannot
train shadow models, insert gradient canaries, or access the model at every
iteration.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.06732v1">Data Efficient Subset Training with Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-03-09T19:05:10Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ninad Jayesh Gandhi, Moparthy Venkata Subrahmanya Sri Harsha</p>
    <p><b>Summary:</b> Private machine learning introduces a trade-off between the privacy budget
and training performance. Training convergence is substantially slower and
extensive hyper parameter tuning is required. Consequently, efficient methods
to conduct private training of models is thoroughly investigated in the
literature. To this end, we investigate the strength of the data efficient
model training methods in the private training setting. We adapt GLISTER
(Killamsetty et al., 2021b) to the private setting and extensively assess its
performance. We empirically find that practical choices of privacy budgets are
too restrictive for data efficient training in the private setting.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.06455v1">Privacy Protection in Prosumer Energy Management Based on Federated
  Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-03-09T05:29:29Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yunfeng Li, Xiaolin Li Zhitao Li, Gangqiang Li</p>
    <p><b>Summary:</b> With the booming development of prosumers, there is an urgent need for a
prosumer energy management system to take full advantage of the flexibility of
prosumers and take into account the interests of other parties. However,
building such a system will undoubtedly reveal users' privacy. In this paper,
by solving the non-independent and identical distribution of data (Non-IID)
problem in federated learning with federated cluster average(FedClusAvg)
algorithm, prosumers' information can efficiently participate in the
intelligent decision making of the system without revealing privacy. In the
proposed FedClusAvg algorithm, each client performs cluster stratified sampling
and multiple iterations. Then, the average weight of the parameters of the
sub-server is determined according to the degree of deviation of the parameter
from the average parameter. Finally, the sub-server multiple local iterations
and updates, and then upload to the main server. The advantages of FedClusAvg
algorithm are the following two parts. First, the accuracy of the model in the
case of Non-IID is improved through the method of clustering and parameter
weighted average. Second, local multiple iterations and three-tier framework
can effectively reduce communication rounds.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.06150v2">Do Fairness Interventions Come at the Cost of Privacy: Evaluations for
  Binary Classifiers</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-03-08T10:21:21Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Huan Tian, Guangsheng Zhang, Bo Liu, Tianqing Zhu, Ming Ding, Wanlei Zhou</p>
    <p><b>Summary:</b> While in-processing fairness approaches show promise in mitigating biased
predictions, their potential impact on privacy leakage remains under-explored.
We aim to address this gap by assessing the privacy risks of fairness-enhanced
binary classifiers via membership inference attacks (MIAs) and attribute
inference attacks (AIAs). Surprisingly, our results reveal that enhancing
fairness does not necessarily lead to privacy compromises. For example, these
fairness interventions exhibit increased resilience against MIAs and AIAs. This
is because fairness interventions tend to remove sensitive information among
extracted features and reduce confidence scores for the majority of training
data for fairer predictions. However, during the evaluations, we uncover a
potential threat mechanism that exploits prediction discrepancies between fair
and biased models, leading to advanced attack results for both MIAs and AIAs.
This mechanism reveals potent vulnerabilities of fair models and poses
significant privacy risks of current fairness methods. Extensive experiments
across multiple datasets, attack methods, and representative fairness
approaches confirm our findings and demonstrate the efficacy of the uncovered
mechanism. Our study exposes the under-explored privacy threats in fairness
studies, advocating for thorough evaluations of potential security
vulnerabilities before model deployments.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.06021v1">FedEM: A Privacy-Preserving Framework for Concurrent Utility
  Preservation in Federated Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-03-08T02:48:00Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mingcong Xu, Xiaojin Zhang, Wei Chen, Hai Jin</p>
    <p><b>Summary:</b> Federated Learning (FL) enables collaborative training of models across
distributed clients without sharing local data, addressing privacy concerns in
decentralized systems. However, the gradient-sharing process exposes private
data to potential leakage, compromising FL's privacy guarantees in real-world
applications. To address this issue, we propose Federated Error Minimization
(FedEM), a novel algorithm that incorporates controlled perturbations through
adaptive noise injection. This mechanism effectively mitigates gradient leakage
attacks while maintaining model performance. Experimental results on benchmark
datasets demonstrate that FedEM significantly reduces privacy risks and
preserves model accuracy, achieving a robust balance between privacy protection
and utility preservation.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.05954v1">A Survey on Tabular Data Generation: Utility, Alignment, Fidelity,
  Privacy, and Beyond</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-03-07T21:47:11Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mihaela Cătălina Stoian, Eleonora Giunchiglia, Thomas Lukasiewicz</p>
    <p><b>Summary:</b> Generative modelling has become the standard approach for synthesising
tabular data. However, different use cases demand synthetic data to comply with
different requirements to be useful in practice. In this survey, we review deep
generative modelling approaches for tabular data from the perspective of four
types of requirements: utility of the synthetic data, alignment of the
synthetic data with domain-specific knowledge, statistical fidelity of the
synthetic data distribution compared to the real data distribution, and
privacy-preserving capabilities. We group the approaches along two levels of
granularity: (i) based on the primary type of requirements they address and
(ii) according to the underlying model they utilise. Additionally, we summarise
the appropriate evaluation methods for each requirement and the specific
characteristics of each model type. Finally, we discuss future directions for
the field, along with opportunities to improve the current evaluation methods.
Overall, this survey can be seen as a user guide to tabular data generation:
helping readers navigate available models and evaluation methods to find those
best suited to their needs.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.05684v1">Fairness-Aware Low-Rank Adaptation Under Demographic Privacy Constraints</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-03-07T18:49:57Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Parameswaran Kamalaruban, Mark Anderson, Stuart Burrell, Maeve Madigan, Piotr Skalski, David Sutton</p>
    <p><b>Summary:</b> Pre-trained foundation models can be adapted for specific tasks using
Low-Rank Adaptation (LoRA). However, the fairness properties of these adapted
classifiers remain underexplored. Existing fairness-aware fine-tuning methods
rely on direct access to sensitive attributes or their predictors, but in
practice, these sensitive attributes are often held under strict consumer
privacy controls, and neither the attributes nor their predictors are available
to model developers, hampering the development of fair models. To address this
issue, we introduce a set of LoRA-based fine-tuning methods that can be trained
in a distributed fashion, where model developers and fairness auditors
collaborate without sharing sensitive attributes or predictors. In this paper,
we evaluate three such methods - sensitive unlearning, adversarial training,
and orthogonality loss - against a fairness-unaware baseline, using experiments
on the CelebA and UTK-Face datasets with an ImageNet pre-trained ViT-Base
model. We find that orthogonality loss consistently reduces bias while
maintaining or improving utility, whereas adversarial training improves False
Positive Rate Parity and Demographic Parity in some cases, and sensitive
unlearning provides no clear benefit. In tasks where significant biases are
present, distributed fairness-aware fine-tuning methods can effectively
eliminate bias without compromising consumer privacy and, in most cases,
improve model utility.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.04980v1">A Consensus Privacy Metrics Framework for Synthetic Data</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-03-06T21:19:02Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Lisa Pilgram, Fida K. Dankar, Jorg Drechsler, Mark Elliot, Josep Domingo-Ferrer, Paul Francis, Murat Kantarcioglu, Linglong Kong, Bradley Malin, Krishnamurty Muralidhar, Puja Myles, Fabian Prasser, Jean Louis Raisaro, Chao Yan, Khaled El Emam</p>
    <p><b>Summary:</b> Synthetic data generation is one approach for sharing individual-level data.
However, to meet legislative requirements, it is necessary to demonstrate that
the individuals' privacy is adequately protected. There is no consolidated
standard for measuring privacy in synthetic data. Through an expert panel and
consensus process, we developed a framework for evaluating privacy in synthetic
data. Our findings indicate that current similarity metrics fail to measure
identity disclosure, and their use is discouraged. For differentially private
synthetic data, a privacy budget other than close to zero was not considered
interpretable. There was consensus on the importance of membership and
attribute disclosure, both of which involve inferring personal information
about an individual without necessarily revealing their identity. The resultant
framework provides precise recommendations for metrics that address these types
of disclosures effectively. Our findings further present specific opportunities
for future research that can help with widespread adoption of synthetic data.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.04707v1">Iris Style Transfer: Enhancing Iris Recognition with Style Features and
  Privacy Preservation through Neural Style Transfer</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-03-06T18:55:21Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mengdi Wang, Efe Bozkir, Enkelejda Kasneci</p>
    <p><b>Summary:</b> Iris texture is widely regarded as a gold standard biometric modality for
authentication and identification. The demand for robust iris recognition
methods, coupled with growing security and privacy concerns regarding iris
attacks, has escalated recently. Inspired by neural style transfer, an advanced
technique that leverages neural networks to separate content and style
features, we hypothesize that iris texture's style features provide a reliable
foundation for recognition and are more resilient to variations like rotation
and perspective shifts than traditional approaches. Our experimental results
support this hypothesis, showing a significantly higher classification accuracy
compared to conventional features. Further, we propose using neural style
transfer to mask identifiable iris style features, ensuring the protection of
sensitive biometric information while maintaining the utility of eye images for
tasks like eye segmentation and gaze estimation. This work opens new avenues
for iris-oriented, secure, and privacy-aware biometric systems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.04652v1">Evaluation of Privacy-aware Support Vector Machine (SVM) Learning using
  Homomorphic Encryption</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-06T17:42:23Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> William J Buchanan, Hisham Ali</p>
    <p><b>Summary:</b> The requirement for privacy-aware machine learning increases as we continue
to use PII (Personally Identifiable Information) within machine training. To
overcome these privacy issues, we can apply Fully Homomorphic Encryption (FHE)
to encrypt data before it is fed into a machine learning model. This involves
creating a homomorphic encryption key pair, and where the associated public key
will be used to encrypt the input data, and the private key will decrypt the
output. But, there is often a performance hit when we use homomorphic
encryption, and so this paper evaluates the performance overhead of using the
SVM machine learning technique with the OpenFHE homomorphic encryption library.
This uses Python and the scikit-learn library for its implementation. The
experiments include a range of variables such as multiplication depth, scale
size, first modulus size, security level, batch size, and ring dimension, along
with two different SVM models, SVM-Poly and SVM-Linear. Overall, the results
show that the two main parameters which affect performance are the ring
dimension and the modulus size, and that SVM-Poly and SVM-Linear show similar
performance levels.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.04451v1">Privacy Preserving and Robust Aggregation for Cross-Silo Federated
  Learning in Non-IID Settings</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-06T14:06:20Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Marco Arazzi, Mert Cihangiroglu, Antonino Nocera</p>
    <p><b>Summary:</b> Federated Averaging remains the most widely used aggregation strategy in
federated learning due to its simplicity and scalability. However, its
performance degrades significantly in non-IID data settings, where client
distributions are highly imbalanced or skewed. Additionally, it relies on
clients transmitting metadata, specifically the number of training samples,
which introduces privacy risks and may conflict with regulatory frameworks like
the European GDPR. In this paper, we propose a novel aggregation strategy that
addresses these challenges by introducing class-aware gradient masking. Unlike
traditional approaches, our method relies solely on gradient updates,
eliminating the need for any additional client metadata, thereby enhancing
privacy protection. Furthermore, our approach validates and dynamically weights
client contributions based on class-specific importance, ensuring robustness
against non-IID distributions, convergence prevention, and backdoor attacks.
Extensive experiments on benchmark datasets demonstrate that our method not
only outperforms FedAvg and other widely accepted aggregation strategies in
non-IID settings but also preserves model integrity in adversarial scenarios.
Our results establish the effectiveness of gradient masking as a practical and
secure solution for federated learning.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.04866v1">Privacy in Responsible AI: Approaches to Facial Recognition from Cloud
  Providers</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-03-06T12:04:12Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Anna Elivanova</p>
    <p><b>Summary:</b> As the use of facial recognition technology is expanding in different
domains, ensuring its responsible use is gaining more importance. This paper
conducts a comprehensive literature review of existing studies on facial
recognition technology from the perspective of privacy, which is one of the key
Responsible AI principles.
  Cloud providers, such as Microsoft, AWS, and Google, are at the forefront of
delivering facial-related technology services, but their approaches to
responsible use of these technologies vary significantly. This paper compares
how these cloud giants implement the privacy principle into their facial
recognition and detection services. By analysing their approaches, it
identifies both common practices and notable differences. The results of this
research will be valuable for developers and businesses by providing them
insights into best practices of three major companies for integration
responsible AI, particularly privacy, into their cloud-based facial recognition
technologies.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.04054v1">Controlled privacy leakage propagation throughout overlapping grouped
  learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2025-03-06T03:14:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shahrzad Kiani, Franziska Boenisch, Stark C. Draper</p>
    <p><b>Summary:</b> Federated Learning (FL) is the standard protocol for collaborative learning.
In FL, multiple workers jointly train a shared model. They exchange model
updates calculated on their data, while keeping the raw data itself local.
Since workers naturally form groups based on common interests and privacy
policies, we are motivated to extend standard FL to reflect a setting with
multiple, potentially overlapping groups. In this setup where workers can
belong and contribute to more than one group at a time, complexities arise in
understanding privacy leakage and in adhering to privacy policies. To address
the challenges, we propose differential private overlapping grouped learning
(DPOGL), a novel method to implement privacy guarantees within overlapping
groups. Under the honest-but-curious threat model, we derive novel privacy
guarantees between arbitrary pairs of workers. These privacy guarantees
describe and quantify two key effects of privacy leakage in DP-OGL: propagation
delay, i.e., the fact that information from one group will leak to other groups
only with temporal offset through the common workers and information
degradation, i.e., the fact that noise addition over model updates limits
information leakage between workers. Our experiments show that applying DP-OGL
enhances utility while maintaining strong privacy compared to standard FL
setups.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.07483v1">Poisoning Attacks to Local Differential Privacy Protocols for Trajectory
  Data</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-03-06T02:31:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> I-Jung Hsu, Chih-Hsun Lin, Chia-Mu Yu, Sy-Yen Kuo, Chun-Ying Huang</p>
    <p><b>Summary:</b> Trajectory data, which tracks movements through geographic locations, is
crucial for improving real-world applications. However, collecting such
sensitive data raises considerable privacy concerns. Local differential privacy
(LDP) offers a solution by allowing individuals to locally perturb their
trajectory data before sharing it. Despite its privacy benefits, LDP protocols
are vulnerable to data poisoning attacks, where attackers inject fake data to
manipulate aggregated results. In this work, we make the first attempt to
analyze vulnerabilities in several representative LDP trajectory protocols. We
propose \textsc{TraP}, a heuristic algorithm for data \underline{P}oisoning
attacks using a prefix-suffix method to optimize fake \underline{Tra}jectory
selection, significantly reducing computational complexity. Our experimental
results demonstrate that our attack can substantially increase target pattern
occurrences in the perturbed trajectory dataset with few fake users. This study
underscores the urgent need for robust defenses and better protocol designs to
safeguard LDP trajectory data against malicious manipulation.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.03988v1">AI-based Programming Assistants for Privacy-related Code Generation: The
  Developers' Experience</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Software Engineering-D91E36">
  <p><b>Published on:</b> 2025-03-06T00:34:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kashumi Madampe, John Grundy, Nalin Arachchilage</p>
    <p><b>Summary:</b> With the popularising of generative AI, the existence of AI-based programming
assistants for developers is no surprise. Developers increasingly use them for
their work, including generating code to fulfil the data protection
requirements (privacy) of the apps they build. We wanted to know if the reality
is the same as expectations of AI-based programming assistants when trying to
fulfil software privacy requirements, and the challenges developers face when
using AI-based programming assistants and how these can be improved. To this
end, we conducted a survey with 51 developers worldwide. We found that AI-based
programming assistants need to be improved in order for developers to better
trust them with generating code that ensures privacy. In this paper, we provide
some practical recommendations for developers to consider following when using
AI-based programming assistants for privacy-related code development, and some
key further research directions.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.03652v1">Token-Level Privacy in Large Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-05T16:27:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Re'em Harel, Niv Gilboa, Yuval Pinter</p>
    <p><b>Summary:</b> The use of language models as remote services requires transmitting private
information to external providers, raising significant privacy concerns. This
process not only risks exposing sensitive data to untrusted service providers
but also leaves it vulnerable to interception by eavesdroppers. Existing
privacy-preserving methods for natural language processing (NLP) interactions
primarily rely on semantic similarity, overlooking the role of contextual
information. In this work, we introduce dchi-stencil, a novel token-level
privacy-preserving mechanism that integrates contextual and semantic
information while ensuring strong privacy guarantees under the dchi
differential privacy framework, achieving 2epsilon-dchi-privacy. By
incorporating both semantic and contextual nuances, dchi-stencil achieves a
robust balance between privacy and utility. We evaluate dchi-stencil using
state-of-the-art language models and diverse datasets, achieving comparable and
even better trade-off between utility and privacy compared to existing methods.
This work highlights the potential of dchi-stencil to set a new standard for
privacy-preserving NLP in modern, high-risk applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.03587v1">"You don't need a university degree to comprehend data protection this
  way": LLM-Powered Interactive Privacy Policy Assessment</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-03-05T15:22:35Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Vincent Freiberger, Arthur Fleig, Erik Buchmann</p>
    <p><b>Summary:</b> Protecting online privacy requires users to engage with and comprehend
website privacy policies, but many policies are difficult and tedious to read.
We present the first qualitative user study on Large Language Model
(LLM)-driven privacy policy assessment. To this end, we build and evaluate an
LLM-based privacy policy assessment browser extension, which helps users
understand the essence of a lengthy, complex privacy policy while browsing. The
tool integrates a dashboard and an LLM chat. In our qualitative user study
(N=22), we evaluate usability, understandability of the information our tool
provides, and its impacts on awareness. While providing a comprehensible quick
overview and a chat for in-depth discussion improves privacy awareness, users
note issues with building trust in the tool. From our insights, we derive
important design implications to guide future policy analysis tools.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.03539v1">Data Sharing, Privacy and Security Considerations in the Energy Sector:
  A Review from Technical Landscape to Regulatory Specifications</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-05T14:23:56Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shiliang Zhang, Sabita Maharjan, Lee Andrew Bygrave, Shui Yu</p>
    <p><b>Summary:</b> Decarbonization, decentralization and digitalization are the three key
elements driving the twin energy transition. The energy system is evolving to a
more data driven ecosystem, leading to the need of communication and storage of
large amount of data of different resolution from the prosumers and other
stakeholders in the energy ecosystem. While the energy system is certainly
advancing, this paradigm shift is bringing in new privacy and security issues
related to collection, processing and storage of data - not only from the
technical dimension, but also from the regulatory perspective. Understanding
data privacy and security in the evolving energy system, regarding regulatory
compliance, is an immature field of research. Contextualized knowledge of how
related issues are regulated is still in its infancy, and the practical and
technical basis for the regulatory framework for data privacy and security is
not clear. To fill this gap, this paper conducts a comprehensive review of the
data-related issues for the energy system by integrating both technical and
regulatory dimensions. We start by reviewing open-access data, data
communication and data-processing techniques for the energy system, and use it
as the basis to connect the analysis of data-related issues from the integrated
perspective. We classify the issues into three categories: (i) data-sharing
among energy end users and stakeholders (ii) privacy of end users, and (iii)
cyber security, and then explore these issues from a regulatory perspective. We
analyze the evolution of related regulations, and introduce the relevant
regulatory initiatives for the categorized issues in terms of regulatory
definitions, concepts, principles, rights and obligations in the context of
energy systems. Finally, we provide reflections on the gaps that still exist,
and guidelines for regulatory frameworks for a truly participatory energy
system.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.03506v1">Rethinking Synthetic Data definitions: A privacy driven approach</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-03-05T13:54:13Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Vibeke Binz Vallevik, Serena Elizabeth Marshall, Aleksandar Babic, Jan Franz Nygaard</p>
    <p><b>Summary:</b> Synthetic data is gaining traction as a cost-effective solution for the
increasing data demands of AI development and can be generated either from
existing knowledge or derived data captured from real-world events. The source
of the synthetic data generation and the technique used significantly impacts
its residual privacy risk and therefore its opportunity for sharing.
Traditional classification of synthetic data types no longer fit the newer
generation techniques and there is a need to better align the classification
with practical needs. We suggest a new way of grouping synthetic data types
that better supports privacy evaluations to aid regulatory policymaking. Our
novel classification provides flexibility to new advancements like deep
generative methods and offers a more practical framework for future
applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.03428v1">Privacy is All You Need: Revolutionizing Wearable Health Data with
  Advanced PETs</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Emerging Technologies-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-03-05T12:01:22Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Karthik Barma, Seshu Babu Barma</p>
    <p><b>Summary:</b> In a world where data is the new currency, wearable health devices offer
unprecedented insights into daily life, continuously monitoring vital signs and
metrics. However, this convenience raises privacy concerns, as these devices
collect sensitive data that can be misused or breached. Traditional measures
often fail due to real-time data processing needs and limited device power.
Users also lack awareness and control over data sharing and usage. We propose a
Privacy-Enhancing Technology (PET) framework for wearable devices, integrating
federated learning, lightweight cryptographic methods, and selectively deployed
blockchain technology. The blockchain acts as a secure ledger triggered only
upon data transfer requests, granting users real-time notifications and
control. By dismantling data monopolies, this approach returns data sovereignty
to individuals. Through real-world applications like secure medical data
sharing, privacy-preserving fitness tracking, and continuous health monitoring,
our framework reduces privacy risks by up to 70 percent while preserving data
utility and performance. This innovation sets a new benchmark for wearable
privacy and can scale to broader IoT ecosystems, including smart homes and
industry. As data continues to shape our digital landscape, our research
underscores the critical need to maintain privacy and user control at the
forefront of technological progress.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.03267v1">Quantum-Inspired Privacy-Preserving Federated Learning Framework for
  Secure Dementia Classification</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2025-03-05T08:49:31Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Gazi Tanbhir, Md. Farhan Shahriyar</p>
    <p><b>Summary:</b> Dementia, a neurological disorder impacting millions globally, presents
significant challenges in diagnosis and patient care. With the rise of privacy
concerns and security threats in healthcare, federated learning (FL) has
emerged as a promising approach to enable collaborative model training across
decentralized datasets without exposing sensitive patient information. However,
FL remains vulnerable to advanced security breaches such as gradient inversion
and eavesdropping attacks. This paper introduces a novel framework that
integrates federated learning with quantum-inspired encryption techniques for
dementia classification, emphasizing privacy preservation and security.
Leveraging quantum key distribution (QKD), the framework ensures secure
transmission of model weights, protecting against unauthorized access and
interception during training. The methodology utilizes a convolutional neural
network (CNN) for dementia classification, with federated training conducted
across distributed healthcare nodes, incorporating QKD-encrypted weight sharing
to secure the aggregation process. Experimental evaluations conducted on MRI
data from the OASIS dataset demonstrate that the proposed framework achieves
identical accuracy levels to a baseline model while enhancing data security and
reducing loss by almost 1% compared to the classical baseline model. The
framework offers significant implications for democratizing access to AI-driven
dementia diagnostics in low- and middle-income countries, addressing critical
resource and privacy constraints. This work contributes a robust, scalable, and
secure federated learning solution for healthcare applications, paving the way
for broader adoption of quantum-inspired techniques in AI-driven medical
research.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.03146v1">PriFFT: Privacy-preserving Federated Fine-tuning of Large Language
  Models via Function Secret Sharing</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-05T03:41:57Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhichao You, Xuewen Dong, Ke Cheng, Xutong Mu, Jiaxuan Fu, Shiyang Ma, Qiang Qu, Yulong Shen</p>
    <p><b>Summary:</b> Fine-tuning large language models (LLMs) raises privacy concerns due to the
risk of exposing sensitive training data. Federated learning (FL) mitigates
this risk by keeping training samples on local devices, but recent studies show
that adversaries can still infer private information from model updates in FL.
Additionally, LLM parameters are typically shared publicly during federated
fine-tuning, while developers are often reluctant to disclose these parameters,
posing further security challenges. Inspired by the above problems, we propose
PriFFT, a privacy-preserving federated fine-tuning mechanism, to protect both
the model updates and parameters. In PriFFT, clients and the server share model
inputs and parameters by secret sharing, performing secure fine-tuning on
shared values without accessing plaintext data. Due to considerable LLM
parameters, privacy-preserving federated fine-tuning invokes complex secure
calculations and requires substantial communication and computation resources.
To optimize the efficiency of privacy-preserving federated fine-tuning of LLMs,
we introduce function secret-sharing protocols for various operations,
including reciprocal calculation, tensor products, natural exponentiation,
softmax, hyperbolic tangent, and dropout. The proposed protocols achieve up to
4.02X speed improvement and reduce 7.19X communication overhead compared to the
implementation based on existing secret sharing methods. Besides, PriFFT
achieves a 2.23X speed improvement and reduces 4.08X communication overhead in
privacy-preserving fine-tuning without accuracy drop compared to the existing
secret sharing methods.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.03087v1">"Watch My Health, Not My Data": Understanding Perceptions, Barriers,
  Emotional Impact, & Coping Strategies Pertaining to IoT Privacy and Security
  in Health Monitoring for Older Adults</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-03-05T01:04:13Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Suleiman Saka, Sanchari Das</p>
    <p><b>Summary:</b> The proliferation of "Internet of Things (IoT)" provides older adults with
critical support for "health monitoring" and independent living, yet
significant concerns about security and privacy persist. In this paper, we
report on these issues through a two-phase user study, including a survey (N =
22) and semi-structured interviews (n = 9) with adults aged 65+. We found that
while 81.82% of our participants are aware of security features like
"two-factor authentication (2FA)" and encryption, 63.64% express serious
concerns about unauthorized access to sensitive health data. Only 13.64% feel
confident in existing protections, citing confusion over "data sharing
policies" and frustration with "complex security settings" which lead to
distrust and anxiety. To cope, our participants adopt various strategies, such
as relying on family or professional support and limiting feature usage leading
to disengagement. Thus, we recommend "adaptive security mechanisms," simplified
interfaces, and real-time transparency notifications to foster trust and ensure
"privacy and security by design" in IoT health systems for older adults.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.03043v1">Leveraging Randomness in Model and Data Partitioning for Privacy
  Amplification</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-04T22:49:59Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Andy Dong, Wei-Ning Chen, Ayfer Ozgur</p>
    <p><b>Summary:</b> We study how inherent randomness in the training process -- where each sample
(or client in federated learning) contributes only to a randomly selected
portion of training -- can be leveraged for privacy amplification. This
includes (1) data partitioning, where a sample participates in only a subset of
training iterations, and (2) model partitioning, where a sample updates only a
subset of the model parameters. We apply our framework to model parallelism in
federated learning, where each client updates a randomly selected subnetwork to
reduce memory and computational overhead, and show that existing methods, e.g.
model splitting or dropout, provide a significant privacy amplification gain
not captured by previous privacy analysis techniques. Additionally, we
introduce Balanced Iteration Subsampling, a new data partitioning method where
each sample (or client) participates in a fixed number of training iterations.
We show that this method yields stronger privacy amplification than Poisson
(i.i.d.) sampling of data (or clients). Our results demonstrate that randomness
in the training process, which is structured rather than i.i.d. and interacts
with data in complex ways, can be systematically leveraged for significant
privacy amplification.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.02968v1">Privacy-Preserving Fair Synthetic Tabular Data</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-04T19:51:00Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Fatima J. Sarmin, Atiquer R. Rahman, Christopher J. Henry, Noman Mohammed</p>
    <p><b>Summary:</b> Sharing of tabular data containing valuable but private information is
limited due to legal and ethical issues. Synthetic data could be an alternative
solution to this sharing problem, as it is artificially generated by machine
learning algorithms and tries to capture the underlying data distribution.
However, machine learning models are not free from memorization and may
introduce biases, as they rely on training data. Producing synthetic data that
preserves privacy and fairness while maintaining utility close to the real data
is a challenging task. This research simultaneously addresses both the privacy
and fairness aspects of synthetic data, an area not explored by other studies.
In this work, we present PF-WGAN, a privacy-preserving, fair synthetic tabular
data generator based on the WGAN-GP model. We have modified the original
WGAN-GP by adding privacy and fairness constraints forcing it to produce
privacy-preserving fair data. This approach will enable the publication of
datasets that protect individual's privacy and remain unbiased toward any
particular group. We compared the results with three state-of-the-art synthetic
data generator models in terms of utility, privacy, and fairness across four
different datasets. We found that the proposed model exhibits a more balanced
trade-off among utility, privacy, and fairness.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.02862v1">Privacy and Accuracy-Aware AI/ML Model Deduplication</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB">
  <p><b>Published on:</b> 2025-03-04T18:40:38Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Hong Guan, Lei Yu, Lixi Zhou, Li Xiong, Kanchan Chowdhury, Lulu Xie, Xusheng Xiao, Jia Zou</p>
    <p><b>Summary:</b> With the growing adoption of privacy-preserving machine learning algorithms,
such as Differentially Private Stochastic Gradient Descent (DP-SGD), training
or fine-tuning models on private datasets has become increasingly prevalent.
This shift has led to the need for models offering varying privacy guarantees
and utility levels to satisfy diverse user requirements. However, managing
numerous versions of large models introduces significant operational
challenges, including increased inference latency, higher resource consumption,
and elevated costs. Model deduplication is a technique widely used by many
model serving and database systems to support high-performance and low-cost
inference queries and model diagnosis queries. However, none of the existing
model deduplication works has considered privacy, leading to unbounded
aggregation of privacy costs for certain deduplicated models and inefficiencies
when applied to deduplicate DP-trained models. We formalize the problems of
deduplicating DP-trained models for the first time and propose a novel privacy-
and accuracy-aware deduplication mechanism to address the problems. We
developed a greedy strategy to select and assign base models to target models
to minimize storage and privacy costs. When deduplicating a target model, we
dynamically schedule accuracy validations and apply the Sparse Vector Technique
to reduce the privacy costs associated with private validation data. Compared
to baselines that do not provide privacy guarantees, our approach improved the
compression ratio by up to $35\times$ for individual models (including large
language models and vision transformers). We also observed up to $43\times$
inference speedup due to the reduction of I/O operations.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.02693v1">Federated Learning for Privacy-Preserving Feedforward Control in
  Multi-Agent Systems</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Multiagent Systems-662E9B">
  <p><b>Published on:</b> 2025-03-04T15:07:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jakob Weber, Markus Gurtner, Benedikt Alt, Adrian Trachte, Andreas Kugi</p>
    <p><b>Summary:</b> Feedforward control (FF) is often combined with feedback control (FB) in many
control systems, improving tracking performance, efficiency, and stability.
However, designing effective data-driven FF controllers in multi-agent systems
requires significant data collection, including transferring private or
proprietary data, which raises privacy concerns and incurs high communication
costs. Therefore, we propose a novel approach integrating Federated Learning
(FL) into FF control to address these challenges. This approach enables
privacy-preserving, communication-efficient, and decentralized continuous
improvement of FF controllers across multiple agents without sharing personal
or proprietary data. By leveraging FL, each agent learns a local, neural FF
controller using its data and contributes only model updates to a global
aggregation process, ensuring data privacy and scalability. We demonstrate the
effectiveness of our method in an autonomous driving use case. Therein,
vehicles equipped with a trajectory-tracking feedback controller are enhanced
by FL-based neural FF control. Simulations highlight significant improvements
in tracking performance compared to pure FB control, analogous to model-based
FF control. We achieve comparable tracking performance without exchanging
private vehicle-specific data compared to a centralized neural FF control. Our
results underscore the potential of FL-based neural FF control to enable
privacy-preserving learning in multi-agent control systems, paving the way for
scalable and efficient autonomous systems applications.</p>
  </details>
</div>

