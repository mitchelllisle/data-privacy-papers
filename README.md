
<h2>2025-01</h2>

<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.19223v1">Through the Looking Glass: LLM-Based Analysis of AR/VR Android
  Applications Privacy Policies</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-01-31T15:30:14Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Abdulaziz Alghamdi, David Mohaisen</p>
    <p><b>Summary:</b> \begin{abstract} This paper comprehensively analyzes privacy policies in
AR/VR applications, leveraging BERT, a state-of-the-art text classification
model, to evaluate the clarity and thoroughness of these policies. By comparing
the privacy policies of AR/VR applications with those of free and premium
websites, this study provides a broad perspective on the current state of
privacy practices within the AR/VR industry. Our findings indicate that AR/VR
applications generally offer a higher percentage of positive segments than free
content but lower than premium websites. The analysis of highlighted segments
and words revealed that AR/VR applications strategically emphasize critical
privacy practices and key terms. This enhances privacy policies' clarity and
effectiveness.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.18862v1">Scalable Distributed Reproduction Numbers of Network Epidemics with
  Differential Privacy</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36">
  <p><b>Published on:</b> 2025-01-31T03:08:57Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Bo Chen, Baike She, Calvin Hawkins, Philip E. Par√©, Matthew T. Hale</p>
    <p><b>Summary:</b> Reproduction numbers are widely used for the estimation and prediction of
epidemic spreading processes over networks. However, conventional reproduction
numbers of an overall network do not indicate where an epidemic is spreading.
Therefore, we propose a novel notion of local distributed reproduction numbers
to capture the spreading behaviors of each node in a network. We first show how
to compute them and then use them to derive new conditions under which an
outbreak can occur. These conditions are then used to derive new conditions for
the existence, uniqueness, and stability of equilibrium states of the
underlying epidemic model. Building upon these local distributed reproduction
numbers, we define cluster distributed reproduction numbers to model the spread
between clusters composed of nodes. Furthermore, we demonstrate that the local
distributed reproduction numbers can be aggregated into cluster distributed
reproduction numbers at different scales. However, both local and cluster
distributed reproduction numbers can reveal the frequency of interactions
between nodes in a network, which raises privacy concerns. Thus, we next
develop a privacy framework that implements a differential privacy mechanism to
provably protect the frequency of interactions between nodes when computing
distributed reproduction numbers. Numerical experiments show that, even under
differential privacy, the distributed reproduction numbers provide accurate
estimates of the epidemic spread while also providing more insights than
conventional reproduction numbers.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.18727v1">Exploring Audio Editing Features as User-Centric Privacy Defenses
  Against Emotion Inference Attacks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Sound-D91E36"> 
  <p><b>Published on:</b> 2025-01-30T20:07:44Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mohd. Farhan Israk Soumik, W. K. M. Mithsara, Abdur R. Shahid, Ahmed Imteaj</p>
    <p><b>Summary:</b> The rapid proliferation of speech-enabled technologies, including virtual
assistants, video conferencing platforms, and wearable devices, has raised
significant privacy concerns, particularly regarding the inference of sensitive
emotional information from audio data. Existing privacy-preserving methods
often compromise usability and security, limiting their adoption in practical
scenarios. This paper introduces a novel, user-centric approach that leverages
familiar audio editing techniques, specifically pitch and tempo manipulation,
to protect emotional privacy without sacrificing usability. By analyzing
popular audio editing applications on Android and iOS platforms, we identified
these features as both widely available and usable. We rigorously evaluated
their effectiveness against a threat model, considering adversarial attacks
from diverse sources, including Deep Neural Networks (DNNs), Large Language
Models (LLMs), and and reversibility testing. Our experiments, conducted on
three distinct datasets, demonstrate that pitch and tempo manipulation
effectively obfuscates emotional data. Additionally, we explore the design
principles for lightweight, on-device implementation to ensure broad
applicability across various devices and platforms.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.18174v1">Advancing Personalized Federated Learning: Integrative Approaches with
  AI for Enhanced Privacy and Customization</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> 
  <p><b>Published on:</b> 2025-01-30T07:03:29Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kevin Cooper, Michael Geller</p>
    <p><b>Summary:</b> In the age of data-driven decision making, preserving privacy while providing
personalized experiences has become paramount. Personalized Federated Learning
(PFL) offers a promising framework by decentralizing the learning process, thus
ensuring data privacy and reducing reliance on centralized data repositories.
However, the integration of advanced Artificial Intelligence (AI) techniques
within PFL remains underexplored. This paper proposes a novel approach that
enhances PFL with cutting-edge AI methodologies including adaptive
optimization, transfer learning, and differential privacy. We present a model
that not only boosts the performance of individual client models but also
ensures robust privacy-preserving mechanisms and efficient resource utilization
across heterogeneous networks. Empirical results demonstrate significant
improvements in model accuracy and personalization, along with stringent
privacy adherence, as compared to conventional federated learning models. This
work paves the way for a new era of truly personalized and privacy-conscious AI
systems, offering significant implications for industries requiring compliance
with stringent data protection regulations.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.17762v2">Improving Privacy Benefits of Redaction</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-01-29T16:53:16Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Vaibhav Gusain, Douglas Leith</p>
    <p><b>Summary:</b> We propose a novel redaction methodology that can be used to sanitize natural
text data. Our new technique provides better privacy benefits than other state
of the art techniques while maintaining lower redaction levels.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.17750v1">Privacy Audit as Bits Transmission: (Im)possibilities for Audit by One
  Run</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-01-29T16:38:51Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zihang Xiang, Tianhao Wang, Di Wang</p>
    <p><b>Summary:</b> Auditing algorithms' privacy typically involves simulating a game-based
protocol that guesses which of two adjacent datasets was the original input.
Traditional approaches require thousands of such simulations, leading to
significant computational overhead. Recent methods propose single-run auditing
of the target algorithm to address this, substantially reducing computational
cost. However, these methods' general applicability and tightness in producing
empirical privacy guarantees remain uncertain.
  This work studies such problems in detail. Our contributions are twofold:
First, we introduce a unifying framework for privacy audits based on
information-theoretic principles, modeling the audit as a bit transmission
problem in a noisy channel. This formulation allows us to derive fundamental
limits and develop an audit approach that yields tight privacy lower bounds for
various DP protocols. Second, leveraging this framework, we demystify the
method of privacy audit by one run, identifying the conditions under which
single-run audits are feasible or infeasible. Our analysis provides general
guidelines for conducting privacy audits and offers deeper insights into the
privacy audit.
  Finally, through experiments, we demonstrate that our approach produces
tighter privacy lower bounds on common differentially private mechanisms while
requiring significantly fewer observations. We also provide a case study
illustrating that our method successfully detects privacy violations in flawed
implementations of private algorithms.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.17634v1">Federated Learning With Individualized Privacy Through Client Sampling</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-01-29T13:11:21Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Lucas Lange, Ole Borchardt, Erhard Rahm</p>
    <p><b>Summary:</b> With growing concerns about user data collection, individualized privacy has
emerged as a promising solution to balance protection and utility by accounting
for diverse user privacy preferences. Instead of enforcing a uniform level of
anonymization for all users, this approach allows individuals to choose privacy
settings that align with their comfort levels. Building on this idea, we
propose an adapted method for enabling Individualized Differential Privacy
(IDP) in Federated Learning (FL) by handling clients according to their
personal privacy preferences. By extending the SAMPLE algorithm from
centralized settings to FL, we calculate client-specific sampling rates based
on their heterogeneous privacy budgets and integrate them into a modified
IDP-FedAvg algorithm. We test this method under realistic privacy distributions
and multiple datasets. The experimental results demonstrate that our approach
achieves clear improvements over uniform DP baselines, reducing the trade-off
between privacy and utility. Compared to the alternative SCALE method in
related work, which assigns differing noise scales to clients, our method
performs notably better. However, challenges remain for complex tasks with
non-i.i.d. data, primarily stemming from the constraints of the decentralized
setting.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.17089v1">CRSet: Non-Interactive Verifiable Credential Revocation with Metadata
  Privacy for Issuers and Everyone Else</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-01-28T17:23:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Felix Hoops, Jonas Gebele, Florian Matthes</p>
    <p><b>Summary:</b> Like any digital certificate, Verifiable Credentials (VCs) require a way to
revoke them in case of an error or key compromise. Existing solutions for VC
revocation, most prominently Bitstring Status List, are not viable for many use
cases since they leak the issuer's behavior, which in turn leaks internal
business metrics. For instance, exact staff fluctuation through issuance and
revocation of employee IDs. We introduce CRSet, a revocation mechanism that
allows an issuer to encode revocation information for years worth of VCs as a
Bloom filter cascade. Padding is used to provide deniability for issuer
metrics. Issuers periodically publish this filter cascade on a decentralized
storage system. Relying Parties (RPs) can download it to perform any number of
revocation checks locally. Compared to existing solutions, CRSet protects the
metadata of subject, RPs, and issuer equally. At the same time, it is
non-interactive, making it work with wallet devices having limited hardware
power and drop-in compatible with existing VC exchange protocols and wallet
applications. We present a prototype using the Ethereum blockchain as
decentralized storage. The recently introduced blob-carrying transactions,
enabling cheaper data writes, allow us to write each CRSet directly to the
chain. We built software for issuers and RPs that we successfully tested
end-to-end with an existing publicly available wallet agents and the OpenID for
Verifiable Credentials protocols. Storage and bandwidth costs paid by issuers
and RP are higher than for Bitstring Status List, but still manageable at
around 1 MB for an issuer issuing hundreds of thousands of VCs annually and
covering decades.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.16885v1">"My Whereabouts, my Location, it's Directly Linked to my Physical
  Security": An Exploratory Qualitative Study of Location-Dependent Security
  and Privacy Perceptions among Activist Tech Users</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2025-01-28T12:13:53Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Christian Eichenm√ºller, Lisa Kuhn, Zinaida Benenson</p>
    <p><b>Summary:</b> Digital-safety research with at-risk users is particularly urgent. At-risk
users are more likely to be digitally attacked or targeted by surveillance and
could be disproportionately harmed by attacks that facilitate physical
assaults. One group of such at-risk users are activists and politically active
individuals. For them, as for other at-risk users, the rise of smart
environments harbors new risks. Since digitization and datafication are no
longer limited to a series of personal devices that can be switched on and off,
but increasingly and continuously surround users, granular geolocation poses
new safety challenges. Drawing on eight exploratory qualitative interviews of
an ongoing research project, this contribution highlights what activists with
powerful adversaries think about evermore data traces, including location data,
and how they intend to deal with emerging risks. Responses of activists include
attempts to control one's immediate technological surroundings and to more
carefully manage device-related location data. For some activists, threat
modeling has also shaped provider choices based on geopolitical considerations.
Since many activists have not enough digital-safety knowledge for effective
protection, feelings of insecurity and paranoia are widespread. Channeling the
concerns and fears of our interlocutors, we call for more research on how
activists can protect themselves against evermore fine-grained location data
tracking.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.16307v1">Privacy-aware Nash Equilibrium Synthesis with Partially Ordered LTL$_f$
  Objectives</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Science and Game Theory-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Logic in Computer Science-662E9B">
  <p><b>Published on:</b> 2025-01-27T18:46:15Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Caleb Probine, Abhishek Kulkarni, Ufuk Topcu</p>
    <p><b>Summary:</b> Nash equilibrium is a fundamental solution concept for modeling the behavior
of self-interested agents. We develop an algorithm to synthesize pure Nash
equilibria in two-player deterministic games on graphs where players have
partial preferences over objectives expressed with linear temporal logic over
finite traces. Previous approaches for Nash equilibrium synthesis assume that
players' preferences are common knowledge. Instead, we allow players'
preferences to remain private but enable communication between players. The
algorithm we design synthesizes Nash equilibria for a complete-information
game, but synthesizes these equilibria in an incomplete-information setting
where players' preferences are private. The algorithm is privacy-aware, as
instead of requiring that players share private preferences, the algorithm
reduces the information sharing to a query interface. Through this interface,
players exchange information about states in the game from which they can
enforce a more desirable outcome. We prove the algorithm's completeness by
showing that it either returns an equilibrium or certifies that one does not
exist. We then demonstrate, via numerical examples, the existence of games
where the queries the players exchange are insufficient to reconstruct players'
preferences, highlighting the privacy-aware nature of the algorithm we propose.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.16033v1">PRISMe: A Novel LLM-Powered Tool for Interactive Privacy Policy
  Assessment</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> 
  <p><b>Published on:</b> 2025-01-27T13:27:04Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Vincent Freiberger, Arthur Fleig, Erik Buchmann</p>
    <p><b>Summary:</b> Protecting online privacy requires users to engage with and comprehend
website privacy policies, but many policies are difficult and tedious to read.
We present PRISMe (Privacy Risk Information Scanner for Me), a novel Large
Language Model (LLM)-driven privacy policy assessment tool, which helps users
to understand the essence of a lengthy, complex privacy policy while browsing.
The tool, a browser extension, integrates a dashboard and an LLM chat. One
major contribution is the first rigorous evaluation of such a tool. In a
mixed-methods user study (N=22), we evaluate PRISMe's efficiency, usability,
understandability of the provided information, and impacts on awareness. While
our tool improves privacy awareness by providing a comprehensible quick
overview and a quality chat for in-depth discussion, users note issues with
consistency and building trust in the tool. From our insights, we derive
important design implications to guide future policy analysis tools.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.18625v1">DUEF-GA: Data Utility and Privacy Evaluation Framework for Graph
  Anonymization</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-01-27T12:22:40Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jordi Casas-Roma</p>
    <p><b>Summary:</b> Anonymization of graph-based data is a problem which has been widely studied
over the last years and several anonymization methods have been developed.
Information loss measures have been used to evaluate data utility and
information loss in the anonymized graphs. However, there is no consensus about
how to evaluate data utility and information loss in privacy-preserving and
anonymization scenarios, where the anonymous datasets were perturbed to hinder
re-identification processes. Authors use diverse metrics to evaluate data
utility and, consequently, it is complex to compare different methods or
algorithms in literature. In this paper we propose a framework to evaluate and
compare anonymous datasets in a common way, providing an objective score to
clearly compare methods and algorithms. Our framework includes metrics based on
generic information loss measures, such as average distance or betweenness
centrality, and also task-specific information loss measures, such as community
detection or information flow. Additionally, we provide some metrics to examine
re-identification and risk assessment. We demonstrate that our framework could
help researchers and practitioners to select the best parametrization and/or
algorithm to reduce information loss and maximize data utility.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.15751v1">A Privacy Model for Classical & Learned Bloom Filters</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-01-27T03:35:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Hayder Tirmazi</p>
    <p><b>Summary:</b> The Classical Bloom Filter (CBF) is a class of Probabilistic Data Structures
(PDS) for handling Approximate Query Membership (AMQ). The Learned Bloom Filter
(LBF) is a recently proposed class of PDS that combines the Classical Bloom
Filter with a Learning Model while preserving the Bloom Filter's one-sided
error guarantees. Bloom Filters have been used in settings where inputs are
sensitive and need to be private in the presence of an adversary with access to
the Bloom Filter through an API or in the presence of an adversary who has
access to the internal state of the Bloom Filter. Prior work has investigated
the privacy of the Classical Bloom Filter providing attacks and defenses under
various privacy definitions. In this work, we formulate a stronger differential
privacy-based model for the Bloom Filter. We propose constructions of the
Classical and Learned Bloom Filter that satisfy $(\epsilon, 0)$-differential
privacy. This is also the first work that analyses and addresses the privacy of
the Learned Bloom Filter under any rigorous model, which is an open problem.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.15744v1">Noise disturbance and lack of privacy: Modeling acoustic dissatisfaction
  in open-plan offices</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Sound-D91E36">
  <p><b>Published on:</b> 2025-01-27T03:10:07Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Manuj Yadav, Jungsoo Kim, Valtteri Hongisto, Densil Cabrera, Richard de Dear</p>
    <p><b>Summary:</b> Open-plan offices are well-known to be adversely affected by acoustic issues.
This study aims to model acoustic dissatisfaction using measurements of room
acoustics, sound environment during occupancy, and occupant surveys (n = 349)
in 28 offices representing a diverse range of workplace parameters. As latent
factors, the contribution of $\textit{lack of privacy}$ (LackPriv) was 25%
higher than $\textit{noise disturbance}$ (NseDstrb) in predicting
$\textit{acoustic dissatisfaction}$ (AcDsat). Room acoustic metrics based on
sound pressure level (SPL) decay of speech ($L_{\text{p,A,s,4m}}$ and
$r_{\text{C}}$) were better in predicting these factors than distraction
distance ($r_{\text{D}}$) based on speech transmission index. This contradicts
previous findings, and the trends for SPL-based metrics in predicting AcDsat
and LackPriv go against expectations based on ISO 3382-3. For sound during
occupation, $L_{\text{A,90}}$ and psychoacoustic loudness ($N_{\text{90}}$)
predicted AcDsat, and a SPL fluctuation metric ($M_{\text{A,eq}}$) predicted
LackPriv. However, these metrics were weaker predictors than ISO 3382-3
metrics. Medium-sized offices exhibited higher dissatisfaction than larger
($\geq$50 occupants) offices. Dissatisfaction varied substantially across
parameters including ceiling heights, number of workstations, and years of
work, but not between offices with fixed seating compared to more flexible and
activity-based working configurations. Overall, these findings highlight the
complexities in characterizing occupants' perceptions using instrumental
acoustic measurements.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.15653v1">A Privacy Enhancing Technique to Evade Detection by Street Video Cameras
  Without Using Adversarial Accessories</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-01-26T19:29:49Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jacob Shams, Ben Nassi, Satoru Koda, Asaf Shabtai, Yuval Elovici</p>
    <p><b>Summary:</b> In this paper, we propose a privacy-enhancing technique leveraging an
inherent property of automatic pedestrian detection algorithms, namely, that
the training of deep neural network (DNN) based methods is generally performed
using curated datasets and laboratory settings, while the operational areas of
these methods are dynamic real-world environments. In particular, we leverage a
novel side effect of this gap between the laboratory and the real world:
location-based weakness in pedestrian detection. We demonstrate that the
position (distance, angle, height) of a person, and ambient light level,
directly impact the confidence of a pedestrian detector when detecting the
person. We then demonstrate that this phenomenon is present in pedestrian
detectors observing a stationary scene of pedestrian traffic, with blind spot
areas of weak detection of pedestrians with low confidence. We show how
privacy-concerned pedestrians can leverage these blind spots to evade detection
by constructing a minimum confidence path between two points in a scene,
reducing the maximum confidence and average confidence of the path by up to
0.09 and 0.13, respectively, over direct and random paths through the scene. To
counter this phenomenon, and force the use of more costly and sophisticated
methods to leverage this vulnerability, we propose a novel countermeasure to
improve the confidence of pedestrian detectors in blind spots, raising the
max/average confidence of paths generated by our technique by 0.09 and 0.05,
respectively. In addition, we demonstrate that our countermeasure improves a
Faster R-CNN-based pedestrian detector's TPR and average true positive
confidence by 0.03 and 0.15, respectively.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.15395v1">Hiding in Plain Sight: An IoT Traffic Camouflage Framework for Enhanced
  Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Networking and Internet Architecture-04E762">
  <p><b>Published on:</b> 2025-01-26T04:33:44Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Daniel Adu Worae, Spyridon Mastorakis</p>
    <p><b>Summary:</b> The rapid growth of Internet of Things (IoT) devices has introduced
significant challenges to privacy, particularly as network traffic analysis
techniques evolve. While encryption protects data content, traffic attributes
such as packet size and timing can reveal sensitive information about users and
devices. Existing single-technique obfuscation methods, such as packet padding,
often fall short in dynamic environments like smart homes due to their
predictability, making them vulnerable to machine learning-based attacks. This
paper introduces a multi-technique obfuscation framework designed to enhance
privacy by disrupting traffic analysis. The framework leverages six
techniques-Padding, Padding with XORing, Padding with Shifting, Constant Size
Padding, Fragmentation, and Delay Randomization-to obscure traffic patterns
effectively. Evaluations on three public datasets demonstrate significant
reductions in classifier performance metrics, including accuracy, precision,
recall, and F1 score. We assess the framework's robustness against adversarial
tactics by retraining and fine-tuning neural network classifiers on obfuscated
traffic. The results reveal a notable degradation in classifier performance,
underscoring the framework's resilience against adaptive attacks. Furthermore,
we evaluate communication and system performance, showing that higher
obfuscation levels enhance privacy but may increase latency and communication
overhead.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.15363v1">AI-Driven Secure Data Sharing: A Trustworthy and Privacy-Preserving
  Approach</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-01-26T02:03:19Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Al Amin, Kamrul Hasan, Sharif Ullah, Liang Hong</p>
    <p><b>Summary:</b> In the era of data-driven decision-making, ensuring the privacy and security
of shared data is paramount across various domains. Applying existing deep
neural networks (DNNs) to encrypted data is critical and often compromises
performance, security, and computational overhead. To address these
limitations, this research introduces a secure framework consisting of a
learnable encryption method based on the block-pixel operation to encrypt the
data and subsequently integrate it with the Vision Transformer (ViT). The
proposed framework ensures data privacy and security by creating unique
scrambling patterns per key, providing robust performance against adversarial
attacks without compromising computational efficiency and data integrity. The
framework was tested on sensitive medical datasets to validate its efficacy,
proving its ability to handle highly confidential information securely. The
suggested framework was validated with a 94\% success rate after extensive
testing on real-world datasets, such as MRI brain tumors and histological scans
of lung and colon cancers. Additionally, the framework was tested under diverse
adversarial attempts against secure data sharing with optimum performance and
demonstrated its effectiveness in various threat scenarios. These comprehensive
analyses underscore its robustness, making it a trustworthy solution for secure
data sharing in critical applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.15032v1">Stealthy Voice Eavesdropping with Acoustic Metamaterials: Unraveling a
  New Privacy Threat</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Sound-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2025-01-25T02:30:03Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhiyuan Ning, Zhanyong Tang, Juan He, Weizhi Meng, Yuntian Chen</p>
    <p><b>Summary:</b> We present SuperEar, a novel privacy threat based on acoustic metamaterials.
Unlike previous research, SuperEar can surreptitiously track and eavesdrop on
the phone calls of a moving outdoor target from a safe distance. To design this
attack, SuperEar overcomes the challenges faced by traditional acoustic
metamaterials, including low low-frequency gain and audio distortion during
reconstruction. It successfully magnifies the speech signal by approximately 20
times, allowing the sound to be captured from the earpiece of the target phone.
In addition, SuperEar optimizes the trade-off between the number and size of
acoustic metamaterials, improving the portability and concealability of the
interceptor while ensuring effective interception performance. This makes it
highly suitable for outdoor tracking and eavesdropping scenarios. Through
extensive experimentation, we have evaluated SuperEar and our results show that
it can achieve an eavesdropping accuracy of over 80% within a range of 4.5
meters in the aforementioned scenario, thus validating its great potential in
real-world applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.14974v1">Private Minimum Hellinger Distance Estimation via Hellinger Distance
  Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Statistics Theory-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Probability-5BC0EB">    
  <p><b>Published on:</b> 2025-01-24T23:15:04Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Fengnan Deng, Anand N. Vidyashankar</p>
    <p><b>Summary:</b> Hellinger distance has been widely used to derive objective functions that
are alternatives to maximum likelihood methods. Motivated by recent regulatory
privacy requirements, estimators satisfying differential privacy constraints
are being derived. In this paper, we describe different notions of privacy
using divergences and establish that Hellinger distance minimizes the added
variance within the class of power divergences for an additive Gaussian
mechanism. We demonstrate that a new definition of privacy, namely Hellinger
differential privacy, shares several features of the standard notion of
differential privacy while allowing for sharper inference. Using these
properties, we develop private versions of gradient descent and Newton-Raphson
algorithms for obtaining private minimum Hellinger distance estimators, which
are robust and first-order efficient. Using numerical experiments, we
illustrate the finite sample performance and verify that they retain their
robustness properties under gross-error contamination.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.14928v1">Decision Making in Changing Environments: Robustness, Query-Based
  Learning, and Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36">  <img alt="Category Badge" src="https://img.shields.io/badge/Statistics Theory-D91E36">  
  <p><b>Published on:</b> 2025-01-24T21:31:50Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Fan Chen, Alexander Rakhlin</p>
    <p><b>Summary:</b> We study the problem of interactive decision making in which the underlying
environment changes over time subject to given constraints. We propose a
framework, which we call \textit{hybrid Decision Making with Structured
Observations} (hybrid DMSO), that provides an interpolation between the
stochastic and adversarial settings of decision making. Within this framework,
we can analyze local differentially private (LDP) decision making, query-based
learning (in particular, SQ learning), and robust and smooth decision making
under the same umbrella, deriving upper and lower bounds based on variants of
the Decision-Estimation Coefficient (DEC). We further establish strong
connections between the DEC's behavior, the SQ dimension, local minimax
complexity, learnability, and joint differential privacy. To showcase the
framework's power, we provide new results for contextual bandits under the LDP
constraint.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.14453v1">Optimal Strategies for Federated Learning Maintaining Client Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-01-24T12:34:38Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Uday Bhaskar, Varul Srivastava, Avyukta Manjunatha Vummintala, Naresh Manwani, Sujit Gujar</p>
    <p><b>Summary:</b> Federated Learning (FL) emerged as a learning method to enable the server to
train models over data distributed among various clients. These clients are
protective about their data being leaked to the server, any other client, or an
external adversary, and hence, locally train the model and share it with the
server rather than sharing the data. The introduction of sophisticated
inferencing attacks enabled the leakage of information about data through
access to model parameters. To tackle this challenge, privacy-preserving
federated learning aims to achieve differential privacy through learning
algorithms like DP-SGD. However, such methods involve adding noise to the
model, data, or gradients, reducing the model's performance.
  This work provides a theoretical analysis of the tradeoff between model
performance and communication complexity of the FL system. We formally prove
that training for one local epoch per global round of training gives optimal
performance while preserving the same privacy budget. We also investigate the
change of utility (tied to privacy) of FL models with a change in the number of
clients and observe that when clients are training using DP-SGD and argue that
for the same privacy budget, the utility improved with increased clients. We
validate our findings through experiments on real-world datasets. The results
from this paper aim to improve the performance of privacy-preserving federated
learning systems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.14313v1">Between Close Enough to Reveal and Far Enough to Protect: a New Privacy
  Region for Correlated Data</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2025-01-24T08:14:54Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Luis Ma√üny, Rawad Bitar, Fangwei Ye, Salim El Rouayheb</p>
    <p><b>Summary:</b> When users make personal privacy choices, correlation between their data can
cause inadvertent leakage about users who do not want to share their data by
other users sharing their data. As a solution, we consider local redaction
mechanisms. As prior works proposed data-independent privatization mechanisms,
we study the family of data-independent local redaction mechanisms and
upper-bound their utility when data correlation is modeled by a stationary
Markov process. In contrast, we derive a novel data-dependent mechanism, which
improves the utility by leveraging a data-dependent leakage measure.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.14309v1">BrainGuard: Privacy-Preserving Multisubject Image Reconstructions from
  Brain Activities</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-01-24T08:10:47Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhibo Tian, Ruijie Quan, Fan Ma, Kun Zhan, Yi Yang</p>
    <p><b>Summary:</b> Reconstructing perceived images from human brain activity forms a crucial
link between human and machine learning through Brain-Computer Interfaces.
Early methods primarily focused on training separate models for each individual
to account for individual variability in brain activity, overlooking valuable
cross-subject commonalities. Recent advancements have explored multisubject
methods, but these approaches face significant challenges, particularly in data
privacy and effectively managing individual variability. To overcome these
challenges, we introduce BrainGuard, a privacy-preserving collaborative
training framework designed to enhance image reconstruction from multisubject
fMRI data while safeguarding individual privacy. BrainGuard employs a
collaborative global-local architecture where individual models are trained on
each subject's local data and operate in conjunction with a shared global model
that captures and leverages cross-subject patterns. This architecture
eliminates the need to aggregate fMRI data across subjects, thereby ensuring
privacy preservation. To tackle the complexity of fMRI data, BrainGuard
integrates a hybrid synchronization strategy, enabling individual models to
dynamically incorporate parameters from the global model. By establishing a
secure and collaborative training environment, BrainGuard not only protects
sensitive brain data but also improves the image reconstructions accuracy.
Extensive experiments demonstrate that BrainGuard sets a new benchmark in both
high-level and low-level metrics, advancing the state-of-the-art in brain
decoding through its innovative design.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.14184v2">Tight Sample Complexity Bounds for Parameter Estimation Under Quantum
  Differential Privacy for Qubits</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2025-01-24T02:23:51Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Farhad Farokhi</p>
    <p><b>Summary:</b> This short note provides tight upper and lower bounds for minimal number of
samples (copies of quantum states) required to attain a prescribed accuracy
(measured by error variance) for scalar parameters using unbiased estimators
under quantum local differential privacy for qubits. In the small privacy
budget $\epsilon$ regime, i.e., $\epsilon\ll 1$, the sample complexity scales
as $\Theta(\epsilon^{-2})$. This bound matches that of classical parameter
estimation under differential privacy. The lower bound loosens (converges to
zero) in the large privacy budget regime, i.e., $\epsilon\gg 1$, but that case
is not particularly interesting as tight bounds for parameter estimation in the
noiseless case are widely known. That being said, extensions to systems with
higher dimensions and tightening the bounds for the large privacy budget regime
are interesting avenues for future research.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.14098v1">Exploring User Perspectives on Data Collection, Data Sharing
  Preferences, and Privacy Concerns with Remote Healthcare Technology</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-01-23T21:09:03Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Daniela Napoli, Heather Molyneaux, Helene Fournier, Sonia Chiasson</p>
    <p><b>Summary:</b> Remote healthcare technology can help tackle societal issues by improving
access to quality healthcare services and enhancing diagnoses through in-place
monitoring. These services can be implemented through a combination of mobile
devices, applications, wearable sensors, and other smart technology. It is
paramount to handle sensitive data that is collected in ways that meet users'
privacy expectations. We surveyed 384 people in Canada aged 20 to 93 years old
to explore participants' comfort with data collection, sharing preferences, and
potential privacy concerns related to remote healthcare technology. We explore
these topics within the context of various healthcare scenarios including
health emergencies and managing chronic health conditions.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.13916v2">PBM-VFL: Vertical Federated Learning with Feature and Sample Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-01-23T18:53:43Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Linh Tran, Timothy Castiglia, Stacy Patterson, Ana Milanova</p>
    <p><b>Summary:</b> We present Poisson Binomial Mechanism Vertical Federated Learning (PBM-VFL),
a communication-efficient Vertical Federated Learning algorithm with
Differential Privacy guarantees. PBM-VFL combines Secure Multi-Party
Computation with the recently introduced Poisson Binomial Mechanism to protect
parties' private datasets during model training. We define the novel concept of
feature privacy and analyze end-to-end feature and sample privacy of our
algorithm. We compare sample privacy loss in VFL with privacy loss in HFL. We
also provide the first theoretical characterization of the relationship between
privacy budget, convergence error, and communication cost in
differentially-private VFL. Finally, we empirically show that our model
performs well with high levels of privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.13904v2">Privacy-Preserving Personalized Federated Prompt Learning for Multimodal
  Large Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-01-23T18:34:09Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Linh Tran, Wei Sun, Stacy Patterson, Ana Milanova</p>
    <p><b>Summary:</b> Multimodal Large Language Models (LLMs) are pivotal in revolutionizing
customer support and operations by integrating multiple modalities such as
text, images, and audio. Federated Prompt Learning (FPL) is a recently proposed
approach that combines pre-trained multimodal LLMs such as vision-language
models with federated learning to create personalized, privacy-preserving AI
systems. However, balancing the competing goals of personalization,
generalization, and privacy remains a significant challenge.
Over-personalization can lead to overfitting, reducing generalizability, while
stringent privacy measures, such as differential privacy, can hinder both
personalization and generalization. In this paper, we propose a Differentially
Private Federated Prompt Learning (DP-FPL) approach to tackle this challenge by
leveraging a low-rank adaptation scheme to capture generalization while
maintaining a residual term that preserves expressiveness for personalization.
To ensure privacy, we introduce a novel method where we apply local
differential privacy to the two low-rank components of the local prompt, and
global differential privacy to the global prompt. Our approach mitigates the
impact of privacy noise on the model performance while balancing the tradeoff
between personalization and generalization. Extensive experiments demonstrate
the effectiveness of our approach over other benchmarks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.13608v1">AirTOWN: A Privacy-Preserving Mobile App for Real-time Pollution-Aware
  POI Suggestion</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Retrieval-5BC0EB">
  <p><b>Published on:</b> 2025-01-23T12:28:22Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Giuseppe Fasano, Yashar Deldjoo, Tommaso Di Noia</p>
    <p><b>Summary:</b> This demo paper presents \airtown, a privacy-preserving mobile application
that provides real-time, pollution-aware recommendations for points of interest
(POIs) in urban environments. By combining real-time Air Quality Index (AQI)
data with user preferences, the proposed system aims to help users make
health-conscious decisions about the locations they visit. The application
utilizes collaborative filtering for personalized suggestions, and federated
learning for privacy protection, and integrates AQI data from sensor networks
in cities such as Bari, Italy, and Cork, UK. In areas with sparse sensor
coverage, interpolation techniques approximate AQI values, ensuring broad
applicability. This system offers a poromsing, health-oriented POI
recommendation solution that adapts dynamically to current urban air quality
conditions while safeguarding user privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.13321v1">Investigation of the Privacy Concerns in AI Systems for Young Digital
  Citizens: A Comparative Stakeholder Analysis</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-01-23T02:07:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Molly Campbell, Ankur Barthwal, Sandhya Joshi, Austin Shouli, Ajay Kumar Shrestha</p>
    <p><b>Summary:</b> The integration of Artificial Intelligence (AI) systems into technologies
used by young digital citizens raises significant privacy concerns. This study
investigates these concerns through a comparative analysis of stakeholder
perspectives. A total of 252 participants were surveyed, with the analysis
focusing on 110 valid responses from parents/educators and 100 from AI
professionals after data cleaning. Quantitative methods, including descriptive
statistics and Partial Least Squares Structural Equation Modeling, examined
five validated constructs: Data Ownership and Control, Parental Data Sharing,
Perceived Risks and Benefits, Transparency and Trust, and Education and
Awareness. Results showed Education and Awareness significantly influenced data
ownership and risk assessment, while Data Ownership and Control strongly
impacted Transparency and Trust. Transparency and Trust, along with Perceived
Risks and Benefits, showed minimal influence on Parental Data Sharing,
suggesting other factors may play a larger role. The study underscores the need
for user-centric privacy controls, tailored transparency strategies, and
targeted educational initiatives. Incorporating diverse stakeholder
perspectives offers actionable insights into ethical AI design and governance,
balancing innovation with robust privacy protections to foster trust in a
digital age.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.13278v1">On Subset Retrieval and Group Testing Problems with Differential Privacy
  Constraints</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2025-01-23T00:05:16Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mira Gonen, Michael Langberg, Alex Sprintson</p>
    <p><b>Summary:</b> This paper focuses on the design and analysis of privacy-preserving
techniques for group testing and infection status retrieval. Our work is
motivated by the need to provide accurate information on the status of disease
spread among a group of individuals while protecting the privacy of the
infection status of any single individual involved. The paper is motivated by
practical scenarios, such as controlling the spread of infectious diseases,
where individuals might be reluctant to participate in testing if their
outcomes are not kept confidential.
  The paper makes the following contributions. First, we present a differential
privacy framework for the subset retrieval problem, which focuses on sharing
the infection status of individuals with administrators and decision-makers. We
characterize the trade-off between the accuracy of subset retrieval and the
degree of privacy guaranteed to the individuals. In particular, we establish
tight lower and upper bounds on the achievable level of accuracy subject to the
differential privacy constraints. We then formulate the differential privacy
framework for the noisy group testing problem in which noise is added either
before or after the pooling process. We establish a reduction between the
private subset retrieval and noisy group testing problems and show that the
converse and achievability schemes for subset retrieval carry over to
differentially private group testing.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.12911v1">A Selective Homomorphic Encryption Approach for Faster
  Privacy-Preserving Federated Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> 
  <p><b>Published on:</b> 2025-01-22T14:37:44Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Abdulkadir Korkmaz, Praveen Rao</p>
    <p><b>Summary:</b> Federated learning is a machine learning method that supports training models
on decentralized devices or servers, where each holds its local data, removing
the need for data exchange. This approach is especially useful in healthcare,
as it enables training on sensitive data without needing to share them. The
nature of federated learning necessitates robust security precautions due to
data leakage concerns during communication. To address this issue, we propose a
new approach that employs selective encryption, homomorphic encryption,
differential privacy, and bit-wise scrambling to minimize data leakage while
achieving good execution performance. Our technique , FAS (fast and secure
federated learning) is used to train deep learning models on medical imaging
data. We implemented our technique using the Flower framework and compared with
a state-of-the-art federated learning approach that also uses selective
homomorphic encryption. Our experiments were run in a cluster of eleven
physical machines to create a real-world federated learning scenario on
different datasets. We observed that our approach is up to 90\% faster than
applying fully homomorphic encryption on the model weights. In addition, we can
avoid the pretraining step that is required by our competitor and can save up
to 20\% in terms of total execution time. While our approach was faster, it
obtained similar security results as the competitor.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.12893v1">Statistical Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">  
  <p><b>Published on:</b> 2025-01-22T14:13:44Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Dennis Breutigam, R√ºdiger Reischuk</p>
    <p><b>Summary:</b> To analyze the privacy guarantee of personal data in a database that is
subject to queries it is necessary to model the prior knowledge of a possible
attacker. Differential privacy considers a worst-case scenario where he knows
almost everything, which in many applications is unrealistic and requires a
large utility loss.
  This paper considers a situation called statistical privacy where an
adversary knows the distribution by which the database is generated, but no
exact data of all (or sufficient many) of its entries. We analyze in detail how
the entropy of the distribution guarantes privacy for a large class of queries
called property queries. Exact formulas are obtained for the privacy
parameters. We analyze how they depend on the probability that an entry
fulfills the property under investigation. These formulas turn out to be
lengthy, but can be used for tight numerical approximations of the privacy
parameters. Such estimations are necessary for applying privacy enhancing
techniques in practice. For this statistical setting we further investigate the
effect of adding noise or applying subsampling and the privacy utility
tradeoff. The dependencies on the parameters are illustrated in detail by a
series of plots. Finally, these results are compared to the differential
privacy model.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.12612v1">T2ISafety: Benchmark for Assessing Fairness, Toxicity, and Privacy in
  Image Generation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-01-22T03:29:43Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Lijun Li, Zhelun Shi, Xuhao Hu, Bowen Dong, Yiran Qin, Xihui Liu, Lu Sheng, Jing Shao</p>
    <p><b>Summary:</b> Text-to-image (T2I) models have rapidly advanced, enabling the generation of
high-quality images from text prompts across various domains. However, these
models present notable safety concerns, including the risk of generating
harmful, biased, or private content. Current research on assessing T2I safety
remains in its early stages. While some efforts have been made to evaluate
models on specific safety dimensions, many critical risks remain unexplored. To
address this gap, we introduce T2ISafety, a safety benchmark that evaluates T2I
models across three key domains: toxicity, fairness, and bias. We build a
detailed hierarchy of 12 tasks and 44 categories based on these three domains,
and meticulously collect 70K corresponding prompts. Based on this taxonomy and
prompt set, we build a large-scale T2I dataset with 68K manually annotated
images and train an evaluator capable of detecting critical risks that previous
work has failed to identify, including risks that even ultra-large proprietary
models like GPTs cannot correctly detect. We evaluate 12 prominent diffusion
models on T2ISafety and reveal several concerns including persistent issues
with racial fairness, a tendency to generate toxic content, and significant
variation in privacy protection across the models, even with defense methods
like concept erasing. Data and evaluator are released under
https://github.com/adwardlee/t2i_safety.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.12537v1">Enhancing Privacy in the Early Detection of Sexual Predators Through
  Federated Learning and Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2025-01-21T23:01:21Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Khaoula Chehbouni, Martine De Cock, Gilles Caporossi, Afaf Taik, Reihaneh Rabbany, Golnoosh Farnadi</p>
    <p><b>Summary:</b> The increased screen time and isolation caused by the COVID-19 pandemic have
led to a significant surge in cases of online grooming, which is the use of
strategies by predators to lure children into sexual exploitation. Previous
efforts to detect grooming in industry and academia have involved accessing and
monitoring private conversations through centrally-trained models or sending
private conversations to a global server. In this work, we implement a
privacy-preserving pipeline for the early detection of sexual predators. We
leverage federated learning and differential privacy in order to create safer
online spaces for children while respecting their privacy. We investigate
various privacy-preserving implementations and discuss their benefits and
shortcomings. Our extensive evaluation using real-world data proves that
privacy and utility can coexist with only a slight reduction in utility.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.12456v1">Deploying Privacy Guardrails for LLMs: A Comparative Analysis of
  Real-World Applications</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Software Engineering-D91E36">
  <p><b>Published on:</b> 2025-01-21T19:04:53Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shubhi Asthana, Bing Zhang, Ruchi Mahindru, Chad DeLuca, Anna Lisa Gentile, Sandeep Gopisetty</p>
    <p><b>Summary:</b> The adoption of Large Language Models (LLMs) has revolutionized AI
applications but poses significant challenges in safeguarding user privacy.
Ensuring compliance with privacy regulations such as GDPR and CCPA while
addressing nuanced privacy risks requires robust and scalable frameworks. This
paper presents a detailed study of OneShield Privacy Guard, a framework
designed to mitigate privacy risks in user inputs and LLM outputs across
enterprise and open-source settings. We analyze two real-world deployments:(1)
a multilingual privacy-preserving system integrated with Data and Model
Factory, focusing on enterprise-scale data governance; and (2) PR Insights, an
open-source repository emphasizing automated triaging and community-driven
refinements. In Deployment 1, OneShield achieved a 0.95 F1 score in detecting
sensitive entities like dates, names, and phone numbers across 26 languages,
outperforming state-of-the-art tool such as StarPII and Presidio by up to 12\%.
Deployment 2, with an average F1 score of 0.86, reduced manual effort by over
300 hours in three months, accurately flagging 8.25\% of 1,256 pull requests
for privacy risks with enhanced context sensitivity. These results demonstrate
OneShield's adaptability and efficacy in diverse environments, offering
actionable insights for context-aware entity recognition, automated compliance,
and ethical AI adoption. This work advances privacy-preserving frameworks,
supporting user trust and compliance across operational contexts.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.12359v1">Measured Hockey-Stick Divergence and its Applications to Quantum
  Pufferfish Privacy</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> 
  <p><b>Published on:</b> 2025-01-21T18:39:48Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Theshani Nuradha, Vishal Singh, Mark M. Wilde</p>
    <p><b>Summary:</b> The hockey-stick divergence is a fundamental quantity characterizing several
statistical privacy frameworks that ensure privacy for classical and quantum
data. In such quantum privacy frameworks, the adversary is allowed to perform
all possible measurements. However, in practice, there are typically
limitations to the set of measurements that can be performed. To this end,
here, we comprehensively analyze the measured hockey-stick divergence under
several classes of practically relevant measurement classes. We prove several
of its properties, including data processing and convexity. We show that it is
efficiently computable by semi-definite programming for some classes of
measurements and can be analytically evaluated for Werner and isotropic states.
Notably, we show that the measured hockey-stick divergence characterizes
optimal privacy parameters in the quantum pufferfish privacy framework. With
this connection and the developed technical tools, we enable methods to
quantify and audit privacy for several practically relevant settings. Lastly,
we introduce the measured hockey-stick divergence of channels and explore its
applications in ensuring privacy for channels.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.12193v1">MyDigiTwin: A Privacy-Preserving Framework for Personalized
  Cardiovascular Risk Prediction and Scenario Exploration</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-01-21T15:01:34Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> H√©ctor Cadavid, Hyunho Mo, Bauke Arends, Katarzyna Dziopa, Esther E. Bron, Daniel Bos, Sonja Georgievska, Pim van der Harst</p>
    <p><b>Summary:</b> Cardiovascular disease (CVD) remains a leading cause of death, and primary
prevention through personalized interventions is crucial. This paper introduces
MyDigiTwin, a framework that integrates health digital twins with personal
health environments to empower patients in exploring personalized health
scenarios while ensuring data privacy. MyDigiTwin uses federated learning to
train predictive models across distributed datasets without transferring raw
data, and a novel data harmonization framework addresses semantic and format
inconsistencies in health data. A proof-of-concept demonstrates the feasibility
of harmonizing and using cohort data to train privacy-preserving CVD prediction
models. This framework offers a scalable solution for proactive, personalized
cardiovascular care and sets the stage for future applications in real-world
healthcare settings.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.12046v1">Communication-Efficient and Privacy-Adaptable Mechanism for Federated
  Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-01-21T11:16:05Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chih Wei Ling, Youqi Wu, Jiande Sun, Cheuk Ting Li, Linqi Song, Weitao Xu</p>
    <p><b>Summary:</b> Training machine learning models on decentralized private data via federated
learning (FL) poses two key challenges: communication efficiency and privacy
protection. In this work, we address these challenges within the trusted
aggregator model by introducing a novel approach called the
Communication-Efficient and Privacy-Adaptable Mechanism (CEPAM), achieving both
objectives simultaneously. In particular, CEPAM leverages the rejection-sampled
universal quantizer (RSUQ), a construction of randomized vector quantizer whose
resulting distortion is equivalent to a prescribed noise, such as Gaussian or
Laplace noise, enabling joint differential privacy and compression. Moreover,
we analyze the trade-offs among user privacy, global utility, and transmission
rate of CEPAM by defining appropriate metrics for FL with differential privacy
and compression. Our CEPAM provides the additional benefit of privacy
adaptability, allowing clients and the server to customize privacy protection
based on required accuracy and protection. We assess CEPAM's utility
performance using MNIST dataset, demonstrating that CEPAM surpasses baseline
models in terms of learning accuracy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.12006v1">The Dilemma of Privacy Protection for Developers in the Metaverse</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Emerging Technologies-F9C80E">
  <p><b>Published on:</b> 2025-01-21T09:56:44Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Argianto Rahartomo, Leonel Merino, Mohammad Ghafari, Yoshiki Ohshima</p>
    <p><b>Summary:</b> To investigate the level of support and awareness developers possess for
dealing with sensitive data in the metaverse, we surveyed developers, consulted
legal frameworks, and analyzed API documentation in the metaverse. Our
preliminary results suggest that privacy is a major concern, but developer
awareness and existing support are limited. Developers lack strategies to
identify sensitive data that are exclusive to the metaverse. The API
documentation contains guidelines for collecting sensitive information, but it
omits instructions for identifying and protecting it. Legal frameworks include
definitions that are subject to individual interpretation. These findings
highlight the urgent need to build a transparent and common ground for privacy
definitions, identify sensitive data, and implement usable protection measures.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.11757v1">An Information Geometric Approach to Local Information Privacy with
  Applications to Max-lift and Local Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2025-01-20T21:34:55Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Amirreza Zamani, Parastoo Sadeghi, Mikael Skoglund</p>
    <p><b>Summary:</b> We study an information-theoretic privacy mechanism design, where an agent
observes useful data $Y$ and wants to reveal the information to a user. Since
the useful data is correlated with the private data $X$, the agent uses a
privacy mechanism to produce disclosed data $U$ that can be released. We assume
that the agent observes $Y$ and has no direct access to $X$, i.e., the private
data is hidden. We study the privacy mechanism design that maximizes the
revealed information about $Y$ while satisfying a bounded Local Information
Privacy (LIP) criterion. When the leakage is sufficiently small, concepts from
information geometry allow us to locally approximate the mutual information. By
utilizing this approximation the main privacy-utility trade-off problem can be
rewritten as a quadratic optimization problem that has closed-form solution
under some constraints. For the cases where the closed-form solution is not
obtained we provide lower bounds on it. In contrast to the previous works that
have complexity issues, here, we provide simple privacy designs with low
complexity which are based on finding the maximum singular value and singular
vector of a matrix. To do so, we follow two approaches where in the first one
we find a lower bound on the main problem and then approximate it, however, in
the second approach we approximate the main problem directly. In this work, we
present geometrical interpretations of the proposed methods and in a numerical
example we compare our results considering both approaches with the optimal
solution and the previous methods. Furthermore, we discuss how our method can
be generalized considering larger amounts for the privacy leakage. Finally, we
discuss how the proposed methods can be applied to deal with differential
privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.11756v1">Everyone's Privacy Matters! An Analysis of Privacy Leakage from
  Real-World Facial Images on Twitter and Associated User Behaviors</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-01-20T21:31:59Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yuqi Niu, Weidong Qiu, Peng Tang, Lifan Wang, Shuo Chen, Shujun Li, Nadin Kokciyan, Ben Niu</p>
    <p><b>Summary:</b> Online users often post facial images of themselves and other people on
online social networks (OSNs) and other Web 2.0 platforms, which can lead to
potential privacy leakage of people whose faces are included in such images.
There is limited research on understanding face privacy in social media while
considering user behavior. It is crucial to consider privacy of subjects and
bystanders separately. This calls for the development of privacy-aware face
detection classifiers that can distinguish between subjects and bystanders
automatically. This paper introduces such a classifier trained on face-based
features, which outperforms the two state-of-the-art methods with a significant
margin (by 13.1% and 3.1% for OSN images, and by 17.9% and 5.9% for non-OSN
images). We developed a semi-automated framework for conducting a large-scale
analysis of the face privacy problem by using our novel bystander-subject
classifier. We collected 27,800 images, each including at least one face,
shared by 6,423 Twitter users. We then applied our framework to analyze this
dataset thoroughly. Our analysis reveals eight key findings of different
aspects of Twitter users' real-world behaviors on face privacy, and we provide
quantitative and qualitative results to better explain these findings. We share
the practical implications of our study to empower online platforms and users
in addressing the face privacy problem efficiently.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.11740v2">PIR Over Wireless Channels: Achieving Privacy With Public Responses</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2025-01-20T20:56:56Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Or Elimelech, Asaf Cohen</p>
    <p><b>Summary:</b> In this paper, we address the problem of Private Information Retrieval (PIR)
over a public Additive White Gaussian Noise (AWGN) channel. In such a setup,
the server's responses are visible to other servers. Thus, a curious server can
listen to the other responses, compromising the user's privacy. Indeed,
previous works on PIR over a shared medium assumed the servers cannot
instantaneously listen to other responses. To address this gap, we present a
novel randomized lattice -- PIR coding scheme that jointly codes for privacy,
channel noise, and curious servers which may listen to other responses. We
demonstrate that a positive PIR rate is achievable even in cases where the
channel to the curious server is stronger than the channel to the user.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.10915v1">LegalGuardian: A Privacy-Preserving Framework for Secure Integration of
  Large Language Models in Legal Practice</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Retrieval-5BC0EB">  
  <p><b>Published on:</b> 2025-01-19T01:43:42Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> M. Mikail Demir, Hakan T. Otal, M. Abdullah Canbaz</p>
    <p><b>Summary:</b> Large Language Models (LLMs) hold promise for advancing legal practice by
automating complex tasks and improving access to justice. However, their
adoption is limited by concerns over client confidentiality, especially when
lawyers include sensitive Personally Identifiable Information (PII) in prompts,
risking unauthorized data exposure. To mitigate this, we introduce
LegalGuardian, a lightweight, privacy-preserving framework tailored for lawyers
using LLM-based tools. LegalGuardian employs Named Entity Recognition (NER)
techniques and local LLMs to mask and unmask confidential PII within prompts,
safeguarding sensitive data before any external interaction. We detail its
development and assess its effectiveness using a synthetic prompt library in
immigration law scenarios. Comparing traditional NER models with one-shot
prompted local LLM, we find that LegalGuardian achieves a F1-score of 93% with
GLiNER and 97% with Qwen2.5-14B in PII detection. Semantic similarity analysis
confirms that the framework maintains high fidelity in outputs, ensuring robust
utility of LLM-based tools. Our findings indicate that legal professionals can
harness advanced AI technologies without compromising client confidentiality or
the quality of legal documents.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.10319v1">Natural Language Processing of Privacy Policies: A Survey</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-01-17T17:47:15Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Andrick Adhikari, Sanchari Das, Rinku Dewri</p>
    <p><b>Summary:</b> Natural Language Processing (NLP) is an essential subset of artificial
intelligence. It has become effective in several domains, such as healthcare,
finance, and media, to identify perceptions, opinions, and misuse, among
others. Privacy is no exception, and initiatives have been taken to address the
challenges of usable privacy notifications to users with the help of NLP. To
this aid, we conduct a literature review by analyzing 109 papers at the
intersection of NLP and privacy policies. First, we provide a brief
introduction to privacy policies and discuss various facets of associated
problems, which necessitate the application of NLP to elevate the current state
of privacy notices and disclosures to users. Subsequently, we a) provide an
overview of the implementation and effectiveness of NLP approaches for better
privacy policy communication; b) identify the methodologies that can be further
enhanced to provide robust privacy policies; and c) identify the gaps in the
current state-of-the-art research. Our systematic analysis reveals that several
research papers focus on annotating and classifying privacy texts for analysis
but need to adequately dwell on other aspects of NLP applications, such as
summarization. More specifically, ample research opportunities exist in this
domain, covering aspects such as corpus generation, summarization vectors,
contextualized word embedding, identification of privacy-relevant statement
categories, fine-grained classification, and domain-specific model tuning.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.10099v1">Several Representations of $Œ±$-Mutual Information and
  Interpretations as Privacy Leakage Measures</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2025-01-17T10:36:05Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Akira Kamatsuka, Takashiro Yoshida</p>
    <p><b>Summary:</b> In this paper, we present several novel representations of $\alpha$-mutual
information ($\alpha$-MI) in terms of R{\' e}nyi divergence and conditional
R{\' e}nyi entropy. The representations are based on the variational
characterizations of $\alpha$-MI using a reverse channel. Based on these
representations, we provide several interpretations of the $\alpha$-MI as
privacy leakage measures using generalized mean and gain functions. Further, as
byproducts of the representations, we propose novel conditional R{\' e}nyi
entropies that satisfy the property that conditioning reduces entropy and
data-processing inequality.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.09191v1">Detecting Vulnerabilities in Encrypted Software Code while Ensuring Code
  Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Software Engineering-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2025-01-15T22:39:50Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jorge Martins, David Dantas, Rafael Ramires, Bernardo Ferreira, Ib√©ria Medeiros</p>
    <p><b>Summary:</b> Software vulnerabilities continue to be the main cause of occurrence for
cyber attacks. In an attempt to reduce them and improve software quality,
software code analysis has emerged as a service offered by companies
specialising in software testing. However, this service requires software
companies to provide access to their software's code, which raises concerns
about code privacy and intellectual property theft. This paper presents a novel
approach to Software Quality and Privacy, in which testing companies can
perform code analysis tasks on encrypted software code provided by software
companies while code privacy is preserved. The approach combines Static Code
Analysis and Searchable Symmetric Encryption in order to process the source
code and build an encrypted inverted index that represents its data and control
flows. The index is then used to discover vulnerabilities by carrying out
static analysis tasks in a confidential way. With this approach, this paper
also defines a new research field -- Confidential Code Analysis --, from which
other types of code analysis tasks and approaches can be derived. We
implemented the approach in a new tool called CoCoA and evaluated it
experimentally with synthetic and real PHP web applications. The results show
that the tool has similar precision as standard (non-confidential) static
analysis tools and a modest average performance overhead of 42.7%.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.08665v1">A Survey on Facial Image Privacy Preservation in Cloud-Based Services</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-01-15T09:00:32Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chen Chen, Mengyuan Sun, Xueluan Gong, Yanjiao Chen, Qian Wang</p>
    <p><b>Summary:</b> Facial recognition models are increasingly employed by commercial
enterprises, government agencies, and cloud service providers for identity
verification, consumer services, and surveillance. These models are often
trained using vast amounts of facial data processed and stored in cloud-based
platforms, raising significant privacy concerns. Users' facial images may be
exploited without their consent, leading to potential data breaches and misuse.
This survey presents a comprehensive review of current methods aimed at
preserving facial image privacy in cloud-based services. We categorize these
methods into two primary approaches: image obfuscation-based protection and
adversarial perturbation-based protection. We provide an in-depth analysis of
both categories, offering qualitative and quantitative comparisons of their
effectiveness. Additionally, we highlight unresolved challenges and propose
future research directions to improve privacy preservation in cloud computing
environments.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.08449v1">A Refreshment Stirred, Not Shaken (II): Invariant-Preserving Deployments
  of Differential Privacy for the US Decennial Census</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B"> 
  <p><b>Published on:</b> 2025-01-14T21:38:01Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> James Bailie, Ruobin Gong, Xiao-Li Meng</p>
    <p><b>Summary:</b> Through the lens of the system of differential privacy specifications
developed in Part I of a trio of articles, this second paper examines two
statistical disclosure control (SDC) methods for the United States Decennial
Census: the Permutation Swapping Algorithm (PSA), which is similar to the 2010
Census's disclosure avoidance system (DAS), and the TopDown Algorithm (TDA),
which was used in the 2020 DAS. To varying degrees, both methods leave
unaltered some statistics of the confidential data $\unicode{x2013}$ which are
called the method's invariants $\unicode{x2013}$ and hence neither can be
readily reconciled with differential privacy (DP), at least as it was
originally conceived. Nevertheless, we establish that the PSA satisfies
$\varepsilon$-DP subject to the invariants it necessarily induces, thereby
showing that this traditional SDC method can in fact still be understood within
our more-general system of DP specifications. By a similar modification to
$\rho$-zero concentrated DP, we also provide a DP specification for the TDA.
Finally, as a point of comparison, we consider the counterfactual scenario in
which the PSA was adopted for the 2020 Census, resulting in a reduction in the
nominal privacy loss, but at the cost of releasing many more invariants.
Therefore, while our results explicate the mathematical guarantees of SDC
provided by the PSA, the TDA and the 2020 DAS in general, care must be taken in
their translation to actual privacy protection $\unicode{x2013}$ just as is the
case for any DP deployment.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.08236v1">Privacy-Preserving Model and Preprocessing Verification for Machine
  Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-01-14T16:21:54Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Wenbiao Li, Anisa Halimi, Xiaoqian Jiang, Jaideep Vaidya, Erman Ayday</p>
    <p><b>Summary:</b> This paper presents a framework for privacy-preserving verification of
machine learning models, focusing on models trained on sensitive data.
Integrating Local Differential Privacy (LDP) with model explanations from LIME
and SHAP, our framework enables robust verification without compromising
individual privacy. It addresses two key tasks: binary classification, to
verify if a target model was trained correctly by applying the appropriate
preprocessing steps, and multi-class classification, to identify specific
preprocessing errors. Evaluations on three real-world datasets-Diabetes, Adult,
and Student Record-demonstrate that while the ML-based approach is particularly
effective in binary tasks, the threshold-based method performs comparably in
multi-class tasks. Results indicate that although verification accuracy varies
across datasets and noise levels, the framework provides effective detection of
preprocessing errors, strong privacy guarantees, and practical applicability
for safeguarding sensitive data.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.07844v2">Towards A Hybrid Quantum Differential Privacy</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-01-14T05:13:37Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Baobao Song, Shiva Raj Pokhrel, Athanasios V. Vasilakos, Tianqing Zhu, Gang Li</p>
    <p><b>Summary:</b> Quantum computing offers unparalleled processing power but raises significant
data privacy challenges. Quantum Differential Privacy (QDP) leverages inherent
quantum noise to safeguard privacy, surpassing traditional DP. This paper
develops comprehensive noise profiles, identifies noise types beneficial for
QDP, and highlights teh need for practical implementations beyond theoretical
models. Existing QDP mechanisms, limited to single noise sources, fail to
reflect teh multi-source noise reality of quantum systems. We propose a
resilient hybrid QDP mechanism utilizing channel and measurement noise,
optimizing privacy budgets to balance privacy and utility. Additionally, we
introduce Lifted Quantum Differential Privacy, offering enhanced randomness for
improved privacy audits and quantum algorithm evaluation.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.07262v1">OblivCDN: A Practical Privacy-preserving CDN with Oblivious Content
  Access</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-01-13T12:23:23Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Viet Vo, Shangqi Lai, Xingliang Yuan, Surya Nepal, Qi Li</p>
    <p><b>Summary:</b> Content providers increasingly utilise Content Delivery Networks (CDNs) to
enhance users' content download experience. However, this deployment scenario
raises significant security concerns regarding content confidentiality and user
privacy due to the involvement of third-party providers. Prior proposals using
private information retrieval (PIR) and oblivious RAM (ORAM) have proven
impractical due to high computation and communication costs, as well as
integration challenges within distributed CDN architectures. In response, we
present \textsf{OblivCDN}, a practical privacy-preserving system meticulously
designed for seamless integration with the existing real-world Internet-CDN
infrastructure. Our design strategically adapts Range ORAM primitives to
optimise memory and disk seeks when accessing contiguous blocks of CDN content,
both at the origin and edge servers, while preserving both content
confidentiality and user access pattern hiding features. Also, we carefully
customise several oblivious building blocks that integrate the distributed
trust model into the ORAM client, thereby eliminating the computational
bottleneck in the origin server and reducing communication costs between the
origin server and edge servers. Moreover, the newly-designed ORAM client also
eliminates the need for trusted hardware on edge servers, and thus
significantly ameliorates the compatibility towards networks with massive
legacy devices.In real-world streaming evaluations, OblivCDN} demonstrates
remarkable performance, downloading a $256$ MB video in just $5.6$ seconds.
This achievement represents a speedup of $90\times$ compared to a strawman
approach (direct ORAM adoption) and a $366\times$ improvement over the prior
art, OblivP2P.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.07209v1">Privacy-Preserving Authentication: Theory vs. Practice</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-01-13T11:04:05Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Daniel Slamanig</p>
    <p><b>Summary:</b> With the increasing use of online services, the protection of the privacy of
users becomes more and more important. This is particularly critical as
authentication and authorization as realized on the Internet nowadays,
typically relies on centralized identity management solutions. Although those
are very convenient from a user's perspective, they are quite intrusive from a
privacy perspective and are currently far from implementing the concept of data
minimization. Fortunately, cryptography offers exciting primitives such as
zero-knowledge proofs and advanced signature schemes to realize various forms
of so-called anonymous credentials. Such primitives allow to realize online
authentication and authorization with a high level of built-in privacy
protection (what we call privacy-preserving authentication). Though these
primitives have already been researched for various decades and are well
understood in the research community, unfortunately, they lack widespread
adoption. In this paper, we look at the problems, what cryptography can do,
some deployment examples, and barriers to widespread adoption. Latter using the
example of the EU Digital Identity Wallet (EUDIW) and the recent discussion and
feedback from cryptography experts around this topic. We also briefly comment
on the transition to post-quantum cryptography.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.09031v1">Synthetic Data and Health Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2025-01-13T10:23:14Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Gw√©nol√© Abgrall, Xavier Monnet, Anmol Arora</p>
    <p><b>Summary:</b> This Viewpoint discusses generative artificial intelligence and safeguarding
privacy by using synthetic data as a substitute for private health data.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.07154v1">Privacy-Preserving Data Quality Assessment for Time-Series IoT Sensors</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2025-01-13T09:28:42Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Novoneel Chakraborty, Abhay Sharma, Jyotirmoy Dutta, Hari Dilip Kumar</p>
    <p><b>Summary:</b> Data from Internet of Things (IoT) sensors has emerged as a key contributor
to decision-making processes in various domains. However, the quality of the
data is crucial to the effectiveness of applications built on it, and
assessment of the data quality is heavily context-dependent. Further,
preserving the privacy of the data during quality assessment is critical in
domains where sensitive data is prevalent. This paper proposes a novel
framework for automated, objective, and privacy-preserving data quality
assessment of time-series data from IoT sensors deployed in smart cities. We
leverage custom, autonomously computable metrics that parameterise the temporal
performance and adherence to a declarative schema document to achieve
objectivity. Additionally, we utilise a trusted execution environment to create
a "data-blind" model that ensures individual privacy, eliminates assessee bias,
and enhances adaptability across data types. This paper describes this data
quality assessment methodology for IoT sensors, emphasising its relevance
within the smart-city context while addressing the growing need for privacy in
the face of extensive data collection practices.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.06913v1">Towards Fair and Privacy-Aware Transfer Learning for Educational
  Predictive Modeling: A Case Study on Retention Prediction in Community
  Colleges</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2025-01-12T19:49:28Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chengyuan Yao, Carmen Cortez, Renzhe Yu</p>
    <p><b>Summary:</b> Predictive analytics is widely used in learning analytics, but many
resource-constrained institutions lack the capacity to develop their own models
or rely on proprietary ones trained in different contexts with little
transparency. Transfer learning holds promise for expanding equitable access to
predictive analytics but remains underexplored due to legal and technical
constraints. This paper examines transfer learning strategies for retention
prediction at U.S. two-year community colleges. We envision a scenario where
community colleges collaborate with each other and four-year universities to
develop retention prediction models under privacy constraints and evaluate
risks and improvement strategies of cross-institutional model transfer. Using
administrative records from 4 research universities and 23 community colleges
covering over 800,000 students across 7 cohorts, we identify performance and
fairness degradation when external models are deployed locally without
adaptation. Publicly available contextual information can forecast these
performance drops and offer early guidance for model portability. For
developers under privacy regulations, sequential training selecting
institutions based on demographic similarities enhances fairness without
compromising performance. For institutions lacking local data to fine-tune
source models, customizing evaluation thresholds for sensitive groups
outperforms standard transfer techniques in improving performance and fairness.
Our findings suggest the value of transfer learning for more accessible
educational predictive modeling and call for judicious use of contextual
information in model training, selection, and deployment to achieve reliable
and equitable model transfer.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.06300v1">Tensorization of neural networks for improved privacy and
  interpretability</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Numerical Analysis-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Numerical Analysis-5BC0EB">  
  <p><b>Published on:</b> 2025-01-10T19:00:06Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jos√© Ram√≥n Pareja Monturiol, Alejandro Pozas-Kerstjens, David P√©rez-Garc√≠a</p>
    <p><b>Summary:</b> We present a tensorization algorithm for constructing tensor train
representations of functions, drawing on sketching and cross interpolation
ideas. The method only requires black-box access to the target function and a
small set of sample points defining the domain of interest. Thus, it is
particularly well-suited for machine learning models, where the domain of
interest is naturally defined by the training dataset. We show that this
approach can be used to enhance the privacy and interpretability of neural
network models. Specifically, we apply our decomposition to (i) obfuscate
neural networks whose parameters encode patterns tied to the training data
distribution, and (ii) estimate topological phases of matter that are easily
accessible from the tensor train representation. Additionally, we show that
this tensorization can serve as an efficient initialization method for
optimizing tensor trains in general settings, and that, for model compression,
our algorithm achieves a superior trade-off between memory and time complexity
compared to conventional tensorization methods of neural networks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.06161v2">RIOT-based smart metering system for privacy-preserving data aggregation
  using watermarking and encryption</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-01-10T18:37:20Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Farzana Kabir, David Megias, Krzysztof Cabaj</p>
    <p><b>Summary:</b> The remarkable advancement of smart grid technology in the IoT sector has
raised concerns over the privacy and security of the data collected and
transferred in real-time. Smart meters generate detailed information about
consumers' energy consumption patterns, increasing the risks of data breaches,
identity theft, and other forms of cyber attacks. This study proposes a
privacy-preserving data aggregation protocol that uses reversible watermarking
and AES cryptography to ensure the security and privacy of the data. There are
two versions of the protocol: one for low-frequency smart meters that uses
LSB-shifting-based reversible watermarking (RLS) and another for high-frequency
smart meters that uses difference expansion-based reversible watermarking
(RDE). This enables the aggregation of smart meter data, maintaining
confidentiality, integrity, and authenticity. The proposed protocol
significantly enhances privacy-preserving measures for smart metering systems,
conducting an experimental evaluation with real hardware implementation using
Nucleo microcontroller boards and the RIOT operating system and comparing the
results to existing security schemes.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.05535v1">On Fair Ordering and Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2025-01-09T19:17:43Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shir Cohen, Neel Basu, Soumya Basu, Lorenzo Alvisi</p>
    <p><b>Summary:</b> In blockchain systems, fair transaction ordering is crucial for a trusted and
regulation-compliant economic ecosystem. Unlike traditional State Machine
Replication (SMR) systems, which focus solely on liveness and safety,
blockchain systems also require a fairness property. This paper examines these
properties and aims to eliminate algorithmic bias in transaction ordering
services.
  We build on the notion of equal opportunity. We characterize transactions in
terms of relevant and irrelevant features, requiring that the order be
determined solely by the relevant ones. Specifically, transactions with
identical relevant features should have an equal chance of being ordered before
one another. We extend this framework to define a property where the greater
the distance in relevant features between transactions, the higher the
probability of prioritizing one over the other.
  We reveal a surprising link between equal opportunity in SMR and Differential
Privacy (DP), showing that any DP mechanism can be used to ensure fairness in
SMR. This connection not only enhances our understanding of the interplay
between privacy and fairness in distributed computing but also opens up new
opportunities for designing fair distributed protocols using well-established
DP techniques.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.05053v1">TAPFed: Threshold Secure Aggregation for Privacy-Preserving Federated
  Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-01-09T08:24:10Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Runhua Xu, Bo Li, Chao Li, James B. D. Joshi, Shuai Ma, Jianxin Li</p>
    <p><b>Summary:</b> Federated learning is a computing paradigm that enhances privacy by enabling
multiple parties to collaboratively train a machine learning model without
revealing personal data. However, current research indicates that traditional
federated learning platforms are unable to ensure privacy due to privacy leaks
caused by the interchange of gradients. To achieve privacy-preserving federated
learning, integrating secure aggregation mechanisms is essential.
Unfortunately, existing solutions are vulnerable to recently demonstrated
inference attacks such as the disaggregation attack. This paper proposes
TAPFed, an approach for achieving privacy-preserving federated learning in the
context of multiple decentralized aggregators with malicious actors. TAPFed
uses a proposed threshold functional encryption scheme and allows for a certain
number of malicious aggregators while maintaining security and privacy. We
provide formal security and privacy analyses of TAPFed and compare it to
various baselines through experimental evaluation. Our results show that TAPFed
offers equivalent performance in terms of model quality compared to
state-of-the-art approaches while reducing transmission overhead by 29%-45%
across different model training scenarios. Most importantly, TAPFed can defend
against recently demonstrated inference attacks caused by curious aggregators,
which the majority of existing approaches are susceptible to.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.04940v1">A New Perspective on Privacy Protection in Federated Learning with
  Granular-Ball Computing</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-01-09T03:14:03Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Guannan Lai, Yihui Feng, Xin Yang, Xiaoyu Deng, Hao Yu, Shuyin Xia, Guoyin Wang, Tianrui Li</p>
    <p><b>Summary:</b> Federated Learning (FL) facilitates collaborative model training while
prioritizing privacy by avoiding direct data sharing. However, most existing
articles attempt to address challenges within the model's internal parameters
and corresponding outputs, while neglecting to solve them at the input level.
To address this gap, we propose a novel framework called Granular-Ball
Federated Learning (GrBFL) for image classification. GrBFL diverges from
traditional methods that rely on the finest-grained input data. Instead, it
segments images into multiple regions with optimal coarse granularity, which
are then reconstructed into a graph structure. We designed a two-dimensional
binary search segmentation algorithm based on variance constraints for GrBFL,
which effectively removes redundant information while preserving key
representative features. Extensive theoretical analysis and experiments
demonstrate that GrBFL not only safeguards privacy and enhances efficiency but
also maintains robust utility, consistently outperforming other
state-of-the-art FL methods. The code is available at
https://github.com/AIGNLAI/GrBFL.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.04420v1">A Closer Look on Gender Stereotypes in Movie Recommender Systems and
  Their Implications with Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Retrieval-5BC0EB">
  <p><b>Published on:</b> 2025-01-08T11:08:58Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Falguni Roy, Yiduo Shen, Na Zhao, Xiaofeng Ding, Md. Omar Faruk</p>
    <p><b>Summary:</b> The movie recommender system typically leverages user feedback to provide
personalized recommendations that align with user preferences and increase
business revenue. This study investigates the impact of gender stereotypes on
such systems through a specific attack scenario. In this scenario, an attacker
determines users' gender, a private attribute, by exploiting gender stereotypes
about movie preferences and analyzing users' feedback data, which is either
publicly available or observed within the system. The study consists of two
phases. In the first phase, a user study involving 630 participants identified
gender stereotypes associated with movie genres, which often influence viewing
choices. In the second phase, four inference algorithms were applied to detect
gender stereotypes by combining the findings from the first phase with users'
feedback data. Results showed that these algorithms performed more effectively
than relying solely on feedback data for gender inference. Additionally, we
quantified the extent of gender stereotypes to evaluate their broader impact on
digital computational science. The latter part of the study utilized two major
movie recommender datasets: MovieLens 1M and Yahoo!Movie. Detailed experimental
information is available on our GitHub repository:
https://github.com/fr-iit/GSMRS</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.04409v1">Lossless Privacy-Preserving Aggregation for Decentralized Federated
  Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-01-08T10:49:06Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xiaoye Miao, Bin Li, Yangyang Wu, Meng Xi, Xinkui Zhao, Jianwei Yin</p>
    <p><b>Summary:</b> Privacy concerns arise as sensitive data proliferate. Despite decentralized
federated learning (DFL) aggregating gradients from neighbors to avoid direct
data transmission, it still poses indirect data leaks from the transmitted
gradients. Existing privacy-preserving methods for DFL add noise to gradients.
They either diminish the model predictive accuracy or suffer from ineffective
gradient protection. In this paper, we propose a novel lossless
privacy-preserving aggregation rule named LPPA to enhance gradient protection
as much as possible but without loss of DFL model predictive accuracy. LPPA
subtly injects the noise difference between the sent and received noise into
transmitted gradients for gradient protection. The noise difference
incorporates neighbors' randomness for each client, effectively safeguarding
against data leaks. LPPA employs the noise flow conservation theory to ensure
that the noise impact can be globally eliminated. The global sum of all noise
differences remains zero, ensuring that accurate gradient aggregation is
unaffected and the model accuracy remains intact. We theoretically prove that
the privacy-preserving capacity of LPPA is \sqrt{2} times greater than that of
noise addition, while maintaining comparable model accuracy to the standard DFL
aggregation without noise injection. Experimental results verify the
theoretical findings and show that LPPA achieves a 13% mean improvement in
accuracy over noise addition. We also demonstrate the effectiveness of LPPA in
protecting raw data and guaranteeing lossless model accuracy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.04323v3">Navigating the Designs of Privacy-Preserving Fine-tuning for Large
  Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-01-08T07:47:43Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Haonan Shi, Tu Ouyang, An Wang</p>
    <p><b>Summary:</b> Instruction tuning has proven effective in enhancing Large Language Models'
(LLMs) performance on downstream tasks. However, real-world fine-tuning faces
inherent conflicts between model providers' intellectual property protection,
clients' data privacy requirements, and tuning costs. While recent approaches
like split learning and offsite tuning demonstrate promising architectures for
privacy-preserving fine-tuning, there is a gap in systematically addressing the
multidimensional trade-offs required for diverse real-world deployments. We
propose several indicative evaluation metrics to guide design trade-offs for
privacy-preserving fine-tuning and a series of example designs, collectively
named GuardedTuning; they result from novel combinations of system
architectures with adapted privacy-enhancement methods and emerging computation
techniques. Each design represents distinct trade-offs across model utility,
privacy guarantees, and costs. Experimental results demonstrate that these
designs protect against data reconstruction attacks while maintaining
competitive fine-tuning performance.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.04222v1">Privacy-Preserving Distributed Online Mirror Descent for Nonconvex
  Optimization</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36">
  <p><b>Published on:</b> 2025-01-08T01:39:10Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yingjie Zhou, Tao Li</p>
    <p><b>Summary:</b> We investigate the distributed online nonconvex optimization problem with
differential privacy over time-varying networks. Each node minimizes the sum of
several nonconvex functions while preserving the node's differential privacy.
We propose a privacy-preserving distributed online mirror descent algorithm for
nonconvex optimization, which uses the mirror descent to update decision
variables and the Laplace differential privacy mechanism to protect privacy.
Unlike the existing works, the proposed algorithm allows the cost functions to
be nonconvex, which is more applicable. Based upon these, we prove that if the
communication network is $B$-strongly connected and the constraint set is
compact, then by choosing the step size properly, the algorithm guarantees
$\epsilon$-differential privacy at each time. Furthermore, we prove that if the
local cost functions are $\beta$-smooth, then the regret over time horizon $T$
grows sublinearly while preserving differential privacy, with an upper bound
$O(\sqrt{T})$. Finally, the effectiveness of the algorithm is demonstrated
through numerical simulations.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.04134v1">Mixing Times and Privacy Analysis for the Projected Langevin Algorithm
  under a Modulus of Continuity</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Optimization and Control-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Statistics Theory-D91E36"> 
  <p><b>Published on:</b> 2025-01-07T20:46:59Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mario Bravo, Juan P. Flores-Mella, Crist√≥bal Guzm√°n</p>
    <p><b>Summary:</b> We study the mixing time of the projected Langevin algorithm (LA) and the
privacy curve of noisy Stochastic Gradient Descent (SGD), beyond nonexpansive
iterations. Specifically, we derive new mixing time bounds for the projected LA
which are, in some important cases, dimension-free and poly-logarithmic on the
accuracy, closely matching the existing results in the smooth convex case.
Additionally, we establish new upper bounds for the privacy curve of the
subsampled noisy SGD algorithm. These bounds show a crucial dependency on the
regularity of gradients, and are useful for a wide range of convex losses
beyond the smooth case. Our analysis relies on a suitable extension of the
Privacy Amplification by Iteration (PABI) framework (Feldman et al., 2018;
Altschuler and Talwar, 2022, 2023) to noisy iterations whose gradient map is
not necessarily nonexpansive. This extension is achieved by designing an
optimization problem which accounts for the best possible R\'enyi divergence
bound obtained by an application of PABI, where the tractability of the problem
is crucially related to the modulus of continuity of the associated gradient
mapping. We show that, in several interesting cases -- including the nonsmooth
convex, weakly smooth and (strongly) dissipative -- such optimization problem
can be solved exactly and explicitly. This yields the tightest possible
PABI-based bounds, where our results are either new or substantially sharper
than those in previous works.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.03941v1">Synthetic Data Privacy Metrics</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-01-07T17:02:33Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Amy Steier, Lipika Ramaswamy, Andre Manoel, Alexa Haushalter</p>
    <p><b>Summary:</b> Recent advancements in generative AI have made it possible to create
synthetic datasets that can be as accurate as real-world data for training AI
models, powering statistical insights, and fostering collaboration with
sensitive datasets while offering strong privacy guarantees. Effectively
measuring the empirical privacy of synthetic data is an important step in the
process. However, while there is a multitude of new privacy metrics being
published every day, there currently is no standardization. In this paper, we
review the pros and cons of popular metrics that include simulations of
adversarial attacks. We also review current best practices for amending
generative models to enhance the privacy of the data they create (e.g.
differential privacy).</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.04058v1">Homomorphic Encryption in Healthcare Industry Applications for
  Protecting Data Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-01-07T07:42:41Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> J. S. Rauthan</p>
    <p><b>Summary:</b> Focussing on two different use cases-Quality Control methods in industrial
contexts and Neural Network algorithms for healthcare diagnostics-this research
investigates the inclusion of Fully Homomorphic Encryption into real-world
applications in the healthcare sector. We evaluate the performance, resource
requirements, and viability of deploying FHE in these settings through
extensive testing and analysis, highlighting the progress made in FHE tooling
and the obstacles still facing addressing the gap between conceptual research
and practical applications. We start our research by describing the specific
case study and trust model were working with. Choosing the two FHE frameworks
most appropriate for industry development, we assess the resources and
performance requirements for implementing each of the two FHE frameworks in the
first scenario, Quality Control algorithms. In conclusion, our findings
demonstrate the effectiveness and resource consumption of the two use
cases-complex NN models and simple QC algorithms-when implemented in an FHE
setting.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.03451v1">Structure-Preference Enabled Graph Embedding Generation under
  Differential Privacy</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Social and Information Networks-662E9B">
  <p><b>Published on:</b> 2025-01-07T00:43:18Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sen Zhang, Qingqing Ye, Haibo Hu</p>
    <p><b>Summary:</b> Graph embedding generation techniques aim to learn low-dimensional vectors
for each node in a graph and have recently gained increasing research
attention. Publishing low-dimensional node vectors enables various graph
analysis tasks, such as structural equivalence and link prediction. Yet,
improper publication opens a backdoor to malicious attackers, who can infer
sensitive information of individuals from the low-dimensional node vectors.
Existing methods tackle this issue by developing deep graph learning models
with differential privacy (DP). However, they often suffer from large noise
injections and cannot provide structural preferences consistent with mining
objectives. Recently, skip-gram based graph embedding generation techniques are
widely used due to their ability to extract customizable structures. Based on
skip-gram, we present SE-PrivGEmb, a structure-preference enabled graph
embedding generation under DP. For arbitrary structure preferences, we design a
unified noise tolerance mechanism via perturbing non-zero vectors. This
mechanism mitigates utility degradation caused by high sensitivity. By
carefully designing negative sampling probabilities in skip-gram, we
theoretically demonstrate that skip-gram can preserve arbitrary proximities,
which quantify structural features in graphs. Extensive experiments show that
our method outperforms existing state-of-the-art methods under structural
equivalence and link prediction tasks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.03391v1">Privacy-Preserving Smart Contracts for Permissioned Blockchains: A
  zk-SNARK-Based Recipe Part-1</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-01-06T21:16:33Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Aldenio Burgos, Eduardo Alchieri</p>
    <p><b>Summary:</b> The Bitcoin white paper introduced blockchain technology, enabling trustful
transactions without intermediaries. Smart contracts emerged with Ethereum and
blockchains expanded beyond cryptocurrency, applying to auctions, crowdfunding
and electronic voting. However, blockchain's transparency raised privacy
concerns and initial anonymity measures proved ineffective. Smart contract
privacy solutions employed zero-knowledge proofs, homomorphic encryption and
trusted execution environments. These approaches have practical drawbacks, such
as limited functionality, high computation times and trust on third parties
requirements, being not fully decentralized. This work proposes a solution
utilizing zk-SNARKs to provide privacy in smart contracts and blockchains. The
solution supports both fungible and nonfungible tokens. Additionally, the
proposal includes a new type of transactions, called delegated transactions,
which enable use cases like Delivery vs Payment (DvP).</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.03222v1">Characterizing the Accuracy-Communication-Privacy Trade-off in
  Distributed Stochastic Convex Optimization</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36">  
  <p><b>Published on:</b> 2025-01-06T18:57:05Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sudeep Salgia, Nikola Pavlovic, Yuejie Chi, Qing Zhao</p>
    <p><b>Summary:</b> We consider the problem of differentially private stochastic convex
optimization (DP-SCO) in a distributed setting with $M$ clients, where each of
them has a local dataset of $N$ i.i.d. data samples from an underlying data
distribution. The objective is to design an algorithm to minimize a convex
population loss using a collaborative effort across $M$ clients, while ensuring
the privacy of the local datasets. In this work, we investigate the
accuracy-communication-privacy trade-off for this problem. We establish
matching converse and achievability results using a novel lower bound and a new
algorithm for distributed DP-SCO based on Vaidya's plane cutting method. Thus,
our results provide a complete characterization of the
accuracy-communication-privacy trade-off for DP-SCO in the distributed setting.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.02893v2">A Volumetric Approach to Privacy of Dynamical Systems</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36">
  <p><b>Published on:</b> 2025-01-06T10:15:21Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chuanghong Weng, Ehsan Nekouei</p>
    <p><b>Summary:</b> Information-theoretic metrics, such as mutual information, have been widely
used to evaluate privacy leakage in dynamic systems. However, these approaches
are typically limited to stochastic systems and face computational challenges.
In this paper, we introduce a novel volumetric framework for analyzing privacy
in systems affected by unknown but bounded noise. Our model considers a dynamic
system comprising public and private states, where an observation set of the
public state is released. An adversary utilizes the observed public state to
infer an uncertainty set of the private state, referred to as the inference
attack. We define the evolution dynamics of these inference attacks and
quantify the privacy level of the private state using the volume of its
uncertainty sets. For linear scalar systems, we derive an explicit formulation
of the uncertainty set. For multi-dimensional linear systems, we develop an
approximate computation method leveraging interval analysis. We investigate the
properties of the proposed volumetric privacy measure and demonstrate that it
is bounded by the information gain derived from the observation set.
Furthermore, we propose an optimization approach to designing privacy filter
using randomization and linear programming based on the proposed privacy
measure. The effectiveness of the optimal privacy filter design is evaluated
through a production-inventory case study, illustrating its robustness against
the inference attack.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.02804v2">Latency and Privacy-Aware Resource Allocation in Vehicular Edge
  Computing</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Performance-F9C80E">
  <p><b>Published on:</b> 2025-01-06T06:44:49Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Hossein Ahmadvand, Fouzhan Foroutan</p>
    <p><b>Summary:</b> The rapid increase in the number of connected vehicles has led to the
generation of vast amounts of data. As a significant portion of this data
pertains to vehicle-to-vehicle and vehicle-to-infrastructure communications, it
is predominantly generated at the edge. Considering the enormous volume of
data, real-time applications, and privacy concerns, it is crucial to process
the data at the edge. Neglecting the management of processing resources in
vehicular edge computing (VEC) could lead to numerous challenges as a
substantial number of vehicles with diverse safety, economic, and entertainment
applications, along with their data processing, emerge in the near future [1].
Previous research in VEC resource allocation has primarily focused on issues
such as response time and privacy preservation techniques. However, an approach
that takes into account privacy-aware resource allocation based on vehicular
network architecture and application requirements has not yet been proposed. In
this paper, we present a privacy and latency-aware approach for allocating
processing resources at the edge of the vehicular network, considering the
specific requirements of different applications. Our approach involves
categorizing vehicular network applications based on their processing accuracy,
real-time processing needs, and privacy preservation requirements. We further
divide the vehicular network edge into two parts: the user layer (OBUs) is
considered for processing applications with privacy requirements, while the
allocation of resources in the RSUs and cloud layer is based on the specific
needs of different applications. In this study, we evaluate the quality of
service based on parameters such as privacy preservation, processing cost,
meeting deadlines, and result quality. Comparative analyses demonstrate that
our approach enhances service quality by 55% compared to existing
state-of-the-art methods.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.02354v1">PrivDPR: Synthetic Graph Publishing with Deep PageRank under
  Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-01-04T18:19:21Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sen Zhang, Haibo Hu, Qingqing Ye, Jianliang Xu</p>
    <p><b>Summary:</b> The objective of privacy-preserving synthetic graph publishing is to
safeguard individuals' privacy while retaining the utility of original data.
Most existing methods focus on graph neural networks under differential privacy
(DP), and yet two fundamental problems in generating synthetic graphs remain
open. First, the current research often encounters high sensitivity due to the
intricate relationships between nodes in a graph. Second, DP is usually
achieved through advanced composition mechanisms that tend to converge
prematurely when working with a small privacy budget. In this paper, inspired
by the simplicity, effectiveness, and ease of analysis of PageRank, we design
PrivDPR, a novel privacy-preserving deep PageRank for graph synthesis. In
particular, we achieve DP by adding noise to the gradient for a specific weight
during learning. Utilizing weight normalization as a bridge, we theoretically
reveal that increasing the number of layers in PrivDPR can effectively mitigate
the high sensitivity and privacy budget splitting. Through formal privacy
analysis, we prove that the synthetic graph generated by PrivDPR satisfies
node-level DP. Experiments on real-world graph datasets show that PrivDPR
preserves high data utility across multiple graph structural properties.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.02091v1">PriveShield: Enhancing User Privacy Using Automatic Isolated Profiles in
  Browsers</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-01-03T20:29:33Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Seyed Ali Akhavani, Engin Kirda, Amin Kharraz</p>
    <p><b>Summary:</b> Online tracking is a widespread practice on the web with questionable ethics,
security, and privacy concerns. While web tracking can offer personalized and
curated content to Internet users, it operates as a sophisticated surveillance
mechanism to gather extensive user information. This paper introduces
PriveShield, a light-weight privacy mechanism that disrupts the information
gathering cycle while offering more control to Internet users to maintain their
privacy. PriveShield is implemented as a browser extension that offers an
adjustable privacy feature to surf the web with multiple identities or accounts
simultaneously without any changes to underlying browser code or services. When
necessary, multiple factors are automatically analyzed on the client side to
isolate cookies and other information that are the basis of online tracking.
PriveShield creates isolated profiles for clients based on their browsing
history, interactions with websites, and the amount of time they spend on
specific websites. This allows the users to easily prevent unwanted browsing
information from being shared with third parties and ad exchanges without the
need for manual configuration. Our evaluation results from 54 real-world
scenarios show that our extension is effective in preventing retargeted ads in
91% of those scenarios.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.01786v1">Advancing privacy in learning analytics using differential privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-01-03T12:36:11Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Qinyi Liu, Ronas Shakya, Mohammad Khalil, Jelena Jovanovic</p>
    <p><b>Summary:</b> This paper addresses the challenge of balancing learner data privacy with the
use of data in learning analytics (LA) by proposing a novel framework by
applying Differential Privacy (DP). The need for more robust privacy protection
keeps increasing, driven by evolving legal regulations and heightened privacy
concerns, as well as traditional anonymization methods being insufficient for
the complexities of educational data. To address this, we introduce the first
DP framework specifically designed for LA and provide practical guidance for
its implementation. We demonstrate the use of this framework through a LA usage
scenario and validate DP in safeguarding data privacy against potential attacks
through an experiment on a well-known LA dataset. Additionally, we explore the
trade-offs between data privacy and utility across various DP settings. Our
work contributes to the field of LA by offering a practical DP framework that
can support researchers and practitioners in adopting DP in their works.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.01639v2">Implications of Artificial Intelligence on Health Data Privacy and
  Confidentiality</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-01-03T05:17:23Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ahmad Momani</p>
    <p><b>Summary:</b> The rapid integration of artificial intelligence (AI) in healthcare is
revolutionizing medical diagnostics, personalized medicine, and operational
efficiency. However, alongside these advancements, significant challenges arise
concerning patient data privacy, ethical considerations, and regulatory
compliance. This paper examines the dual impact of AI on healthcare,
highlighting its transformative potential and the critical need for
safeguarding sensitive health information. It explores the role of the Health
Insurance Portability and Accountability Act (HIPAA) as a regulatory framework
for ensuring data privacy and security, emphasizing the importance of robust
safeguards and ethical standards in AI-driven healthcare. Through case studies,
including AI applications in diabetic retinopathy, oncology, and the
controversies surrounding data sharing, this study underscores the ethical and
legal complexities of AI implementation. A balanced approach that fosters
innovation while maintaining patient trust and privacy is imperative. The
findings emphasize the importance of continuous education, transparency, and
adherence to regulatory frameworks to harness AI's full potential responsibly
and ethically in healthcare.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.01353v1">Privacy Preservation in MIMO-OFDM Localization Systems: A Beamforming
  Approach</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2025-01-02T17:08:15Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yuchen Zhang, Hui Chen, Musa Furkan Keskin, Alireza Pourafzal, Pinjun Zheng, Henk Wymeersch, Tareq Y. Al-Naffouri</p>
    <p><b>Summary:</b> We investigate an uplink MIMO-OFDM localization scenario where a legitimate
base station (BS) aims to localize a user equipment (UE) using pilot signals
transmitted by the UE, while an unauthorized BS attempts to localize the UE by
eavesdropping on these pilots, posing a risk to the UE's location privacy. To
enhance legitimate localization performance while protecting the UE's privacy,
we formulate an optimization problem regarding the beamformers at the UE,
aiming to minimize the Cram\'er-Rao bound (CRB) for legitimate localization
while constraining the CRB for unauthorized localization above a threshold. A
penalty dual decomposition optimization framework is employed to solve the
problem, leading to a novel beamforming approach for location privacy
preservation. Numerical results confirm the effectiveness of the proposed
approach and demonstrate its superiority over existing benchmarks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.01131v1">Privacy Bills of Materials: A Transparent Privacy Information Inventory
  for Collaborative Privacy Notice Generation in Mobile App Development</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Software Engineering-D91E36">
  <p><b>Published on:</b> 2025-01-02T08:14:52Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhen Tao, Shidong Pan, Zhenchang Xing, Xiaoyu Sun, Omar Haggag, John Grundy, Ze Shi Li, Jingjie Li, Liming Zhu</p>
    <p><b>Summary:</b> Privacy regulations mandate that developers must provide authentic and
comprehensive privacy notices, e.g., privacy policies or labels, to inform
users of their apps' privacy practices. However, due to a lack of knowledge of
privacy requirements, developers often struggle to create accurate privacy
notices, especially for sophisticated mobile apps with complex features and in
crowded development teams. To address these challenges, we introduce Privacy
Bills of Materials (PriBOM), a systematic software engineering approach that
leverages different development team roles to better capture and coordinate
mobile app privacy information. PriBOM facilitates transparency-centric privacy
documentation and specific privacy notice creation, enabling traceability and
trackability of privacy practices. We present a pre-fill of PriBOM based on
static analysis and privacy notice analysis techniques. We demonstrate the
perceived usefulness of PriBOM through a human evaluation with 150 diverse
participants. Our findings suggest that PriBOM could serve as a significant
solution for providing privacy support in DevOps for mobile apps.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.01063v1">FAPL-DM-BC: A Secure and Scalable FL Framework with Adaptive Privacy and
  Dynamic Masking, Blockchain, and XAI for the IoVs</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-01-02T05:21:52Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sathwik Narkedimilli, Amballa Venkata Sriram, Sujith Makam, MSVPJ Sathvik, Sai Prashanth Mallellu</p>
    <p><b>Summary:</b> The FAPL-DM-BC solution is a new FL-based privacy, security, and scalability
solution for the Internet of Vehicles (IoV). It leverages Federated Adaptive
Privacy-Aware Learning (FAPL) and Dynamic Masking (DM) to learn and adaptively
change privacy policies in response to changing data sensitivity and state in
real-time, for the optimal privacy-utility tradeoff. Secure Logging and
Verification, Blockchain-based provenance and decentralized validation, and
Cloud Microservices Secure Aggregation using FedAvg (Federated Averaging) and
Secure Multi-Party Computation (SMPC). Two-model feedback, driven by
Model-Agnostic Explainable AI (XAI), certifies local predictions and
explanations to drive it to the next level of efficiency. Combining local
feedback with world knowledge through a weighted mean computation, FAPL-DM-BC
assures federated learning that is secure, scalable, and interpretable.
Self-driving cars, traffic management, and forecasting, vehicular network
cybersecurity in real-time, and smart cities are a few possible applications of
this integrated, privacy-safe, and high-performance IoV platform.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.00824v2">Information Sifting Funnel: Privacy-preserving Collaborative Inference
  Against Model Inversion Attacks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2025-01-01T13:00:01Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Rongke Liu</p>
    <p><b>Summary:</b> The complexity of neural networks and inference tasks, coupled with demands
for computational efficiency and real-time feedback, poses significant
challenges for resource-constrained edge devices. Collaborative inference
mitigates this by assigning shallow feature extraction to edge devices and
offloading features to the cloud for further inference, reducing computational
load. However, transmitted features remain susceptible to model inversion
attacks (MIAs), which can reconstruct original input data. Current defenses,
such as perturbation and information bottleneck techniques, offer explainable
protection but face limitations, including the lack of standardized criteria
for assessing MIA difficulty, challenges in mutual information estimation, and
trade-offs among usability, privacy, and deployability.
  To address these challenges, we introduce the first criterion to evaluate MIA
difficulty in collaborative inference, supported by theoretical analysis of
existing attacks and defenses, validated using experiments with the Mutual
Information Neural Estimator (MINE). Based on these findings, we propose
SiftFunnel, a privacy-preserving framework for collaborative inference. The
edge model is trained with linear and non-linear correlation constraints to
reduce redundant information in transmitted features, enhancing privacy
protection. Label smoothing and a cloud-based upsampling module are added to
balance usability and privacy. To improve deployability, the edge model
incorporates a funnel-shaped structure and attention mechanisms, preserving
both privacy and usability. Extensive experiments demonstrate that SiftFunnel
outperforms state-of-the-art defenses against MIAs, achieving superior privacy
protection with less than 3% accuracy loss and striking an optimal balance
among usability, privacy, and practicality.</p>
  </details>
</div>



<h2>2024-12</h2>

<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.00588v2">Privacy-Preserving Distributed Defense Framework for DC Microgrids
  Against Exponentially Unbounded False Data Injection Attacks</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36">
  <p><b>Published on:</b> 2024-12-31T18:25:05Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yi Zhang, Mohamadamin Rajabinezhad, Yichao Wang, Junbo Zhao, Shan Zuo</p>
    <p><b>Summary:</b> This paper introduces a novel, fully distributed control framework for DC
microgrids, enhancing resilience against exponentially unbounded false data
injection (EU-FDI) attacks. Our framework features a consensus-based secondary
control for each converter, effectively addressing these advanced threats. To
further safeguard sensitive operational data, a privacy-preserving mechanism is
incorporated into the control design, ensuring that critical information
remains secure even under adversarial conditions. Rigorous Lyapunov stability
analysis confirms the framework's ability to maintain critical DC microgrid
operations like voltage regulation and load sharing under EU-FDI threats. The
framework's practicality is validated through hardware-in-the-loop experiments,
demonstrating its enhanced resilience and robust privacy protection against the
complex challenges posed by quick variant FDI attacks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.00363v1">SPDZCoder: Teaching LLMs to Synthesize Privacy Computing Code without
  Massive Training Data</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Software Engineering-D91E36">
  <p><b>Published on:</b> 2024-12-31T09:29:38Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xiaoning Dong, Peilin Xin, Wei Xu</p>
    <p><b>Summary:</b> Privacy computing receives increasing attention but writing privacy computing
code remains challenging for developers due to limited library functions that
necessitate extensive function implementation from scratch as well as the
data-oblivious requirement which contradicts intuitive thinking and usual
practices of programmers. Large language models (LLMs) have demonstrated
surprising capabilities in coding tasks and achieved state-of-the-art
performance across many benchmarks. However, even with extensive prompting,
existing LLMs struggle with code translation task for privacy computing, such
as translating Python to MP-SPDZ, due to the scarcity of MP-SPDZ data required
for effective pre-training or fine-tuning. To address the limitation, this
paper proposes SPDZCoder, a rule-based framework to teach LLMs to synthesize
privacy computing code without asking experts to write tons of code and by
leveraging the instruction-following and in-context learning ability of LLMs.
Specifically, SPDZCoder decouples the translation task into the refactoring
stage and the generation stage, which can mitigate the semantic-expressing
differences at different levels. In addition, SPDZCoder can further improve its
performance by a feedback stage. SPDZCoder does not require fine-tuning since
it adopts an in-context learning paradigm of LLMs. To evaluate SPDZCoder, we
manually created a benchmark dataset, named SPDZEval, containing six classes of
difficult tasks to implement in MP-SPDZ. We conduct experiments on SPDZEval and
the experimental results shows that SPDZCoder achieves the state-of-the-art
performance in pass@1 and pass@2 across six data splits. Specifically,
SPDZCoder achieves an overall correctness of 85.94% and 92.01% in pass@1 and
pass@2, respectively, significantly surpassing baselines (at most 30.35% and
49.84% in pass@1 and pass@2, respectively) by a large margin.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.21069v1">Privacy-Aware Multi-Device Cooperative Edge Inference with Distributed
  Resource Bidding</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Networking and Internet Architecture-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36">
  <p><b>Published on:</b> 2024-12-30T16:37:17Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Wenhao Zhuang, Yuyi Mao</p>
    <p><b>Summary:</b> Mobile edge computing (MEC) has empowered mobile devices (MDs) in supporting
artificial intelligence (AI) applications through collaborative efforts with
proximal MEC servers. Unfortunately, despite the great promise of device-edge
cooperative AI inference, data privacy becomes an increasing concern. In this
paper, we develop a privacy-aware multi-device cooperative edge inference
system for classification tasks, which integrates a distributed bidding
mechanism for the MEC server's computational resources. Intermediate feature
compression is adopted as a principled approach to minimize data privacy
leakage. To determine the bidding values and feature compression ratios in a
distributed fashion, we formulate a decentralized partially observable Markov
decision process (DEC-POMDP) model, for which, a multi-agent deep deterministic
policy gradient (MADDPG)-based algorithm is developed. Simulation results
demonstrate the effectiveness of the proposed algorithm in privacy-preserving
cooperative edge inference. Specifically, given a sufficient level of data
privacy protection, the proposed algorithm achieves 0.31-0.95% improvements in
classification accuracy compared to the approach being agnostic to the wireless
channel conditions. The performance is further enhanced by 1.54-1.67% by
considering the difficulties of inference data.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.20798v2">A Tale of Two Imperatives: Privacy and Explainability</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-12-30T08:43:28Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Supriya Manna, Niladri Sett</p>
    <p><b>Summary:</b> Deep learning's preponderance across scientific domains has reshaped
high-stakes decision-making, making it essential to follow rigorous operational
frameworks that include both Right-to-Privacy (RTP) and Right-to-Explanation
(RTE). This paper examines the complexities of combining these two
requirements. For RTP, we focus on `Differential privacy' (DP), which is
considered the current \textit{gold standard} for privacy-preserving machine
learning due to its strong quantitative guarantee of privacy. For RTE, we focus
on post-hoc explainers: they are the \textit{go-to} option for model auditing
as they operate independently of model training. We formally investigate DP
models and various commonly-used post-hoc explainers: how to evaluate these
explainers subject to RTP, and analyze the intrinsic interactions between DP
models and these explainers. Furthermore, our work throws light on how RTP and
RTE can be effectively combined in high-stakes applications. Our study
concludes by outlining an industrial software pipeline, with the example of a
wildly used use-case, that respects both RTP and RTE requirements.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.20762v1">Enhancing Privacy in Federated Learning through Quantum Teleportation
  Integration</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">  
  <p><b>Published on:</b> 2024-12-30T07:15:21Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Koffka Khan</p>
    <p><b>Summary:</b> Federated learning enables collaborative model training across multiple
clients without sharing raw data, thereby enhancing privacy. However, the
exchange of model updates can still expose sensitive information. Quantum
teleportation, a process that transfers quantum states between distant
locations without physical transmission of the particles themselves, has
recently been implemented in real-world networks. This position paper explores
the potential of integrating quantum teleportation into federated learning
frameworks to bolster privacy. By leveraging quantum entanglement and the
no-cloning theorem, quantum teleportation ensures that data remains secure
during transmission, as any eavesdropping attempt would be detectable. We
propose a novel architecture where quantum teleportation facilitates the secure
exchange of model parameters and gradients among clients and servers. This
integration aims to mitigate risks associated with data leakage and adversarial
attacks inherent in classical federated learning setups. We also discuss the
practical challenges of implementing such a system, including the current
limitations of quantum network infrastructure and the need for hybrid
quantum-classical protocols. Our analysis suggests that, despite these
challenges, the convergence of quantum communication technologies and federated
learning presents a promising avenue for achieving unprecedented levels of
privacy in distributed machine learning.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.20733v1">Towards nation-wide analytical healthcare infrastructures: A
  privacy-preserving augmented knee rehabilitation case study</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Multimedia-5BC0EB">
  <p><b>Published on:</b> 2024-12-30T06:14:48Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Boris Baƒçiƒá, Claudiu Vasile, Chengwei Feng, Marian G. CiucƒÉ</p>
    <p><b>Summary:</b> The purpose of this paper is to contribute towards the near-future
privacy-preserving big data analytical healthcare platforms, capable of
processing streamed or uploaded timeseries data or videos from patients. The
experimental work includes a real-life knee rehabilitation video dataset
capturing a set of exercises from simple and personalised to more general and
challenging movements aimed for returning to sport. To convert video from
mobile into privacy-preserving diagnostic timeseries data, we employed Google
MediaPipe pose estimation. The developed proof-of-concept algorithms can
augment knee exercise videos by overlaying the patient with stick figure
elements while updating generated timeseries plot with knee angle estimation
streamed as CSV file format. For patients and physiotherapists, video with
side-to-side timeseries visually indicating potential issues such as excessive
knee flexion or unstable knee movements or stick figure overlay errors is
possible by setting a-priori knee-angle parameters. To address adherence to
rehabilitation programme and quantify exercise sets and repetitions, our
adaptive algorithm can correctly identify (91.67%-100%) of all exercises from
side- and front-view videos. Transparent algorithm design for adaptive visual
analysis of various knee exercise patterns contributes towards the
interpretable AI and will inform near-future privacy-preserving, non-vendor
locking, open-source developments for both end-user computing devices and as
on-premises non-proprietary cloud platforms that can be deployed within the
national healthcare system.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.20641v1">SafeSynthDP: Leveraging Large Language Models for Privacy-Preserving
  Synthetic Data Generation Using Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-12-30T01:10:10Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Md Mahadi Hasan Nahid, Sadid Bin Hasan</p>
    <p><b>Summary:</b> Machine learning (ML) models frequently rely on training data that may
include sensitive or personal information, raising substantial privacy
concerns. Legislative frameworks such as the General Data Protection Regulation
(GDPR) and the California Consumer Privacy Act (CCPA) have necessitated the
development of strategies that preserve privacy while maintaining the utility
of data. In this paper, we investigate the capability of Large Language Models
(LLMs) to generate synthetic datasets integrated with Differential Privacy (DP)
mechanisms, thereby enabling data-driven research and model training without
direct exposure of sensitive information. Our approach incorporates DP-based
noise injection methods, including Laplace and Gaussian distributions, into the
data generation process. We then evaluate the utility of these DP-enhanced
synthetic datasets by comparing the performance of ML models trained on them
against models trained on the original data. To substantiate privacy
guarantees, we assess the resilience of the generated synthetic data to
membership inference attacks and related threats. The experimental results
demonstrate that integrating DP within LLM-driven synthetic data generation
offers a viable balance between privacy protection and data utility. This study
provides a foundational methodology and insight into the privacy-preserving
capabilities of LLMs, paving the way for compliant and effective ML research
and applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.20603v1">Privacy-Preserving Identity and Access Management in Multiple Cloud
  Environments: Models, Issues, and Solutions</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-12-29T22:15:19Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Alfredo Cuzzocrea, Islam Belmerabet</p>
    <p><b>Summary:</b> This paper focuses the attention on privacy-preserving identity and access
management in multiple Cloud environments, which is an annoying problem in the
modern big data era. Within this conceptual context, the paper describes
contemporaneous models and issues, and put the basis for future solid
solutions. Finally, we provide a summary table where we embed an innovative
taxonomy of state-of-the-art research proposals in the reference scientific
field.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.19496v1">Multi-P$^2$A: A Multi-perspective Benchmark on Privacy Assessment for
  Large Vision-Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-12-27T07:33:39Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jie Zhang, Xiangkui Cao, Zhouyu Han, Shiguang Shan, Xilin Chen</p>
    <p><b>Summary:</b> Large Vision-Language Models (LVLMs) exhibit impressive potential across
various tasks but also face significant privacy risks, limiting their practical
applications. Current researches on privacy assessment for LVLMs is limited in
scope, with gaps in both assessment dimensions and privacy categories. To
bridge this gap, we propose Multi-P$^2$A, a comprehensive benchmark for
evaluating the privacy preservation capabilities of LVLMs in terms of privacy
awareness and leakage. Privacy awareness measures the model's ability to
recognize the privacy sensitivity of input data, while privacy leakage assesses
the risk of the model unintentionally disclosing privacy information in its
output. We design a range of sub-tasks to thoroughly evaluate the model's
privacy protection offered by LVLMs. Multi-P$^2$A covers 26 categories of
personal privacy, 15 categories of trade secrets, and 18 categories of state
secrets, totaling 31,962 samples. Based on Multi-P$^2$A, we evaluate the
privacy preservation capabilities of 21 open-source and 2 closed-source LVLMs.
Our results reveal that current LVLMs generally pose a high risk of
facilitating privacy breaches, with vulnerabilities varying across personal
privacy, trade secret, and state secret.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.19291v2">RAG with Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-12-26T17:34:26Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Nicolas Grislain</p>
    <p><b>Summary:</b> Retrieval-Augmented Generation (RAG) has emerged as the dominant technique to
provide \emph{Large Language Models} (LLM) with fresh and relevant context,
mitigating the risk of hallucinations and improving the overall quality of
responses in environments with large and fast moving knowledge bases. However,
the integration of external documents into the generation process raises
significant privacy concerns. Indeed, when added to a prompt, it is not
possible to guarantee a response will not inadvertently expose confidential
data, leading to potential breaches of privacy and ethical dilemmas. This paper
explores a practical solution to this problem suitable to general knowledge
extraction from personal data. It shows \emph{differentially private token
generation} is a viable approach to private RAG.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.18992v2">Optimal Federated Learning for Functional Mean Estimation under
  Heterogeneous Privacy Constraints</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Statistics Theory-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">  
  <p><b>Published on:</b> 2024-12-25T22:06:12Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tony Cai, Abhinav Chakraborty, Lasse Vuursteen</p>
    <p><b>Summary:</b> Federated learning (FL) is a distributed machine learning technique designed
to preserve data privacy and security, and it has gained significant importance
due to its broad range of applications. This paper addresses the problem of
optimal functional mean estimation from discretely sampled data in a federated
setting.
  We consider a heterogeneous framework where the number of individuals,
measurements per individual, and privacy parameters vary across one or more
servers, under both common and independent design settings. In the common
design setting, the same design points are measured for each individual,
whereas in the independent design, each individual has their own random
collection of design points. Within this framework, we establish minimax upper
and lower bounds for the estimation error of the underlying mean function,
highlighting the nuanced differences between common and independent designs
under distributed privacy constraints.
  We propose algorithms that achieve the optimal trade-off between privacy and
accuracy and provide optimality results that quantify the fundamental limits of
private functional mean estimation across diverse distributed settings. These
results characterize the cost of privacy and offer practical insights into the
potential for privacy-preserving statistical analysis in federated
environments.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.18716v1">Design and Evaluation of Privacy-Preserving Protocols for
  Agent-Facilitated Mobile Money Services in Kenya</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2024-12-25T00:27:13Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Karen Sowon, Collins W. Munyendo, Lily Klucinec, Eunice Maingi, Gerald Suleh, Lorrie Faith Cranor, Giulia Fanti, Conrad Tucker, Assane Gueye</p>
    <p><b>Summary:</b> Mobile Money (MoMo), a technology that allows users to complete digital
financial transactions using a mobile phone without requiring a bank account,
has become a common method for processing financial transactions in Africa and
other developing regions. Operationally, users can deposit (exchange cash for
mobile money tokens) and withdraw with the help of human agents who facilitate
a near end-to-end process from customer onboarding to authentication and
recourse. During deposit and withdraw operations, know-your-customer (KYC)
processes require agents to access and verify customer information such as name
and ID number, which can introduce privacy and security risks. In this work, we
design alternative protocols for mobile money deposits and withdrawals that
protect users' privacy while enabling KYC checks. These workflows redirect the
flow of sensitive information from the agent to the MoMo provider, thus
allowing the agent to facilitate transactions without accessing a customer's
personal information. We evaluate the usability and efficiency of our proposed
protocols in a role play and semi-structured interview study with 32 users and
15 agents in Kenya. We find that users and agents both generally appear to
prefer the new protocols, due in part to convenient and efficient verification
using biometrics, better data privacy and access control, as well as better
security mechanisms for delegated transactions. Our results also highlight some
challenges and limitations that suggest the need for more work to build
deployable solutions.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.19837v1">Data Poisoning Attacks to Local Differential Privacy Protocols for
  Graphs</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB">
  <p><b>Published on:</b> 2024-12-23T11:16:23Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xi He, Kai Huang, Qingqing Ye, Haibo Hu</p>
    <p><b>Summary:</b> Graph analysis has become increasingly popular with the prevalence of big
data and machine learning. Traditional graph data analysis methods often assume
the existence of a trusted third party to collect and store the graph data,
which does not align with real-world situations. To address this, some research
has proposed utilizing Local Differential Privacy (LDP) to collect graph data
or graph metrics (e.g., clustering coefficient). This line of research focuses
on collecting two atomic graph metrics (the adjacency bit vectors and node
degrees) from each node locally under LDP to synthesize an entire graph or
generate graph metrics. However, they have not considered the security issues
of LDP for graphs.
  In this paper, we bridge the gap by demonstrating that an attacker can inject
fake users into LDP protocols for graphs and design data poisoning attacks to
degrade the quality of graph metrics. In particular, we present three data
poisoning attacks to LDP protocols for graphs. As a proof of concept, we focus
on data poisoning attacks on two classical graph metrics: degree centrality and
clustering coefficient. We further design two countermeasures for these data
poisoning attacks. Experimental study on real-world datasets demonstrates that
our attacks can largely degrade the quality of collected graph metrics, and the
proposed countermeasures cannot effectively offset the effect, which calls for
the development of new defenses.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.17317v1">Better Knowledge Enhancement for Privacy-Preserving Cross-Project Defect
  Prediction</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Software Engineering-D91E36">
  <p><b>Published on:</b> 2024-12-23T06:21:15Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yuying Wang, Yichen Li, Haozhao Wang, Lei Zhao, Xiaofang Zhang</p>
    <p><b>Summary:</b> Cross-Project Defect Prediction (CPDP) poses a non-trivial challenge to
construct a reliable defect predictor by leveraging data from other projects,
particularly when data owners are concerned about data privacy. In recent
years, Federated Learning (FL) has become an emerging paradigm to guarantee
privacy information by collaborative training a global model among multiple
parties without sharing raw data. While the direct application of FL to the
CPDP task offers a promising solution to address privacy concerns, the data
heterogeneity arising from proprietary projects across different companies or
organizations will bring troubles for model training. In this paper, we study
the privacy-preserving cross-project defect prediction with data heterogeneity
under the federated learning framework. To address this problem, we propose a
novel knowledge enhancement approach named FedDP with two simple but effective
solutions: 1. Local Heterogeneity Awareness and 2. Global Knowledge
Distillation. Specifically, we employ open-source project data as the
distillation dataset and optimize the global model with the heterogeneity-aware
local model ensemble via knowledge distillation. Experimental results on 19
projects from two datasets demonstrate that our method significantly
outperforms baselines.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.17038v3">ErasableMask: A Robust and Erasable Privacy Protection Scheme against
  Black-box Face Recognition Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-12-22T14:30:26Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sipeng Shen, Yunming Zhang, Dengpan Ye, Xiuwen Shi, Long Tang, Haoran Duan, Jiacheng Deng, Ziyi Liu</p>
    <p><b>Summary:</b> While face recognition (FR) models have brought remarkable convenience in
face verification and identification, they also pose substantial privacy risks
to the public. Existing facial privacy protection schemes usually adopt
adversarial examples to disrupt face verification of FR models. However, these
schemes often suffer from weak transferability against black-box FR models and
permanently damage the identifiable information that cannot fulfill the
requirements of authorized operations such as forensics and authentication. To
address these limitations, we propose ErasableMask, a robust and erasable
privacy protection scheme against black-box FR models. Specifically, via
rethinking the inherent relationship between surrogate FR models, ErasableMask
introduces a novel meta-auxiliary attack, which boosts black-box
transferability by learning more general features in a stable and balancing
optimization strategy. It also offers a perturbation erasion mechanism that
supports the erasion of semantic perturbations in protected face without
degrading image quality. To further improve performance, ErasableMask employs a
curriculum learning strategy to mitigate optimization conflicts between
adversarial attack and perturbation erasion. Extensive experiments on the
CelebA-HQ and FFHQ datasets demonstrate that ErasableMask achieves the
state-of-the-art performance in transferability, achieving over 72% confidence
on average in commercial FR systems. Moreover, ErasableMask also exhibits
outstanding perturbation erasion performance, achieving over 90% erasion
success rate.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.16916v1">On the Differential Privacy and Interactivity of Privacy Sandbox Reports</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-12-22T08:22:57Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Badih Ghazi, Charlie Harrison, Arpana Hosabettu, Pritish Kamath, Alexander Knop, Ravi Kumar, Ethan Leeman, Pasin Manurangsi, Vikas Sahu</p>
    <p><b>Summary:</b> The Privacy Sandbox initiative from Google includes APIs for enabling
privacy-preserving advertising functionalities as part of the effort around
limiting third-party cookies. In particular, the Private Aggregation API (PAA)
and the Attribution Reporting API (ARA) can be used for ad measurement while
providing different guardrails for safeguarding user privacy, including a
framework for satisfying differential privacy (DP). In this work, we provide a
formal model for analyzing the privacy of these APIs and show that they satisfy
a formal DP guarantee under certain assumptions. Our analysis handles the case
where both the queries and database can change interactively based on previous
responses from the API.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.16893v1">Preventing Non-intrusive Load Monitoring Privacy Invasion: A Precise
  Adversarial Attack Scheme for Networked Smart Meters</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-12-22T07:06:46Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jialing He, Jiacheng Wang, Ning Wang, Shangwei Guo, Liehuang Zhu, Dusit Niyato, Tao Xiang</p>
    <p><b>Summary:</b> Smart grid, through networked smart meters employing the non-intrusive load
monitoring (NILM) technique, can considerably discern the usage patterns of
residential appliances. However, this technique also incurs privacy leakage. To
address this issue, we propose an innovative scheme based on adversarial attack
in this paper. The scheme effectively prevents NILM models from violating
appliance-level privacy, while also ensuring accurate billing calculation for
users. To achieve this objective, we overcome two primary challenges. First, as
NILM models fall under the category of time-series regression models, direct
application of traditional adversarial attacks designed for classification
tasks is not feasible. To tackle this issue, we formulate a novel adversarial
attack problem tailored specifically for NILM and providing a theoretical
foundation for utilizing the Jacobian of the NILM model to generate
imperceptible perturbations. Leveraging the Jacobian, our scheme can produce
perturbations, which effectively misleads the signal prediction of NILM models
to safeguard users' appliance-level privacy. The second challenge pertains to
fundamental utility requirements, where existing adversarial attack schemes
struggle to achieve accurate billing calculation for users. To handle this
problem, we introduce an additional constraint, mandating that the sum of added
perturbations within a billing period must be precisely zero. Experimental
validation on real-world power datasets REDD and UK-DALE demonstrates the
efficacy of our proposed solutions, which can significantly amplify the
discrepancy between the output of the targeted NILM model and the actual power
signal of appliances, and enable accurate billing at the same time.
Additionally, our solutions exhibit transferability, making the generated
perturbation signal from one target model applicable to other diverse NILM
models.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.16825v1">SoK: Usability Studies in Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-12-22T02:21:57Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Onyinye Dibia, Brad Stenger, Steven Baldasty, Mako Bates, Ivoline C. Ngong, Yuanyuan Feng, Joseph P. Near</p>
    <p><b>Summary:</b> Differential Privacy (DP) has emerged as a pivotal approach for safeguarding
individual privacy in data analysis, yet its practical adoption is often
hindered by challenges in usability in implementation and communication of the
privacy protection levels. This paper presents a comprehensive systematization
of existing research on the usability of and communication about DP,
synthesizing insights from studies on both the practical use of DP tools and
strategies for conveying DP parameters that determine the privacy protection
levels such as epsilon. By reviewing and analyzing these studies, we identify
core usability challenges, best practices, and critical gaps in current DP
tools that affect adoption across diverse user groups, including developers,
data analysts, and non-technical stakeholders. Our analysis highlights
actionable insights and pathways for future research that emphasizes
user-centered design and clear communication, fostering the development of more
accessible DP tools that meet practical needs and support broader adoption.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.16669v1">Label Privacy in Split Learning for Large Models with
  Parameter-Efficient Training</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-12-21T15:32:03Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Philip Zmushko, Marat Mansurov, Ruslan Svirschevski, Denis Kuznedelev, Max Ryabinin, Aleksandr Beznosikov</p>
    <p><b>Summary:</b> As deep learning models become larger and more expensive, many practitioners
turn to fine-tuning APIs. These web services allow fine-tuning a model between
two parties: the client that provides the data, and the server that hosts the
model. While convenient, these APIs raise a new concern: the data of the client
is at risk of privacy breach during the training procedure. This challenge
presents an important practical case of vertical federated learning, where the
two parties perform parameter-efficient fine-tuning (PEFT) of a large model. In
this study, we systematically search for a way to fine-tune models over an API
while keeping the labels private. We analyze the privacy of LoRA, a popular
approach for parameter-efficient fine-tuning when training over an API. Using
this analysis, we propose P$^3$EFT, a multi-party split learning algorithm that
takes advantage of existing PEFT properties to maintain privacy at a lower
performance overhead. To validate our algorithm, we fine-tune
DeBERTa-v2-XXLarge, Flan-T5 Large and LLaMA-2 7B using LoRA adapters on a range
of NLP tasks. We find that P$^3$EFT is competitive with existing
privacy-preserving methods in multi-party and two-party setups while having
higher accuracy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.16667v1">The Good, the Bad, and the (Un)Usable: A Rapid Literature Review on
  Privacy as Code</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Software Engineering-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2024-12-21T15:30:17Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Nicol√°s E. D√≠az Ferreyra, Sirine Khelifi, Nalin Arachchilage, Riccardo Scandariato</p>
    <p><b>Summary:</b> Privacy and security are central to the design of information systems endowed
with sound data protection and cyber resilience capabilities. Still, developers
often struggle to incorporate these properties into software projects as they
either lack proper cybersecurity training or do not consider them a priority.
Prior work has tried to support privacy and security engineering activities
through threat modeling methods for scrutinizing flaws in system architectures.
Moreover, several techniques for the automatic identification of
vulnerabilities and the generation of secure code implementations have also
been proposed in the current literature. Conversely, such as-code approaches
seem under-investigated in the privacy domain, with little work elaborating on
(i) the automatic detection of privacy properties in source code or (ii) the
generation of privacy-friendly code. In this work, we seek to characterize the
current research landscape of Privacy as Code (PaC) methods and tools by
conducting a rapid literature review. Our results suggest that PaC research is
in its infancy, especially regarding the performance evaluation and usability
assessment of the existing approaches. Based on these findings, we outline and
discuss prospective research directions concerning empirical studies with
software practitioners, the curation of benchmark datasets, and the role of
generative AI technologies.</p>
  </details>
</div>

