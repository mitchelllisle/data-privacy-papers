
<h2>2024-08</h2>

<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.17378v1">Empowering Open Data Sharing for Social Good: A Privacy-Aware Approach</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB">
  <p><b>Published on:</b> 2024-08-30T16:14:32Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tânia Carvalho, Luís Antunes, Cristina Costa, Nuno Moniz</p>
    <p><b>Summary:</b> The Covid-19 pandemic has affected the world at multiple levels. Data sharing
was pivotal for advancing research to understand the underlying causes and
implement effective containment strategies. In response, many countries have
promoted the availability of daily cases to support research initiatives,
fostering collaboration between organisations and making such data available to
the public through open data platforms. Despite the several advantages of data
sharing, one of the major concerns before releasing health data is its impact
on individuals' privacy. Such a sharing process should be based on
state-of-the-art methods in Data Protection by Design and by Default. In this
paper, we use a data set related to Covid-19 cases in the second largest
hospital in Portugal to show how it is feasible to ensure data privacy while
improving the quality and maintaining the utility of the data. Our goal is to
demonstrate how knowledge exchange in multidisciplinary teams of healthcare
practitioners, data privacy, and data science experts is crucial to
co-developing strategies that ensure high utility of de-identified data.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.17354v1">Forget to Flourish: Leveraging Machine-Unlearning on Pretrained Language
  Models for Privacy Leakage</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-30T15:35:09Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Md Rafi Ur Rashid, Jing Liu, Toshiaki Koike-Akino, Shagufta Mehnaz, Ye Wang</p>
    <p><b>Summary:</b> Fine-tuning large language models on private data for downstream applications
poses significant privacy risks in potentially exposing sensitive information.
Several popular community platforms now offer convenient distribution of a
large variety of pre-trained models, allowing anyone to publish without
rigorous verification. This scenario creates a privacy threat, as pre-trained
models can be intentionally crafted to compromise the privacy of fine-tuning
datasets. In this study, we introduce a novel poisoning technique that uses
model-unlearning as an attack tool. This approach manipulates a pre-trained
language model to increase the leakage of private data during the fine-tuning
process. Our method enhances both membership inference and data extraction
attacks while preserving model utility. Experimental results across different
models, datasets, and fine-tuning setups demonstrate that our attacks
significantly surpass baseline performance. This work serves as a cautionary
note for users who download pre-trained models from unverified sources,
highlighting the potential risks involved.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.17263v1">Privacy-Preserving Set-Based Estimation Using Differential Privacy and
  Zonotopes</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-30T13:05:38Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mohammed M. Dawoud, Changxin Liu, Karl H. Johansson, Amr Alanwar</p>
    <p><b>Summary:</b> For large-scale cyber-physical systems, the collaboration of spatially
distributed sensors is often needed to perform the state estimation process.
Privacy concerns arise from disclosing sensitive measurements to a cloud
estimator. To solve this issue, we propose a differentially private set-based
estimation protocol that guarantees true state containment in the estimated set
and differential privacy for the sensitive measurements throughout the
set-based state estimation process within the central and local differential
privacy models. Zonotopes are employed in the proposed differentially private
set-based estimator, offering computational advantages in set operations. We
consider a plant of a non-linear discrete-time dynamical system with bounded
modeling uncertainties, sensors that provide sensitive measurements with
bounded measurement uncertainties, and a cloud estimator that predicts the
system's state. The privacy-preserving noise perturbs the centers of
measurement zonotopes, thereby concealing the precise position of these
zonotopes, i.e., ensuring privacy preservation for the sets containing
sensitive measurements. Compared to existing research, our approach achieves
less privacy loss and utility loss through the central and local differential
privacy models by leveraging a numerically optimized truncated noise
distribution. The proposed estimator is perturbed by weaker noise than the
analytical approaches in the literature to guarantee the same level of privacy,
therefore improving the estimation utility. Numerical and comparison
experiments with truncated Laplace noise are presented to support our approach.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.17151v1">Investigating Privacy Leakage in Dimensionality Reduction Methods via
  Reconstruction Attack</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-08-30T09:40:52Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chayadon Lumbut, Donlapark Ponnoprat</p>
    <p><b>Summary:</b> This study investigates privacy leakage in dimensionality reduction methods
through a novel machine learning-based reconstruction attack. Employing an
\emph{informed adversary} threat model, we develop a neural network capable of
reconstructing high-dimensional data from low-dimensional embeddings.
  We evaluate six popular dimensionality reduction techniques: PCA, sparse
random projection (SRP), multidimensional scaling (MDS), Isomap, $t$-SNE, and
UMAP. Using both MNIST and NIH Chest X-ray datasets, we perform a qualitative
analysis to identify key factors affecting reconstruction quality. Furthermore,
we assess the effectiveness of an additive noise mechanism in mitigating these
reconstruction attacks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.17049v1">SPOQchain: Platform for Secure, Scalable, and Privacy-Preserving Supply
  Chain Tracing and Counterfeit Protection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-30T07:15:43Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Moritz Finke, Alexandra Dmitrienko, Jasper Stang</p>
    <p><b>Summary:</b> Product lifecycle tracing is increasingly in the focus of regulators and
producers, as shown with the initiative of the Digital Product Pass. Likewise,
new methods of counterfeit detection are developed that are, e.g., based on
Physical Unclonable Functions (PUFs). In order to ensure trust and integrity of
product lifecycle data, multiple existing supply chain tracing systems are
built on blockchain technology. However, only few solutions employ secure
identifiers such as PUFs. Furthermore, existing systems that publish the data
of individual products, in part fully transparently, have a detrimental impact
on scalability and the privacy of users. This work proposes SPOQchain, a novel
blockchain-based platform that provides comprehensive lifecycle traceability
and originality verification while ensuring high efficiency and user privacy.
The improved efficiency is achieved by a sophisticated batching mechanism that
removes lifecycle redundancies. In addition to the successful evaluation of
SPOQchain's scalability, this work provides a comprehensive analysis of privacy
and security aspects, demonstrating the need and qualification of SPOQchain for
the future of supply chain tracing.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.16913v1">Analyzing Inference Privacy Risks Through Gradients in Machine Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2024-08-29T21:21:53Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhuohang Li, Andrew Lowy, Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, Bradley Malin, Ye Wang</p>
    <p><b>Summary:</b> In distributed learning settings, models are iteratively updated with shared
gradients computed from potentially sensitive user data. While previous work
has studied various privacy risks of sharing gradients, our paper aims to
provide a systematic approach to analyze private information leakage from
gradients. We present a unified game-based framework that encompasses a broad
range of attacks including attribute, property, distributional, and user
disclosures. We investigate how different uncertainties of the adversary affect
their inferential power via extensive experiments on five datasets across
various data modalities. Our results demonstrate the inefficacy of solely
relying on data aggregation to achieve privacy against inference attacks in
distributed learning. We further evaluate five types of defenses, namely,
gradient pruning, signed gradient descent, adversarial perturbations,
variational information bottleneck, and differential privacy, under both static
and adaptive adversary settings. We provide an information-theoretic view for
analyzing the effectiveness of these defenses against inference from gradients.
Finally, we introduce a method for auditing attribute inference privacy,
improving the empirical estimation of worst-case privacy through crafting
adversarial canary records.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.16304v1">Understanding Privacy Norms through Web Forms</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-29T07:11:09Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Hao Cui, Rahmadi Trimananda, Athina Markopoulou</p>
    <p><b>Summary:</b> Web forms are one of the primary ways to collect personal information online,
yet they are relatively under-studied. Unlike web tracking, data collection
through web forms is explicit and contextualized. Users (i) are asked to input
specific personal information types, and (ii) know the specific context (i.e.,
on which website and for what purpose). For web forms to be trusted by users,
they must meet the common sense standards of appropriate data collection
practices within a particular context (i.e., privacy norms). In this paper, we
extract the privacy norms embedded within web forms through a measurement
study. First, we build a specialized crawler to discover web forms on websites.
We run it on 11,500 popular websites, and we create a dataset of 293K web
forms. Second, to process data of this scale, we develop a cost-efficient way
to annotate web forms with form types and personal information types, using
text classifiers trained with assistance of large language models (LLMs).
Third, by analyzing the annotated dataset, we reveal common patterns of data
collection practices. We find that (i) these patterns are explained by
functional necessities and legal obligations, thus reflecting privacy norms,
and that (ii) deviations from the observed norms often signal unnecessary data
collection. In addition, we analyze the privacy policies that accompany web
forms. We show that, despite their wide adoption and use, there is a disconnect
between privacy policy disclosures and the observed privacy norms.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.15694v1">Protecting Privacy in Federated Time Series Analysis: A Pragmatic
  Technology Review for Application Developers</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-28T10:41:31Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Daniel Bachlechner, Ruben Hetfleisch, Stephan Krenn, Thomas Lorünser, Michael Rader</p>
    <p><b>Summary:</b> The federated analysis of sensitive time series has huge potential in various
domains, such as healthcare or manufacturing. Yet, to fully unlock this
potential, requirements imposed by various stakeholders must be fulfilled,
regarding, e.g., efficiency or trust assumptions. While many of these
requirements can be addressed by deploying advanced secure computation
paradigms such as fully homomorphic encryption, certain aspects require an
integration with additional privacy-preserving technologies.
  In this work, we perform a qualitative requirements elicitation based on
selected real-world use cases. We match the derived requirements categories
against the features and guarantees provided by available technologies. For
each technology, we additionally perform a maturity assessment, including the
state of standardization and availability on the market. Furthermore, we
provide a decision tree supporting application developers in identifying the
most promising technologies available matching their needs. Finally, existing
gaps are identified, highlighting research potential to advance the field.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.15688v1">PDSR: A Privacy-Preserving Diversified Service Recommendation Method on
  Distributed Data</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Retrieval-5BC0EB">
  <p><b>Published on:</b> 2024-08-28T10:25:36Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Lina Wang, Huan Yang, Yiran Shen, Chao Liu, Lianyong Qi, Xiuzhen Cheng, Feng Li</p>
    <p><b>Summary:</b> The last decade has witnessed a tremendous growth of service computing, while
efficient service recommendation methods are desired to recommend high-quality
services to users. It is well known that collaborative filtering is one of the
most popular methods for service recommendation based on QoS, and many existing
proposals focus on improving recommendation accuracy, i.e., recommending
high-quality redundant services. Nevertheless, users may have different
requirements on QoS, and hence diversified recommendation has been attracting
increasing attention in recent years to fulfill users' diverse demands and to
explore potential services. Unfortunately, the recommendation performances
relies on a large volume of data (e.g., QoS data), whereas the data may be
distributed across multiple platforms. Therefore, to enable data sharing across
the different platforms for diversified service recommendation, we propose a
Privacy-preserving Diversified Service Recommendation (PDSR) method.
Specifically, we innovate in leveraging the Locality-Sensitive Hashing (LSH)
mechanism such that privacy-preserved data sharing across different platforms
is enabled to construct a service similarity graph. Based on the similarity
graph, we propose a novel accuracy-diversity metric and design a
$2$-approximation algorithm to select $K$ services to recommend by maximizing
the accuracy-diversity measure. Extensive experiments on real datasets are
conducted to verify the efficacy of our PDSR method.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.15621v1">Convergent Differential Privacy Analysis for General Federated Learning:
  the f-DP Perspective</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-28T08:22:21Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yan Sun, Li Shen, Dacheng Tao</p>
    <p><b>Summary:</b> Federated learning (FL) is an efficient collaborative training paradigm
extensively developed with a focus on local privacy protection, and
differential privacy (DP) is a classical approach to capture and ensure the
reliability of local privacy. The powerful cooperation of FL and DP provides a
promising learning framework for large-scale private clients, juggling both
privacy securing and trustworthy learning. As the predominant algorithm of DP,
the noisy perturbation has been widely studied and incorporated into various
federated algorithms, theoretically proven to offer significant privacy
protections. However, existing analyses in noisy FL-DP mostly rely on the
composition theorem and cannot tightly quantify the privacy leakage challenges,
which is nearly tight for small numbers of communication rounds but yields an
arbitrarily loose and divergent bound under the large communication rounds.
This implies a counterintuitive judgment, suggesting that FL may not provide
adequate privacy protection during long-term training. To further investigate
the convergent privacy and reliability of the FL-DP framework, in this paper,
we comprehensively evaluate the worst privacy of two classical methods under
the non-convex and smooth objectives based on the f-DP analysis, i.e.
Noisy-FedAvg and Noisy-FedProx methods. With the aid of the
shifted-interpolation technique, we successfully prove that the worst privacy
of the Noisy-FedAvg method achieves a tight convergent lower bound. Moreover,
in the Noisy-FedProx method, with the regularization of the proxy term, the
worst privacy has a stable constant lower bound. Our analysis further provides
a solid theoretical foundation for the reliability of privacy protection in
FL-DP. Meanwhile, our conclusions can also be losslessly converted to other
classical DP analytical frameworks, e.g. $(\epsilon,\delta)$-DP and
R$\acute{\text{e}}$nyi-DP (RDP).</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.15391v1">Examining the Interplay Between Privacy and Fairness for Speech
  Processing: A Review and Perspective</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Sound-D91E36">
  <p><b>Published on:</b> 2024-08-27T20:32:01Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Anna Leschanowsky, Sneha Das</p>
    <p><b>Summary:</b> Speech technology has been increasingly deployed in various areas of daily
life including sensitive domains such as healthcare and law enforcement. For
these technologies to be effective, they must work reliably for all users while
preserving individual privacy. Although tradeoffs between privacy and utility,
as well as fairness and utility, have been extensively researched, the specific
interplay between privacy and fairness in speech processing remains
underexplored. This review and position paper offers an overview of emerging
privacy-fairness tradeoffs throughout the entire machine learning lifecycle for
speech processing. By drawing on well-established frameworks on fairness and
privacy, we examine existing biases and sources of privacy harm that coexist
during the development of speech processing models. We then highlight how
corresponding privacy-enhancing technologies have the potential to
inadvertently increase these biases and how bias mitigation strategies may
conversely reduce privacy. By raising open questions, we advocate for a
comprehensive evaluation of privacy-fairness tradeoffs for speech technology
and the development of privacy-enhancing and fairness-aware algorithms in this
domain.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.15077v2">MMASD+: A Novel Dataset for Privacy-Preserving Behavior Analysis of
  Children with Autism Spectrum Disorder</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-08-27T14:05:48Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Pavan Uttej Ravva, Behdokht Kiafar, Pinar Kullu, Jicheng Li, Anjana Bhat, Roghayeh Leila Barmaki</p>
    <p><b>Summary:</b> Autism spectrum disorder (ASD) is characterized by significant challenges in
social interaction and comprehending communication signals. Recently,
therapeutic interventions for ASD have increasingly utilized Deep learning
powered-computer vision techniques to monitor individual progress over time.
These models are trained on private, non-public datasets from the autism
community, creating challenges in comparing results across different models due
to privacy-preserving data-sharing issues. This work introduces MMASD+, an
enhanced version of the novel open-source dataset called Multimodal ASD
(MMASD). MMASD+ consists of diverse data modalities, including 3D-Skeleton, 3D
Body Mesh, and Optical Flow data. It integrates the capabilities of Yolov8 and
Deep SORT algorithms to distinguish between the therapist and children,
addressing a significant barrier in the original dataset. Additionally, a
Multimodal Transformer framework is proposed to predict 11 action types and the
presence of ASD. This framework achieves an accuracy of 95.03% for predicting
action types and 96.42% for predicting ASD presence, demonstrating over a 10%
improvement compared to models trained on single data modalities. These
findings highlight the advantages of integrating multiple data modalities
within the Multimodal Transformer framework.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.14830v1">PolicyLR: A Logic Representation For Privacy Policies</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2024-08-27T07:27:16Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ashish Hooda, Rishabh Khandelwal, Prasad Chalasani, Kassem Fawaz, Somesh Jha</p>
    <p><b>Summary:</b> Privacy policies are crucial in the online ecosystem, defining how services
handle user data and adhere to regulations such as GDPR and CCPA. However,
their complexity and frequent updates often make them difficult for
stakeholders to understand and analyze. Current automated analysis methods,
which utilize natural language processing, have limitations. They typically
focus on individual tasks and fail to capture the full context of the policies.
We propose PolicyLR, a new paradigm that offers a comprehensive
machine-readable representation of privacy policies, serving as an all-in-one
solution for multiple downstream tasks. PolicyLR converts privacy policies into
a machine-readable format using valuations of atomic formulae, allowing for
formal definitions of tasks like compliance and consistency. We have developed
a compiler that transforms unstructured policy text into this format using
off-the-shelf Large Language Models (LLMs). This compiler breaks down the
transformation task into a two-stage translation and entailment procedure. This
procedure considers the full context of the privacy policy to infer a complex
formula, where each formula consists of simpler atomic formulae. The advantage
of this model is that PolicyLR is interpretable by design and grounded in
segments of the privacy policy. We evaluated the compiler using ToS;DR, a
community-annotated privacy policy entailment dataset. Utilizing open-source
LLMs, our compiler achieves precision and recall values of 0.91 and 0.88,
respectively. Finally, we demonstrate the utility of PolicyLR in three privacy
tasks: Policy Compliance, Inconsistency Detection, and Privacy Comparison
Shopping.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.14753v1">CoopASD: Cooperative Machine Anomalous Sound Detection with Privacy
  Concerns</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Sound-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB"> 
  <p><b>Published on:</b> 2024-08-27T03:07:03Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Anbai Jiang, Yuchen Shi, Pingyi Fan, Wei-Qiang Zhang, Jia Liu</p>
    <p><b>Summary:</b> Machine anomalous sound detection (ASD) has emerged as one of the most
promising applications in the Industrial Internet of Things (IIoT) due to its
unprecedented efficacy in mitigating risks of malfunctions and promoting
production efficiency. Previous works mainly investigated the machine ASD task
under centralized settings. However, developing the ASD system under
decentralized settings is crucial in practice, since the machine data are
dispersed in various factories and the data should not be explicitly shared due
to privacy concerns. To enable these factories to cooperatively develop a
scalable ASD model while preserving their privacy, we propose a novel framework
named CoopASD, where each factory trains an ASD model on its local dataset, and
a central server aggregates these local models periodically. We employ a
pre-trained model as the backbone of the ASD model to improve its robustness
and develop specialized techniques to stabilize the model under a completely
non-iid and domain shift setting. Compared with previous state-of-the-art
(SOTA) models trained in centralized settings, CoopASD showcases competitive
results with negligible degradation of 0.08%. We also conduct extensive
ablation studies to demonstrate the effectiveness of CoopASD.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.14735v1">PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework
  with Correlated Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Multimedia-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2024-08-27T02:03:36Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xianzhi Zhang, Yipeng Zhou, Di Wu, Quan Z. Sheng, Miao Hu, Linchang Xiao</p>
    <p><b>Summary:</b> Online video streaming has evolved into an integral component of the
contemporary Internet landscape. Yet, the disclosure of user requests presents
formidable privacy challenges. As users stream their preferred online videos,
their requests are automatically seized by video content providers, potentially
leaking users' privacy.
  Unfortunately, current protection methods are not well-suited to preserving
user request privacy from content providers while maintaining high-quality
online video services. To tackle this challenge, we introduce a novel
Privacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge
devices to pre-fetch and cache videos, ensuring the privacy of users' requests
while optimizing the efficiency of edge caching. More specifically, we design
PPVF with three core components: (1) \textit{Online privacy budget scheduler},
which employs a theoretically guaranteed online algorithm to select
non-requested videos as candidates with assigned privacy budgets. Alternative
videos are chosen by an online algorithm that is theoretically guaranteed to
consider both video utilities and available privacy budgets. (2) \textit{Noisy
video request generator}, which generates redundant video requests (in addition
to original ones) utilizing correlated differential privacy to obfuscate
request privacy. (3) \textit{Online video utility predictor}, which leverages
federated learning to collaboratively evaluate video utility in an online
fashion, aiding in video selection in (1) and noise generation in (2). Finally,
we conduct extensive experiments using real-world video request traces from
Tencent Video. The results demonstrate that PPVF effectively safeguards user
request privacy while upholding high video caching performance.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.14689v1">Federated User Preference Modeling for Privacy-Preserving Cross-Domain
  Recommendation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Retrieval-5BC0EB">
  <p><b>Published on:</b> 2024-08-26T23:29:03Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Li Wang, Shoujin Wang, Quangui Zhang, Qiang Wu, Min Xu</p>
    <p><b>Summary:</b> Cross-domain recommendation (CDR) aims to address the data-sparsity problem
by transferring knowledge across domains. Existing CDR methods generally assume
that the user-item interaction data is shareable between domains, which leads
to privacy leakage. Recently, some privacy-preserving CDR (PPCDR) models have
been proposed to solve this problem. However, they primarily transfer simple
representations learned only from user-item interaction histories, overlooking
other useful side information, leading to inaccurate user preferences.
Additionally, they transfer differentially private user-item interaction
matrices or embeddings across domains to protect privacy. However, these
methods offer limited privacy protection, as attackers may exploit external
information to infer the original data. To address these challenges, we propose
a novel Federated User Preference Modeling (FUPM) framework. In FUPM, first, a
novel comprehensive preference exploration module is proposed to learn users'
comprehensive preferences from both interaction data and additional data
including review texts and potentially positive items. Next, a private
preference transfer module is designed to first learn differentially private
local and global prototypes, and then privately transfer the global prototypes
using a federated learning strategy. These prototypes are generalized
representations of user groups, making it difficult for attackers to infer
individual information. Extensive experiments on four CDR tasks conducted on
the Amazon and Douban datasets validate the superiority of FUPM over SOTA
baselines. Code is available at https://github.com/Lili1013/FUPM.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.14329v1">PHEVA: A Privacy-preserving Human-centric Video Anomaly Detection
  Dataset</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-08-26T14:55:23Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ghazal Alinezhad Noghre, Shanle Yao, Armin Danesh Pazho, Babak Rahimi Ardabili, Vinit Katariya, Hamed Tabkhi</p>
    <p><b>Summary:</b> PHEVA, a Privacy-preserving Human-centric Ethical Video Anomaly detection
dataset. By removing pixel information and providing only de-identified human
annotations, PHEVA safeguards personally identifiable information. The dataset
includes seven indoor/outdoor scenes, featuring one novel, context-specific
camera, and offers over 5x the pose-annotated frames compared to the largest
previous dataset. This study benchmarks state-of-the-art methods on PHEVA using
a comprehensive set of metrics, including the 10% Error Rate (10ER), a metric
used for anomaly detection for the first time providing insights relevant to
real-world deployment. As the first of its kind, PHEVA bridges the gap between
conventional training and real-world deployment by introducing continual
learning benchmarks, with models outperforming traditional methods in 82.14% of
cases. The dataset is publicly available at
https://github.com/TeCSAR-UNCC/PHEVA.git.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.13460v1">DOPPLER: Differentially Private Optimizers with Low-pass Filter for
  Privacy Noise Reduction</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2024-08-24T04:27:07Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xinwei Zhang, Zhiqi Bu, Mingyi Hong, Meisam Razaviyayn</p>
    <p><b>Summary:</b> Privacy is a growing concern in modern deep-learning systems and
applications. Differentially private (DP) training prevents the leakage of
sensitive information in the collected training data from the trained machine
learning models. DP optimizers, including DP stochastic gradient descent
(DPSGD) and its variants, privatize the training procedure by gradient clipping
and DP noise injection. However, in practice, DP models trained using DPSGD and
its variants often suffer from significant model performance degradation. Such
degradation prevents the application of DP optimization in many key tasks, such
as foundation model pretraining. In this paper, we provide a novel signal
processing perspective to the design and analysis of DP optimizers. We show
that a ``frequency domain'' operation called low-pass filtering can be used to
effectively reduce the impact of DP noise. More specifically, by defining the
``frequency domain'' for both the gradient and differential privacy (DP) noise,
we have developed a new component, called DOPPLER. This component is designed
for DP algorithms and works by effectively amplifying the gradient while
suppressing DP noise within this frequency domain. As a result, it maintains
privacy guarantees and enhances the quality of the DP-protected model. Our
experiments show that the proposed DP optimizers with a low-pass filter
outperform their counterparts without the filter by 3%-10% in test accuracy on
various models and datasets. Both theoretical and practical evidence suggest
that the DOPPLER is effective in closing the gap between DP and non-DP
training.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.13424v1">Enabling Humanitarian Applications with Targeted Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-24T01:34:37Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Nitin Kohli, Joshua Blumenstock</p>
    <p><b>Summary:</b> The proliferation of mobile phones in low- and middle-income countries has
suddenly and dramatically increased the extent to which the world's poorest and
most vulnerable populations can be observed and tracked by governments and
corporations. Millions of historically "off the grid" individuals are now
passively generating digital data; these data, in turn, are being used to make
life-altering decisions about those individuals -- including whether or not
they receive government benefits, and whether they qualify for a consumer loan.
  This paper develops an approach to implementing algorithmic decisions based
on personal data, while also providing formal privacy guarantees to data
subjects. The approach adapts differential privacy to applications that require
decisions about individuals, and gives decision makers granular control over
the level of privacy guaranteed to data subjects. We show that stronger privacy
guarantees typically come at some cost, and use data from two real-world
applications -- an anti-poverty program in Togo and a consumer lending platform
in Nigeria -- to illustrate those costs. Our empirical results quantify the
tradeoff between privacy and predictive accuracy, and characterize how
different privacy guarantees impact overall program effectiveness. More
broadly, our results demonstrate a way for humanitarian programs to responsibly
use personal data, and better equip program designers to make informed
decisions about data privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.13038v1">Improving the Classification Effect of Clinical Images of Diseases for
  Multi-Source Privacy Protection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-08-23T12:52:24Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tian Bowen, Xu Zhengyang, Yin Zhihao, Wang Jingying, Yue Yutao</p>
    <p><b>Summary:</b> Privacy data protection in the medical field poses challenges to data
sharing, limiting the ability to integrate data across hospitals for training
high-precision auxiliary diagnostic models. Traditional centralized training
methods are difficult to apply due to violations of privacy protection
principles. Federated learning, as a distributed machine learning framework,
helps address this issue, but it requires multiple hospitals to participate in
training simultaneously, which is hard to achieve in practice. To address these
challenges, we propose a medical privacy data training framework based on data
vectors. This framework allows each hospital to fine-tune pre-trained models on
private data, calculate data vectors (representing the optimization direction
of model parameters in the solution space), and sum them up to generate
synthetic weights that integrate model information from multiple hospitals.
This approach enhances model performance without exchanging private data or
requiring synchronous training. Experimental results demonstrate that this
method effectively utilizes dispersed private data resources while protecting
patient privacy. The auxiliary diagnostic model trained using this approach
significantly outperforms models trained independently by a single hospital,
providing a new perspective for resolving the conflict between medical data
privacy protection and model training and advancing the development of medical
intelligence.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.12787v1">LLM-PBE: Assessing Data Privacy in Large Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-08-23T01:37:29Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Qinbin Li, Junyuan Hong, Chulin Xie, Jeffrey Tan, Rachel Xin, Junyi Hou, Xavier Yin, Zhun Wang, Dan Hendrycks, Zhangyang Wang, Bo Li, Bingsheng He, Dawn Song</p>
    <p><b>Summary:</b> Large Language Models (LLMs) have become integral to numerous domains,
significantly advancing applications in data management, mining, and analysis.
Their profound capabilities in processing and interpreting complex language
data, however, bring to light pressing concerns regarding data privacy,
especially the risk of unintentional training data leakage. Despite the
critical nature of this issue, there has been no existing literature to offer a
comprehensive assessment of data privacy risks in LLMs. Addressing this gap,
our paper introduces LLM-PBE, a toolkit crafted specifically for the systematic
evaluation of data privacy risks in LLMs. LLM-PBE is designed to analyze
privacy across the entire lifecycle of LLMs, incorporating diverse attack and
defense strategies, and handling various data types and metrics. Through
detailed experimentation with multiple LLMs, LLM-PBE facilitates an in-depth
exploration of data privacy concerns, shedding light on influential factors
such as model size, data characteristics, and evolving temporal dimensions.
This study not only enriches the understanding of privacy issues in LLMs but
also serves as a vital resource for future research in the field. Aimed at
enhancing the breadth of knowledge in this area, the findings, resources, and
our full technical report are made available at https://llm-pbe.github.io/,
providing an open platform for academic and practical advancements in LLM
privacy assessment.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.12385v1">Sharper Bounds for Chebyshev Moment Matching with Applications to
  Differential Privacy and Beyond</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-08-22T13:26:41Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Cameron Musco, Christopher Musco, Lucas Rosenblatt, Apoorv Vikram Singh</p>
    <p><b>Summary:</b> We study the problem of approximately recovering a probability distribution
given noisy measurements of its Chebyshev polynomial moments. We sharpen prior
work, proving that accurate recovery in the Wasserstein distance is possible
with more noise than previously known.
  As a main application, our result yields a simple "linear query" algorithm
for constructing a differentially private synthetic data distribution with
Wasserstein-1 error $\tilde{O}(1/n)$ based on a dataset of $n$ points in
$[-1,1]$. This bound is optimal up to log factors and matches a recent
breakthrough of Boedihardjo, Strohmer, and Vershynin [Probab. Theory. Rel.,
2024], which uses a more complex "superregular random walk" method to beat an
$O(1/\sqrt{n})$ accuracy barrier inherent to earlier approaches.
  We illustrate a second application of our new moment-based recovery bound in
numerical linear algebra: by improving an approach of Braverman, Krishnan, and
Musco [STOC 2022], our result yields a faster algorithm for estimating the
spectral density of a symmetric matrix up to small error in the Wasserstein
distance.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.12353v1">Distributed quasi-Newton robust estimation under differential privacy</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Statistics Theory-D91E36"> 
  <p><b>Published on:</b> 2024-08-22T12:51:28Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chuhan Wang, Lixing Zhu, Xuehu Zhu</p>
    <p><b>Summary:</b> For distributed computing with Byzantine machines under Privacy Protection
(PP) constraints, this paper develops a robust PP distributed quasi-Newton
estimation, which only requires the node machines to transmit five vectors to
the central processor with high asymptotic relative efficiency. Compared with
the gradient descent strategy which requires more rounds of transmission and
the Newton iteration strategy which requires the entire Hessian matrix to be
transmitted, the novel quasi-Newton iteration has advantages in reducing
privacy budgeting and transmission cost. Moreover, our PP algorithm does not
depend on the boundedness of gradients and second-order derivatives. When
gradients and second-order derivatives follow sub-exponential distributions, we
offer a mechanism that can ensure PP with a sufficiently high probability.
Furthermore, this novel estimator can achieve the optimal convergence rate and
the asymptotic normality. The numerical studies on synthetic and real data sets
evaluate the performance of the proposed algorithm.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.12010v1">Confounding Privacy and Inverse Composition</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-21T21:45:13Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tao Zhang, Bradley A. Malin, Netanel Raviv, Yevgeniy Vorobeychik</p>
    <p><b>Summary:</b> We introduce a novel privacy notion of ($\epsilon, \delta$)-confounding
privacy that generalizes both differential privacy and Pufferfish privacy. In
differential privacy, sensitive information is contained in the dataset while
in Pufferfish privacy, sensitive information determines data distribution.
Consequently, both assume a chain-rule relationship between the sensitive
information and the output of privacy mechanisms. Confounding privacy, in
contrast, considers general causal relationships between the dataset and
sensitive information. One of the key properties of differential privacy is
that it can be easily composed over multiple interactions with the mechanism
that maps private data to publicly shared information. In contrast, we show
that the quantification of the privacy loss under the composition of
independent ($\epsilon, \delta$)-confounding private mechanisms using the
optimal composition of differential privacy \emph{underestimates} true privacy
loss. To address this, we characterize an inverse composition framework to
tightly implement a target global ($\epsilon_{g}, \delta_{g}$)-confounding
privacy under composition while keeping individual mechanisms independent and
private. In particular, we propose a novel copula-perturbation method which
ensures that (1) each individual mechanism $i$ satisfies a target local
($\epsilon_{i}, \delta_{i}$)-confounding privacy and (2) the target global
($\epsilon_{g}, \delta_{g}$)-confounding privacy is tightly implemented by
solving an optimization problem. Finally, we study inverse composition
empirically on real datasets.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.11649v1">Video-to-Text Pedestrian Monitoring (VTPM): Leveraging Computer Vision
  and Large Language Models for Privacy-Preserve Pedestrian Activity Monitoring
  at Intersections</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-08-21T14:21:53Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ahmed S. Abdelrahman, Mohamed Abdel-Aty, Dongdong Wang</p>
    <p><b>Summary:</b> Computer vision has advanced research methodologies, enhancing system
services across various fields. It is a core component in traffic monitoring
systems for improving road safety; however, these monitoring systems don't
preserve the privacy of pedestrians who appear in the videos, potentially
revealing their identities. Addressing this issue, our paper introduces
Video-to-Text Pedestrian Monitoring (VTPM), which monitors pedestrian movements
at intersections and generates real-time textual reports, including traffic
signal and weather information. VTPM uses computer vision models for pedestrian
detection and tracking, achieving a latency of 0.05 seconds per video frame.
Additionally, it detects crossing violations with 90.2% accuracy by
incorporating traffic signal data. The proposed framework is equipped with
Phi-3 mini-4k to generate real-time textual reports of pedestrian activity
while stating safety concerns like crossing violations, conflicts, and the
impact of weather on their behavior with latency of 0.33 seconds. To enhance
comprehensive analysis of the generated textual reports, Phi-3 medium is
fine-tuned for historical analysis of these generated textual reports. This
fine-tuning enables more reliable analysis about the pedestrian safety at
intersections, effectively detecting patterns and safety critical events. The
proposed VTPM offers a more efficient alternative to video footage by using
textual reports reducing memory usage, saving up to 253 million percent,
eliminating privacy issues, and enabling comprehensive interactive historical
analysis.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.11290v1">Privacy Preservation in Delay-Based Localization Systems: Artificial
  Noise or Artificial Multipath?</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2024-08-21T02:38:44Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yuchen Zhang, Hui Chen, Henk Wymeersch</p>
    <p><b>Summary:</b> Localization plays an increasingly pivotal role in 5G/6G systems, enabling
various applications. This paper focuses on the privacy concerns associated
with delay-based localization, where unauthorized base stations attempt to
infer the location of the end user. We propose a method to disrupt localization
at unauthorized nodes by injecting artificial components into the pilot signal,
exploiting model mismatches inherent in these nodes. Specifically, we
investigate the effectiveness of two techniques, namely artificial multipath
(AM) and artificial noise (AN), in mitigating location leakage. By leveraging
the misspecified Cram\'er-Rao bound framework, we evaluate the impact of these
techniques on unauthorized localization performance. Our results demonstrate
that pilot manipulation significantly degrades the accuracy of unauthorized
localization while minimally affecting legitimate localization. Moreover, we
find that the superiority of AM over AN varies depending on the specific
scenario.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.11263v1">Privacy-Preserving Data Management using Blockchains</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB">
  <p><b>Published on:</b> 2024-08-21T01:10:39Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Michael Mireku Kwakye</p>
    <p><b>Summary:</b> Privacy-preservation policies are guidelines formulated to protect data
providers private data. Previous privacy-preservation methodologies have
addressed privacy in which data are permanently stored in repositories and
disconnected from changing data provider privacy preferences. This occurrence
becomes evident as data moves to another data repository. Hence, the need for
data providers to control and flexibly update their existing privacy
preferences due to changing data usage continues to remain a problem. This
paper proposes a blockchain-based methodology for preserving data providers
private and sensitive data. The research proposes to tightly couple data
providers private attribute data element to privacy preferences and data
accessor data element into a privacy tuple. The implementation presents a
framework of tightly-coupled relational database and blockchains. This delivers
secure, tamper-resistant, and query-efficient platform for data management and
query processing. The evaluation analysis from the implementation validates
efficient query processing of privacy-aware queries on the privacy
infrastructure.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.12387v1">Makeup-Guided Facial Privacy Protection via Untrained Neural Network
  Priors</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-08-20T17:59:39Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Fahad Shamshad, Muzammal Naseer, Karthik Nandakumar</p>
    <p><b>Summary:</b> Deep learning-based face recognition (FR) systems pose significant privacy
risks by tracking users without their consent. While adversarial attacks can
protect privacy, they often produce visible artifacts compromising user
experience. To mitigate this issue, recent facial privacy protection approaches
advocate embedding adversarial noise into the natural looking makeup styles.
However, these methods require training on large-scale makeup datasets that are
not always readily available. In addition, these approaches also suffer from
dataset bias. For instance, training on makeup data that predominantly contains
female faces could compromise protection efficacy for male faces. To handle
these issues, we propose a test-time optimization approach that solely
optimizes an untrained neural network to transfer makeup style from a reference
to a source image in an adversarial manner. We introduce two key modules: a
correspondence module that aligns regions between reference and source images
in latent space, and a decoder with conditional makeup layers. The untrained
decoder, optimized via carefully designed structural and makeup consistency
losses, generates a protected image that resembles the source but incorporates
adversarial makeup to deceive FR models. As our approach does not rely on
training with makeup face datasets, it avoids potential male/female dataset
biases while providing effective protection. We further extend the proposed
approach to videos by leveraging on temporal correlations. Experiments on
benchmark datasets demonstrate superior performance in face verification and
identification tasks and effectiveness against commercial FR systems. Our code
and models will be available at
https://github.com/fahadshamshad/deep-facial-privacy-prior</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.10715v1">Fine-Tuning a Local LLaMA-3 Large Language Model for Automated
  Privacy-Preserving Physician Letter Generation in Radiation Oncology</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-08-20T10:31:36Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yihao Hou, Christoph Bert, Ahmed Gomaa, Godehard Lahmer, Daniel Hoefler, Thomas Weissmann, Raphaela Voigt, Philipp Schubert, Charlotte Schmitter, Alina Depardon, Sabine Semrau, Andreas Maier, Rainer Fietkau, Yixing Huang, Florian Putz</p>
    <p><b>Summary:</b> Generating physician letters is a time-consuming task in daily clinical
practice. This study investigates local fine-tuning of large language models
(LLMs), specifically LLaMA models, for physician letter generation in a
privacy-preserving manner within the field of radiation oncology. Our findings
demonstrate that base LLaMA models, without fine-tuning, are inadequate for
effectively generating physician letters. The QLoRA algorithm provides an
efficient method for local intra-institutional fine-tuning of LLMs with limited
computational resources (i.e., a single 48 GB GPU workstation within the
hospital). The fine-tuned LLM successfully learns radiation oncology-specific
information and generates physician letters in an institution-specific style.
ROUGE scores of the generated summary reports highlight the superiority of the
8B LLaMA-3 model over the 13B LLaMA-2 model. Further multidimensional physician
evaluations of 10 cases reveal that, although the fine-tuned LLaMA-3 model has
limited capacity to generate content beyond the provided input data, it
successfully generates salutations, diagnoses and treatment histories,
recommendations for further treatment, and planned schedules. Overall, clinical
benefit was rated highly by the clinical experts (average score of 3.44 on a
4-point scale). With careful physician review and correction, automated
LLM-based physician letter generation has significant practical value.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.10648v1">Smart Contract Coordinated Privacy Preserving Crowd-Sensing Campaigns</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Networking and Internet Architecture-04E762">
  <p><b>Published on:</b> 2024-08-20T08:41:57Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Luca Bedogni, Stefano Ferretti</p>
    <p><b>Summary:</b> Crowd-sensing has emerged as a powerful data retrieval model, enabling
diverse applications by leveraging active user participation. However, data
availability and privacy concerns pose significant challenges. Traditional
methods like data encryption and anonymization, while essential, may not fully
address these issues. For instance, in sparsely populated areas, anonymized
data can still be traced back to individual users. Additionally, the volume of
data generated by users can reveal their identities. To develop credible
crowd-sensing systems, data must be anonymized, aggregated and separated into
uniformly sized chunks. Furthermore, decentralizing the data management
process, rather than relying on a single server, can enhance security and
trust. This paper proposes a system utilizing smart contracts and blockchain
technologies to manage crowd-sensing campaigns. The smart contract handles user
subscriptions, data encryption, and decentralized storage, creating a secure
data marketplace. Incentive policies within the smart contract encourage user
participation and data diversity. Simulation results confirm the system's
viability, highlighting the importance of user participation for data
credibility and the impact of geographical data scarcity on rewards. This
approach aims to balance data origin and reduce cheating risks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.10647v1">Privacy-preserving Universal Adversarial Defense for Black-box Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2024-08-20T08:40:39Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Qiao Li, Cong Wu, Jing Chen, Zijun Zhang, Kun He, Ruiying Du, Xinxin Wang, Qingchuang Zhao, Yang Liu</p>
    <p><b>Summary:</b> Deep neural networks (DNNs) are increasingly used in critical applications
such as identity authentication and autonomous driving, where robustness
against adversarial attacks is crucial. These attacks can exploit minor
perturbations to cause significant prediction errors, making it essential to
enhance the resilience of DNNs. Traditional defense methods often rely on
access to detailed model information, which raises privacy concerns, as model
owners may be reluctant to share such data. In contrast, existing black-box
defense methods fail to offer a universal defense against various types of
adversarial attacks. To address these challenges, we introduce DUCD, a
universal black-box defense method that does not require access to the target
model's parameters or architecture. Our approach involves distilling the target
model by querying it with data, creating a white-box surrogate while preserving
data privacy. We further enhance this surrogate model using a certified defense
based on randomized smoothing and optimized noise selection, enabling robust
defense against a broad range of adversarial attacks. Comparative evaluations
between the certified defenses of the surrogate and target models demonstrate
the effectiveness of our approach. Experiments on multiple image classification
datasets show that DUCD not only outperforms existing black-box defenses but
also matches the accuracy of white-box defenses, all while enhancing data
privacy and reducing the success rate of membership inference attacks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.10468v3">Tracing Privacy Leakage of Language Models to Training Data via Adjusted
  Influence Functions</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-20T00:40:49Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jinxin Liu, Zao Yang</p>
    <p><b>Summary:</b> The responses generated by Large Language Models (LLMs) can include sensitive
information from individuals and organizations, leading to potential privacy
leakage. This work implements Influence Functions (IFs) to trace privacy
leakage back to the training data, thereby mitigating privacy concerns of
Language Models (LMs). However, we notice that current IFs struggle to
accurately estimate the influence of tokens with large gradient norms,
potentially overestimating their influence. When tracing the most influential
samples, this leads to frequently tracing back to samples with large gradient
norm tokens, overshadowing the actual most influential samples even if their
influences are well estimated. To address this issue, we propose Heuristically
Adjusted IF (HAIF), which reduces the weight of tokens with large gradient
norms, thereby significantly improving the accuracy of tracing the most
influential samples. To establish easily obtained groundtruth for tracing
privacy leakage, we construct two datasets, PII-E and PII-CR, representing two
distinct scenarios: one with identical text in the model outputs and
pre-training data, and the other where models leverage their reasoning
abilities to generate text divergent from pre-training data. HAIF significantly
improves tracing accuracy, enhancing it by 20.96% to 73.71% on the PII-E
dataset and 3.21% to 45.93% on the PII-CR dataset, compared to the best SOTA
IFs against various GPT-2 and QWen-1.5 models. HAIF also outperforms SOTA IFs
on real-world pretraining data CLUECorpus2020, demonstrating strong robustness
regardless prompt and response lengths.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.10442v1">Feasibility of assessing cognitive impairment via distributed camera
  network and privacy-preserving edge computing</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-08-19T22:34:43Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chaitra Hegde, Yashar Kiarashi, Allan I Levey, Amy D Rodriguez, Hyeokhyen Kwon, Gari D Clifford</p>
    <p><b>Summary:</b> INTRODUCTION: Mild cognitive impairment (MCI) is characterized by a decline
in cognitive functions beyond typical age and education-related expectations.
Since, MCI has been linked to reduced social interactions and increased aimless
movements, we aimed to automate the capture of these behaviors to enhance
longitudinal monitoring.
  METHODS: Using a privacy-preserving distributed camera network, we collected
movement and social interaction data from groups of individuals with MCI
undergoing therapy within a 1700$m^2$ space. We developed movement and social
interaction features, which were then used to train a series of machine
learning algorithms to distinguish between higher and lower cognitive
functioning MCI groups.
  RESULTS: A Wilcoxon rank-sum test revealed statistically significant
differences between high and low-functioning cohorts in features such as linear
path length, walking speed, change in direction while walking, entropy of
velocity and direction change, and number of group formations in the indoor
space. Despite lacking individual identifiers to associate with specific levels
of MCI, a machine learning approach using the most significant features
provided a 71% accuracy.
  DISCUSSION: We provide evidence to show that a privacy-preserving low-cost
camera network using edge computing framework has the potential to distinguish
between different levels of cognitive impairment from the movements and social
interactions captured during group activities.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.10053v1">Privacy Checklist: Privacy Violation Detection Grounding on Contextual
  Integrity Theory</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-19T14:48:04Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Haoran Li, Wei Fan, Yulin Chen, Jiayang Cheng, Tianshu Chu, Xuebing Zhou, Peizhao Hu, Yangqiu Song</p>
    <p><b>Summary:</b> Privacy research has attracted wide attention as individuals worry that their
private data can be easily leaked during interactions with smart devices,
social platforms, and AI applications. Computer science researchers, on the
other hand, commonly study privacy issues through privacy attacks and defenses
on segmented fields. Privacy research is conducted on various sub-fields,
including Computer Vision (CV), Natural Language Processing (NLP), and Computer
Networks. Within each field, privacy has its own formulation. Though pioneering
works on attacks and defenses reveal sensitive privacy issues, they are
narrowly trapped and cannot fully cover people's actual privacy concerns.
Consequently, the research on general and human-centric privacy research
remains rather unexplored. In this paper, we formulate the privacy issue as a
reasoning problem rather than simple pattern matching. We ground on the
Contextual Integrity (CI) theory which posits that people's perceptions of
privacy are highly correlated with the corresponding social context. Based on
such an assumption, we develop the first comprehensive checklist that covers
social identities, private attributes, and existing privacy regulations. Unlike
prior works on CI that either cover limited expert annotated norms or model
incomplete social context, our proposed privacy checklist uses the whole Health
Insurance Portability and Accountability Act of 1996 (HIPAA) as an example, to
show that we can resort to large language models (LLMs) to completely cover the
HIPAA's regulations. Additionally, our checklist also gathers expert
annotations across multiple ontologies to determine private information
including but not limited to personally identifiable information (PII). We use
our preliminary results on the HIPAA to shed light on future context-centric
privacy research to cover more privacy regulations, social norms and standards.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.09943v2">Calibrating Noise for Group Privacy in Subsampled Mechanisms</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-19T12:32:50Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yangfan Jiang, Xinjian Luo, Yin Yang, Xiaokui Xiao</p>
    <p><b>Summary:</b> Given a group size m and a sensitive dataset D, group privacy (GP) releases
information about D with the guarantee that the adversary cannot infer with
high confidence whether the underlying data is D or a neighboring dataset D'
that differs from D by m records. GP generalizes the well-established notion of
differential privacy (DP) for protecting individuals' privacy; in particular,
when m=1, GP reduces to DP. Compared to DP, GP is capable of protecting the
sensitive aggregate information of a group of up to m individuals, e.g., the
average annual income among members of a yacht club. Despite its longstanding
presence in the research literature and its promising applications, GP is often
treated as an afterthought, with most approaches first developing a DP
mechanism and then using a generic conversion to adapt it for GP, treating the
DP solution as a black box. As we point out in the paper, this methodology is
suboptimal when the underlying DP solution involves subsampling, e.g., in the
classic DP-SGD method for training deep learning models. In this case, the
DP-to-GP conversion is overly pessimistic in its analysis, leading to low
utility in the published results under GP.
  Motivated by this, we propose a novel analysis framework that provides tight
privacy accounting for subsampled GP mechanisms. Instead of converting a
black-box DP mechanism to GP, our solution carefully analyzes and utilizes the
inherent randomness in subsampled mechanisms, leading to a substantially
improved bound on the privacy loss with respect to GP. The proposed solution
applies to a wide variety of foundational mechanisms with subsampling.
Extensive experiments with real datasets demonstrate that compared to the
baseline convert-from-blackbox-DP approach, our GP mechanisms achieve noise
reductions of over an order of magnitude in several practical settings,
including deep neural network training.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.09935v1">Privacy Technologies for Financial Intelligence</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-19T12:13:53Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yang Li, Thilina Ranbaduge, Kee Siong Ng</p>
    <p><b>Summary:</b> Financial crimes like terrorism financing and money laundering can have real
impacts on society, including the abuse and mismanagement of public funds,
increase in societal problems such as drug trafficking and illicit gambling
with attendant economic costs, and loss of innocent lives in the case of
terrorism activities. Complex financial crimes can be hard to detect primarily
because data related to different pieces of the overall puzzle is usually
distributed across a network of financial institutions, regulators, and
law-enforcement agencies and they cannot be easily shared due to privacy
constraints. Recent advances in Privacy-Preserving Data Matching and Machine
Learning provide an opportunity for regulators and the financial industry to
come together to solve the risk-discovery problem with technology. This paper
provides a survey of the financial intelligence landscape and where
opportunities lie for privacy technologies to improve the state-of-the-art in
financial-crime detection.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.09659v1">An Algorithm for Enhancing Privacy-Utility Tradeoff in the Privacy
  Funnel and Other Lift-based Measures</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2024-08-19T02:43:18Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mohammad Amin Zarrabian, Parastoo Sadeghi</p>
    <p><b>Summary:</b> This paper investigates the privacy funnel, a privacy-utility tradeoff
problem in which mutual information quantifies both privacy and utility. The
objective is to maximize utility while adhering to a specified privacy budget.
However, the privacy funnel represents a non-convex optimization problem,
making it challenging to achieve an optimal solution. An existing proposed
approach to this problem involves substituting the mutual information with the
lift (the exponent of information density) and then solving the optimization.
Since mutual information is the expectation of the information density, this
substitution overestimates the privacy loss and results in a final smaller
bound on the privacy of mutual information than what is allowed in the budget.
This significantly compromises the utility. To overcome this limitation, we
propose using a privacy measure that is more relaxed than the lift but stricter
than mutual information while still allowing the optimization to be efficiently
solved. Instead of directly using information density, our proposed measure is
the average of information density over the sensitive data distribution for
each observed data realization. We then introduce a heuristic algorithm capable
of achieving solutions that produce extreme privacy values, which enhances
utility. The numerical results confirm improved utility at the same privacy
budget compared to existing solutions in the literature. Additionally, we
explore two other privacy measures, $\ell_{1}$-norm and strong
$\chi^2$-divergence, demonstrating the applicability of our algorithm to these
lift-based measures. We evaluate the performance of our method by comparing its
output with previous works. Finally, we validate our heuristic approach with a
theoretical framework that estimates the optimal utility for strong
$\chi^2$-divergence, numerically showing a perfect match.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.08722v1">A Novel Buffered Federated Learning Framework for Privacy-Driven Anomaly
  Detection in IIoT</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-16T13:01:59Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Samira Kamali Poorazad, Chafika Benzaid, Tarik Taleb</p>
    <p><b>Summary:</b> Industrial Internet of Things (IIoT) is highly sensitive to data privacy and
cybersecurity threats. Federated Learning (FL) has emerged as a solution for
preserving privacy, enabling private data to remain on local IIoT clients while
cooperatively training models to detect network anomalies. However, both
synchronous and asynchronous FL architectures exhibit limitations, particularly
when dealing with clients with varying speeds due to data heterogeneity and
resource constraints. Synchronous architecture suffers from straggler effects,
while asynchronous methods encounter communication bottlenecks. Additionally,
FL models are prone to adversarial inference attacks aimed at disclosing
private training data. To address these challenges, we propose a Buffered FL
(BFL) framework empowered by homomorphic encryption for anomaly detection in
heterogeneous IIoT environments. BFL utilizes a novel weighted average time
approach to mitigate both straggler effects and communication bottlenecks,
ensuring fairness between clients with varying processing speeds through
collaboration with a buffer-based server. The performance results, derived from
two datasets, show the superiority of BFL compared to state-of-the-art FL
methods, demonstrating improved accuracy and convergence speed while enhancing
privacy preservation.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.08666v1">A Multivocal Literature Review on Privacy and Fairness in Federated
  Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-08-16T11:15:52Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Beatrice Balbierer, Lukas Heinlein, Domenique Zipperling, Niklas Kühl</p>
    <p><b>Summary:</b> Federated Learning presents a way to revolutionize AI applications by
eliminating the necessity for data sharing. Yet, research has shown that
information can still be extracted during training, making additional
privacy-preserving measures such as differential privacy imperative. To
implement real-world federated learning applications, fairness, ranging from a
fair distribution of performance to non-discriminative behaviour, must be
considered. Particularly in high-risk applications (e.g. healthcare), avoiding
the repetition of past discriminatory errors is paramount. As recent research
has demonstrated an inherent tension between privacy and fairness, we conduct a
multivocal literature review to examine the current methods to integrate
privacy and fairness in federated learning. Our analyses illustrate that the
relationship between privacy and fairness has been neglected, posing a critical
risk for real-world applications. We highlight the need to explore the
relationship between privacy, fairness, and performance, advocating for the
creation of integrated federated learning frameworks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.08642v1">The Power of Bias: Optimizing Client Selection in Federated Learning
  with Heterogeneous Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-08-16T10:19:27Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jiating Ma, Yipeng Zhou, Qi Li, Quan Z. Sheng, Laizhong Cui, Jiangchuan Liu</p>
    <p><b>Summary:</b> To preserve the data privacy, the federated learning (FL) paradigm emerges in
which clients only expose model gradients rather than original data for
conducting model training. To enhance the protection of model gradients in FL,
differentially private federated learning (DPFL) is proposed which incorporates
differentially private (DP) noises to obfuscate gradients before they are
exposed. Yet, an essential but largely overlooked problem in DPFL is the
heterogeneity of clients' privacy requirement, which can vary significantly
between clients and extremely complicates the client selection problem in DPFL.
In other words, both the data quality and the influence of DP noises should be
taken into account when selecting clients. To address this problem, we conduct
convergence analysis of DPFL under heterogeneous privacy, a generic client
selection strategy, popular DP mechanisms and convex loss. Based on convergence
analysis, we formulate the client selection problem to minimize the value of
loss function in DPFL with heterogeneous privacy, which is a convex
optimization problem and can be solved efficiently. Accordingly, we propose the
DPFL-BCS (biased client selection) algorithm. The extensive experiment results
with real datasets under both convex and non-convex loss functions indicate
that DPFL-BCS can remarkably improve model utility compared with the SOTA
baselines.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.08529v1">Privacy-Preserving Vision Transformer Using Images Encrypted with
  Restricted Random Permutation Matrices</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-08-16T04:57:21Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kouki Horio, Kiyoshi Nishikawa, Hitoshi Kiya</p>
    <p><b>Summary:</b> We propose a novel method for privacy-preserving fine-tuning vision
transformers (ViTs) with encrypted images. Conventional methods using encrypted
images degrade model performance compared with that of using plain images due
to the influence of image encryption. In contrast, the proposed encryption
method using restricted random permutation matrices can provide a higher
performance than the conventional ones.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.08475v2">Models Matter: Setting Accurate Privacy Expectations for Local and
  Central Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2024-08-16T01:21:57Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mary Anne Smart, Priyanka Nanayakkara, Rachel Cummings, Gabriel Kaptchuk, Elissa Redmiles</p>
    <p><b>Summary:</b> Differential privacy is a popular privacy-enhancing technology that has been
deployed both in industry and government agencies. Unfortunately, existing
explanations of differential privacy fail to set accurate privacy expectations
for data subjects, which depend on the choice of deployment model. We design
and evaluate new explanations of differential privacy for the local and central
models, drawing inspiration from prior work explaining other privacy-enhancing
technologies. We find that consequences-focused explanations in the style of
privacy nutrition labels that lay out the implications of differential privacy
are a promising approach for setting accurate privacy expectations. Further, we
find that while process-focused explanations are not enough to set accurate
privacy expectations, combining consequences-focused explanations with a brief
description of how differential privacy works leads to greater trust.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.08107v1">Communication-robust and Privacy-safe Distributed Estimation for
  Heterogeneous Community-level Behind-the-meter Solar Power Generation</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36">
  <p><b>Published on:</b> 2024-08-15T12:11:03Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jinglei Feng, Zhengshuo Li</p>
    <p><b>Summary:</b> The rapid growth of behind-the-meter (BTM) solar power generation systems
presents challenges for distribution system planning and scheduling due to
invisible solar power generation. To address the data leakage problem of
centralized machine-learning methods in BTM solar power generation estimation,
the federated learning (FL) method has been investigated for its distributed
learning capability. However, the conventional FL method has encountered
various challenges, including heterogeneity, communication failures, and
malicious privacy attacks. To overcome these challenges, this study proposes a
communication-robust and privacy-safe distributed estimation method for
heterogeneous community-level BTM solar power generation. Specifically, this
study adopts multi-task FL as the main structure and learns the common and
unique features of all communities. Simultaneously, it embeds an updated
parameters estimation method into the multi-task FL, automatically identifies
similarities between any two clients, and estimates the updated parameters for
unavailable clients to mitigate the negative effects of communication failures.
Finally, this study adopts a differential privacy mechanism under the dynamic
privacy budget allocation strategy to combat malicious privacy attacks and
improve model training efficiency. Case studies show that in the presence of
heterogeneity and communication failures, the proposed method exhibits better
estimation accuracy and convergence performance as compared with traditional FL
and localized learning methods, while providing stronger privacy protection.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.08002v1">Practical Privacy-Preserving Identity Verification using Third-Party
  Cloud Services and FHE (Role of Data Encoding in Circuit Depth Management)</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-15T08:12:07Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Deep Inder Mohan, Srinivas Vivek</p>
    <p><b>Summary:</b> National digital identity verification systems have played a critical role in
the effective distribution of goods and services, particularly, in developing
countries. Due to the cost involved in deploying and maintaining such systems,
combined with a lack of in-house technical expertise, governments seek to
outsource this service to third-party cloud service providers to the extent
possible. This leads to increased concerns regarding the privacy of users'
personal data. In this work, we propose a practical privacy-preserving digital
identity (ID) verification protocol where the third-party cloud services
process the identity data encrypted using a (single-key) Fully Homomorphic
Encryption (FHE) scheme such as BFV. Though the role of a trusted entity such
as government is not completely eliminated, our protocol does significantly
reduces the computation load on such parties.
  A challenge in implementing a privacy-preserving ID verification protocol
using FHE is to support various types of queries such as exact and/or fuzzy
demographic and biometric matches including secure age comparisons. From a
cryptographic engineering perspective, our main technical contribution is a
user data encoding scheme that encodes demographic and biometric user data in
only two BFV ciphertexts and yet facilitates us to outsource various types of
ID verification queries to a third-party cloud. Our encoding scheme also
ensures that the only computation done by the trusted entity is a
query-agnostic "extended" decryption. This is in stark contrast with recent
works that outsource all the non-arithmetic operations to a trusted server. We
implement our protocol using the Microsoft SEAL FHE library and demonstrate its
practicality.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.07892v3">Personhood credentials: Artificial intelligence and the value of
  privacy-preserving tools to distinguish who is real online</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2024-08-15T02:41:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Steven Adler, Zoë Hitzig, Shrey Jain, Catherine Brewer, Wayne Chang, Renée DiResta, Eddy Lazzarin, Sean McGregor, Wendy Seltzer, Divya Siddarth, Nouran Soliman, Tobin South, Connor Spelliscy, Manu Sporny, Varya Srivastava, John Bailey, Brian Christian, Andrew Critch, Ronnie Falcon, Heather Flanagan, Kim Hamilton Duffy, Eric Ho, Claire R. Leibowicz, Srikanth Nadhamuni, Alan Z. Rozenshtein, David Schnurr, Evan Shapiro, Lacey Strahm, Andrew Trask, Zoe Weinberg, Cedric Whitney, Tom Zick</p>
    <p><b>Summary:</b> Anonymity is an important principle online. However, malicious actors have
long used misleading identities to conduct fraud, spread disinformation, and
carry out other deceptive schemes. With the advent of increasingly capable AI,
bad actors can amplify the potential scale and effectiveness of their
operations, intensifying the challenge of balancing anonymity and
trustworthiness online. In this paper, we analyze the value of a new tool to
address this challenge: "personhood credentials" (PHCs), digital credentials
that empower users to demonstrate that they are real people -- not AIs -- to
online services, without disclosing any personal information. Such credentials
can be issued by a range of trusted institutions -- governments or otherwise. A
PHC system, according to our definition, could be local or global, and does not
need to be biometrics-based. Two trends in AI contribute to the urgency of the
challenge: AI's increasing indistinguishability from people online (i.e.,
lifelike content and avatars, agentic activity), and AI's increasing
scalability (i.e., cost-effectiveness, accessibility). Drawing on a long
history of research into anonymous credentials and "proof-of-personhood"
systems, personhood credentials give people a way to signal their
trustworthiness on online platforms, and offer service providers new tools for
reducing misuse by bad actors. In contrast, existing countermeasures to
automated deception -- such as CAPTCHAs -- are inadequate against sophisticated
AI, while stringent identity verification solutions are insufficiently private
for many use-cases. After surveying the benefits of personhood credentials, we
also examine deployment risks and design challenges. We conclude with
actionable next steps for policymakers, technologists, and standards bodies to
consider in consultation with the public.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.07614v1">Practical Considerations for Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-14T15:28:28Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kareem Amin, Alex Kulesza, Sergei Vassilvitskii</p>
    <p><b>Summary:</b> Differential privacy is the gold standard for statistical data release. Used
by governments, companies, and academics, its mathematically rigorous
guarantees and worst-case assumptions on the strength and knowledge of
attackers make it a robust and compelling framework for reasoning about
privacy. However, even with landmark successes, differential privacy has not
achieved widespread adoption in everyday data use and data protection. In this
work we examine some of the practical obstacles that stand in the way.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.07021v1">Improved Counting under Continual Observation with Pure Differential
  Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B">
  <p><b>Published on:</b> 2024-08-13T16:36:33Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Joel Daniel Andersson, Rasmus Pagh, Sahel Torkamani</p>
    <p><b>Summary:</b> Counting under continual observation is a well-studied problem in the area of
differential privacy. Given a stream of updates $x_1,x_2,\dots,x_T \in \{0,1\}$
the problem is to continuously release estimates of the prefix sums
$\sum_{i=1}^t x_i$ for $t=1,\dots,T$ while protecting each input $x_i$ in the
stream with differential privacy. Recently, significant leaps have been made in
our understanding of this problem under $\textit{approximate}$ differential
privacy, aka. $(\varepsilon,\delta)$$\textit{-differential privacy}$. However,
for the classical case of $\varepsilon$-differential privacy, we are not aware
of any improvement in mean squared error since the work of Honaker (TPDP 2015).
In this paper we present such an improvement, reducing the mean squared error
by a factor of about 4, asymptotically. The key technique is a new
generalization of the binary tree mechanism that uses a $k$-ary number system
with $\textit{negative digits}$ to improve the privacy-accuracy trade-off. Our
mechanism improves the mean squared error over all 'optimal'
$(\varepsilon,\delta)$-differentially private factorization mechanisms based on
Gaussian noise whenever $\delta$ is sufficiently small. Specifically, using
$k=19$ we get an asymptotic improvement over the bound given in the work by
Henzinger, Upadhyay and Upadhyay (SODA 2023) when $\delta = O(T^{-0.92})$.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.07006v1">The Complexities of Differential Privacy for Survey Data</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-13T16:15:42Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jörg Drechsler, James Bailie</p>
    <p><b>Summary:</b> The concept of differential privacy (DP) has gained substantial attention in
recent years, most notably since the U.S. Census Bureau announced the adoption
of the concept for its 2020 Decennial Census. However, despite its attractive
theoretical properties, implementing DP in practice remains challenging,
especially when it comes to survey data. In this paper we present some results
from an ongoing project funded by the U.S. Census Bureau that is exploring the
possibilities and limitations of DP for survey data. Specifically, we identify
five aspects that need to be considered when adopting DP in the survey context:
the multi-staged nature of data production; the limited privacy amplification
from complex sampling designs; the implications of survey-weighted estimates;
the weighting adjustments for nonresponse and other data deficiencies, and the
imputation of missing values. We summarize the project's key findings with
respect to each of these aspects and also discuss some of the challenges that
still need to be addressed before DP could become the new data protection
standard at statistical agencies.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.07004v1">Casper: Prompt Sanitization for Protecting User Privacy in Web-Based
  Large Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-08-13T16:08:37Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chun Jie Chong, Chenxi Hou, Zhihao Yao, Seyed Mohammadjavad Seyed Talebi</p>
    <p><b>Summary:</b> Web-based Large Language Model (LLM) services have been widely adopted and
have become an integral part of our Internet experience. Third-party plugins
enhance the functionalities of LLM by enabling access to real-world data and
services. However, the privacy consequences associated with these services and
their third-party plugins are not well understood. Sensitive prompt data are
stored, processed, and shared by cloud-based LLM providers and third-party
plugins. In this paper, we propose Casper, a prompt sanitization technique that
aims to protect user privacy by detecting and removing sensitive information
from user inputs before sending them to LLM services. Casper runs entirely on
the user's device as a browser extension and does not require any changes to
the online LLM services. At the core of Casper is a three-layered sanitization
mechanism consisting of a rule-based filter, a Machine Learning (ML)-based
named entity recognizer, and a browser-based local LLM topic identifier. We
evaluate Casper on a dataset of 4000 synthesized prompts and show that it can
effectively filter out Personal Identifiable Information (PII) and
privacy-sensitive topics with high accuracy, at 98.5% and 89.9%, respectively.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.08909v1">An Adaptive Differential Privacy Method Based on Federated Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2024-08-13T13:08:11Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhiqiang Wang, Xinyue Yu, Qianli Huang, Yongguang Gong</p>
    <p><b>Summary:</b> Differential privacy is one of the methods to solve the problem of privacy
protection in federated learning. Setting the same privacy budget for each
round will result in reduced accuracy in training. The existing methods of the
adjustment of privacy budget consider fewer influencing factors and tend to
ignore the boundaries, resulting in unreasonable privacy budgets. Therefore, we
proposed an adaptive differential privacy method based on federated learning.
The method sets the adjustment coefficient and scoring function according to
accuracy, loss, training rounds, and the number of datasets and clients. And
the privacy budget is adjusted based on them. Then the local model update is
processed according to the scaling factor and the noise. Fi-nally, the server
aggregates the noised local model update and distributes the noised global
model. The range of parameters and the privacy of the method are analyzed.
Through the experimental evaluation, it can reduce the privacy budget by about
16%, while the accuracy remains roughly the same.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.06460v1">Evaluating Privacy Measures for Load Hiding</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36">   
  <p><b>Published on:</b> 2024-08-12T19:21:34Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Vadim Arzamasov, Klemens Böhm</p>
    <p><b>Summary:</b> In smart grids, the use of smart meters to measure electricity consumption at
a household level raises privacy concerns. To address them, researchers have
designed various load hiding algorithms that manipulate the electricity
consumption measured. To compare how well these algorithms preserve privacy,
various privacy measures have been proposed. However, there currently is no
consensus on which privacy measure is most appropriate to use. In this study,
we aim to identify the most effective privacy measure(s) for load hiding
algorithms. We have crafted a series of experiments to assess the effectiveness
of these measures. found 20 of the 25 measures studied to be ineffective. Next,
focused on the well-known "appliance usage" secret, we have designed synthetic
data to find the measure that best deals with this secret. We observe that such
a measure, a variant of mutual information, actually exists.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.08904v1">Privacy in Federated Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-12T18:41:58Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jaydip Sen, Hetvi Waghela, Sneha Rakshit</p>
    <p><b>Summary:</b> Federated Learning (FL) represents a significant advancement in distributed
machine learning, enabling multiple participants to collaboratively train
models without sharing raw data. This decentralized approach enhances privacy
by keeping data on local devices. However, FL introduces new privacy
challenges, as model updates shared during training can inadvertently leak
sensitive information. This chapter delves into the core privacy concerns
within FL, including the risks of data reconstruction, model inversion attacks,
and membership inference. It explores various privacy-preserving techniques,
such as Differential Privacy (DP) and Secure Multi-Party Computation (SMPC),
which are designed to mitigate these risks. The chapter also examines the
trade-offs between model accuracy and privacy, emphasizing the importance of
balancing these factors in practical implementations. Furthermore, it discusses
the role of regulatory frameworks, such as GDPR, in shaping the privacy
standards for FL. By providing a comprehensive overview of the current state of
privacy in FL, this chapter aims to equip researchers and practitioners with
the knowledge necessary to navigate the complexities of secure federated
learning environments. The discussion highlights both the potential and
limitations of existing privacy-enhancing techniques, offering insights into
future research directions and the development of more robust solutions.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.06197v1">Lancelot: Towards Efficient and Privacy-Preserving Byzantine-Robust
  Federated Learning within Fully Homomorphic Encryption</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2024-08-12T14:48:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Siyang Jiang, Hao Yang, Qipeng Xie, Chuan Ma, Sen Wang, Guoliang Xing</p>
    <p><b>Summary:</b> In sectors such as finance and healthcare, where data governance is subject
to rigorous regulatory requirements, the exchange and utilization of data are
particularly challenging. Federated Learning (FL) has risen as a pioneering
distributed machine learning paradigm that enables collaborative model training
across multiple institutions while maintaining data decentralization. Despite
its advantages, FL is vulnerable to adversarial threats, particularly poisoning
attacks during model aggregation, a process typically managed by a central
server. However, in these systems, neural network models still possess the
capacity to inadvertently memorize and potentially expose individual training
instances. This presents a significant privacy risk, as attackers could
reconstruct private data by leveraging the information contained in the model
itself. Existing solutions fall short of providing a viable, privacy-preserving
BRFL system that is both completely secure against information leakage and
computationally efficient. To address these concerns, we propose Lancelot, an
innovative and computationally efficient BRFL framework that employs fully
homomorphic encryption (FHE) to safeguard against malicious client activities
while preserving data privacy. Our extensive testing, which includes medical
imaging diagnostics and widely-used public image datasets, demonstrates that
Lancelot significantly outperforms existing methods, offering more than a
twenty-fold increase in processing speed, all while maintaining data privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.06167v1">Blind-Match: Efficient Homomorphic Encryption-Based 1:N Matching for
  Privacy-Preserving Biometric Identification</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-12T14:13:08Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Hyunmin Choi, Jiwon Kim, Chiyoung Song, Simon S. Woo, Hyoungshick Kim</p>
    <p><b>Summary:</b> We present Blind-Match, a novel biometric identification system that
leverages homomorphic encryption (HE) for efficient and privacy-preserving 1:N
matching. Blind-Match introduces a HE-optimized cosine similarity computation
method, where the key idea is to divide the feature vector into smaller parts
for processing rather than computing the entire vector at once. By optimizing
the number of these parts, Blind-Match minimizes execution time while ensuring
data privacy through HE. Blind-Match achieves superior performance compared to
state-of-the-art methods across various biometric datasets. On the LFW face
dataset, Blind-Match attains a 99.63% Rank-1 accuracy with a 128-dimensional
feature vector, demonstrating its robustness in face recognition tasks. For
fingerprint identification, Blind-Match achieves a remarkable 99.55% Rank-1
accuracy on the PolyU dataset, even with a compact 16-dimensional feature
vector, significantly outperforming the state-of-the-art method, Blind-Touch,
which achieves only 59.17%. Furthermore, Blind-Match showcases practical
efficiency in large-scale biometric identification scenarios, such as Naver
Cloud's FaceSign, by processing 6,144 biometric samples in 0.74 seconds using a
128-dimensional feature vector.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.06395v1">Fast John Ellipsoid Computation with Differential Privacy Optimization</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-08-12T03:47:55Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jiuxiang Gu, Xiaoyu Li, Yingyu Liang, Zhenmei Shi, Zhao Song, Junwei Yu</p>
    <p><b>Summary:</b> Determining the John ellipsoid - the largest volume ellipsoid contained
within a convex polytope - is a fundamental problem with applications in
machine learning, optimization, and data analytics. Recent work has developed
fast algorithms for approximating the John ellipsoid using sketching and
leverage score sampling techniques. However, these algorithms do not provide
privacy guarantees for sensitive input data. In this paper, we present the
first differentially private algorithm for fast John ellipsoid computation. Our
method integrates noise perturbation with sketching and leverage score sampling
to achieve both efficiency and privacy. We prove that (1) our algorithm
provides $(\epsilon,\delta)$-differential privacy, and the privacy guarantee
holds for neighboring datasets that are $\epsilon_0$-close, allowing
flexibility in the privacy definition; (2) our algorithm still converges to a
$(1+\xi)$-approximation of the optimal John ellipsoid in
$O(\xi^{-2}(\log(n/\delta_0) + (L\epsilon_0)^{-2}))$ iterations where $n$ is
the number of data point, $L$ is the Lipschitz constant, $\delta_0$ is the
failure probability, and $\epsilon_0$ is the closeness of neighboring input
datasets. Our theoretical analysis demonstrates the algorithm's convergence and
privacy properties, providing a robust approach for balancing utility and
privacy in John ellipsoid computation. This is the first differentially private
algorithm for fast John ellipsoid computation, opening avenues for future
research in privacy-preserving optimization techniques.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.05723v1">Deep Learning with Data Privacy via Residual Perturbation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-08-11T08:26:43Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Wenqi Tao, Huaming Ling, Zuoqiang Shi, Bao Wang</p>
    <p><b>Summary:</b> Protecting data privacy in deep learning (DL) is of crucial importance.
Several celebrated privacy notions have been established and used for
privacy-preserving DL. However, many existing mechanisms achieve privacy at the
cost of significant utility degradation and computational overhead. In this
paper, we propose a stochastic differential equation-based residual
perturbation for privacy-preserving DL, which injects Gaussian noise into each
residual mapping of ResNets. Theoretically, we prove that residual perturbation
guarantees differential privacy (DP) and reduces the generalization gap of DL.
Empirically, we show that residual perturbation is computationally efficient
and outperforms the state-of-the-art differentially private stochastic gradient
descent (DPSGD) in utility maintenance without sacrificing membership privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.05543v1">PixelFade: Privacy-preserving Person Re-identification with Noise-guided
  Progressive Replacement</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-08-10T12:52:54Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Delong Zhang, Yi-Xing Peng, Xiao-Ming Wu, Ancong Wu, Wei-Shi Zheng</p>
    <p><b>Summary:</b> Online person re-identification services face privacy breaches from potential
data leakage and recovery attacks, exposing cloud-stored images to malicious
attackers and triggering public concern. The privacy protection of pedestrian
images is crucial. Previous privacy-preserving person re-identification methods
are unable to resist recovery attacks and compromise accuracy. In this paper,
we propose an iterative method (PixelFade) to optimize pedestrian images into
noise-like images to resist recovery attacks. We first give an in-depth study
of protected images from previous privacy methods, which reveal that the chaos
of protected images can disrupt the learning of recovery models. Accordingly,
Specifically, we propose Noise-guided Objective Function with the feature
constraints of a specific authorization model, optimizing pedestrian images to
normal-distributed noise images while preserving their original identity
information as per the authorization model. To solve the above non-convex
optimization problem, we propose a heuristic optimization algorithm that
alternately performs the Constraint Operation and the Partial Replacement
Operation. This strategy not only safeguards that original pixels are replaced
with noises to protect privacy, but also guides the images towards an improved
optimization direction to effectively preserve discriminative features.
Extensive experiments demonstrate that our PixelFade outperforms previous
methods in resisting recovery attacks and Re-ID performance. The code is
available at https://github.com/iSEE-Laboratory/PixelFade.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.05212v1">Preserving Privacy in Large Language Models: A Survey on Current Threats
  and Solutions</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-08-10T05:41:19Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Michele Miranda, Elena Sofia Ruzzetti, Andrea Santilli, Fabio Massimo Zanzotto, Sébastien Bratières, Emanuele Rodolà</p>
    <p><b>Summary:</b> Large Language Models (LLMs) represent a significant advancement in
artificial intelligence, finding applications across various domains. However,
their reliance on massive internet-sourced datasets for training brings notable
privacy issues, which are exacerbated in critical domains (e.g., healthcare).
Moreover, certain application-specific scenarios may require fine-tuning these
models on private data. This survey critically examines the privacy threats
associated with LLMs, emphasizing the potential for these models to memorize
and inadvertently reveal sensitive information. We explore current threats by
reviewing privacy attacks on LLMs and propose comprehensive solutions for
integrating privacy mechanisms throughout the entire learning pipeline. These
solutions range from anonymizing training datasets to implementing differential
privacy during training or inference and machine unlearning after training. Our
comprehensive review of existing literature highlights ongoing challenges,
available tools, and future directions for preserving privacy in LLMs. This
work aims to guide the development of more secure and trustworthy AI systems by
providing a thorough understanding of privacy preservation methods and their
effectiveness in mitigating risks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.05092v1">PriPHiT: Privacy-Preserving Hierarchical Training of Deep Neural
  Networks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">  
  <p><b>Published on:</b> 2024-08-09T14:33:34Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yamin Sepehri, Pedram Pad, Pascal Frossard, L. Andrea Dunbar</p>
    <p><b>Summary:</b> The training phase of deep neural networks requires substantial resources and
as such is often performed on cloud servers. However, this raises privacy
concerns when the training dataset contains sensitive content, e.g., face
images. In this work, we propose a method to perform the training phase of a
deep learning model on both an edge device and a cloud server that prevents
sensitive content being transmitted to the cloud while retaining the desired
information. The proposed privacy-preserving method uses adversarial early
exits to suppress the sensitive content at the edge and transmits the
task-relevant information to the cloud. This approach incorporates noise
addition during the training phase to provide a differential privacy guarantee.
We extensively test our method on different facial datasets with diverse face
attributes using various deep learning architectures, showcasing its
outstanding performance. We also demonstrate the effectiveness of privacy
preservation through successful defenses against different white-box and deep
reconstruction attacks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.04931v1">Privacy-Preserved Taxi Demand Prediction System Utilizing Distributed
  Data</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2024-08-09T08:24:47Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ren Ozeki, Haruki Yonekura, Hamada Rizk, Hirozumi Yamaguchi</p>
    <p><b>Summary:</b> Accurate taxi-demand prediction is essential for optimizing taxi operations
and enhancing urban transportation services. However, using customers' data in
these systems raises significant privacy and security concerns. Traditional
federated learning addresses some privacy issues by enabling model training
without direct data exchange but often struggles with accuracy due to varying
data distributions across different regions or service providers. In this
paper, we propose CC-Net: a novel approach using collaborative learning
enhanced with contrastive learning for taxi-demand prediction. Our method
ensures high performance by enabling multiple parties to collaboratively train
a demand-prediction model through hierarchical federated learning. In this
approach, similar parties are clustered together, and federated learning is
applied within each cluster. The similarity is defined without data exchange,
ensuring privacy and security. We evaluated our approach using real-world data
from five taxi service providers in Japan over fourteen months. The results
demonstrate that CC-Net maintains the privacy of customers' data while
improving prediction accuracy by at least 2.2% compared to existing techniques.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.04888v1">Locally Private Histograms in All Privacy Regimes</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Discrete Mathematics-04E762">
  <p><b>Published on:</b> 2024-08-09T06:22:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Clément L. Canonne, Abigail Gentle</p>
    <p><b>Summary:</b> Frequency estimation, a.k.a. histograms, is a workhorse of data analysis, and
as such has been thoroughly studied under differentially privacy. In
particular, computing histograms in the local model of privacy has been the
focus of a fruitful recent line of work, and various algorithms have been
proposed, achieving the order-optimal $\ell_\infty$ error in the high-privacy
(small $\varepsilon$) regime while balancing other considerations such as time-
and communication-efficiency. However, to the best of our knowledge, the
picture is much less clear when it comes to the medium- or low-privacy regime
(large $\varepsilon$), despite its increased relevance in practice. In this
paper, we investigate locally private histograms, and the very related
distribution learning task, in this medium-to-low privacy regime, and establish
near-tight (and somewhat unexpected) bounds on the $\ell_\infty$ error
achievable. Our theoretical findings emerge from a novel analysis, which
appears to improve bounds across the board for the locally private histogram
problem. We back our theoretical findings by an empirical comparison of
existing algorithms in all privacy regimes, to assess their typical performance
and behaviour beyond the worst-case setting.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.04684v1">Moving beyond privacy and airspace safety: Guidelines for just drones in
  policing</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2024-08-08T09:04:01Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mateusz Dolata, Gerhard Schwabe</p>
    <p><b>Summary:</b> The use of drones offers police forces potential gains in efficiency and
safety. However, their use may also harm public perception of the police if
drones are refused. Therefore, police forces should consider the perception of
bystanders and broader society to maximize drones' potential. This article
examines the concerns expressed by members of the public during a field trial
involving 52 test participants. Analysis of the group interviews suggests that
their worries go beyond airspace safety and privacy, broadly discussed in
existing literature and regulations. The interpretation of the results
indicates that the perceived justice of drone use is a significant factor in
acceptance. Leveraging the concept of organizational justice and data
collected, we propose a catalogue of guidelines for just operation of drones to
supplement the existing policy. We present the organizational justice
perspective as a framework to integrate the concerns of the public and
bystanders into legal work. Finally, we discuss the relevance of justice for
the legitimacy of the police's actions and provide implications for research
and practice.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.04315v1">Federated Cubic Regularized Newton Learning with
  Sparsification-amplified Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36"> 
  <p><b>Published on:</b> 2024-08-08T08:48:54Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Wei Huo, Changxin Liu, Kemi Ding, Karl Henrik Johansson, Ling Shi</p>
    <p><b>Summary:</b> This paper investigates the use of the cubic-regularized Newton method within
a federated learning framework while addressing two major concerns that
commonly arise in federated learning: privacy leakage and communication
bottleneck. We introduce a federated learning algorithm called Differentially
Private Federated Cubic Regularized Newton (DP-FCRN). By leveraging
second-order techniques, our algorithm achieves lower iteration complexity
compared to first-order methods. We also incorporate noise perturbation during
local computations to ensure privacy. Furthermore, we employ sparsification in
uplink transmission, which not only reduces the communication costs but also
amplifies the privacy guarantee. Specifically, this approach reduces the
necessary noise intensity without compromising privacy protection. We analyze
the convergence properties of our algorithm and establish the privacy
guarantee. Finally, we validate the effectiveness of the proposed algorithm
through experiments on a benchmark dataset.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.04188v1">Trustworthy Semantic-Enabled 6G Communication: A Task-oriented and
  Privacy-preserving Perspective</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36"> 
  <p><b>Published on:</b> 2024-08-08T03:16:42Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shuaishuai Guo, Anbang Zhang, Yanhu Wang, Chenyuan Feng, Tony Q. S. Quek</p>
    <p><b>Summary:</b> Trustworthy task-oriented semantic communication (ToSC) emerges as an
innovative approach in the 6G landscape, characterized by the transmission of
only vital information that is directly pertinent to a specific task. While
ToSC offers an efficient mode of communication, it concurrently raises concerns
regarding privacy, as sophisticated adversaries might possess the capability to
reconstruct the original data from the transmitted features. This article
provides an in-depth analysis of privacy-preserving strategies specifically
designed for ToSC relying on deep neural network-based joint source and channel
coding (DeepJSCC). The study encompasses a detailed comparative assessment of
trustworthy feature perturbation methods such as differential privacy and
encryption, alongside intrinsic security incorporation approaches like
adversarial learning to train the JSCC and learning-based vector quantization
(LBVQ). This comparative analysis underscores the integration of advanced
explainable learning algorithms into communication systems, positing a new
benchmark for privacy standards in the forthcoming 6G era.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.03897v1">Speech privacy-preserving methods using secret key for convolutional
  neural network models and their robustness evaluation</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Sound-D91E36">
  <p><b>Published on:</b> 2024-08-07T16:51:39Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shoko Niwa, Sayaka Shiota, Hitoshi Kiya</p>
    <p><b>Summary:</b> In this paper, we propose privacy-preserving methods with a secret key for
convolutional neural network (CNN)-based models in speech processing tasks. In
environments where untrusted third parties, like cloud servers, provide
CNN-based systems, ensuring the privacy of speech queries becomes essential.
This paper proposes encryption methods for speech queries using secret keys and
a model structure that allows for encrypted queries to be accepted without
decryption. Our approach introduces three types of secret keys: Shuffling,
Flipping, and random orthogonal matrix (ROM). In experiments, we demonstrate
that when the proposed methods are used with the correct key, identification
performance did not degrade. Conversely, when an incorrect key is used, the
performance significantly decreased. Particularly, with the use of ROM, we show
that even with a relatively small key space, high privacy-preserving
performance can be maintained many speech processing tasks. Furthermore, we
also demonstrate the difficulty of recovering original speech from encrypted
queries in various robustness evaluations.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.03578v1">Unraveling Privacy Threat Modeling Complexity: Conceptual Privacy
  Analysis Layers</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-07T06:30:20Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kim Wuyts, Avi Douglen</p>
    <p><b>Summary:</b> Analyzing privacy threats in software products is an essential part of
software development to ensure systems are privacy-respecting; yet it is still
a far from trivial activity. While there have been many advancements in the
past decade, they tend to focus on describing 'what' the threats are. What
isn't entirely clear yet is 'how' to actually find these threats. Privacy is a
complex domain. We propose to use four conceptual layers (feature, ecosystem,
business context, and environment) to capture this privacy complexity. These
layers can be used as a frame to structure and specify the privacy analysis
support in a more tangible and actionable way, thereby improving applicability
of the analysis process.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.03185v1">MaskAnyone Toolkit: Offering Strategies for Minimizing Privacy Risks and
  Maximizing Utility in Audio-Visual Data Archiving</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Multimedia-5BC0EB">
  <p><b>Published on:</b> 2024-08-06T13:35:27Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Babajide Alamu Owoyele, Martin Schilling, Rohan Sawahn, Niklas Kaemer, Pavel Zherebenkov, Bhuvanesh Verma, Wim Pouw, Gerard de Melo</p>
    <p><b>Summary:</b> This paper introduces MaskAnyone, a novel toolkit designed to navigate some
privacy and ethical concerns of sharing audio-visual data in research.
MaskAnyone offers a scalable, user-friendly solution for de-identifying
individuals in video and audio content through face-swapping and voice
alteration, supporting multi-person masking and real-time bulk processing. By
integrating this tool within research practices, we aim to enhance data
reproducibility and utility in social science research. Our approach draws on
Design Science Research, proposing that MaskAnyone can facilitate safer data
sharing and potentially reduce the storage of fully identifiable data. We
discuss the development and capabilities of MaskAnyone, explore its integration
into ethical research practices, and consider the broader implications of
audio-visual data masking, including issues of consent and the risk of misuse.
The paper concludes with a preliminary evaluation framework for assessing the
effectiveness and ethical integration of masking tools in such research
settings.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.02927v1">HARMONIC: Harnessing LLMs for Tabular Data Synthesis and Privacy
  Protection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-06T03:21:13Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yuxin Wang, Duanyu Feng, Yongfu Dai, Zhengyu Chen, Jimin Huang, Sophia Ananiadou, Qianqian Xie, Hao Wang</p>
    <p><b>Summary:</b> Data serves as the fundamental foundation for advancing deep learning,
particularly tabular data presented in a structured format, which is highly
conducive to modeling. However, even in the era of LLM, obtaining tabular data
from sensitive domains remains a challenge due to privacy or copyright
concerns. Hence, exploring how to effectively use models like LLMs to generate
realistic and privacy-preserving synthetic tabular data is urgent. In this
paper, we take a step forward to explore LLMs for tabular data synthesis and
privacy protection, by introducing a new framework HARMONIC for tabular data
generation and evaluation. In the tabular data generation of our framework,
unlike previous small-scale LLM-based methods that rely on continued
pre-training, we explore the larger-scale LLMs with fine-tuning to generate
tabular data and enhance privacy. Based on idea of the k-nearest neighbors
algorithm, an instruction fine-tuning dataset is constructed to inspire LLMs to
discover inter-row relationships. Then, with fine-tuning, LLMs are trained to
remember the format and connections of the data rather than the data itself,
which reduces the risk of privacy leakage. In the evaluation part of our
framework, we develop specific privacy risk metrics DLT for LLM synthetic data
generation, as well as performance evaluation metrics LLE for downstream LLM
tasks. Our experiments find that this tabular data generation framework
achieves equivalent performance to existing methods with better privacy, which
also demonstrates our evaluation framework for the effectiveness of synthetic
data and privacy risks in LLM scenarios.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.02750v1">Privacy-Safe Iris Presentation Attack Detection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> 
  <p><b>Published on:</b> 2024-08-05T18:09:02Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mahsa Mitcheff, Patrick Tinsley, Adam Czajka</p>
    <p><b>Summary:</b> This paper proposes a framework for a privacy-safe iris presentation attack
detection (PAD) method, designed solely with synthetically-generated,
identity-leakage-free iris images. Once trained, the method is evaluated in a
classical way using state-of-the-art iris PAD benchmarks. We designed two
generative models for the synthesis of ISO/IEC 19794-6-compliant iris images.
The first model synthesizes bona fide-looking samples. To avoid ``identity
leakage,'' the generated samples that accidentally matched those used in the
model's training were excluded. The second model synthesizes images of irises
with textured contact lenses and is conditioned by a given contact lens brand
to have better control over textured contact lens appearance when forming the
training set. Our experiments demonstrate that models trained solely on
synthetic data achieve a lower but still reasonable performance when compared
to solutions trained with iris images collected from human subjects. This is
the first-of-its-kind attempt to use solely synthetic data to train a
fully-functional iris PAD solution, and despite the performance gap between
regular and the proposed methods, this study demonstrates that with the
increasing fidelity of generative models, creating such privacy-safe iris PAD
methods may be possible. The source codes and generative models trained for
this work are offered along with the paper.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.02373v1">Operationalizing Contextual Integrity in Privacy-Conscious Assistants</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-08-05T10:53:51Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sahra Ghalebikesabi, Eugene Bagdasaryan, Ren Yi, Itay Yona, Ilia Shumailov, Aneesh Pappu, Chongyang Shi, Laura Weidinger, Robert Stanforth, Leonard Berrada, Pushmeet Kohli, Po-Sen Huang, Borja Balle</p>
    <p><b>Summary:</b> Advanced AI assistants combine frontier LLMs and tool access to autonomously
perform complex tasks on behalf of users. While the helpfulness of such
assistants can increase dramatically with access to user information including
emails and documents, this raises privacy concerns about assistants sharing
inappropriate information with third parties without user supervision. To steer
information-sharing assistants to behave in accordance with privacy
expectations, we propose to operationalize $\textit{contextual integrity}$
(CI), a framework that equates privacy with the appropriate flow of information
in a given context. In particular, we design and evaluate a number of
strategies to steer assistants' information-sharing actions to be CI compliant.
Our evaluation is based on a novel form filling benchmark composed of synthetic
data and human annotations, and it reveals that prompting frontier LLMs to
perform CI-based reasoning yields strong results.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.01711v1">Privacy in networks of quantum sensors</a></h3>
  
  <p><b>Published on:</b> 2024-08-03T08:39:44Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Majid Hassani, Santiago Scheiner, Matteo G. A. Paris, Damian Markham</p>
    <p><b>Summary:</b> We treat privacy in a network of quantum sensors where accessible information
is limited to specific functions of the network parameters, and all other
information remains private. We develop an analysis of privacy in terms of a
manipulation of the quantum Fisher information matrix, and find the optimal
state achieving maximum privacy in the estimation of linear combination of the
unknown parameters in a network of quantum sensors. We also discuss the effect
of uncorrelated noise on the privacy of the network. Moreover, we illustrate
our results with an example where the goal is to estimate the average value of
the unknown parameters in the network. In this example, we also introduce the
notion of quasi-privacy ($\epsilon$-privacy), quantifying how close the state
is to being private.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.01609v1">Fed-RD: Privacy-Preserving Federated Learning for Financial Crime
  Detection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computational Engineering, Finance, and Science-5BC0EB">
  <p><b>Published on:</b> 2024-08-03T00:07:10Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Md. Saikat Islam Khan, Aparna Gupta, Oshani Seneviratne, Stacy Patterson</p>
    <p><b>Summary:</b> We introduce Federated Learning for Relational Data (Fed-RD), a novel
privacy-preserving federated learning algorithm specifically developed for
financial transaction datasets partitioned vertically and horizontally across
parties. Fed-RD strategically employs differential privacy and secure
multiparty computation to guarantee the privacy of training data. We provide
theoretical analysis of the end-to-end privacy of the training algorithm and
present experimental results on realistic synthetic datasets. Our results
demonstrate that Fed-RD achieves high model accuracy with minimal degradation
as privacy increases, while consistently surpassing benchmark results.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.01228v2">The Phantom Menace: Unmasking Privacy Leakages in Vision-Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-08-02T12:36:13Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Simone Caldarella, Massimiliano Mancini, Elisa Ricci, Rahaf Aljundi</p>
    <p><b>Summary:</b> Vision-Language Models (VLMs) combine visual and textual understanding,
rendering them well-suited for diverse tasks like generating image captions and
answering visual questions across various domains. However, these capabilities
are built upon training on large amount of uncurated data crawled from the web.
The latter may include sensitive information that VLMs could memorize and leak,
raising significant privacy concerns. In this paper, we assess whether these
vulnerabilities exist, focusing on identity leakage. Our study leads to three
key findings: (i) VLMs leak identity information, even when the vision-language
alignment and the fine-tuning use anonymized data; (ii) context has little
influence on identity leakage; (iii) simple, widely used anonymization
techniques, like blurring, are not sufficient to address the problem. These
findings underscore the urgent need for robust privacy protection strategies
when deploying VLMs. Ethical awareness and responsible development practices
are essential to mitigate these risks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.01040v1">Privacy-Preserving Split Learning with Vision Transformers using
  Patch-Wise Random and Noisy CutMix</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-08-02T06:24:39Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Seungeun Oh, Sihun Baek, Jihong Park, Hyelin Nam, Praneeth Vepakomma, Ramesh Raskar, Mehdi Bennis, Seong-Lyun Kim</p>
    <p><b>Summary:</b> In computer vision, the vision transformer (ViT) has increasingly superseded
the convolutional neural network (CNN) for improved accuracy and robustness.
However, ViT's large model sizes and high sample complexity make it difficult
to train on resource-constrained edge devices. Split learning (SL) emerges as a
viable solution, leveraging server-side resources to train ViTs while utilizing
private data from distributed devices. However, SL requires additional
information exchange for weight updates between the device and the server,
which can be exposed to various attacks on private training data. To mitigate
the risk of data breaches in classification tasks, inspired from the CutMix
regularization, we propose a novel privacy-preserving SL framework that injects
Gaussian noise into smashed data and mixes randomly chosen patches of smashed
data across clients, coined DP-CutMixSL. Our analysis demonstrates that
DP-CutMixSL is a differentially private (DP) mechanism that strengthens privacy
protection against membership inference attacks during forward propagation.
Through simulations, we show that DP-CutMixSL improves privacy protection
against membership inference attacks, reconstruction attacks, and label
inference attacks, while also improving accuracy compared to DP-SL and
DP-MixSL.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.00950v1">PrivateGaze: Preserving User Privacy in Black-box Mobile Gaze Tracking
  Services</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-08-01T23:11:03Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Lingyu Du, Jinyuan Jia, Xucong Zhang, Guohao Lan</p>
    <p><b>Summary:</b> Eye gaze contains rich information about human attention and cognitive
processes. This capability makes the underlying technology, known as gaze
tracking, a critical enabler for many ubiquitous applications and has triggered
the development of easy-to-use gaze estimation services. Indeed, by utilizing
the ubiquitous cameras on tablets and smartphones, users can readily access
many gaze estimation services. In using these services, users must provide
their full-face images to the gaze estimator, which is often a black box. This
poses significant privacy threats to the users, especially when a malicious
service provider gathers a large collection of face images to classify
sensitive user attributes. In this work, we present PrivateGaze, the first
approach that can effectively preserve users' privacy in black-box gaze
tracking services without compromising gaze estimation performance.
Specifically, we proposed a novel framework to train a privacy preserver that
converts full-face images into obfuscated counterparts, which are effective for
gaze estimation while containing no privacy information. Evaluation on four
datasets shows that the obfuscated image can protect users' private
information, such as identity and gender, against unauthorized attribute
classification. Meanwhile, when used directly by the black-box gaze estimator
as inputs, the obfuscated images lead to comparable tracking performance to the
conventional, unprotected full-face images.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.00639v1">Privacy-preserving datasets by capturing feature distributions with
  Conditional VAEs</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> 
  <p><b>Published on:</b> 2024-08-01T15:26:24Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Francesco Di Salvo, David Tafler, Sebastian Doerrich, Christian Ledig</p>
    <p><b>Summary:</b> Large and well-annotated datasets are essential for advancing deep learning
applications, however often costly or impossible to obtain by a single entity.
In many areas, including the medical domain, approaches relying on data sharing
have become critical to address those challenges. While effective in increasing
dataset size and diversity, data sharing raises significant privacy concerns.
Commonly employed anonymization methods based on the k-anonymity paradigm often
fail to preserve data diversity, affecting model robustness. This work
introduces a novel approach using Conditional Variational Autoencoders (CVAEs)
trained on feature vectors extracted from large pre-trained vision foundation
models. Foundation models effectively detect and represent complex patterns
across diverse domains, allowing the CVAE to faithfully capture the embedding
space of a given data distribution to generate (sample) a diverse,
privacy-respecting, and potentially unbounded set of synthetic feature vectors.
Our method notably outperforms traditional approaches in both medical and
natural image domains, exhibiting greater dataset diversity and higher
robustness against perturbations while preserving sample privacy. These results
underscore the potential of generative models to significantly impact deep
learning applications in data-scarce and privacy-sensitive environments. The
source code is available at
https://github.com/francescodisalvo05/cvae-anonymization .</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.00382v1">Long-Term Conversation Analysis: Privacy-Utility Trade-off under Noise
  and Reverberation</a></h3>
  
  <p><b>Published on:</b> 2024-08-01T08:43:46Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jule Pohlhausen, Francesco Nespoli, Joerg Bitzer</p>
    <p><b>Summary:</b> Recordings in everyday life require privacy preservation of the speech
content and speaker identity. This contribution explores the influence of noise
and reverberation on the trade-off between privacy and utility for low-cost
privacy-preserving methods feasible for edge computing. These methods
compromise spectral and temporal smoothing, speaker anonymization using the
McAdams coefficient, sampling with a very low sampling rate, and combinations.
Privacy is assessed by automatic speech and speaker recognition, while our
utility considers voice activity detection and speaker diarization. Overall,
our evaluation shows that additional noise degrades the performance of all
models more than reverberation. This degradation corresponds to enhanced speech
privacy, while utility is less deteriorated for some methods.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.00294v1">RDP: Ranked Differential Privacy for Facial Feature Protection in
  Multiscale Sparsified Subspace</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Retrieval-5BC0EB">
  <p><b>Published on:</b> 2024-08-01T05:41:59Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Lu Ou, Shaolin Liao, Shihui Gao, Guandong Huang, Zheng Qi</p>
    <p><b>Summary:</b> With the widespread sharing of personal face images in applications' public
databases, face recognition systems faces real threat of being breached by
potential adversaries who are able to access users' face images and use them to
intrude the face recognition systems. In this paper, we propose a novel privacy
protection method in the multiscale sparsified feature subspaces to protect
sensitive facial features, by taking care of the influence or weight ranked
feature coefficients on the privacy budget, named "Ranked Differential Privacy
(RDP)". After the multiscale feature decomposition, the lightweight Laplacian
noise is added to the dimension-reduced sparsified feature coefficients
according to the geometric superposition method. Then, we rigorously prove that
the RDP satisfies Differential Privacy. After that, the nonlinear Lagrange
Multiplier (LM) method is formulated for the constraint optimization problem of
maximizing the utility of the visualization quality protected face images with
sanitizing noise, under a given facial features privacy budget. Then, two
methods are proposed to solve the nonlinear LM problem and obtain the optimal
noise scale parameters: 1) the analytical Normalization Approximation (NA)
method with identical average noise scale parameter for real-time online
applications; and 2) the LM optimization Gradient Descent (LMGD) numerical
method to obtain the nonlinear solution through iterative updating for more
accurate offline applications. Experimental results on two real-world datasets
show that our proposed RDP outperforms other state-of-the-art methods: at a
privacy budget of 0.2, the PSNR (Peak Signal-to-Noise Ratio) of the RDP is
about ~10 dB higher than (10 times as high as) the highest PSNR of all compared
methods.</p>
  </details>
</div>



<h2>2024-07</h2>

<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.21691v1">Explainable Artificial Intelligence for Quantifying Interfering and
  High-Risk Behaviors in Autism Spectrum Disorder in a Real-World Classroom
  Environment Using Privacy-Preserving Video Analysis</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-07-31T15:37:52Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Barun Das, Conor Anderson, Tania Villavicencio, Johanna Lantz, Jenny Foster, Theresa Hamlin, Ali Bahrami Rad, Gari D. Clifford, Hyeokhyen Kwon</p>
    <p><b>Summary:</b> Rapid identification and accurate documentation of interfering and high-risk
behaviors in ASD, such as aggression, self-injury, disruption, and restricted
repetitive behaviors, are important in daily classroom environments for
tracking intervention effectiveness and allocating appropriate resources to
manage care needs. However, having a staff dedicated solely to observing is
costly and uncommon in most educational settings. Recently, multiple research
studies have explored developing automated, continuous, and objective tools
using machine learning models to quantify behaviors in ASD. However, the
majority of the work was conducted under a controlled environment and has not
been validated for real-world conditions. In this work, we demonstrate that the
latest advances in video-based group activity recognition techniques can
quantify behaviors in ASD in real-world activities in classroom environments
while preserving privacy. Our explainable model could detect the episode of
problem behaviors with a 77% F1-score and capture distinctive behavior features
in different types of behaviors in ASD. To the best of our knowledge, this is
the first work that shows the promise of objectively quantifying behaviors in
ASD in a real-world environment, which is an important step toward the
development of a practical tool that can ease the burden of data collection for
classroom staff.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.21624v1">Grid-Based Decompositions for Spatial Data under Local Differential
  Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-07-31T14:17:44Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Berkay Kemal Balioglu, Alireza Khodaie, Ameer Taweel, Mehmet Emre Gursoy</p>
    <p><b>Summary:</b> Local differential privacy (LDP) has recently emerged as a popular privacy
standard. With the growing popularity of LDP, several recent works have applied
LDP to spatial data, and grid-based decompositions have been a common building
block in the collection and analysis of spatial data under DP and LDP. In this
paper, we study three grid-based decomposition methods for spatial data under
LDP: Uniform Grid (UG), PrivAG, and AAG. UG is a static approach that consists
of equal-sized cells. To enable data-dependent decomposition, PrivAG was
proposed by Yang et al. as the most recent adaptive grid method. To advance the
state-of-the-art in adaptive grids, in this paper we propose the Advanced
Adaptive Grid (AAG) method. For each grid cell, following the intuition that
the cell's intra-cell density distribution will be affected by its neighbors,
AAG performs uneven cell divisions depending on the neighboring cells'
densities. We experimentally compare UG, PrivAG, and AAG using three real-world
location datasets, varying privacy budgets, and query sizes. Results show that
AAG provides higher utility than PrivAG, demonstrating the superiority of our
proposed approach. Furthermore, UG's performance is heavily dependent on the
choice of grid size. When the grid size is chosen optimally in UG, AAG still
beats UG for small queries, but UG beats AAG for large (coarse-grained)
queries.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.21141v1">FL-DECO-BC: A Privacy-Preserving, Provably Secure, and
  Provenance-Preserving Federated Learning Framework with Decentralized Oracles
  on Blockchain for VANETs</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-07-30T19:09:10Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sathwik Narkedimilli, Rayachoti Arun Kumar, N. V. Saran Kumar, Ramapathruni Praneeth Reddy, Pavan Kumar C</p>
    <p><b>Summary:</b> Vehicular Ad-Hoc Networks (VANETs) hold immense potential for improving
traffic safety and efficiency. However, traditional centralized approaches for
machine learning in VANETs raise concerns about data privacy and security.
Federated Learning (FL) offers a solution that enables collaborative model
training without sharing raw data. This paper proposes FL-DECO-BC as a novel
privacy-preserving, provably secure, and provenance-preserving federated
learning framework specifically designed for VANETs. FL-DECO-BC leverages
decentralized oracles on blockchain to securely access external data sources
while ensuring data privacy through advanced techniques. The framework
guarantees provable security through cryptographic primitives and formal
verification methods. Furthermore, FL-DECO-BC incorporates a
provenance-preserving design to track data origin and history, fostering trust
and accountability. This combination of features empowers VANETs with secure
and privacy-conscious machine-learning capabilities, paving the way for
advanced traffic management and safety applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.20830v1">Federated Knowledge Recycling: Privacy-Preserving Synthetic Data Sharing</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-07-30T13:56:26Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Eugenio Lomurno, Matteo Matteucci</p>
    <p><b>Summary:</b> Federated learning has emerged as a paradigm for collaborative learning,
enabling the development of robust models without the need to centralise
sensitive data. However, conventional federated learning techniques have
privacy and security vulnerabilities due to the exposure of models, parameters
or updates, which can be exploited as an attack surface. This paper presents
Federated Knowledge Recycling (FedKR), a cross-silo federated learning approach
that uses locally generated synthetic data to facilitate collaboration between
institutions. FedKR combines advanced data generation techniques with a dynamic
aggregation process to provide greater security against privacy attacks than
existing methods, significantly reducing the attack surface. Experimental
results on generic and medical datasets show that FedKR achieves competitive
performance, with an average improvement in accuracy of 4.24% compared to
training models from local data, demonstrating particular effectiveness in data
scarcity scenarios.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.20640v1">Improved Bounds for Pure Private Agnostic Learning: Item-Level and
  User-Level Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-07-30T08:35:26Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Bo Li, Wei Wang, Peng Ye</p>
    <p><b>Summary:</b> Machine Learning has made remarkable progress in a wide range of fields. In
many scenarios, learning is performed on datasets involving sensitive
information, in which privacy protection is essential for learning algorithms.
In this work, we study pure private learning in the agnostic model -- a
framework reflecting the learning process in practice. We examine the number of
users required under item-level (where each user contributes one example) and
user-level (where each user contributes multiple examples) privacy and derive
several improved upper bounds. For item-level privacy, our algorithm achieves a
near optimal bound for general concept classes. We extend this to the
user-level setting, rendering a tighter upper bound than the one proved by
Ghazi et al. (2023). Lastly, we consider the problem of learning thresholds
under user-level privacy and present an algorithm with a nearly tight user
complexity.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.19828v1">Federated Learning based Latent Factorization of Tensors for
  Privacy-Preserving QoS Prediction</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-07-29T09:30:00Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shuai Zhong, Zengtong Tang, Di Wu</p>
    <p><b>Summary:</b> In applications related to big data and service computing, dynamic
connections tend to be encountered, especially the dynamic data of
user-perspective quality of service (QoS) in Web services. They are transformed
into high-dimensional and incomplete (HDI) tensors which include abundant
temporal pattern information. Latent factorization of tensors (LFT) is an
extremely efficient and typical approach for extracting such patterns from an
HDI tensor. However, current LFT models require the QoS data to be maintained
in a central place (e.g., a central server), which is impossible for
increasingly privacy-sensitive users. To address this problem, this article
creatively designs a federated learning based on latent factorization of
tensors (FL-LFT). It builds a data-density -oriented federated learning model
to enable isolated users to collaboratively train a global LFT model while
protecting user's privacy. Extensive experiments on a QoS dataset collected
from the real world verify that FL-LFT shows a remarkable increase in
prediction accuracy when compared to state-of-the-art federated learning (FL)
approaches.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.19703v1">Efficient Byzantine-Robust and Provably Privacy-Preserving Federated
  Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-07-29T04:55:30Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chenfei Nie, Qiang Li, Yuxin Yang, Yuede Ji, Binghui Wang</p>
    <p><b>Summary:</b> Federated learning (FL) is an emerging distributed learning paradigm without
sharing participating clients' private data. However, existing works show that
FL is vulnerable to both Byzantine (security) attacks and data reconstruction
(privacy) attacks. Almost all the existing FL defenses only address one of the
two attacks. A few defenses address the two attacks, but they are not efficient
and effective enough. We propose BPFL, an efficient Byzantine-robust and
provably privacy-preserving FL method that addresses all the issues.
Specifically, we draw on state-of-the-art Byzantine-robust FL methods and use
similarity metrics to measure the robustness of each participating client in
FL. The validity of clients are formulated as circuit constraints on similarity
metrics and verified via a zero-knowledge proof. Moreover, the client models
are masked by a shared random vector, which is generated based on homomorphic
encryption. In doing so, the server receives the masked client models rather
than the true ones, which are proven to be private. BPFL is also efficient due
to the usage of non-interactive zero-knowledge proof. Experimental results on
various datasets show that our BPFL is efficient, Byzantine-robust, and
privacy-preserving.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.19677v1">Navigating the United States Legislative Landscape on Voice Privacy:
  Existing Laws, Proposed Bills, Protection for Children, and Synthetic Data
  for AI</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Sound-D91E36">  
  <p><b>Published on:</b> 2024-07-29T03:43:16Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Satwik Dutta, John H. L. Hansen</p>
    <p><b>Summary:</b> Privacy is a hot topic for policymakers across the globe, including the
United States. Evolving advances in AI and emerging concerns about the misuse
of personal data have pushed policymakers to draft legislation on trustworthy
AI and privacy protection for its citizens. This paper presents the state of
the privacy legislation at the U.S. Congress and outlines how voice data is
considered as part of the legislation definition. This paper also reviews
additional privacy protection for children. This paper presents a holistic
review of enacted and proposed privacy laws, and consideration for voice data,
including guidelines for processing children's data, in those laws across the
fifty U.S. states. As a groundbreaking alternative to actual human data,
ethically generated synthetic data allows much flexibility to keep AI
innovation in progress. Given the consideration of synthetic data in AI
legislation by policymakers to be relatively new, as compared to that of
privacy laws, this paper reviews regulatory considerations for synthetic data.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.19401v1">Complete Security and Privacy for AI Inference in Decentralized Systems</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-07-28T05:09:17Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Hongyang Zhang, Yue Zhao, Claudio Angione, Harry Yang, James Buban, Ahmad Farhan, Fielding Johnston, Patrick Colangelo</p>
    <p><b>Summary:</b> The need for data security and model integrity has been accentuated by the
rapid adoption of AI and ML in data-driven domains including healthcare,
finance, and security. Large models are crucial for tasks like diagnosing
diseases and forecasting finances but tend to be delicate and not very
scalable. Decentralized systems solve this issue by distributing the workload
and reducing central points of failure. Yet, data and processes spread across
different nodes can be at risk of unauthorized access, especially when they
involve sensitive information. Nesa solves these challenges with a
comprehensive framework using multiple techniques to protect data and model
outputs. This includes zero-knowledge proofs for secure model verification. The
framework also introduces consensus-based verification checks for consistent
outputs across nodes and confirms model integrity. Split Learning divides
models into segments processed by different nodes for data privacy by
preventing full data access at any single point. For hardware-based security,
trusted execution environments are used to protect data and computations within
secure zones. Nesa's state-of-the-art proofs and principles demonstrate the
framework's effectiveness, making it a promising approach for securely
democratizing artificial intelligence.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.19364v1">Defogger: A Visual Analysis Approach for Data Exploration of Sensitive
  Data Protected by Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-07-28T02:14:12Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xumeng Wang, Shuangcheng Jiao, Chris Bryan</p>
    <p><b>Summary:</b> Differential privacy ensures the security of individual privacy but poses
challenges to data exploration processes because the limited privacy budget
incapacitates the flexibility of exploration and the noisy feedback of data
requests leads to confusing uncertainty. In this study, we take the lead in
describing corresponding exploration scenarios, including underlying
requirements and available exploration strategies. To facilitate practical
applications, we propose a visual analysis approach to the formulation of
exploration strategies. Our approach applies a reinforcement learning model to
provide diverse suggestions for exploration strategies according to the
exploration intent of users. A novel visual design for representing uncertainty
in correlation patterns is integrated into our prototype system to support the
proposed approach. Finally, we implemented a user study and two case studies.
The results of these studies verified that our approach can help develop
strategies that satisfy the exploration intent of users.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.19354v1">The Emerged Security and Privacy of LLM Agent: A Survey with Case
  Studies</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-07-28T00:26:24Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Feng He, Tianqing Zhu, Dayong Ye, Bo Liu, Wanlei Zhou, Philip S. Yu</p>
    <p><b>Summary:</b> Inspired by the rapid development of Large Language Models (LLMs), LLM agents
have evolved to perform complex tasks. LLM agents are now extensively applied
across various domains, handling vast amounts of data to interact with humans
and execute tasks. The widespread applications of LLM agents demonstrate their
significant commercial value; however, they also expose security and privacy
vulnerabilities. At the current stage, comprehensive research on the security
and privacy of LLM agents is highly needed. This survey aims to provide a
comprehensive overview of the newly emerged privacy and security issues faced
by LLM agents. We begin by introducing the fundamental knowledge of LLM agents,
followed by a categorization and analysis of the threats. We then discuss the
impacts of these threats on humans, environment, and other agents.
Subsequently, we review existing defensive strategies, and finally explore
future trends. Additionally, the survey incorporates diverse case studies to
facilitate a more accessible understanding. By highlighting these critical
security and privacy issues, the survey seeks to stimulate future research
towards enhancing the security and privacy of LLM agents, thereby increasing
their reliability and trustworthiness in future applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.19147v1">Reexamination of the realtime protection for user privacy in practical
  quantum private query</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-07-27T02:19:35Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chun-Yan Wei, Xiao-Qiu Cai, Tian-Yin Wang</p>
    <p><b>Summary:</b> Quantum private query (QPQ) is the quantum version for symmetrically private
retrieval. However, the user privacy in QPQ is generally guarded in the
non-realtime and cheat sensitive way. That is, the dishonest database holder's
cheating to elicit user privacy can only be discovered after the protocol is
finished (when the user finds some errors in the retrieved database item). Such
delayed detection may cause very unpleasant results for the user in real-life
applications. Current efforts to protect user privacy in realtime in existing
QPQ protocols mainly use two techniques, i.e., adding an honesty checking on
the database or allowing the user to reorder the qubits. We reexamine these two
kinds of QPQ protocols and find neither of them can work well. We give concrete
cheating strategies for both participants and show that honesty checking of
inner participant should be dealt more carefully in for example the choosing of
checking qubits. We hope such discussion can supply new concerns when detection
of dishonest participant is considered in quantum multi-party secure
computations.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.19119v1">Accuracy-Privacy Trade-off in the Mitigation of Membership Inference
  Attack in Federated Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-07-26T22:44:41Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sayyed Farid Ahamed, Soumya Banerjee, Sandip Roy, Devin Quinn, Marc Vucovich, Kevin Choi, Abdul Rahman, Alison Hu, Edward Bowen, Sachin Shetty</p>
    <p><b>Summary:</b> Over the last few years, federated learning (FL) has emerged as a prominent
method in machine learning, emphasizing privacy preservation by allowing
multiple clients to collaboratively build a model while keeping their training
data private. Despite this focus on privacy, FL models are susceptible to
various attacks, including membership inference attacks (MIAs), posing a
serious threat to data confidentiality. In a recent study, Rezaei \textit{et
al.} revealed the existence of an accuracy-privacy trade-off in deep ensembles
and proposed a few fusion strategies to overcome it. In this paper, we aim to
explore the relationship between deep ensembles and FL. Specifically, we
investigate whether confidence-based metrics derived from deep ensembles apply
to FL and whether there is a trade-off between accuracy and privacy in FL with
respect to MIA. Empirical investigations illustrate a lack of a non-monotonic
correlation between the number of clients and the accuracy-privacy trade-off.
By experimenting with different numbers of federated clients, datasets, and
confidence-metric-based fusion strategies, we identify and analytically justify
the clear existence of the accuracy-privacy trade-off.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.18789v1">Granularity is crucial when applying differential privacy to text: An
  investigation for neural machine translation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2024-07-26T14:52:37Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Doan Nam Long Vu, Timour Igamberdiev, Ivan Habernal</p>
    <p><b>Summary:</b> Applying differential privacy (DP) by means of the DP-SGD algorithm to
protect individual data points during training is becoming increasingly popular
in NLP. However, the choice of granularity at which DP is applied is often
neglected. For example, neural machine translation (NMT) typically operates on
the sentence-level granularity. From the perspective of DP, this setup assumes
that each sentence belongs to a single person and any two sentences in the
training dataset are independent. This assumption is however violated in many
real-world NMT datasets, e.g. those including dialogues. For proper application
of DP we thus must shift from sentences to entire documents. In this paper, we
investigate NMT at both the sentence and document levels, analyzing the
privacy/utility trade-off for both scenarios, and evaluating the risks of not
using the appropriate privacy granularity in terms of leaking personally
identifiable information (PII). Our findings indicate that the document-level
NMT system is more resistant to membership inference attacks, emphasizing the
significance of using the appropriate granularity when working with DP.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.18564v1">Unveiling Privacy Vulnerabilities: Investigating the Role of Structure
  in Graph Data</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Social and Information Networks-662E9B">
  <p><b>Published on:</b> 2024-07-26T07:40:54Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Hanyang Yuan, Jiarong Xu, Cong Wang, Ziqi Yang, Chunping Wang, Keting Yin, Yang Yang</p>
    <p><b>Summary:</b> The public sharing of user information opens the door for adversaries to
infer private data, leading to privacy breaches and facilitating malicious
activities. While numerous studies have concentrated on privacy leakage via
public user attributes, the threats associated with the exposure of user
relationships, particularly through network structure, are often neglected.
This study aims to fill this critical gap by advancing the understanding and
protection against privacy risks emanating from network structure, moving
beyond direct connections with neighbors to include the broader implications of
indirect network structural patterns. To achieve this, we first investigate the
problem of Graph Privacy Leakage via Structure (GPS), and introduce a novel
measure, the Generalized Homophily Ratio, to quantify the various mechanisms
contributing to privacy breach risks in GPS. Based on this insight, we develop
a novel graph private attribute inference attack, which acts as a pivotal tool
for evaluating the potential for privacy leakage through network structures
under worst-case scenarios. To protect users' private data from such
vulnerabilities, we propose a graph data publishing method incorporating a
learnable graph sampling technique, effectively transforming the original graph
into a privacy-preserving version. Extensive experiments demonstrate that our
attack model poses a significant threat to user privacy, and our graph data
publishing method successfully achieves the optimal privacy-utility trade-off
compared to baselines.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.18503v1">Homomorphic Encryption-Enabled Federated Learning for Privacy-Preserving
  Intrusion Detection in Resource-Constrained IoV Networks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-07-26T04:19:37Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Bui Duc Manh, Chi-Hieu Nguyen, Dinh Thai Hoang, Diep N. Nguyen</p>
    <p><b>Summary:</b> This paper aims to propose a novel framework to address the data privacy
issue for Federated Learning (FL)-based Intrusion Detection Systems (IDSs) in
Internet-of-Vehicles(IoVs) with limited computational resources. In particular,
in conventional FL systems, it is usually assumed that the computing nodes have
sufficient computational resources to process the training tasks. However, in
practical IoV systems, vehicles usually have limited computational resources to
process intensive training tasks, compromising the effectiveness of deploying
FL in IDSs. While offloading data from vehicles to the cloud can mitigate this
issue, it introduces significant privacy concerns for vehicle users (VUs). To
resolve this issue, we first propose a highly-effective framework using
homomorphic encryption to secure data that requires offloading to a centralized
server for processing. Furthermore, we develop an effective training algorithm
tailored to handle the challenges of FL-based systems with encrypted data. This
algorithm allows the centralized server to directly compute on quantum-secure
encrypted ciphertexts without needing decryption. This approach not only
safeguards data privacy during the offloading process from VUs to the
centralized server but also enhances the efficiency of utilizing FL for IDSs in
IoV systems. Our simulation results show that our proposed approach can achieve
a performance that is as close to that of the solution without encryption, with
a gap of less than 0.8%.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.18433v1">Investigating the Privacy Risk of Using Robot Vacuum Cleaners in Smart
  Environments</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-07-26T00:00:53Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Benjamin Ulsmaag, Jia-Chun Lin, Ming-Chang Lee</p>
    <p><b>Summary:</b> Robot vacuum cleaners have become increasingly popular and are widely used in
various smart environments. To improve user convenience, manufacturers also
introduced smartphone applications that enable users to customize cleaning
settings or access information about their robot vacuum cleaners. While this
integration enhances the interaction between users and their robot vacuum
cleaners, it results in potential privacy concerns because users' personal
information may be exposed. To address these concerns, end-to-end encryption is
implemented between the application, cloud service, and robot vacuum cleaners
to secure the exchanged information. Nevertheless, network header metadata
remains unencrypted and it is still vulnerable to network eavesdropping. In
this paper, we investigate the potential risk of private information exposure
through such metadata. A popular robot vacuum cleaner was deployed in a real
smart environment where passive network eavesdropping was conducted during
several selected cleaning events. Our extensive analysis, based on Association
Rule Learning, demonstrates that it is feasible to identify certain events
using only the captured Internet traffic metadata, thereby potentially exposing
private user information and raising privacy concerns.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.18353v1">Privacy-Preserving Model-Distributed Inference at the Edge</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-07-25T19:39:03Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Fatemeh Jafarian Dehkordi, Yasaman Keshtkarjahromi, Hulya Seferoglu</p>
    <p><b>Summary:</b> This paper focuses on designing a privacy-preserving Machine Learning (ML)
inference protocol for a hierarchical setup, where clients own/generate data,
model owners (cloud servers) have a pre-trained ML model, and edge servers
perform ML inference on clients' data using the cloud server's ML model. Our
goal is to speed up ML inference while providing privacy to both data and the
ML model. Our approach (i) uses model-distributed inference (model
parallelization) at the edge servers and (ii) reduces the amount of
communication to/from the cloud server. Our privacy-preserving hierarchical
model-distributed inference, privateMDI design uses additive secret sharing and
linearly homomorphic encryption to handle linear calculations in the ML
inference, and garbled circuit and a novel three-party oblivious transfer are
used to handle non-linear functions. privateMDI consists of offline and online
phases. We designed these phases in a way that most of the data exchange is
done in the offline phase while the communication overhead of the online phase
is reduced. In particular, there is no communication to/from the cloud server
in the online phase, and the amount of communication between the client and
edge servers is minimized. The experimental results demonstrate that privateMDI
significantly reduces the ML inference time as compared to the baselines.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.18157v1">Enhanced Privacy Bound for Shuffle Model with Personalized Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB">
  <p><b>Published on:</b> 2024-07-25T16:11:56Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yixuan Liu, Yuhan Liu, Li Xiong, Yujie Gu, Hong Chen</p>
    <p><b>Summary:</b> The shuffle model of Differential Privacy (DP) is an enhanced privacy
protocol which introduces an intermediate trusted server between local users
and a central data curator. It significantly amplifies the central DP guarantee
by anonymizing and shuffling the local randomized data. Yet, deriving a tight
privacy bound is challenging due to its complicated randomization protocol.
While most existing work are focused on unified local privacy settings, this
work focuses on deriving the central privacy bound for a more practical setting
where personalized local privacy is required by each user. To bound the privacy
after shuffling, we first need to capture the probability of each user
generating clones of the neighboring data points. Second, we need to quantify
the indistinguishability between two distributions of the number of clones on
neighboring datasets. Existing works either inaccurately capture the
probability, or underestimate the indistinguishability between neighboring
datasets. Motivated by this, we develop a more precise analysis, which yields a
general and tighter bound for arbitrary DP mechanisms. Firstly, we derive the
clone-generating probability by hypothesis testing %from a randomizer-specific
perspective, which leads to a more accurate characterization of the
probability. Secondly, we analyze the indistinguishability in the context of
$f$-DP, where the convexity of the distributions is leveraged to achieve a
tighter privacy bound. Theoretical and numerical results demonstrate that our
bound remarkably outperforms the existing results in the literature.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.18096v1">Privacy Threats and Countermeasures in Federated Learning for Internet
  of Things: A Systematic Review</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-07-25T15:01:56Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Adel ElZemity, Budi Arief</p>
    <p><b>Summary:</b> Federated Learning (FL) in the Internet of Things (IoT) environments can
enhance machine learning by utilising decentralised data, but at the same time,
it might introduce significant privacy and security concerns due to the
constrained nature of IoT devices. This represents a research challenge that we
aim to address in this paper. We systematically analysed recent literature to
identify privacy threats in FL within IoT environments, and evaluate the
defensive measures that can be employed to mitigate these threats. Using a
Systematic Literature Review (SLR) approach, we searched five publication
databases (Scopus, IEEE Xplore, Wiley, ACM, and Science Direct), collating
relevant papers published between 2017 and April 2024, a period which spans
from the introduction of FL until now. Guided by the PRISMA protocol, we
selected 49 papers to focus our systematic review on. We analysed these papers,
paying special attention to the privacy threats and defensive measures --
specifically within the context of IoT -- using inclusion and exclusion
criteria tailored to highlight recent advances and critical insights. We
identified various privacy threats, including inference attacks, poisoning
attacks, and eavesdropping, along with defensive measures such as Differential
Privacy and Secure Multi-Party Computation. These defences were evaluated for
their effectiveness in protecting privacy without compromising the functional
integrity of FL in IoT settings. Our review underscores the necessity for
robust and efficient privacy-preserving strategies tailored for IoT
environments. Notably, there is a need for strategies against replay, evasion,
and model stealing attacks. Exploring lightweight defensive measures and
emerging technologies such as blockchain may help improve the privacy of FL in
IoT, leading to the creation of FL models that can operate under variable
network conditions.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.05218v1">Comment on "An Efficient Privacy-Preserving Ranked Multi-Keyword
  Retrieval for Multiple Data Owners in Outsourced Cloud"</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-07-25T05:01:07Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Uma Sankararao Varri</p>
    <p><b>Summary:</b> Protecting the privacy of keywords in the field of search over outsourced
cloud data is a challenging task. In IEEE Transactions on Services Computing
(Vol. 17 No. 2, March/April 2024), Li et al. proposed PRMKR: efficient
privacy-preserving ranked multi-keyword retrieval scheme, which was claimed to
resist keyword guessing attack. However, we show that the scheme fails to
resist keyword guessing attack, index privacy, and trapdoor privacy. Further,
we propose a solution to address the above said issues by correcting the errors
in the important equations of the scheme.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.17663v1">Explaining the Model, Protecting Your Data: Revealing and Mitigating the
  Data Privacy Risks of Post-Hoc Model Explanations via Membership Inference</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-07-24T22:16:37Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Catherine Huang, Martin Pawelczyk, Himabindu Lakkaraju</p>
    <p><b>Summary:</b> Predictive machine learning models are becoming increasingly deployed in
high-stakes contexts involving sensitive personal data; in these contexts,
there is a trade-off between model explainability and data privacy. In this
work, we push the boundaries of this trade-off: with a focus on foundation
models for image classification fine-tuning, we reveal unforeseen privacy risks
of post-hoc model explanations and subsequently offer mitigation strategies for
such risks. First, we construct VAR-LRT and L1/L2-LRT, two new membership
inference attacks based on feature attribution explanations that are
significantly more successful than existing explanation-leveraging attacks,
particularly in the low false-positive rate regime that allows an adversary to
identify specific training set members with confidence. Second, we find
empirically that optimized differentially private fine-tuning substantially
diminishes the success of the aforementioned attacks, while maintaining high
model accuracy. We carry out a systematic empirical investigation of our 2 new
attacks with 5 vision transformer architectures, 5 benchmark datasets, 4
state-of-the-art post-hoc explanation methods, and 4 privacy strength settings.</p>
  </details>
</div>

