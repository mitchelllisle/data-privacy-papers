
<h2>2025-10</h2>

<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2510.05959v1">Distributed Platoon Control Under Quantization: Stability Analysis and
  Privacy Preservation</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36">
  <p><b>Published on:</b> 2025-10-07T14:16:59Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kaixiang Zhang, Zhaojian Li, Wei Lin</p>
    <p><b>Summary:</b> Distributed control of connected and automated vehicles has attracted
considerable interest for its potential to improve traffic efficiency and
safety. However, such control schemes require sharing privacy-sensitive vehicle
data, which introduces risks of information leakage and potential malicious
activities. This paper investigates the stability and privacy-preserving
properties of distributed platoon control under two types of quantizers:
deterministic and probabilistic. For deterministic quantization, we show that
the resulting control strategy ensures the system errors remain uniformly
ultimately bounded. Moreover, in the absence of auxiliary information, an
eavesdropper cannot uniquely infer sensitive vehicle states. In contrast, the
use of probabilistic quantization enables asymptotic convergence of the vehicle
platoon in expectation with bounded variance. Importantly, probabilistic
quantizers can satisfy differential privacy guarantees, thereby preserving
privacy even when the eavesdropper possesses arbitrary auxiliary information.
We further analyze the trade-off between control performance and privacy by
formulating an optimization problem that characterizes the impact of the
quantization step on both metrics. Numerical simulations are provided to
illustrate the performance differences between the two quantization strategies.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2510.05860v1">Automated Boilerplate: Prevalence and Quality of Contract Generators in
  the Context of Swiss Privacy Policies</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-10-07T12:30:01Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Luka Nenadic, David Rodriguez</p>
    <p><b>Summary:</b> It has become increasingly challenging for firms to comply with a plethora of
novel digital regulations. This is especially true for smaller businesses that
often lack both the resources and know-how to draft complex legal documents.
Instead of seeking costly legal advice from attorneys, firms may turn to
cheaper alternative legal service providers such as automated contract
generators. While these services have a long-standing presence, there is little
empirical evidence on their prevalence and output quality.
  We address this gap in the context of a 2023 Swiss privacy law revision. To
enable a systematic evaluation, we create and annotate a multilingual benchmark
dataset that captures key compliance obligations under Swiss and EU privacy
law. Using this dataset, we validate a novel GPT-5-based method for large-scale
compliance assessment of privacy policies, allowing us to measure the impact of
the revision. We observe compliance increases indicating an effect of the
revision. Generators, explicitly referenced by 18% of local websites, are
associated with substantially higher levels of compliance, with increases of up
to 15 percentage points compared to privacy policies without generator use.
These findings contribute to three debates: the potential of LLMs for
cross-lingual legal analysis, the Brussels Effect of EU regulations, and,
crucially, the role of automated tools in improving compliance and contractual
quality.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2510.05807v1">Privacy-Preserving On-chain Permissioning for KYC-Compliant
  Decentralized Applications</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-10-07T11:24:51Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Fabian Piper, Karl Wolf, Jonathan Heiss</p>
    <p><b>Summary:</b> Decentralized applications (dApps) in Decentralized Finance (DeFi) face a
fundamental tension between regulatory compliance requirements like Know Your
Customer (KYC) and maintaining decentralization and privacy. Existing
permissioned DeFi solutions often fail to adequately protect private attributes
of dApp users and introduce implicit trust assumptions, undermining the
blockchain's decentralization. Addressing these limitations, this paper
presents a novel synthesis of Self-Sovereign Identity (SSI), Zero-Knowledge
Proofs (ZKPs), and Attribute-Based Access Control to enable privacy-preserving
on-chain permissioning based on decentralized policy decisions. We provide a
comprehensive framework for permissioned dApps that aligns decentralized trust,
privacy, and transparency, harmonizing blockchain principles with regulatory
compliance. Our framework supports multiple proof types (equality, range,
membership, and time-dependent) with efficient proof generation through a
commit-and-prove scheme that moves credential authenticity verification outside
the ZKP circuit. Experimental evaluation of our KYC-compliant DeFi
implementation shows considerable performance improvement for different proof
types compared to baseline approaches. We advance the state-of-the-art through
a holistic approach, flexible proof mechanisms addressing diverse real-world
requirements, and optimized proof generation enabling practical deployment.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2510.05803v1">The Five Safes as a Privacy Context</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2025-10-07T11:19:22Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> James Bailie, Ruobin Gong</p>
    <p><b>Summary:</b> The Five Safes is a framework used by national statistical offices (NSO) for
assessing and managing the disclosure risk of data sharing. This paper makes
two points: Firstly, the Five Safes can be understood as a specialization of a
broader concept $\unicode{x2013}$ contextual integrity $\unicode{x2013}$ to the
situation of statistical dissemination by an NSO. We demonstrate this by
mapping the five parameters of contextual integrity onto the five dimensions of
the Five Safes. Secondly, the Five Safes contextualizes narrow, technical
notions of privacy within a holistic risk assessment. We demonstrate this with
the example of differential privacy (DP). This contextualization allows NSOs to
place DP within their Five Safes toolkit while also guiding the design of DP
implementations within the broader privacy context, as delineated by both their
regulation and the relevant social norms.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2510.05535v1">Permutation-Invariant Representation Learning for Robust and
  Privacy-Preserving Feature Selection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-10-07T02:53:32Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Rui Liu, Tao Zhe, Yanjie Fu, Feng Xia, Ted Senator, Dongjie Wang</p>
    <p><b>Summary:</b> Feature selection eliminates redundancy among features to improve downstream
task performance while reducing computational overhead. Existing methods often
struggle to capture intricate feature interactions and adapt across diverse
application scenarios. Recent advances employ generative intelligence to
alleviate these drawbacks. However, these methods remain constrained by
permutation sensitivity in embedding and reliance on convexity assumptions in
gradient-based search. To address these limitations, our initial work
introduces a novel framework that integrates permutation-invariant embedding
with policy-guided search. Although effective, it still left opportunities to
adapt to realistic distributed scenarios. In practice, data across local
clients is highly imbalanced, heterogeneous and constrained by strict privacy
regulations, limiting direct sharing. These challenges highlight the need for a
framework that can integrate feature selection knowledge across clients without
exposing sensitive information. In this extended journal version, we advance
the framework from two perspectives: 1) developing a privacy-preserving
knowledge fusion strategy to derive a unified representation space without
sharing sensitive raw data. 2) incorporating a sample-aware weighting strategy
to address distributional imbalance among heterogeneous local clients.
Extensive experiments validate the effectiveness, robustness, and efficiency of
our framework. The results further demonstrate its strong generalization
ability in federated learning scenarios. The code and data are publicly
available: https://anonymous.4open.science/r/FedCAPS-08BF.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2510.05288v1">DP-Adam-AC: Privacy-preserving Fine-Tuning of Localizable Language
  Models Using Adam Optimization with Adaptive Clipping</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-10-06T18:56:15Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ruoxing Yang</p>
    <p><b>Summary:</b> Large language models (LLMs) such as ChatGPT have evolved into powerful and
ubiquitous tools. Fine-tuning on small datasets allows LLMs to acquire
specialized skills for specific tasks efficiently. Although LLMs provide great
utility in both general and task-specific use cases, they are limited by two
security-related concerns. First, traditional LLM hardware requirements make
them infeasible to run locally on consumer-grade devices. A remote network
connection with the LLM provider's server is usually required, making the
system vulnerable to network attacks. Second, fine-tuning an LLM for a
sensitive task may involve sensitive data. Non-private fine-tuning algorithms
produce models vulnerable to training data reproduction attacks. Our work
addresses these security concerns by enhancing differentially private
optimization algorithms and applying them to fine-tune localizable language
models. We introduce adaptable gradient clipping along with other engineering
enhancements to the standard DP-Adam optimizer to create DP-Adam-AC. We use our
optimizer to fine-tune examples of two localizable LLM designs, small language
model (Qwen2.5-0.5B) and 1.58 bit quantization (Bitnet-b1.58-2B). We
demonstrate promising improvements in loss through experimentation with two
synthetic datasets.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2510.05068v1">Multi-Agent Distributed Optimization With Feasible Set Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Networking and Internet Architecture-04E762">  
  <p><b>Published on:</b> 2025-10-06T17:45:57Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shreya Meel, Sennur Ulukus</p>
    <p><b>Summary:</b> We consider the problem of decentralized constrained optimization with
multiple agents $E_1,\ldots,E_N$ who jointly wish to learn the optimal solution
set while keeping their feasible sets $\mathcal{P}_1,\ldots,\mathcal{P}_N$
private from each other. We assume that the objective function $f$ is known to
all agents and each feasible set is a collection of points from a universal
alphabet $\mathcal{P}_{alph}$. A designated agent (leader) starts the
communication with the remaining (non-leader) agents, and is the first to
retrieve the solution set. The leader searches for the solution by sending
queries to and receiving answers from the non-leaders, such that the
information on the individual feasible sets revealed to the leader should be no
more than nominal, i.e., what is revealed from learning the solution set alone.
We develop achievable schemes for obtaining the solution set at nominal
information leakage, and characterize their communication costs under two
communication setups between agents. In this work, we focus on two kinds of
network setups: i) ring, where each agent communicates with two adjacent
agents, and ii) star, where only the leader communicates with the remaining
agents. We show that, if the leader first learns the joint feasible set through
an existing private set intersection (PSI) protocol and then deduces the
solution set, the information leaked to the leader is greater than nominal.
Moreover, we draw connection of our schemes to threshold PSI (ThPSI), which is
a PSI-variant where the intersection is revealed only when its cardinality is
larger than a threshold value. Finally, for various realizations of $f$ mapped
uniformly at random to a fixed range of values, our schemes are more
communication-efficient with a high probability compared to retrieving the
entire feasible set through PSI.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2510.04527v1">Quantum capacity amplification via privacy</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36">   <img alt="Category Badge" src="https://img.shields.io/badge/Mathematical Physics-F9C80E">
  <p><b>Published on:</b> 2025-10-06T06:35:19Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Peixue Wu, Yunkai Wang</p>
    <p><b>Summary:</b> We investigate superadditivity of quantum capacity through private channels
whose Choi-Jamiolkowski operators are private states. This perspective links
the security structure of private states to quantum capacity and clarifies the
role of the shield system: information encoded in the shield system that would
otherwise leak to the environment can be recycled when paired with an assisting
channel, thereby boosting capacity. Our main contributions are threefold:
Firstly, we develop a general framework that provides a sufficient condition
for capacity amplification, which is formulated in terms of the assisting
channel's Holevo information. As examples, we give explicit, dimension and
parameter dependent amplification thresholds for erasure and depolarizing
channels. Secondly, assuming the Spin alignment conjecture, we derive a
single-letter expression for the quantum capacity of a family of private
channels that are neither degradable, anti-degradable, nor PPT; as an
application, we construct channels with vanishing quantum capacity yet
unbounded private capacity. Thirdly, we further analyze approximate private
channels: we give an alternative proof of superactivation that extends its
validity to a broader parameter regime, and, by combining amplification bounds
with continuity estimates, we establish a metric separation showing that
channels exhibiting capacity amplification have nonzero diamond distance from
the set of anti-degradable channels, indicating that existing approximate
(anti-)degradability bounds are not tight. We also revisit the computability of
the regularized quantum capacity and modestly suggest that this fundamental
question still remains open.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2510.04465v1">Autonomy Matters: A Study on Personalization-Privacy Dilemma in LLM
  Agents</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-10-06T03:38:54Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhiping Zhang, Yi Evie Zhang, Freda Shi, Tianshi Li</p>
    <p><b>Summary:</b> Large Language Model (LLM) agents require personal information for
personalization in order to better act on users' behalf in daily tasks, but
this raises privacy concerns and a personalization-privacy dilemma. Agent's
autonomy introduces both risks and opportunities, yet its effects remain
unclear. To better understand this, we conducted a 3$\times$3 between-subjects
experiment ($N=450$) to study how agent's autonomy level and personalization
influence users' privacy concerns, trust and willingness to use, as well as the
underlying psychological processes. We find that personalization without
considering users' privacy preferences increases privacy concerns and decreases
trust and willingness to use. Autonomy moderates these effects: Intermediate
autonomy flattens the impact of personalization compared to No- and Full
autonomy conditions. Our results suggest that rather than aiming for perfect
model alignment in output generation, balancing autonomy of agent's action and
user control offers a promising path to mitigate the personalization-privacy
dilemma.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2510.04261v1">VortexPIA: Indirect Prompt Injection Attack against LLMs for Efficient
  Extraction of User Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-10-05T15:58:55Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yu Cui, Sicheng Pan, Yifei Liu, Haibin Zhang, Cong Zuo</p>
    <p><b>Summary:</b> Large language models (LLMs) have been widely deployed in Conversational AIs
(CAIs), while exposing privacy and security threats. Recent research shows that
LLM-based CAIs can be manipulated to extract private information from human
users, posing serious security threats. However, the methods proposed in that
study rely on a white-box setting that adversaries can directly modify the
system prompt. This condition is unlikely to hold in real-world deployments.
The limitation raises a critical question: can unprivileged attackers still
induce such privacy risks in practical LLM-integrated applications? To address
this question, we propose \textsc{VortexPIA}, a novel indirect prompt injection
attack that induces privacy extraction in LLM-integrated applications under
black-box settings. By injecting token-efficient data containing false
memories, \textsc{VortexPIA} misleads LLMs to actively request private
information in batches. Unlike prior methods, \textsc{VortexPIA} allows
attackers to flexibly define multiple categories of sensitive data. We evaluate
\textsc{VortexPIA} on six LLMs, covering both traditional and reasoning LLMs,
across four benchmark datasets. The results show that \textsc{VortexPIA}
significantly outperforms baselines and achieves state-of-the-art (SOTA)
performance. It also demonstrates efficient privacy requests, reduced token
consumption, and enhanced robustness against defense mechanisms. We further
validate \textsc{VortexPIA} on multiple realistic open-source LLM-integrated
applications, demonstrating its practical effectiveness.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2510.04153v1">ObCLIP: Oblivious CLoud-Device Hybrid Image Generation with Privacy
  Preservation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-10-05T11:09:10Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Haoqi Wu, Wei Dai, Ming Xu, Li Wang, Qiang Yan</p>
    <p><b>Summary:</b> Diffusion Models have gained significant popularity due to their remarkable
capabilities in image generation, albeit at the cost of intensive computation
requirement. Meanwhile, despite their widespread deployment in inference
services such as Midjourney, concerns about the potential leakage of sensitive
information in uploaded user prompts have arisen. Existing solutions either
lack rigorous privacy guarantees or fail to strike an effective balance between
utility and efficiency. To bridge this gap, we propose ObCLIP, a plug-and-play
safeguard that enables oblivious cloud-device hybrid generation. By oblivious,
each input prompt is transformed into a set of semantically similar candidate
prompts that differ only in sensitive attributes (e.g., gender, ethnicity). The
cloud server processes all candidate prompts without knowing which one is the
real one, thus preventing any prompt leakage. To mitigate server cost, only a
small portion of denoising steps is performed upon the large cloud model. The
intermediate latents are then sent back to the client, which selects the
targeted latent and completes the remaining denoising using a small device
model. Additionally, we analyze and incorporate several cache-based
accelerations that leverage temporal and batch redundancy, effectively reducing
computation cost with minimal utility degradation. Extensive experiments across
multiple datasets demonstrate that ObCLIP provides rigorous privacy and
comparable utility to cloud models with slightly increased server cost.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2510.05172v1">Learning More with Less: A Generalizable, Self-Supervised Framework for
  Privacy-Preserving Capacity Estimation with EV Charging Data</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-10-05T08:58:35Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Anushiya Arunan, Yan Qin, Xiaoli Li, U-Xuan Tan, H. Vincent Poor, Chau Yuen</p>
    <p><b>Summary:</b> Accurate battery capacity estimation is key to alleviating consumer concerns
about battery performance and reliability of electric vehicles (EVs). However,
practical data limitations imposed by stringent privacy regulations and labeled
data shortages hamper the development of generalizable capacity estimation
models that remain robust to real-world data distribution shifts. While
self-supervised learning can leverage unlabeled data, existing techniques are
not particularly designed to learn effectively from challenging field data --
let alone from privacy-friendly data, which are often less feature-rich and
noisier. In this work, we propose a first-of-its-kind capacity estimation model
based on self-supervised pre-training, developed on a large-scale dataset of
privacy-friendly charging data snippets from real-world EV operations. Our
pre-training framework, snippet similarity-weighted masked input
reconstruction, is designed to learn rich, generalizable representations even
from less feature-rich and fragmented privacy-friendly data. Our key innovation
lies in harnessing contrastive learning to first capture high-level
similarities among fragmented snippets that otherwise lack meaningful context.
With our snippet-wise contrastive learning and subsequent similarity-weighted
masked reconstruction, we are able to learn rich representations of both
granular charging patterns within individual snippets and high-level
associative relationships across different snippets. Bolstered by this rich
representation learning, our model consistently outperforms state-of-the-art
baselines, achieving 31.9% lower test error than the best-performing benchmark,
even under challenging domain-shifted settings affected by both manufacturer
and age-induced distribution shifts.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2510.04027v1">Multi-Class Support Vector Machine with Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-10-05T04:25:16Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jinseong Park, Yujin Choi, Jaewook Lee</p>
    <p><b>Summary:</b> With the increasing need to safeguard data privacy in machine learning
models, differential privacy (DP) is one of the major frameworks to build
privacy-preserving models. Support Vector Machines (SVMs) are widely used
traditional machine learning models due to their robust margin guarantees and
strong empirical performance in binary classification. However, applying DP to
multi-class SVMs is inadequate, as the standard one-versus-rest (OvR) and
one-versus-one (OvO) approaches repeatedly query each data sample when building
multiple binary classifiers, thus consuming the privacy budget proportionally
to the number of classes. To overcome this limitation, we explore all-in-one
SVM approaches for DP, which access each data sample only once to construct
multi-class SVM boundaries with margin maximization properties. We propose a
novel differentially Private Multi-class SVM (PMSVM) with weight and gradient
perturbation methods, providing rigorous sensitivity and convergence analyses
to ensure DP in all-in-one SVMs. Empirical results demonstrate that our
approach surpasses existing DP-SVM methods in multi-class scenarios.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2510.03996v1">FHEON: A Configurable Framework for Developing Privacy-Preserving Neural
  Networks Using Homomorphic Encryption</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-10-05T02:12:44Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Nges Brian Njungle, Eric Jahns, Michel A. Kinsy</p>
    <p><b>Summary:</b> The widespread adoption of Machine Learning as a Service raises critical
privacy and security concerns, particularly about data confidentiality and
trust in both cloud providers and the machine learning models. Homomorphic
Encryption (HE) has emerged as a promising solution to this problems, allowing
computations on encrypted data without decryption. Despite its potential,
existing approaches to integrate HE into neural networks are often limited to
specific architectures, leaving a wide gap in providing a framework for easy
development of HE-friendly privacy-preserving neural network models similar to
what we have in the broader field of machine learning. In this paper, we
present FHEON, a configurable framework for developing privacy-preserving
convolutional neural network (CNN) models for inference using HE. FHEON
introduces optimized and configurable implementations of privacy-preserving CNN
layers including convolutional layers, average pooling layers, ReLU activation
functions, and fully connected layers. These layers are configured using
parameters like input channels, output channels, kernel size, stride, and
padding to support arbitrary CNN architectures. We assess the performance of
FHEON using several CNN architectures, including LeNet-5, VGG-11, VGG- 16,
ResNet-20, and ResNet-34. FHEON maintains encrypted-domain accuracies within
+/- 1% of their plaintext counterparts for ResNet-20 and LeNet-5 models.
Notably, on a consumer-grade CPU, the models build on FHEON achieved 98.5%
accuracy with a latency of 13 seconds on MNIST using LeNet-5, and 92.2%
accuracy with a latency of 403 seconds on CIFAR-10 using ResNet-20.
Additionally, FHEON operates within a practical memory budget requiring not
more than 42.3 GB for VGG-16.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2510.03860v1">Privacy Enhancement in Over-the-Air Federated Learning via Adaptive
  Receive Scaling</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36">  
  <p><b>Published on:</b> 2025-10-04T16:15:19Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Faeze Moradi Kalarde, Ben Liang, Min Dong, Yahia A. Eldemerdash Ahmed, Ho Ting Cheng</p>
    <p><b>Summary:</b> In Federated Learning (FL) with over-the-air aggregation, the quality of the
signal received at the server critically depends on the receive scaling
factors. While a larger scaling factor can reduce the effective noise power and
improve training performance, it also compromises the privacy of devices by
reducing uncertainty. In this work, we aim to adaptively design the receive
scaling factors across training rounds to balance the trade-off between
training convergence and privacy in an FL system under dynamic channel
conditions. We formulate a stochastic optimization problem that minimizes the
overall R\'enyi differential privacy (RDP) leakage over the entire training
process, subject to a long-term constraint that ensures convergence of the
global loss function. Our problem depends on unknown future information, and we
observe that standard Lyapunov optimization is not applicable. Thus, we develop
a new online algorithm, termed AdaScale, based on a sequence of novel per-round
problems that can be solved efficiently. We further derive upper bounds on the
dynamic regret and constraint violation of AdaSacle, establishing that it
achieves diminishing dynamic regret in terms of time-averaged RDP leakage while
ensuring convergence of FL training to a stationary point. Numerical
experiments on canonical classification tasks show that our approach
effectively reduces RDP and DP leakages compared with state-of-the-art
benchmarks without compromising learning performance.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2510.03662v1">Operationalizing Data Minimization for Privacy-Preserving LLM Prompting</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-10-04T04:20:18Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jijie Zhou, Niloofar Mireshghallah, Tianshi Li</p>
    <p><b>Summary:</b> The rapid deployment of large language models (LLMs) in consumer applications
has led to frequent exchanges of personal information. To obtain useful
responses, users often share more than necessary, increasing privacy risks via
memorization, context-based personalization, or security breaches. We present a
framework to formally define and operationalize data minimization: for a given
user prompt and response model, quantifying the least privacy-revealing
disclosure that maintains utility, and we propose a priority-queue tree search
to locate this optimal point within a privacy-ordered transformation space. We
evaluated the framework on four datasets spanning open-ended conversations
(ShareGPT, WildChat) and knowledge-intensive tasks with single-ground-truth
answers (CaseHold, MedQA), quantifying achievable data minimization with nine
LLMs as the response model. Our results demonstrate that larger frontier LLMs
can tolerate stronger data minimization while maintaining task quality than
smaller open-source models (85.7% redaction for GPT-5 vs. 19.3% for
Qwen2.5-0.5B). By comparing with our search-derived benchmarks, we find that
LLMs struggle to predict optimal data minimization directly, showing a bias
toward abstraction that leads to oversharing. This suggests not just a privacy
gap, but a capability gap: models may lack awareness of what information they
actually need to solve a task.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2510.03559v1">PrivacyMotiv: Speculative Persona Journeys for Empathic and Motivating
  Privacy Reviews in UX Design</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-10-03T23:14:22Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zeya Chen, Jianing Wen, Ruth Schmidt, Yaxing Yao, Toby Jia-Jun Li, Tianshi Li</p>
    <p><b>Summary:</b> UX professionals routinely conduct design reviews, yet privacy concerns are
often overlooked -- not only due to limited tools, but more critically because
of low intrinsic motivation. Limited privacy knowledge, weak empathy for
unexpectedly affected users, and low confidence in identifying harms make it
difficult to address risks. We present PrivacyMotiv, an LLM-powered system that
supports privacy-oriented design diagnosis by generating speculative personas
with UX user journeys centered on individuals vulnerable to privacy risks.
Drawing on narrative strategies, the system constructs relatable and
attention-drawing scenarios that show how ordinary design choices may cause
unintended harms, expanding the scope of privacy reflection in UX. In a
within-subjects study with professional UX practitioners (N=16), we compared
participants' self-proposed methods with PrivacyMotiv across two privacy review
tasks. Results show significant improvements in empathy, intrinsic motivation,
and perceived usefulness. This work contributes a promising privacy review
approach which addresses the motivational barriers in privacy-aware UX.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2510.03513v1">A Lightweight Federated Learning Approach for Privacy-Preserving Botnet
  Detection in IoT</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2025-10-03T20:54:58Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Taha M. Mahmoud, Naima Kaabouch</p>
    <p><b>Summary:</b> The rapid growth of the Internet of Things (IoT) has expanded opportunities
for innovation but also increased exposure to botnet-driven cyberattacks.
Conventional detection methods often struggle with scalability, privacy, and
adaptability in resource-constrained IoT environments. To address these
challenges, we present a lightweight and privacy-preserving botnet detection
framework based on federated learning. This approach enables distributed
devices to collaboratively train models without exchanging raw data, thus
maintaining user privacy while preserving detection accuracy. A
communication-efficient aggregation strategy is introduced to reduce overhead,
ensuring suitability for constrained IoT networks. Experiments on benchmark IoT
botnet datasets demonstrate that the framework achieves high detection accuracy
while substantially reducing communication costs. These findings highlight
federated learning as a practical path toward scalable, secure, and
privacy-aware intrusion detection for IoT ecosystems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2510.03035v1">Protecting Persona Biometric Data: The Case of Facial Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2025-10-03T14:16:33Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Lambert Hogenhout, Rinzin Wangmo</p>
    <p><b>Summary:</b> The proliferation of digital technologies has led to unprecedented data
collection, with facial data emerging as a particularly sensitive commodity.
Companies are increasingly leveraging advanced facial recognition technologies,
often without the explicit consent or awareness of individuals, to build
sophisticated surveillance capabilities. This practice, fueled by weak and
fragmented laws in many jurisdictions, has created a regulatory vacuum that
allows for the commercialization of personal identity and poses significant
threats to individual privacy and autonomy. This article introduces the concept
of Facial Privacy. It analyzes the profound challenges posed by unregulated
facial recognition by conducting a comprehensive review of existing legal
frameworks. It examines and compares regulations such as the GDPR, Brazil's
LGPD, Canada's PIPEDA, and privacy acts in China, Singapore, South Korea, and
Japan, alongside sector-specific laws in the United States like the Illinois
Biometric Information Privacy Act (BIPA). The analysis highlights the societal
impacts of this technology, including the potential for discriminatory bias and
the long-lasting harm that can result from the theft of immutable biometric
data. Ultimately, the paper argues that existing legal loopholes and
ambiguities leave individuals vulnerable. It proposes a new policy framework
that shifts the paradigm from data as property to a model of inalienable
rights, ensuring that fundamental human rights are upheld against unchecked
technological expansion.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2510.02487v1">Interplay between Security, Privacy and Trust in 6G-enabled Intelligent
  Transportation Systems</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Networking and Internet Architecture-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-10-02T18:47:36Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ahmed Danladi Abdullahi, Erfan Bahrami, Tooska Dargahi, Mohammed Al-Khalidi, Mohammad Hammoudeh</p>
    <p><b>Summary:</b> The advancement of 6G technology has the potential to revolutionize the
transportation sector and significantly improve how we travel. 6G-enabled
Intelligent Transportation Systems (ITS) promise to offer high-speed,
low-latency communication and advanced data analytics capabilities, supporting
the development of safer, more efficient, and more sustainable transportation
solutions. However, various security and privacy challenges were identified in
the literature that must be addressed to enable the safe and secure deployment
of 6G-ITS and ensure people's trust in using these technologies. This paper
reviews the opportunities and challenges of 6G-ITS, particularly focusing on
trust, security, and privacy, with special attention to quantum technologies
that both enhance security through quantum key distribution and introduce new
vulnerabilities. It discusses the potential benefits of 6G technology in the
transportation sector, including improved communication, device
interoperability support, data analytic capabilities, and increased automation
for different components, such as transportation management and communication
systems. A taxonomy of different attack models in 6G-ITS is proposed, and a
comparison of the security threats in 5G-ITS and 6G-ITS is provided, along with
potential mitigating solutions. This research highlights the urgent need for a
comprehensive, multi-layered security framework spanning physical
infrastructure protection, network protocol security, data management
safeguards, application security measures, and trust management systems to
effectively mitigate emerging security and privacy risks and ensure the
integrity and resilience of future transportation ecosystems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2510.01793v1">Sensitivity, Specificity, and Consistency: A Tripartite Evaluation of
  Privacy Filters for Synthetic Data Generation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-10-02T08:32:20Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Adil Koeken, Alexander Ziller, Moritz Knolle, Daniel Rueckert</p>
    <p><b>Summary:</b> The generation of privacy-preserving synthetic datasets is a promising avenue
for overcoming data scarcity in medical AI research. Post-hoc privacy filtering
techniques, designed to remove samples containing personally identifiable
information, have recently been proposed as a solution. However, their
effectiveness remains largely unverified. This work presents a rigorous
evaluation of a filtering pipeline applied to chest X-ray synthesis. Contrary
to claims from the original publications, our results demonstrate that current
filters exhibit limited specificity and consistency, achieving high sensitivity
only for real images while failing to reliably detect near-duplicates generated
from training data. These results demonstrate a critical limitation of post-hoc
filtering: rather than effectively safeguarding patient privacy, these methods
may provide a false sense of security while leaving unacceptable levels of
patient information exposed. We conclude that substantial advances in filter
design are needed before these methods can be confidently deployed in sensitive
applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2510.01645v1">Position: Privacy Is Not Just Memorization!</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-10-02T04:02:06Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Niloofar Mireshghallah, Tianshi Li</p>
    <p><b>Summary:</b> The discourse on privacy risks in Large Language Models (LLMs) has
disproportionately focused on verbatim memorization of training data, while a
constellation of more immediate and scalable privacy threats remain
underexplored. This position paper argues that the privacy landscape of LLM
systems extends far beyond training data extraction, encompassing risks from
data collection practices, inference-time context leakage, autonomous agent
capabilities, and the democratization of surveillance through deep inference
attacks. We present a comprehensive taxonomy of privacy risks across the LLM
lifecycle -- from data collection through deployment -- and demonstrate through
case studies how current privacy frameworks fail to address these multifaceted
threats. Through a longitudinal analysis of 1,322 AI/ML privacy papers
published at leading conferences over the past decade (2016--2025), we reveal
that while memorization receives outsized attention in technical research, the
most pressing privacy harms lie elsewhere, where current technical approaches
offer little traction and viable paths forward remain unclear. We call for a
fundamental shift in how the research community approaches LLM privacy, moving
beyond the narrow focus of current technical solutions and embracing
interdisciplinary approaches that address the sociotechnical nature of these
emerging threats.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2510.01113v1">Privacy Preserved Federated Learning with Attention-Based Aggregation
  for Biometric Recognition</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-10-01T16:58:59Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kassahun Azezew, Minyechil Alehegn, Tsega Asresa, Bitew Mekuria, Tizazu Bayh, Ayenew Kassie, Amsalu Tesema, Animut Embiyale</p>
    <p><b>Summary:</b> Because biometric data is sensitive, centralized training poses a privacy
risk, even though biometric recognition is essential for contemporary
applications. Federated learning (FL), which permits decentralized training,
provides a privacy-preserving substitute. Conventional FL, however, has trouble
with interpretability and heterogeneous data (non-IID). In order to handle
non-IID biometric data, this framework adds an attention mechanism at the
central server that weights local model updates according to their
significance. Differential privacy and secure update protocols safeguard data
while preserving accuracy. The A3-FL framework is evaluated in this study using
FVC2004 fingerprint data, with each client's features extracted using a Siamese
Convolutional Neural Network (Siamese-CNN). By dynamically modifying client
contributions, the attention mechanism increases the accuracy of the global
model.The accuracy, convergence speed, and robustness of the A3-FL framework
are superior to those of standard FL (FedAvg) and static baselines, according
to experimental evaluations using fingerprint data (FVC2004). The accuracy of
the attention-based approach was 0.8413, while FedAvg, Local-only, and
Centralized approaches were 0.8164, 0.7664, and 0.7997, respectively. Accuracy
stayed high at 0.8330 even with differential privacy. A scalable and
privacy-sensitive biometric system for secure and effective recognition in
dispersed environments is presented in this work.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2510.00909v1">"We are not Future-ready": Understanding AI Privacy Risks and Existing
  Mitigation Strategies from the Perspective of AI Developers in Europe</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-10-01T13:51:33Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Alexandra Klymenko, Stephen Meisenbacher, Patrick Gage Kelley, Sai Teja Peddinti, Kurt Thomas, Florian Matthes</p>
    <p><b>Summary:</b> The proliferation of AI has sparked privacy concerns related to training
data, model interfaces, downstream applications, and more. We interviewed 25 AI
developers based in Europe to understand which privacy threats they believe
pose the greatest risk to users, developers, and businesses and what protective
strategies, if any, would help to mitigate them. We find that there is little
consensus among AI developers on the relative ranking of privacy risks. These
differences stem from salient reasoning patterns that often relate to human
rather than purely technical factors. Furthermore, while AI developers are
aware of proposed mitigation strategies for addressing these risks, they
reported minimal real-world adoption. Our findings highlight both gaps and
opportunities for empowering AI developers to better address privacy risks in
AI.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2510.00478v1">Vicinity-Guided Discriminative Latent Diffusion for Privacy-Preserving
  Domain Adaptation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-10-01T03:58:26Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jing Wang, Wonho Bae, Jiahong Chen, Wenxu Wang, Junhyug Noh</p>
    <p><b>Summary:</b> Recent work on latent diffusion models (LDMs) has focused almost exclusively
on generative tasks, leaving their potential for discriminative transfer
largely unexplored. We introduce Discriminative Vicinity Diffusion (DVD), a
novel LDM-based framework for a more practical variant of source-free domain
adaptation (SFDA): the source provider may share not only a pre-trained
classifier but also an auxiliary latent diffusion module, trained once on the
source data and never exposing raw source samples. DVD encodes each source
feature's label information into its latent vicinity by fitting a Gaussian
prior over its k-nearest neighbors and training the diffusion network to drift
noisy samples back to label-consistent representations. During adaptation, we
sample from each target feature's latent vicinity, apply the frozen diffusion
module to generate source-like cues, and use a simple InfoNCE loss to align the
target encoder to these cues, explicitly transferring decision boundaries
without source access. Across standard SFDA benchmarks, DVD outperforms
state-of-the-art methods. We further show that the same latent diffusion module
enhances the source classifier's accuracy on in-domain data and boosts
performance in supervised classification and domain generalization experiments.
DVD thus reinterprets LDMs as practical, privacy-preserving bridges for
explicit knowledge transfer, addressing a core challenge in source-free domain
adaptation that prior methods have yet to solve.</p>
  </details>
</div>



<h2>2025-09</h2>

<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2510.00350v1">Security and Privacy Analysis of Tile's Location Tracking Protocol</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-09-30T23:25:59Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Akshaya Kumar, Anna Raymaker, Michael Specter</p>
    <p><b>Summary:</b> We conduct the first comprehensive security analysis of Tile, the second most
popular crowd-sourced location-tracking service behind Apple's AirTags. We
identify several exploitable vulnerabilities and design flaws, disproving many
of the platform's claimed security and privacy guarantees: Tile's servers can
persistently learn the location of all users and tags, unprivileged adversaries
can track users through Bluetooth advertisements emitted by Tile's devices, and
Tile's anti-theft mode is easily subverted.
  Despite its wide deployment -- millions of users, devices, and purpose-built
hardware tags -- Tile provides no formal description of its protocol or threat
model. Worse, Tile intentionally weakens its antistalking features to support
an antitheft use-case and relies on a novel "accountability" mechanism to
punish those abusing the system to stalk victims.
  We examine Tile's accountability mechanism, a unique feature of independent
interest; no other provider attempts to guarantee accountability. While an
ideal accountability mechanism may disincentivize abuse in crowd-sourced
location tracking protocols, we show that Tile's implementation is subvertible
and introduces new exploitable vulnerabilities. We conclude with a discussion
on the need for new, formal definitions of accountability in this setting.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2510.00165v1">Privacy-Preserving Learning-Augmented Data Structures</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Retrieval-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B">
  <p><b>Published on:</b> 2025-09-30T18:37:39Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Prabhav Goyal, Vinesh Sridhar, Wilson Zheng</p>
    <p><b>Summary:</b> Learning-augmented data structures use predicted frequency estimates to
retrieve frequently occurring database elements faster than standard data
structures. Recent work has developed data structures that optimally exploit
these frequency estimates while maintaining robustness to adversarial
prediction errors. However, the privacy and security implications of this
setting remain largely unexplored.
  In the event of a security breach, data structures should reveal minimal
information beyond their current contents. This is even more crucial for
learning-augmented data structures, whose layout adapts to the data. A data
structure is history independent if its memory representation reveals no
information about past operations except what is inferred from its current
contents. In this work, we take the first step towards privacy and security
guarantees in this setting by proposing the first learning-augmented data
structure that is strongly history independent, robust, and supports dynamic
updates.
  To achieve this, we introduce two techniques: thresholding, which
automatically makes any learning-augmented data structure robust, and pairing,
a simple technique that provides strong history independence in the dynamic
setting. Our experimental results demonstrate a tradeoff between security and
efficiency but are still competitive with the state of the art.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2510.00164v1">Calyx: Privacy-Preserving Multi-Token Optimistic-Rollup Protocol</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-09-30T18:35:31Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Dominik Apel, Zeta Avarikioti, Matteo Maffei, Yuheng Wang</p>
    <p><b>Summary:</b> Rollup protocols have recently received significant attention as a promising
class of Layer 2 (L2) scalability solutions. By utilizing the Layer 1 (L1)
blockchain solely as a bulletin board for a summary of the executed
transactions and state changes, rollups enable secure off-chain execution while
avoiding the complexity of other L2 mechanisms. However, to ensure data
availability, current rollup protocols require the plaintext of executed
transactions to be published on-chain, resulting in inherent privacy
limitations.
  In this paper, we address this problem by introducing Calyx, the first
privacy-preserving multi-token optimistic-Rollup protocol. Calyx guarantees
full payment privacy for all L2 transactions, revealing no information about
the sender, recipient, transferred amount, or token type. The protocol further
supports atomic execution of multiple multi-token transactions and introduces a
transaction fee scheme to enable broader application scenarios while ensuring
the sustainable operation of the protocol. To enforce correctness, Calyx adopts
an efficient one-step fraud-proof mechanism. We analyze the security and
privacy guarantees of the protocol and provide an implementation and
evaluation. Our results show that executing a single transaction costs
approximately $0.06 (0.00002 ETH) and incurs only constant-size on-chain cost
in asymptotic terms.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.25906v1">Federated Learning with Enhanced Privacy via Model Splitting and Random
  Client Participation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-09-30T07:51:06Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yiwei Li, Shuai Wang, Zhuojun Tian, Xiuhua Wang, Shijian Su</p>
    <p><b>Summary:</b> Federated Learning (FL) often adopts differential privacy (DP) to protect
client data, but the added noise required for privacy guarantees can
substantially degrade model accuracy. To resolve this challenge, we propose
model-splitting privacy-amplified federated learning (MS-PAFL), a novel
framework that combines structural model splitting with statistical privacy
amplification. In this framework, each client's model is partitioned into a
private submodel, retained locally, and a public submodel, shared for global
aggregation. The calibrated Gaussian noise is injected only into the public
submodel, thereby confining its adverse impact while preserving the utility of
the local model. We further present a rigorous theoretical analysis that
characterizes the joint privacy amplification achieved through random client
participation and local data subsampling under this architecture. The analysis
provides tight bounds on both single-round and total privacy loss,
demonstrating that MS-PAFL significantly reduces the noise necessary to satisfy
a target privacy protection level. Extensive experiments validate our
theoretical findings, showing that MS-PAFL consistently attains a superior
privacy-utility trade-off and enables the training of highly accurate models
under strong privacy guarantees.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.25525v1">Defeating Cerberus: Concept-Guided Privacy-Leakage Mitigation in
  Multimodal Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-09-29T21:27:10Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Boyang Zhang, Istemi Ekin Akkus, Ruichuan Chen, Alice Dethise, Klaus Satzke, Ivica Rimac, Yang Zhang</p>
    <p><b>Summary:</b> Multimodal large language models (MLLMs) have demonstrated remarkable
capabilities in processing and reasoning over diverse modalities, but their
advanced abilities also raise significant privacy concerns, particularly
regarding Personally Identifiable Information (PII) leakage. While relevant
research has been conducted on single-modal language models to some extent, the
vulnerabilities in the multimodal setting have yet to be fully investigated. In
this work, we investigate these emerging risks with a focus on vision language
models (VLMs), a representative subclass of MLLMs that covers the two
modalities most relevant for PII leakage, vision and text. We introduce a
concept-guided mitigation approach that identifies and modifies the model's
internal states associated with PII-related content. Our method guides VLMs to
refuse PII-sensitive tasks effectively and efficiently, without requiring
re-training or fine-tuning. We also address the current lack of multimodal PII
datasets by constructing various ones that simulate real-world scenarios.
Experimental results demonstrate that the method can achieve an average refusal
rate of 93.3% for various PII-related tasks with minimal impact on unrelated
model performances. We further examine the mitigation's performance under
various conditions to show the adaptability of our proposed method.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.25469v1">Balancing Compliance and Privacy in Offline CBDC Transactions Using a
  Secure Element-based System</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-09-29T20:19:31Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Panagiotis Michalopoulos, Anthony Mack, Cameron Clark, Linus Chen, Johannes Sedlmeir, Andreas Veneris</p>
    <p><b>Summary:</b> Blockchain technology has spawned a vast ecosystem of digital currencies with
Central Bank Digital Currencies (CBDCs) -- digital forms of fiat currency --
being one of them. An important feature of digital currencies is facilitating
transactions without network connectivity, which can enhance the scalability of
cryptocurrencies and the privacy of CBDC users. However, in the case of CBDCs,
this characteristic also introduces new regulatory challenges, particularly
when it comes to applying established Anti-Money Laundering and Countering the
Financing of Terrorism (AML/CFT) frameworks. This paper introduces a prototype
for offline digital currency payments, equally applicable to cryptocurrencies
and CBDCs, that leverages Secure Elements and digital credentials to address
the tension of offline payment support with regulatory compliance. Performance
evaluation results suggest that the prototype can be flexibly adapted to
different regulatory environments, with a transaction latency comparable to
real-life commercial payment systems. Furthermore, we conceptualize how the
integration of Zero-Knowledge Proofs into our design could accommodate various
tiers of enhanced privacy protection.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.25072v1">Optimizing Privacy-Preserving Primitives to Support LLM-Scale
  Applications</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-09-29T17:16:51Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yaman Jandali, Ruisi Zhang, Nojan Sheybani, Farinaz Koushanfar</p>
    <p><b>Summary:</b> Privacy-preserving technologies have introduced a paradigm shift that allows
for realizable secure computing in real-world systems. The significant barrier
to the practical adoption of these primitives is the computational and
communication overhead that is incurred when applied at scale. In this paper,
we present an overview of our efforts to bridge the gap between this overhead
and practicality for privacy-preserving learning systems using multi-party
computation (MPC), zero-knowledge proofs (ZKPs), and fully homomorphic
encryption (FHE). Through meticulous hardware/software/algorithm co-design, we
show progress towards enabling LLM-scale applications in privacy-preserving
settings. We demonstrate the efficacy of our solutions in several contexts,
including DNN IP ownership, ethical LLM usage enforcement, and transformer
inference.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.24488v1">Sanitize Your Responses: Mitigating Privacy Leakage in Large Language
  Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-09-29T08:59:44Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Wenjie Fu, Huandong Wang, Junyao Gao, Guoan Wan, Tao Jiang</p>
    <p><b>Summary:</b> As Large Language Models (LLMs) achieve remarkable success across a wide
range of applications, such as chatbots and code copilots, concerns surrounding
the generation of harmful content have come increasingly into focus. Despite
significant advances in aligning LLMs with safety and ethical standards,
adversarial prompts can still be crafted to elicit undesirable responses.
Existing mitigation strategies are predominantly based on post-hoc filtering,
which introduces substantial latency or computational overhead, and is
incompatible with token-level streaming generation. In this work, we introduce
Self-Sanitize, a novel LLM-driven mitigation framework inspired by cognitive
psychology, which emulates human self-monitor and self-repair behaviors during
conversations. Self-Sanitize comprises a lightweight Self-Monitor module that
continuously inspects high-level intentions within the LLM at the token level
via representation engineering, and a Self-Repair module that performs in-place
correction of harmful content without initiating separate review dialogues.
This design allows for real-time streaming monitoring and seamless repair, with
negligible impact on latency and resource utilization. Given that
privacy-invasive content has often been insufficiently focused in previous
studies, we perform extensive experiments on four LLMs across three privacy
leakage scenarios. The results demonstrate that Self-Sanitize achieves superior
mitigation performance with minimal overhead and without degrading the utility
of LLMs, offering a practical and robust solution for safer LLM deployments.
Our code is available at the following link:
https://github.com/wjfu99/LLM_Self_Sanitize</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.24345v1">Regulating Online Algorithmic Pricing: A Comparative Study of Privacy
  and Data Protection Laws in the EU and US</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2025-09-29T06:46:56Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zihao Li</p>
    <p><b>Summary:</b> The emergence of big data, AI and machine learning has allowed sellers and
online platforms to tailor pricing for customers in real-time. While online
algorithmic pricing can increase efficiency, market welfare, and optimize
pricing strategies for sellers and companies, it poses a threat to the
fundamental values of privacy, digital autonomy, and non-discrimination,
raising legal and ethical concerns. On both sides of the Atlantic, legislators
have endeavoured to regulate online algorithmic pricing in different ways in
the context of privacy and personal data protection. Represented by the GDPR,
the EU adopts an omnibus approach to regulate algorithmic pricing and is
supplemented by the Digital Service Act and the Digital Market Act. The US
combines federal and state laws to regulate online algorithmic pricing and
focuses on industrial regulations. Therefore, a comparative analysis of these
legal frameworks is necessary to ascertain the effectiveness of these
approaches. Taking a comparative approach, this working paper aims to explore
how EU and US respective data protection and privacy laws address the issues
posed by online algorithmic pricing. The paper evaluates whether the current
legal regime is effective in protecting individuals against the perils of
online algorithmic pricing in the EU and the US. It particularly analyses the
new EU regulatory paradigm, the Digital Service Act (DSA) and the Digital
Market Act (DMA), as supplementary mechanisms to the EU data protection law, in
order to draw lessons for US privacy law and vice versa.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.24173v1">Fundamental Limit of Discrete Distribution Estimation under
  Utility-Optimized Local Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2025-09-29T01:41:36Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sun-Moon Yoon, Hyun-Young Park, Seung-Hyun Nam, Si-Hyeon Lee</p>
    <p><b>Summary:</b> We study the problem of discrete distribution estimation under
utility-optimized local differential privacy (ULDP), which enforces local
differential privacy (LDP) on sensitive data while allowing more accurate
inference on non-sensitive data. In this setting, we completely characterize
the fundamental privacy-utility trade-off. The converse proof builds on several
key ideas, including a generalized uniform asymptotic Cram\'er-Rao lower bound,
a reduction showing that it suffices to consider a newly defined class of
extremal ULDP mechanisms, and a novel distribution decomposition technique
tailored to ULDP constraints. For the achievability, we propose a class of
utility-optimized block design (uBD) schemes, obtained as nontrivial
modifications of the block design mechanism known to be optimal under standard
LDP constraints, while incorporating the distribution decomposition idea used
in the converse proof and a score-based linear estimator. These results provide
a tight characterization of the estimation accuracy achievable under ULDP and
reveal new insights into the structure of optimal mechanisms for
privacy-preserving statistical inference.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.24153v1">DNS in the Time of Curiosity: A Tale of Collaborative User Privacy
  Protection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-09-29T01:09:09Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Philip Sjsvrd, Hongyu Jin, Panos Papadimitratos</p>
    <p><b>Summary:</b> The Domain Name System (DNS) is central to all Internet user activity,
resolving accessed domain names into Internet Protocol (IP) addresses. As a
result, curious DNS resolvers can learn everything about Internet users'
interests. Public DNS resolvers are rising in popularity, offering low-latency
resolution, high reliability, privacy-preserving policies, and support for
encrypted DNS queries. However, client-resolver traffic encryption,
increasingly deployed to protect users from eavesdroppers, does not protect
users against curious resolvers. Similarly, privacy-preserving policies are
based solely on written commitments and do not provide technical safeguards.
Although DNS query relay schemes can separate duties to limit data accessible
by each entity, they cannot prevent colluding entities from sharing user
traffic logs. Thus, a key challenge remains: organizations operating public DNS
resolvers, accounting for the majority of DNS resolutions, can potentially
collect and analyze massive volumes of Internet user activity data. With DNS
infrastructure that cannot be fully trusted, can we safeguard user privacy? We
answer positively and advocate for a user-driven approach to reduce exposure to
DNS services. We will discuss key ideas of the proposal, which aims to achieve
a high level of privacy without sacrificing performance: maintaining low
latency, network bandwidth, memory/storage overhead, and computational
overhead.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2510.03284v1">Edge-FIT: Federated Instruction Tuning of Quantized LLMs for
  Privacy-Preserving Smart Home Environments</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-09-28T20:06:37Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Vinay Venkatesh, Vamsidhar R Kamanuru, Lav Kumar, Nikita Kothari</p>
    <p><b>Summary:</b> This paper proposes Edge-FIT (Federated Instruction Tuning on the Edge), a
scalable framework for Federated Instruction Tuning (FIT) of Large Language
Models (LLMs). Traditional Federated Learning (TFL) methods, like FedAvg, fail
when confronted with the massive parameter size of LLMs [3], [6]. Our Edge-FIT
framework combines federated learning with 4-bit Quantized Low-Rank Adaptation
(QLORA), mitigating the core issues of communication and computational
overhead. We demonstrate this by filtering the general-purpose Databricks Dolly
15k dataset for the IoT domain. Experimental results show the Edge-FIT tuned
Llama 2(7B) achieves an F1-Score of 0.89. We also demonstrate a viable
trade-off using the 3.8B Phi-3-mini model, validating Edge-FIT as a scalable
framework for decentralized LLM deployment on home compute gateways.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.23834v1">GPM: The Gaussian Pancake Mechanism for Planting Undetectable Backdoors
  in Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB">
  <p><b>Published on:</b> 2025-09-28T12:14:06Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Haochen Sun, Xi He</p>
    <p><b>Summary:</b> Differential privacy (DP) has become the gold standard for preserving
individual privacy in data analysis. However, an implicit yet fundamental
assumption underlying these rigorous privacy guarantees is the correct
implementation and execution of DP mechanisms. Several incidents of unintended
privacy loss have occurred due to numerical issues and inappropriate
configurations of DP software, which have been successfully exploited in
privacy attacks. To better understand the seriousness of defective DP software,
we ask the following question: is it possible to elevate these passive defects
into active privacy attacks while maintaining covertness?
  To address this question, we present the Gaussian pancake mechanism (GPM), a
novel mechanism that is computationally indistinguishable from the widely used
Gaussian mechanism (GM), yet exhibits arbitrarily weaker statistical DP
guarantees. This unprecedented separation enables a new class of backdoor
attacks: by indistinguishably passing off as the authentic GM, GPM can covertly
degrade statistical privacy. Unlike the unintentional privacy loss caused by
GM's numerical issues, GPM is an adversarial yet undetectable backdoor attack
against data privacy. We formally prove GPM's covertness, characterize its
statistical leakage, and demonstrate a concrete distinguishing attack that can
achieve near-perfect success rates under suitable parameter choices, both
theoretically and empirically.
  Our results underscore the importance of using transparent, open-source DP
libraries and highlight the need for rigorous scrutiny and formal verification
of DP implementations to prevent subtle, undetectable privacy compromises in
real-world systems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.23827v1">Assessing Visual Privacy Risks in Multimodal AI: A Novel
  Taxonomy-Grounded Evaluation of Vision-Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-09-28T12:04:54Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Efthymios Tsaprazlis, Tiantian Feng, Anil Ramakrishna, Rahul Gupta, Shrikanth Narayanan</p>
    <p><b>Summary:</b> Artificial Intelligence have profoundly transformed the technological
landscape in recent years. Large Language Models (LLMs) have demonstrated
impressive abilities in reasoning, text comprehension, contextual pattern
recognition, and integrating language with visual understanding. While these
advances offer significant benefits, they also reveal critical limitations in
the models' ability to grasp the notion of privacy. There is hence substantial
interest in determining if and how these models can understand and enforce
privacy principles, particularly given the lack of supporting resources to test
such a task. In this work, we address these challenges by examining how legal
frameworks can inform the capabilities of these emerging technologies. To this
end, we introduce a comprehensive, multi-level Visual Privacy Taxonomy that
captures a wide range of privacy issues, designed to be scalable and adaptable
to existing and future research needs. Furthermore, we evaluate the
capabilities of several state-of-the-art Vision-Language Models (VLMs),
revealing significant inconsistencies in their understanding of contextual
privacy. Our work contributes both a foundational taxonomy for future research
and a critical benchmark of current model limitations, demonstrating the urgent
need for more robust, privacy-aware AI systems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.23680v1">A First Look at Privacy Risks of Android Task-executable Voice Assistant
  Applications</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Software Engineering-D91E36">
  <p><b>Published on:</b> 2025-09-28T06:47:06Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shidong Pan, Yikai Ge, Xiaoyu Sun</p>
    <p><b>Summary:</b> With the development of foundation AI technologies, task-executable voice
assistants (VAs) have become more popular, enhancing user convenience and
expanding device functionality. Android task-executable VAs are applications
that are capable of understanding complex tasks and performing corresponding
operations. Given their prevalence and great autonomy, there is no existing
work examine the privacy risks within the voice assistants from the
task-execution pattern in a holistic manner. To fill this research gap, this
paper presents a user-centric comprehensive empirical study on privacy risks in
Android task-executable VA applications. We collect ten mainstream VAs as our
research target and analyze their operational characteristics. We then
cross-check their privacy declarations across six sources, including privacy
labels, policies, and manifest files, and our findings reveal widespread
inconsistencies. Moreover, we uncover three significant privacy threat models:
(1) privacy misdisclosure in mega apps, where integrated mini apps such as
Alexa skills are inadequately represented; (2) privilege escalation via
inter-application interactions, which exploit Android's communication
mechanisms to bypass user consent; and (3) abuse of Google system applications,
enabling apps to evade the declaration of dangerous permissions. Our study
contributes actionable recommendations for practitioners and underscores
broader relevance of these privacy risks to emerging autonomous AI agents.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2510.02357v1">Privacy in the Age of AI: A Taxonomy of Data Risks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> 
  <p><b>Published on:</b> 2025-09-28T00:20:03Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Grace Billiris, Asif Gill, Madhushi Bandara</p>
    <p><b>Summary:</b> Artificial Intelligence (AI) systems introduce unprecedented privacy
challenges as they process increasingly sensitive data. Traditional privacy
frameworks prove inadequate for AI technologies due to unique characteristics
such as autonomous learning and black-box decision-making. This paper presents
a taxonomy classifying AI privacy risks, synthesised from 45 studies identified
through systematic review. We identify 19 key risks grouped under four
categories: Dataset-Level, Model-Level, Infrastructure-Level, and Insider
Threat Risks. Findings reveal a balanced distribution across these dimensions,
with human error (9.45%) emerging as the most significant factor. This taxonomy
challenges conventional security approaches that typically prioritise technical
controls over human factors, highlighting gaps in holistic understanding. By
bridging technical and behavioural dimensions of AI privacy, this paper
contributes to advancing trustworthy AI development and provides a foundation
for future research.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2510.02356v1">Measuring Physical-World Privacy Awareness of Large Language Models: An
  Evaluation Benchmark</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-09-27T23:39:56Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xinjie Shen, Mufei Li, Pan Li</p>
    <p><b>Summary:</b> The deployment of Large Language Models (LLMs) in embodied agents creates an
urgent need to measure their privacy awareness in the physical world. Existing
evaluation methods, however, are confined to natural language based scenarios.
To bridge this gap, we introduce EAPrivacy, a comprehensive evaluation
benchmark designed to quantify the physical-world privacy awareness of
LLM-powered agents. EAPrivacy utilizes procedurally generated scenarios across
four tiers to test an agent's ability to handle sensitive objects, adapt to
changing environments, balance task execution with privacy constraints, and
resolve conflicts with social norms. Our measurements reveal a critical deficit
in current models. The top-performing model, Gemini 2.5 Pro, achieved only 59\%
accuracy in scenarios involving changing physical environments. Furthermore,
when a task was accompanied by a privacy request, models prioritized completion
over the constraint in up to 86\% of cases. In high-stakes situations pitting
privacy against critical social norms, leading models like GPT-4o and
Claude-3.5-haiku disregarded the social norm over 15\% of the time. These
findings, demonstrated by our benchmark, underscore a fundamental misalignment
in LLMs regarding physically grounded privacy and establish the need for more
robust, physically-aware alignment.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.23525v1">Privy: Envisioning and Mitigating Privacy Risks for Consumer-facing AI
  Product Concepts</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-09-27T23:08:24Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Hao-Ping Lee, Yu-Ju Yang, Matthew Bilik, Isadora Krsek, Thomas Serban von Davier, Kyzyl Monteiro, Jason Lin, Shivani Agarwal, Jodi Forlizzi, Sauvik Das</p>
    <p><b>Summary:</b> AI creates and exacerbates privacy risks, yet practitioners lack effective
resources to identify and mitigate these risks. We present Privy, a tool that
guides practitioners through structured privacy impact assessments to: (i)
identify relevant risks in novel AI product concepts, and (ii) propose
appropriate mitigations. Privy was shaped by a formative study with 11
practitioners, which informed two versions -- one LLM-powered, the other
template-based. We evaluated these two versions of Privy through a
between-subjects, controlled study with 24 separate practitioners, whose
assessments were reviewed by 13 independent privacy experts. Results show that
Privy helps practitioners produce privacy assessments that experts deemed high
quality: practitioners identified relevant risks and proposed appropriate
mitigation strategies. These effects were augmented in the LLM-powered version.
Practitioners themselves rated Privy as being useful and usable, and their
feedback illustrates how it helps overcome long-standing awareness, motivation,
and ability barriers in privacy work.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.23459v2">MaskSQL: Safeguarding Privacy for LLM-Based Text-to-SQL via Abstraction</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-09-27T19:07:50Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sepideh Abedini, Shubhankar Mohapatra, D. B. Emerson, Masoumeh Shafieinejad, Jesse C. Cresswell, Xi He</p>
    <p><b>Summary:</b> Large language models (LLMs) have shown promising performance on tasks that
require reasoning, such as text-to-SQL, code generation, and debugging.
However, regulatory frameworks with strict privacy requirements constrain their
integration into sensitive systems. State-of-the-art LLMs are also proprietary,
costly, and resource-intensive, making local deployment impractical.
Consequently, utilizing such LLMs often requires sharing data with third-party
providers, raising privacy concerns and risking noncompliance with regulations.
Although fine-tuned small language models (SLMs) can outperform LLMs on certain
tasks and be deployed locally to mitigate privacy concerns, they underperform
on more complex tasks such as text-to-SQL translation. In this work, we
introduce MaskSQL, a text-to-SQL framework that utilizes abstraction as a
privacy protection mechanism to mask sensitive information in LLM prompts.
Unlike redaction, which removes content entirely, or generalization, which
broadens tokens, abstraction retains essential information while discarding
unnecessary details, striking an effective privacy-utility balance for the
text-to-SQL task. Moreover, by providing mechanisms to control the
privacy-utility tradeoff, MaskSQL facilitates adoption across a broader range
of use cases. Our experimental results show that MaskSQL outperforms leading
SLM-based text-to-SQL models and achieves performance approaching
state-of-the-art LLM-based models, while preserving privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.23444v1">HoloTrace: a Location Privacy Preservation Solution for mmWave MIMO-OFDM
  Systems</a></h3>
  
  <p><b>Published on:</b> 2025-09-27T18:26:30Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Lorenzo Italiano, Alireza Pourafzal, Hui Chen, Mattia Brambilla, Gonzalo Seco-Granados, Monica Nicoli, Henk Wymeersch</p>
    <p><b>Summary:</b> The technological innovation towards 6G cellular networks introduces
unprecedented capabilities for user equipment (UE) localization, but it also
raises serious concerns about physical layer location privacy. This paper
introduces HoloTrace, a signal-level privacy preservation framework that relies
on user-side spoofing of localization-relevant features to prevent the
extraction of precise location information from the signals received by a base
station (BS) in a mmWave MIMO-OFDM system. Spoofing is performed by the user on
location parameters such as angle of arrival (AoA), angle of departure (AoD),
and time difference of arrival (TDoA). Without requiring any protocol
modification nor network-side support, our method strategically perturbs pilot
transmissions to prevent a BS from performing non-consensual UE localization.
The methodology allows the UE to spoof its position, keeping the precoder
unchanged. We formulate spoofing as a unified rank-constrained projection
problem, and provide closed-form solutions under varying levels of channel
state information (CSI) at the UE, including scenarios with and without CSI
knowledge. Simulation results confirm that the proposed approach enables the UE
to deceive the BS, inducing significant localization errors, while the impact
on link capacity varies depending on the spoofed position. Our findings
establish HoloTrace as a practical and robust privacy-preserving solution for
future 6G networks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.23246v1">Adaptive Token-Weighted Differential Privacy for LLMs: Not All Tokens
  Require Equal Protection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-09-27T10:51:07Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Manjiang Yu, Priyanka Singh, Xue Li, Yang Cao</p>
    <p><b>Summary:</b> Large language models (LLMs) frequently memorize sensitive or personal
information, raising significant privacy concerns. Existing variants of
differential privacy stochastic gradient descent (DPSGD) inject uniform noise
into every gradient step, significantly extending training time and reducing
model accuracy. We propose that concentrating noise primarily on gradients
associated with sensitive tokens can substantially decrease DP training time,
strengthen the protection of sensitive information, and simultaneously preserve
the model's performance on non-sensitive data. We operationalize this insight
through Adaptive Token-Weighted Differential Privacy (ATDP), a modification of
vanilla DP-SGD that adaptively assigns different gradient weights to sensitive
and non-sensitive tokens. By employing a larger noise scale at the early stage
of training, ATDP rapidly disrupts memorization of sensitive content. As a
result, ATDP only requires a few additional epochs of lightweight
post-processing following standard fine-tuning, injecting targeted noise
primarily on parameters corresponding to sensitive tokens, thus minimally
affecting the model's general capabilities. ATDP can be seamlessly integrated
into any existing DP-based fine-tuning pipeline or directly applied to
non-private models as a fast privacy-enhancing measure. Additionally, combined
with an initial redacted fine-tuning phase, ATDP forms a streamlined DP
pipeline that achieves comparable canary protection to state-of-the-art DP-SGD
methods, significantly reduces the computational overhead of DP fine-tuning,
shortening training time by approximately 90 percent, while achieving
comparable or superior privacy protection and minimal accuracy degradation.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.23190v1">CoSIFL: Collaborative Secure and Incentivized Federated Learning with
  Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-09-27T08:45:40Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhanhong Xie, Meifan Zhang, Lihua Yin</p>
    <p><b>Summary:</b> Federated learning (FL) has emerged as a promising paradigm for collaborative
model training while preserving data locality. However, it still faces
challenges from malicious or compromised clients, as well as difficulties in
incentivizing participants to contribute high-quality data under strict privacy
requirements. Motivated by these considerations, we propose CoSIFL, a novel
framework that integrates proactive alarming for robust security and local
differential privacy (LDP) for inference attacks, together with a
Stackelberg-based incentive scheme to encourage client participation and data
sharing. Specifically, CoSIFL uses an active alarming mechanism and robust
aggregation to defend against Byzantine and inference attacks, while a Tullock
contest-inspired incentive module rewards honest clients for both data
contributions and reliable alarm triggers. We formulate the interplay between
the server and clients as a two-stage game: in the first stage, the server
determines total rewards, selects participants, and fixes global iteration
settings, whereas in the second stage, each client decides its mini-batch size,
privacy noise scale, and alerting strategy. We prove that the server-client
game admits a unique equilibrium, and analyze how clients' multi-dimensional
attributes - such as non-IID degrees and privacy budgets - jointly affect
system efficiency. Experimental results on standard benchmarks demonstrate that
CoSIFL outperforms state-of-the-art solutions in improving model robustness and
reducing total server costs, highlighting the effectiveness of our integrated
design.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.23091v1">FedBit: Accelerating Privacy-Preserving Federated Learning via
  Bit-Interleaved Packing and Cross-Layer Co-Design</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Hardware Architecture-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-09-27T03:58:16Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xiangchen Meng, Yangdi Lyu</p>
    <p><b>Summary:</b> Federated learning (FL) with fully homomorphic encryption (FHE) effectively
safeguards data privacy during model aggregation by encrypting local model
updates before transmission, mitigating threats from untrusted servers or
eavesdroppers in transmission. However, the computational burden and ciphertext
expansion associated with homomorphic encryption can significantly increase
resource and communication overhead. To address these challenges, we propose
FedBit, a hardware/software co-designed framework optimized for the
Brakerski-Fan-Vercauteren (BFV) scheme. FedBit employs bit-interleaved data
packing to embed multiple model parameters into a single ciphertext
coefficient, thereby minimizing ciphertext expansion and maximizing
computational parallelism. Additionally, we integrate a dedicated FPGA
accelerator to handle cryptographic operations and an optimized dataflow to
reduce the memory overhead. Experimental results demonstrate that FedBit
achieves a speedup of two orders of magnitude in encryption and lowers average
communication overhead by 60.7%, while maintaining high accuracy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.23030v1">DPFNAS: Differential Privacy-Enhanced Federated Neural Architecture
  Search for 6G Edge Intelligence</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-09-27T01:03:26Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yang Lv, Jin Cao, Ben Niu, Zhe Sun, Fengwei Wang, Fenghua Li, Hui Li</p>
    <p><b>Summary:</b> The Sixth-Generation (6G) network envisions pervasive artificial intelligence
(AI) as a core goal, enabled by edge intelligence through on-device data
utilization. To realize this vision, federated learning (FL) has emerged as a
key paradigm for collaborative training across edge devices. However, the
sensitivity and heterogeneity of edge data pose key challenges to FL: parameter
sharing risks data reconstruction, and a unified global model struggles to
adapt to diverse local distributions. In this paper, we propose a novel
federated learning framework that integrates personalized differential privacy
(DP) and adaptive model design. To protect training data, we leverage
sample-level representations for knowledge sharing and apply a personalized DP
strategy to resist reconstruction attacks. To ensure distribution-aware
adaptation under privacy constraints, we develop a privacy-aware neural
architecture search (NAS) algorithm that generates locally customized
architectures and hyperparameters. To the best of our knowledge, this is the
first personalized DP solution tailored for representation-based FL with
theoretical convergence guarantees. Our scheme achieves strong privacy
guarantees for training data while significantly outperforming state-of-the-art
methods in model performance. Experiments on benchmark datasets such as
CIFAR-10 and CIFAR-100 demonstrate that our scheme improves accuracy by 6.82\%
over the federated NAS method PerFedRLNAS, while reducing model size to 1/10
and communication cost to 1/20.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.23022v2">Copyright Infringement Detection in Text-to-Image Diffusion Models via
  Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-09-27T00:38:12Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xiafeng Man, Zhipeng Wei, Jingjing Chen</p>
    <p><b>Summary:</b> The widespread deployment of large vision models such as Stable Diffusion
raises significant legal and ethical concerns, as these models can memorize and
reproduce copyrighted content without authorization. Existing detection
approaches often lack robustness and fail to provide rigorous theoretical
underpinnings. To address these gaps, we formalize the concept of copyright
infringement and its detection from the perspective of Differential Privacy
(DP), and introduce the conditional sensitivity metric, a concept analogous to
sensitivity in DP, that quantifies the deviation in a diffusion model's output
caused by the inclusion or exclusion of a specific training data point. To
operationalize this metric, we propose D-Plus-Minus (DPM), a novel post-hoc
detection framework that identifies copyright infringement in text-to-image
diffusion models. Specifically, DPM simulates inclusion and exclusion processes
by fine-tuning models in two opposing directions: learning or unlearning.
Besides, to disentangle concept-specific influence from the global parameter
shifts induced by fine-tuning, DPM computes confidence scores over orthogonal
prompt distributions using statistical metrics. Moreover, to facilitate
standardized benchmarking, we also construct the Copyright Infringement
Detection Dataset (CIDD), a comprehensive resource for evaluating detection
across diverse categories. Our results demonstrate that DPM reliably detects
infringement content without requiring access to the original training dataset
or text prompts, offering an interpretable and practical solution for
safeguarding intellectual property in the era of generative AI.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.22900v1">Towards Context-aware Mobile Privacy Notice: Implementation of A
  Deployable Contextual Privacy Policies Generator</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Software Engineering-D91E36">
  <p><b>Published on:</b> 2025-09-26T20:26:32Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Haochen Gong, Zhen Tao, Shidong Pan, Zhenchang Xing, Xiaoyu Sun</p>
    <p><b>Summary:</b> Lengthy and legally phrased privacy policies impede users' understanding of
how mobile applications collect and process personal data. Prior work proposed
Contextual Privacy Policies (CPPs) for mobile apps to display shorter policy
snippets only in the corresponding user interface contexts, but the pipeline
could not be deployable in real-world mobile environments. In this paper, we
present PrivScan, the first deployable CPP Software Development Kit (SDK) for
Android. It captures live app screenshots to identify GUI elements associated
with types of personal data and displays CPPs in a concise, user-facing format.
We provide a lightweight floating button that offers low-friction, on-demand
control. The architecture leverages remote deployment to decouple the
multimodal backend pipeline from a mobile client comprising five modular
components, thereby reducing on-device resource demands and easing
cross-platform portability. A feasibility-oriented evaluation shows an average
execution time of 9.15\,s, demonstrating the practicality of our approach. The
source code of PrivScan is available at https://github.com/buyanghc/PrivScan
and the demo video can be found at https://www.youtube.com/watch?v=ck-25otfyHc.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.22857v1">PAPER: Privacy-Preserving ResNet Models using Low-Degree Polynomial
  Approximations and Structural Optimizations on Leveled FHE</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-09-26T19:10:23Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Eduardo Chielle, Manaar Alam, Jinting Liu, Jovan Kascelan, Michail Maniatakos</p>
    <p><b>Summary:</b> Recent work has made non-interactive privacy-preserving inference more
practical by running deep Convolution Neural Network (CNN) with Fully
Homomorphic Encryption (FHE). However, these methods remain limited by their
reliance on bootstrapping, a costly FHE operation applied across multiple
layers, severely slowing inference. They also depend on high-degree polynomial
approximations of non-linear activations, which increase multiplicative depth
and reduce accuracy by 2-5% compared to plaintext ReLU models. In this work, we
focus on ResNets, a widely adopted benchmark architecture in privacy-preserving
inference, and close the accuracy gap between their FHE-based non-interactive
models and plaintext counterparts, while also achieving faster inference than
existing methods. We use a quadratic polynomial approximation of ReLU, which
achieves the theoretical minimum multiplicative depth for non-linear
activations, along with a penalty-based training strategy. We further introduce
structural optimizations such as node fusing, weight redistribution, and tower
reuse. These optimizations reduce the required FHE levels in CNNs by nearly a
factor of five compared to prior work, allowing us to run ResNet models under
leveled FHE without bootstrapping. To further accelerate inference and recover
accuracy typically lost with polynomial approximations, we introduce parameter
clustering along with a joint strategy of data encoding layout and ensemble
techniques. Experiments with ResNet-18, ResNet-20, and ResNet-32 on CIFAR-10
and CIFAR-100 show that our approach achieves up to 4x faster private inference
than prior work with comparable accuracy to plaintext ReLU models.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.22428v1">Privacy Mechanism Design based on Empirical Distributions</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2025-09-26T14:46:59Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Leonhard Grosse, Sara Saeidian, Mikael Skoglund, Tobias J. Oechtering</p>
    <p><b>Summary:</b> Pointwise maximal leakage (PML) is a per-outcome privacy measure based on
threat models from quantitative information flow. Privacy guarantees with PML
rely on knowledge about the distribution that generated the private data. In
this work, we propose a framework for PML privacy assessment and mechanism
design with empirical estimates of this data-generating distribution. By
extending the PML framework to consider sets of data-generating distributions,
we arrive at bounds on the worst-case leakage within a given set. We use these
bounds alongside large-deviation bounds from the literature to provide a method
for obtaining distribution-independent $(\varepsilon,\delta)$-PML guarantees
when the data-generating distribution is estimated from available data samples.
We provide an optimal binary mechanism, and show that mechanism design with
this type of uncertainty about the data-generating distribution reduces to a
linearly constrained convex program. Further, we show that optimal mechanisms
designed for a distribution estimate can be used. Finally, we apply these tools
to leakage assessment of the Laplace mechanism and the Gaussian mechanism for
binary private data, and numerically show that the presented approach to
mechanism design can yield significant utility increase compared to local
differential privacy, while retaining similar privacy guarantees.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.22213v1">Accuracy-First Rnyi Differential Privacy and Post-Processing Immunity</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-09-26T11:27:36Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ossi Ris, Antti Koskela, Antti Honkela</p>
    <p><b>Summary:</b> The accuracy-first perspective of differential privacy addresses an important
shortcoming by allowing a data analyst to adaptively adjust the quantitative
privacy bound instead of sticking to a predetermined bound. Existing works on
the accuracy-first perspective have neglected an important property of
differential privacy known as post-processing immunity, which ensures that an
adversary is not able to weaken the privacy guarantee by post-processing. We
address this gap by determining which existing definitions in the
accuracy-first perspective have post-processing immunity, and which do not. The
only definition with post-processing immunity, pure ex-post privacy, lacks
useful tools for practical problems, such as an ex-post analogue of the
Gaussian mechanism, and an algorithm to check if accuracy on separate private
validation set is high enough. To address this, we propose a new definition
based on R\'enyi differential privacy that has post-processing immunity, and we
develop basic theory and tools needed for practical applications. We
demonstrate the practicality of our theory with an application to synthetic
data generation, where our algorithm successfully adjusts the privacy bound
until an accuracy threshold is met on a private validation dataset.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.22103v1">Privacy in Distributed Quantum Sensing with Gaussian Quantum Networks</a></h3>
   
  <p><b>Published on:</b> 2025-09-26T09:24:24Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Uesli Alushi, Roberto Di Candia</p>
    <p><b>Summary:</b> We study the privacy properties of distributed quantum sensing protocols in a
Gaussian quantum network, where each node encodes a parameter via a local phase
shift. For networks with more than two nodes, achieving perfect privacy is
possible only asymptotically, in the limit of large photon numbers. However, we
show that optimized fully symmetric Gaussian states enable rapidly approaching
perfect privacy while maintaining near-optimal sensing performance. We show
that local homodyne detection achieves a quadratic scaling of precision with
the total number of photons. We further analyze the impact of thermal noise in
the preparation stage on both privacy and estimation precision. Our results
pave the way for the development of practical, private distributed quantum
sensing protocols in continuous-variable quantum networks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.21800v1">Federated Learning of Quantile Inference under Local Differential
  Privacy</a></h3>
   
  <p><b>Published on:</b> 2025-09-26T02:56:39Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Leheng Cai, Qirui Hu, Shuyuan Wu</p>
    <p><b>Summary:</b> In this paper, we investigate federated learning for quantile inference under
local differential privacy (LDP). We propose an estimator based on local
stochastic gradient descent (SGD), whose local gradients are perturbed via a
randomized mechanism with global parameters, making the procedure tolerant of
communication and storage constraints without compromising statistical
efficiency. Although the quantile loss and its corresponding gradient do not
satisfy standard smoothness conditions typically assumed in existing
literature, we establish asymptotic normality for our estimator as well as a
functional central limit theorem. The proposed method accommodates data
heterogeneity and allows each server to operate with an individual privacy
budget. Furthermore, we construct confidence intervals for the target value
through a self-normalization approach, thereby circumventing the need to
estimate additional nuisance parameters. Extensive numerical experiments and
real data application validate the theoretical guarantees of the proposed
methodology.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.21762v1">Privacy-Preserving Performance Profiling of In-The-Wild GPUs</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Hardware Architecture-04E762">
  <p><b>Published on:</b> 2025-09-26T01:49:50Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ian McDougall, Michael Davies, Rahul Chatterjee, Somesh Jha, Karthikeyan Sankaralingam</p>
    <p><b>Summary:</b> GPUs are the dominant platform for many important applications today
including deep learning, accelerated computing, and scientific simulation.
However, as the complexity of both applications and hardware increases, GPU
chip manufacturers face a significant challenge: how to gather comprehensive
performance characteristics and value profiles from GPUs deployed in real-world
scenarios. Such data, encompassing the types of kernels executed and the time
spent in each, is crucial for optimizing chip design and enhancing application
performance. Unfortunately, despite the availability of low-level tools like
NSYS and NCU, current methodologies fall short, offering data collection
capabilities only on an individual user basis rather than a broader, more
informative fleet-wide scale. This paper takes on the problem of realizing a
system that allows planet-scale real-time GPU performance profiling of
low-level hardware characteristics. The three fundamental problems we solve
are: i) user experience of achieving this with no slowdown; ii) preserving user
privacy, so that no 3rd party is aware of what applications any user runs; iii)
efficacy in showing we are able to collect data and assign it applications even
when run on 1000s of GPUs. Our results simulate a 100,000 size GPU deployment,
running applications from the Torchbench suite, showing our system addresses
all 3 problems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.21712v1">Not My Agent, Not My Boundary? Elicitation of Personal Privacy
  Boundaries in AI-Delegated Information Sharing</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-09-26T00:20:30Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Bingcan Guo, Eryue Xu, Zhiping Zhang, Tianshi Li</p>
    <p><b>Summary:</b> Aligning AI systems with human privacy preferences requires understanding
individuals' nuanced disclosure behaviors beyond general norms. Yet eliciting
such boundaries remains challenging due to the context-dependent nature of
privacy decisions and the complex trade-offs involved. We present an AI-powered
elicitation approach that probes individuals' privacy boundaries through a
discriminative task. We conducted a between-subjects study that systematically
varied communication roles and delegation conditions, resulting in 1,681
boundary specifications from 169 participants for 61 scenarios. We examined how
these contextual factors and individual differences influence the boundary
specification. Quantitative results show that communication roles influence
individuals' acceptance of detailed and identifiable disclosure, AI delegation
and individuals' need for privacy heighten sensitivity to disclosed
identifiers, and AI delegation results in less consensus across individuals.
Our findings highlight the importance of situating privacy preference
elicitation within real-world data flows. We advocate using nuanced privacy
boundaries as an alignment goal for future AI systems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.21704v1">PQFed: A Privacy-Preserving Quality-Controlled Federated Learning
  Framework</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-09-25T23:56:24Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Weiqi Yue, Wenbiao Li, Yuzhou Jiang, Anisa Halimi, Roger French, Erman Ayday</p>
    <p><b>Summary:</b> Federated learning enables collaborative model training without sharing raw
data, but data heterogeneity consistently challenges the performance of the
global model. Traditional optimization methods often rely on collaborative
global model training involving all clients, followed by local adaptation to
improve individual performance. In this work, we focus on early-stage quality
control and propose PQFed, a novel privacy-preserving personalized federated
learning framework that designs customized training strategies for each client
prior to the federated training process. PQFed extracts representative features
from each client's raw data and applies clustering techniques to estimate
inter-client dataset similarity. Based on these similarity estimates, the
framework implements a client selection strategy that enables each client to
collaborate with others who have compatible data distributions. We evaluate
PQFed on two benchmark datasets, CIFAR-10 and MNIST, integrated with three
existing federated learning algorithms. Experimental results show that PQFed
consistently improves the target client's model performance, even with a
limited number of participants. We further benchmark PQFed against a baseline
cluster-based algorithm, IFCA, and observe that PQFed also achieves better
performance in low-participation scenarios. These findings highlight PQFed's
scalability and effectiveness in personalized federated learning settings.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2510.02325v1">Agentic-AI Healthcare: Multilingual, Privacy-First Framework with MCP
  Agents</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> 
  <p><b>Published on:</b> 2025-09-25T21:25:52Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mohammed A. Shehab</p>
    <p><b>Summary:</b> This paper introduces Agentic-AI Healthcare, a privacy-aware, multilingual,
and explainable research prototype developed as a single-investigator project.
The system leverages the emerging Model Context Protocol (MCP) to orchestrate
multiple intelligent agents for patient interaction, including symptom
checking, medication suggestions, and appointment scheduling. The platform
integrates a dedicated Privacy and Compliance Layer that applies role-based
access control (RBAC), AES-GCM field-level encryption, and tamper-evident audit
logging, aligning with major healthcare data protection standards such as HIPAA
(US), PIPEDA (Canada), and PHIPA (Ontario). Example use cases demonstrate
multilingual patient-doctor interaction (English, French, Arabic) and
transparent diagnostic reasoning powered by large language models. As an
applied AI contribution, this work highlights the feasibility of combining
agentic orchestration, multilingual accessibility, and compliance-aware
architecture in healthcare applications. This platform is presented as a
research prototype and is not a certified medical device.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.20867v1">Federated Markov Imputation: Privacy-Preserving Temporal Imputation in
  Multi-Centric ICU Environments</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-09-25T08:00:05Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Christoph Dsing, Philipp Cimiano</p>
    <p><b>Summary:</b> Missing data is a persistent challenge in federated learning on electronic
health records, particularly when institutions collect time-series data at
varying temporal granularities. To address this, we propose Federated Markov
Imputation (FMI), a privacy-preserving method that enables Intensive Care Units
(ICUs) to collaboratively build global transition models for temporal
imputation. We evaluate FMI on a real-world sepsis onset prediction task using
the MIMIC-IV dataset and show that it outperforms local imputation baselines,
especially in scenarios with irregular sampling intervals across ICUs.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.20838v1">Zero-Shot Privacy-Aware Text Rewriting via Iterative Tree Search</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-09-25T07:23:52Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shuo Huang, Xingliang Yuan, Gholamreza Haffari, Lizhen Qu</p>
    <p><b>Summary:</b> The increasing adoption of large language models (LLMs) in cloud-based
services has raised significant privacy concerns, as user inputs may
inadvertently expose sensitive information. Existing text anonymization and
de-identification techniques, such as rule-based redaction and scrubbing, often
struggle to balance privacy preservation with text naturalness and utility. In
this work, we propose a zero-shot, tree-search-based iterative sentence
rewriting algorithm that systematically obfuscates or deletes private
information while preserving coherence, relevance, and naturalness. Our method
incrementally rewrites privacy-sensitive segments through a structured search
guided by a reward model, enabling dynamic exploration of the rewriting space.
Experiments on privacy-sensitive datasets show that our approach significantly
outperforms existing baselines, achieving a superior balance between privacy
protection and utility preservation.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.20460v2">Differential Privacy of Network Parameters from a System Identification
  Perspective</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2025-09-24T18:06:11Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Andrew Campbell, Anna Scaglione, Hang Liu, Victor Elvira, Sean Peisert, Daniel Arnold</p>
    <p><b>Summary:</b> This paper addresses the problem of protecting network information from
privacy system identification (SI) attacks when sharing cyber-physical system
simulations. We model analyst observations of networked states as time-series
outputs of a graph filter driven by differentially private (DP) nodal
excitations, with the analyst aiming to infer the underlying graph shift
operator (GSO). Unlike traditional SI, which estimates system parameters, we
study the inverse problem: what assumptions prevent adversaries from
identifying the GSO while preserving utility for legitimate analysis. We show
that applying DP mechanisms to inputs provides formal privacy guarantees for
the GSO, linking the $(\epsilon,\delta)$-DP bound to the spectral properties of
the graph filter and noise covariance. More precisely, for DP Gaussian signals,
the spectral characteristics of both the filter and noise covariance determine
the privacy bound, with smooth filters and low-condition-number covariance
yielding greater privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.20454v1">Bridging Privacy and Utility: Synthesizing anonymized EEG with
  constraining utility functions</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-09-24T18:02:41Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kay Fuhrmeister, Arne Pelzer, Fabian Radke, Julia Lechinger, Mahzad Gharleghi, Thomas Kllmer, Insa Wolf</p>
    <p><b>Summary:</b> Electroencephalography (EEG) is widely used for recording brain activity and
has seen numerous applications in machine learning, such as detecting sleep
stages and neurological disorders. Several studies have successfully shown the
potential of EEG data for re-identification and leakage of other personal
information. Therefore, the increasing availability of EEG consumer devices
raises concerns about user privacy, motivating us to investigate how to
safeguard this sensitive data while retaining its utility for EEG applications.
To address this challenge, we propose a transformer-based autoencoder to create
EEG data that does not allow for subject re-identification while still
retaining its utility for specific machine learning tasks. We apply our
approach to automatic sleep staging by evaluating the re-identification and
utility potential of EEG data before and after anonymization. The results show
that the re-identifiability of the EEG signal can be substantially reduced
while preserving its utility for machine learning.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.20324v1">RAG Security and Privacy: Formalizing the Threat Model and Attack
  Surface</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-09-24T17:11:35Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Atousa Arzanipour, Rouzbeh Behnia, Reza Ebrahimi, Kaushik Dutta</p>
    <p><b>Summary:</b> Retrieval-Augmented Generation (RAG) is an emerging approach in natural
language processing that combines large language models (LLMs) with external
document retrieval to produce more accurate and grounded responses. While RAG
has shown strong potential in reducing hallucinations and improving factual
consistency, it also introduces new privacy and security challenges that differ
from those faced by traditional LLMs. Existing research has demonstrated that
LLMs can leak sensitive information through training data memorization or
adversarial prompts, and RAG systems inherit many of these vulnerabilities. At
the same time, reliance of RAG on an external knowledge base opens new attack
surfaces, including the potential for leaking information about the presence or
content of retrieved documents, or for injecting malicious content to
manipulate model behavior. Despite these risks, there is currently no formal
framework that defines the threat landscape for RAG systems. In this paper, we
address a critical gap in the literature by proposing, to the best of our
knowledge, the first formal threat model for retrieval-RAG systems. We
introduce a structured taxonomy of adversary types based on their access to
model components and data, and we formally define key threat vectors such as
document-level membership inference and data poisoning, which pose serious
privacy and integrity risks in real-world deployments. By establishing formal
definitions and attack models, our work lays the foundation for a more rigorous
and principled understanding of privacy and security in RAG systems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.20283v1">Monitoring Violations of Differential Privacy over Time</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Statistics Theory-D91E36">  
  <p><b>Published on:</b> 2025-09-24T16:15:51Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> nder Askin, Tim Kutta, Holger Dette</p>
    <p><b>Summary:</b> Auditing differential privacy has emerged as an important area of research
that supports the design of privacy-preserving mechanisms. Privacy audits help
to obtain empirical estimates of the privacy parameter, to expose flawed
implementations of algorithms and to compare practical with theoretical privacy
guarantees. In this work, we investigate an unexplored facet of privacy
auditing: the sustained auditing of a mechanism that can go through changes
during its development or deployment. Monitoring the privacy of algorithms over
time comes with specific challenges. Running state-of-the-art (static) auditors
repeatedly requires excessive sampling efforts, while the reliability of such
methods deteriorates over time without proper adjustments. To overcome these
obstacles, we present a new monitoring procedure that extracts information from
the entire deployment history of the algorithm. This allows us to reduce
sampling efforts, while sustaining reliable outcomes of our auditor. We derive
formal guarantees with regard to the soundness of our methods and evaluate
their performance for important mechanisms from the literature. Our theoretical
findings and experiments demonstrate the efficacy of our approach.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.20153v2">Affective Computing and Emotional Data: Challenges and Implications in
  Privacy Regulations, The AI Act, and Ethics in Large Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-09-24T14:18:41Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Nicola Fabiano</p>
    <p><b>Summary:</b> This paper examines the integration of emotional intelligence into artificial
intelligence systems, with a focus on affective computing and the growing
capabilities of Large Language Models (LLMs), such as ChatGPT and Claude, to
recognize and respond to human emotions. Drawing on interdisciplinary research
that combines computer science, psychology, and neuroscience, the study
analyzes foundational neural architectures - CNNs for processing facial
expressions and RNNs for sequential data, such as speech and text - that enable
emotion recognition. It examines the transformation of human emotional
experiences into structured emotional data, addressing the distinction between
explicit emotional data collected with informed consent in research settings
and implicit data gathered passively through everyday digital interactions.
That raises critical concerns about lawful processing, AI transparency, and
individual autonomy over emotional expressions in digital environments. The
paper explores implications across various domains, including healthcare,
education, and customer service, while addressing challenges of cultural
variations in emotional expression and potential biases in emotion recognition
systems across different demographic groups. From a regulatory perspective, the
paper examines emotional data in the context of the GDPR and the EU AI Act
frameworks, highlighting how emotional data may be considered sensitive
personal data that requires robust safeguards, including purpose limitation,
data minimization, and meaningful consent mechanisms.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.20024v1">Generative Adversarial Networks Applied for Privacy Preservation in
  Biometric-Based Authentication and Identification</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-09-24T11:39:40Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Lubos Mjachky, Ivan Homoliak</p>
    <p><b>Summary:</b> Biometric-based authentication systems are getting broadly adopted in many
areas. However, these systems do not allow participating users to influence the
way their data is used. Furthermore, the data may leak and can be misused
without the users' knowledge. In this paper, we propose a new authentication
method that preserves the privacy of individuals and is based on a generative
adversarial network (GAN). Concretely, we suggest using the GAN for translating
images of faces to a visually private domain (e.g., flowers or shoes).
Classifiers, which are used for authentication purposes, are then trained on
the images from the visually private domain. Based on our experiments, the
method is robust against attacks and still provides meaningful utility.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.19925v1">CON-QA: Privacy-Preserving QA using cloud LLMs in Contract Domain</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-09-24T09:29:17Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ajeet Kumar Singh, Rajsabi Surya, Anurag Tripathi, Santanu Choudhury, Sudhir Bisane</p>
    <p><b>Summary:</b> As enterprises increasingly integrate cloud-based large language models
(LLMs) such as ChatGPT and Gemini into their legal document workflows,
protecting sensitive contractual information - including Personally
Identifiable Information (PII) and commercially sensitive clauses - has emerged
as a critical challenge. In this work, we propose CON-QA, a hybrid
privacy-preserving framework designed specifically for secure question
answering over enterprise contracts, effectively combining local and
cloud-hosted LLMs. The CON-QA framework operates through three stages: (i)
semantic query decomposition and query-aware document chunk retrieval using a
locally deployed LLM analysis, (ii) anonymization of detected sensitive
entities via a structured one-to-many mapping scheme, ensuring semantic
coherence while preventing cross-session entity inference attacks, and (iii)
anonymized response generation by a cloud-based LLM, with accurate
reconstruction of the original answer locally using a session-consistent
many-to-one reverse mapping. To rigorously evaluate CON-QA, we introduce
CUAD-QA, a corpus of 85k question-answer pairs generated over 510 real-world
CUAD contract documents, encompassing simple, complex, and summarization-style
queries. Empirical evaluations, complemented by detailed human assessments,
confirm that CON-QA effectively maintains both privacy and utility, preserves
answer quality, maintains fidelity to legal clause semantics, and significantly
mitigates privacy risks, demonstrating its practical suitability for secure,
enterprise-level contract documents.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.19906v1">Voice Privacy Preservation with Multiple Random Orthogonal Secret Keys:
  Attack Resistance Analysis</a></h3>
  
  <p><b>Published on:</b> 2025-09-24T09:05:13Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kohei Tanaka, Hitoshi Kiya, Sayaka Shiota</p>
    <p><b>Summary:</b> Recently, opportunities to transmit speech data to deep learning models
executed in the cloud have increased. This has led to growing concerns about
speech privacy, including both speaker-specific information and the linguistic
content of utterances. As an approach to preserving speech privacy, a speech
privacy-preserving method based on encryption using a secret key with a random
orthogonal matrix has been proposed. This method enables cloud-based model
inference while concealing both the speech content and the speaker identity.
However, the method has limited attack resistance and is constrained in terms
of the deep learning models to which the encryption can be applied. In this
work, we propose a method that enhances the attack resistance of the
conventional speech privacy-preserving technique by employing multiple random
orthogonal matrices as secret keys. We also introduce approaches to relax the
model constraints, enabling the application of our method to a broader range of
deep learning models. Furthermore, we investigate the robustness of the
proposed method against attacks using extended attack scenarios based on the
scenarios employed in the Voice Privacy Challenge. Our experimental results
confirmed that the proposed method maintains privacy protection performance for
speaker concealment, even under more powerful attack scenarios not considered
in prior work.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.19661v1">Consistent Estimation of Numerical Distributions under Local
  Differential Privacy by Wavelet Expansion</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-09-24T00:37:22Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Puning Zhao, Zhikun Zhang, Bo Sun, Li Shen, Liang Zhang, Shaowei Wang, Zhe Liu</p>
    <p><b>Summary:</b> Distribution estimation under local differential privacy (LDP) is a
fundamental and challenging task. Significant progresses have been made on
categorical data. However, due to different evaluation metrics, these methods
do not work well when transferred to numerical data. In particular, we need to
prevent the probability mass from being misplaced far away. In this paper, we
propose a new approach that express the sample distribution using wavelet
expansions. The coefficients of wavelet series are estimated under LDP. Our
method prioritizes the estimation of low-order coefficients, in order to ensure
accurate estimation at macroscopic level. Therefore, the probability mass is
prevented from being misplaced too far away from its ground truth. We establish
theoretical guarantees for our methods. Experiments show that our wavelet
expansion method significantly outperforms existing solutions under Wasserstein
and KS distances.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.19599v1">Knowledge Base-Aware Orchestration: A Dynamic, Privacy-Preserving Method
  for Multi-Agent Systems</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Multiagent Systems-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-09-23T21:46:38Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Danilo Trombino, Vincenzo Pecorella, Alessandro de Giulii, Davide Tresoldi</p>
    <p><b>Summary:</b> Multi-agent systems (MAS) are increasingly tasked with solving complex,
knowledge-intensive problems where effective agent orchestration is critical.
Conventional orchestration methods rely on static agent descriptions, which
often become outdated or incomplete. This limitation leads to inefficient task
routing, particularly in dynamic environments where agent capabilities
continuously evolve. We introduce Knowledge Base-Aware (KBA) Orchestration, a
novel approach that augments static descriptions with dynamic,
privacy-preserving relevance signals derived from each agent's internal
knowledge base (KB). In the proposed framework, when static descriptions are
insufficient for a clear routing decision, the orchestrator prompts the
subagents in parallel. Each agent then assesses the task's relevance against
its private KB, returning a lightweight ACK signal without exposing the
underlying data. These collected signals populate a shared semantic cache,
providing dynamic indicators of agent suitability for future queries. By
combining this novel mechanism with static descriptions, our method achieves
more accurate and adaptive task routing preserving agent autonomy and data
confidentiality. Benchmarks show that our KBA Orchestration significantly
outperforms static description-driven methods in routing precision and overall
system efficiency, making it suitable for large-scale systems that require
higher accuracy than standard description-driven routing.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.19041v1">Position: Human-Robot Interaction in Embodied Intelligence Demands a
  Shift From Static Privacy Controls to Dynamic Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-09-23T14:10:00Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shuning Zhang, Hong Jia, Simin Li, Ting Dang, Yongquan `Owen' Hu, Xin Yi, Hewu Li</p>
    <p><b>Summary:</b> The reasoning capabilities of embodied agents introduce a critical,
under-explored inferential privacy challenge, where the risk of an agent
generate sensitive conclusions from ambient data. This capability creates a
fundamental tension between an agent's utility and user privacy, rendering
traditional static controls ineffective. To address this, this position paper
proposes a framework that reframes privacy as a dynamic learning problem
grounded in theory of Contextual Integrity (CI). Our approach enables agents to
proactively learn and adapt to individual privacy norms through interaction,
outlining a research agenda to develop embodied agents that are both capable
and function as trustworthy safeguards of user privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.18949v1">Towards Privacy-Aware Bayesian Networks: A Credal Approach</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-09-23T12:58:32Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Niccol Rocchi, Fabio Stella, Cassio de Campos</p>
    <p><b>Summary:</b> Bayesian networks (BN) are probabilistic graphical models that enable
efficient knowledge representation and inference. These have proven effective
across diverse domains, including healthcare, bioinformatics and economics. The
structure and parameters of a BN can be obtained by domain experts or directly
learned from available data. However, as privacy concerns escalate, it becomes
increasingly critical for publicly released models to safeguard sensitive
information in training data. Typically, released models do not prioritize
privacy by design. In particular, tracing attacks from adversaries can combine
the released BN with auxiliary data to determine whether specific individuals
belong to the data from which the BN was learned. State-of-the-art protection
tecniques involve introducing noise into the learned parameters. While this
offers robust protection against tracing attacks, it significantly impacts the
model's utility, in terms of both the significance and accuracy of the
resulting inferences. Hence, high privacy may be attained at the cost of
releasing a possibly ineffective model. This paper introduces credal networks
(CN) as a novel solution for balancing the model's privacy and utility. After
adapting the notion of tracing attacks, we demonstrate that a CN enables the
masking of the learned BN, thereby reducing the probability of successful
attacks. As CNs are obfuscated but not noisy versions of BNs, they can achieve
meaningful inferences while safeguarding privacy. Moreover, we identify key
learning information that must be concealed to prevent attackers from
recovering the underlying BN. Finally, we conduct a set of numerical
experiments to analyze how privacy gains can be modulated by tuning the CN
hyperparameters. Our results confirm that CNs provide a principled, practical,
and effective approach towards the development of privacy-aware probabilistic
graphical models.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.18871v2">Uncovering Privacy Vulnerabilities through Analytical Gradient Inversion
  Attacks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-09-23T10:10:12Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tamer Ahmed Eltaras, Qutaibah Malluhi, Alessandro Savino, Stefano Di Carlo, Adnan Qayyum</p>
    <p><b>Summary:</b> Federated learning has emerged as a prominent privacy-preserving technique
for leveraging large-scale distributed datasets by sharing gradients instead of
raw data. However, recent studies indicate that private training data can still
be exposed through gradient inversion attacks. While earlier analytical methods
have demonstrated success in reconstructing input data from fully connected
layers, their effectiveness significantly diminishes when applied to
convolutional layers, high-dimensional inputs, and scenarios involving multiple
training examples. This paper extends our previous work \cite{eltaras2024r} and
proposes three advanced algorithms to broaden the applicability of gradient
inversion attacks. The first algorithm presents a novel data leakage method
that efficiently exploits convolutional layer gradients, demonstrating that
even with non-fully invertible activation functions, such as ReLU, training
samples can be analytically reconstructed directly from gradients without the
need to reconstruct intermediate layer outputs. Building on this foundation,
the second algorithm extends this analytical approach to support
high-dimensional input data, substantially enhancing its utility across complex
real-world datasets. The third algorithm introduces an innovative analytical
method for reconstructing mini-batches, addressing a critical gap in current
research that predominantly focuses on reconstructing only a single training
example. Unlike previous studies that focused mainly on the weight constraints
of convolutional layers, our approach emphasizes the pivotal role of gradient
constraints, revealing that successful attacks can be executed with fewer than
5\% of the constraints previously deemed necessary in certain layers.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.18696v1">FlowCrypt: Flow-Based Lightweight Encryption with Near-Lossless Recovery
  for Cloud Photo Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-09-23T06:25:35Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xiaohui Yang, Ping Ping, Feng Xu</p>
    <p><b>Summary:</b> The widespread adoption of smartphone photography has led users to
increasingly rely on cloud storage for personal photo archiving and sharing,
raising critical privacy concerns. Existing deep learning-based image
encryption schemes, typically built upon CNNs or GANs, often depend on
traditional cryptographic algorithms and lack inherent architectural
reversibility, resulting in limited recovery quality and poor robustness.
Invertible neural networks (INNs) have emerged to address this issue by
enabling reversible transformations, yet the first INN-based encryption scheme
still relies on an auxiliary reference image and discards by-product
information before decryption, leading to degraded recovery and limited
practicality. To address these limitations, this paper proposes FlowCrypt, a
novel flow-based image encryption framework that simultaneously achieves
near-lossless recovery, high security, and lightweight model design. FlowCrypt
begins by applying a key-conditioned random split to the input image, enhancing
forward-process randomness and encryption strength. The resulting components
are processed through a Flow-based Encryption/Decryption (FED) module composed
of invertible blocks, which share parameters across encryption and decryption.
Thanks to its reversible architecture and reference-free design, FlowCrypt
ensures high-fidelity image recovery. Extensive experiments show that FlowCrypt
achieves recovery quality with 100dB on three datasets, produces uniformly
distributed cipher images, and maintains a compact architecture with only 1M
parameters, making it suitable for mobile and edge-device applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.20388v1">Can You Trust Your Copilot? A Privacy Scorecard for AI Coding Assistants</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-09-22T21:45:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Amir AL-Maamari</p>
    <p><b>Summary:</b> The rapid integration of AI-powered coding assistants into developer
workflows has raised significant privacy and trust concerns. As developers
entrust proprietary code to services like OpenAI's GPT, Google's Gemini, and
GitHub Copilot, the unclear data handling practices of these tools create
security and compliance risks. This paper addresses this challenge by
introducing and applying a novel, expert-validated privacy scorecard. The
methodology involves a detailed analysis of four document types; from legal
policies to external audits; to score five leading assistants against 14
weighted criteria. A legal expert and a data protection officer refined these
criteria and their weighting. The results reveal a distinct hierarchy of
privacy protections, with a 20-point gap between the highest- and lowest-ranked
tools. The analysis uncovers common industry weaknesses, including the
pervasive use of opt-out consent for model training and a near-universal
failure to filter secrets from user prompts proactively. The resulting
scorecard provides actionable guidance for developers and organizations,
enabling evidence-based tool selection. This work establishes a new benchmark
for transparency and advocates for a shift towards more user-centric privacy
standards in the AI industry.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.18413v1">VoxGuard: Evaluating User and Attribute Privacy in Speech via Membership
  Inference Attacks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-09-22T20:57:48Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Efthymios Tsaprazlis, Thanathai Lertpetchpun, Tiantian Feng, Sai Praneeth Karimireddy, Shrikanth Narayanan</p>
    <p><b>Summary:</b> Voice anonymization aims to conceal speaker identity and attributes while
preserving intelligibility, but current evaluations rely almost exclusively on
Equal Error Rate (EER) that obscures whether adversaries can mount
high-precision attacks. We argue that privacy should instead be evaluated in
the low false-positive rate (FPR) regime, where even a small number of
successful identifications constitutes a meaningful breach. To this end, we
introduce VoxGuard, a framework grounded in differential privacy and membership
inference that formalizes two complementary notions: User Privacy, preventing
speaker re-identification, and Attribute Privacy, protecting sensitive traits
such as gender and accent. Across synthetic and real datasets, we find that
informed adversaries, especially those using fine-tuned models and
max-similarity scoring, achieve orders-of-magnitude stronger attacks at low-FPR
despite similar EER. For attributes, we show that simple transparent attacks
recover gender and accent with near-perfect accuracy even after anonymization.
Our results demonstrate that EER substantially underestimates leakage,
highlighting the need for low-FPR evaluation, and recommend VoxGuard as a
benchmark for evaluating privacy leakage.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.18311v1">Fine-Tuning Robot Policies While Maintaining User Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Robotics-F9C80E">
  <p><b>Published on:</b> 2025-09-22T18:36:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Benjamin A. Christie, Sagar Parekh, Dylan P. Losey</p>
    <p><b>Summary:</b> Recent works introduce general-purpose robot policies. These policies provide
a strong prior over how robots should behave -- e.g., how a robot arm should
manipulate food items. But in order for robots to match an individual person's
needs, users typically fine-tune these generalized policies -- e.g., showing
the robot arm how to make their own preferred dinners. Importantly, during the
process of personalizing robots, end-users leak data about their preferences,
habits, and styles (e.g., the foods they prefer to eat). Other agents can
simply roll-out the fine-tuned policy and see these personally-trained
behaviors. This leads to a fundamental challenge: how can we develop robots
that personalize actions while keeping learning private from external agents?
We here explore this emerging topic in human-robot interaction and develop
PRoP, a model-agnostic framework for personalized and private robot policies.
Our core idea is to equip each user with a unique key; this key is then used to
mathematically transform the weights of the robot's network. With the correct
key, the robot's policy switches to match that user's preferences -- but with
incorrect keys, the robot reverts to its baseline behaviors. We show the
general applicability of our method across multiple model types in imitation
learning, reinforcement learning, and classification tasks. PRoP is practically
advantageous because it retains the architecture and behaviors of the original
policy, and experimentally outperforms existing encoder-based approaches. See
videos and code here: https://prop-icra26.github.io.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.18014v1">Synth-MIA: A Testbed for Auditing Privacy Leakage in Tabular Data
  Synthesis</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2025-09-22T16:53:38Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Joshua Ward, Xiaofeng Lin, Chi-Hua Wang, Guang Cheng</p>
    <p><b>Summary:</b> Tabular Generative Models are often argued to preserve privacy by creating
synthetic datasets that resemble training data. However, auditing their
empirical privacy remains challenging, as commonly used similarity metrics fail
to effectively characterize privacy risk. Membership Inference Attacks (MIAs)
have recently emerged as a method for evaluating privacy leakage in synthetic
data, but their practical effectiveness is limited. Numerous attacks exist
across different threat models, each with distinct implementations targeting
various sources of privacy leakage, making them difficult to apply
consistently. Moreover, no single attack consistently outperforms the others,
leading to a routine underestimation of privacy risk.
  To address these issues, we propose a unified, model-agnostic threat
framework that deploys a collection of attacks to estimate the maximum
empirical privacy leakage in synthetic datasets. We introduce Synth-MIA, an
open-source Python library that streamlines this auditing process through a
novel testbed that integrates seamlessly into existing synthetic data
evaluation pipelines through a Scikit-Learn-like API. Our software implements
13 attack methods through a Scikit-Learn-like API, designed to enable fast
systematic estimation of privacy leakage for practitioners as well as
facilitate the development of new attacks and experiments for researchers.
  We demonstrate our framework's utility in the largest tabular synthesis
privacy benchmark to date, revealing that higher synthetic data quality
corresponds to greater privacy leakage, that similarity-based privacy metrics
show weak correlation with MIA results, and that the differentially private
generator PATEGAN can fail to preserve privacy under such attacks. This
underscores the necessity of MIA-based auditing when designing and deploying
Tabular Generative Models.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.17871v2">B-Privacy: Defining and Enforcing Privacy in Weighted Voting</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-09-22T15:11:12Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Samuel Breckenridge, Dani Vilardell, Andrs Fbrega, Amy Zhao, Patrick McCorry, Rafael Solari, Ari Juels</p>
    <p><b>Summary:</b> In traditional, one-vote-per-person voting systems, privacy equates with
ballot secrecy: voting tallies are published, but individual voters' choices
are concealed.
  Voting systems that weight votes in proportion to token holdings, though, are
now prevalent in cryptocurrency and web3 systems. We show that these
weighted-voting systems overturn existing notions of voter privacy. Our
experiments demonstrate that even with secret ballots, publishing raw tallies
often reveals voters' choices.
  Weighted voting thus requires a new framework for privacy. We introduce a
notion called B-privacy whose basis is bribery, a key problem in voting systems
today. B-privacy captures the economic cost to an adversary of bribing voters
based on revealed voting tallies.
  We propose a mechanism to boost B-privacy by noising voting tallies. We prove
bounds on its tradeoff between B-privacy and transparency, meaning
reported-tally accuracy. Analyzing 3,582 proposals across 30 Decentralized
Autonomous Organizations (DAOs), we find that the prevalence of large voters
("whales") limits the effectiveness of any B-Privacy-enhancing technique.
However, our mechanism proves to be effective in cases without extreme voting
weight concentration: among proposals requiring coalitions of $\geq5$ voters to
flip outcomes, our mechanism raises B-privacy by a geometric mean factor of
$4.1\times$.
  Our work offers the first principled guidance on transparency-privacy
tradeoffs in weighted-voting systems, complementing existing approaches that
focus on ballot secrecy and revealing fundamental constraints that voting
weight concentration imposes on privacy mechanisms.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.17488v1">Privacy in Action: Towards Realistic Privacy Mitigation and Evaluation
  for LLM-Powered Agents</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-09-22T08:19:06Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shouju Wang, Fenglin Yu, Xirui Liu, Xiaoting Qin, Jue Zhang, Qingwei Lin, Dongmei Zhang, Saravan Rajmohan</p>
    <p><b>Summary:</b> The increasing autonomy of LLM agents in handling sensitive communications,
accelerated by Model Context Protocol (MCP) and Agent-to-Agent (A2A)
frameworks, creates urgent privacy challenges. While recent work reveals
significant gaps between LLMs' privacy Q&A performance and their agent
behavior, existing benchmarks remain limited to static, simplified scenarios.
We present PrivacyChecker, a model-agnostic, contextual integrity based
mitigation approach that effectively reduces privacy leakage from 36.08% to
7.30% on DeepSeek-R1 and from 33.06% to 8.32% on GPT-4o, all while preserving
task helpfulness. We also introduce PrivacyLens-Live, transforming static
benchmarks into dynamic MCP and A2A environments that reveal substantially
higher privacy risks in practical. Our modular mitigation approach integrates
seamlessly into agent protocols through three deployment strategies, providing
practical privacy protection for the emerging agentic ecosystem. Our data and
code will be made available at https://aka.ms/privacy_in_action.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.17266v1">Privacy-Preserving State Estimation with Crowd Sensors: An
  Information-Theoretic Respective</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36">  
  <p><b>Published on:</b> 2025-09-21T22:44:34Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Farhad Farokhi</p>
    <p><b>Summary:</b> Privacy-preserving state estimation for linear time-invariant dynamical
systems with crowd sensors is considered. At any time step, the estimator has
access to measurements from a randomly selected sensor from a pool of sensors
with pre-specified models and noise profiles. A Luenberger-like observer is
used to fuse the measurements with the underlying model of the system to
recursively generate the state estimates. An additive privacy-preserving noise
is used to constrain information leakage. Information leakage is measured via
mutual information between the identity of the sensors and the state estimate
conditioned on the actual state of the system. This captures an omnipotent
adversary that not only can access state estimates but can also gather direct
high-quality state measurements. Any prescribed level of information leakage is
shown to be achievable by appropriately selecting the variance of the
privacy-preserving noise. Therefore, privacy-utility trade-off can be
fine-tuned.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.16962v1">Temporal Drift in Privacy Recall: Users Misremember From Verbatim Loss
  to Gist-Based Overexposure Over Time</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-09-21T07:50:19Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Haoze Guo, Ziqi Wei</p>
    <p><b>Summary:</b> With social media content traversing the different platforms, occasionally
resurfacing after periods of time, users are increasingly prone to unintended
disclosure resulting from a misremembered acceptance of privacy. Context
collapse and interface cues are two factors considered by prior researchers,
yet we know less about how time-lapse basically alters recall of past audiences
destined for exposure. Likewise, the design space for mitigating this temporal
exposure risk remains underexplored. Our work theorizes temporal drift in
privacy recall as verbatim memory of prior settings blowing apart and
eventually settling with gist-based heuristics, which more often than not
select an audience larger than the original one. Grounded in memory research,
contextual integrity, and usable privacy, we examine why such a drift occurs,
why it tends to bias toward broader sharing, and how it compounds upon repeat
exposure. Following that, we suggest provenance-forward interface schemes and a
risk-based evaluation framework that mutates recall into recognition. The merit
of our work lies in establishing a temporal awareness of privacy design as an
essential safety rail against inadvertent overexposure.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.16915v1">Differential Privacy for Euclidean Jordan Algebra with Applications to
  Private Symmetric Cone Programming</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Optimization and Control-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-09-21T04:34:12Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhao Song, Jianfei Xue, Lichen Zhang</p>
    <p><b>Summary:</b> In this paper, we study differentially private mechanisms for functions whose
outputs lie in a Euclidean Jordan algebra. Euclidean Jordan algebras capture
many important mathematical structures and form the foundation of linear
programming, second-order cone programming, and semidefinite programming. Our
main contribution is a generic Gaussian mechanism for such functions, with
sensitivity measured in $\ell_2$, $\ell_1$, and $\ell_\infty$ norms. Notably,
this framework includes the important case where the function outputs are
symmetric matrices, and sensitivity is measured in the Frobenius, nuclear, or
spectral norm. We further derive private algorithms for solving symmetric cone
programs under various settings, using a combination of the multiplicative
weights update method and our generic Gaussian mechanism. As an application, we
present differentially private algorithms for semidefinite programming,
resolving a major open question posed by [Hsu, Roth, Roughgarden, and Ullman,
ICALP 2014].</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.25205v1">Polynomial Contrastive Learning for Privacy-Preserving Representation
  Learning on Graphs</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Rings and Algebras-662E9B">
  <p><b>Published on:</b> 2025-09-19T20:00:30Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Daksh Pandey</p>
    <p><b>Summary:</b> Self-supervised learning (SSL) has emerged as a powerful paradigm for
learning representations on graph data without requiring manual labels.
However, leading SSL methods like GRACE are fundamentally incompatible with
privacy-preserving technologies such as Homomorphic Encryption (HE) due to
their reliance on non-polynomial operations. This paper introduces Poly-GRACE,
a novel framework for HE-compatible self-supervised learning on graphs. Our
approach consists of a fully polynomial-friendly Graph Convolutional Network
(GCN) encoder and a novel, polynomial-based contrastive loss function. Through
experiments on three benchmark datasets -- Cora, CiteSeer, and PubMed -- we
demonstrate that Poly-GRACE not only enables private pre-training but also
achieves performance that is highly competitive with, and in the case of
CiteSeer, superior to the standard non-private baseline. Our work represents a
significant step towards practical and high-performance privacy-preserving
graph representation learning.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.15755v1">Utility-based Privacy Preserving Data Mining</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB">
  <p><b>Published on:</b> 2025-09-19T08:30:41Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Qingfeng Zhou, Wensheng Gan, Zhenlian Qi, Philip S. Yu</p>
    <p><b>Summary:</b> With the advent of big data, periodic pattern mining has demonstrated
significant value in real-world applications, including smart home systems,
healthcare systems, and the medical field. However, advances in network
technology have enabled malicious actors to extract sensitive information from
publicly available datasets, posing significant threats to data providers and,
in severe cases, hindering societal development. To mitigate such risks,
privacy-preserving utility mining (PPUM) has been proposed. However, PPUM is
unsuitable for addressing privacy concerns in periodic information mining. To
address this issue, we innovatively extend the existing PPUM framework and
propose two algorithms, Maximum sensitive Utility-MAximum maxPer item (MU-MAP)
and Maximum sensitive Utility-MInimum maxPer item (MU-MIP). These algorithms
aim to hide sensitive periodic high-utility itemsets while generating sanitized
datasets. To enhance the efficiency of the algorithms, we designed two novel
data structures: the Sensitive Itemset List (SISL) and the Sensitive Item List
(SIL), which store essential information about sensitive itemsets and their
constituent items. Moreover, several performance metrics were employed to
evaluate the performance of our algorithms compared to the state-of-the-art
PPUM algorithms. The experimental results show that our proposed algorithms
achieve an Artificial Cost (AC) value of 0 on all datasets when hiding
sensitive itemsets. In contrast, the traditional PPUM algorithm yields non-zero
AC. This indicates that our algorithms can successfully hide sensitive periodic
itemsets without introducing misleading patterns, whereas the PPUM algorithm
generates additional itemsets that may interfere with user decision-making.
Moreover, the results also reveal that our algorithms maintain Database Utility
Similarity (DUS) of over 90\% after the sensitive itemsets are hidden.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.18187v1">V-SenseDrive: A Privacy-Preserving Road Video and In-Vehicle Sensor
  Fusion Framework for Road Safety & Driver Behaviour Modelling</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-09-18T21:55:14Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Muhammad Naveed, Nazia Perwaiz, Sidra Sultana, Mohaira Ahmad, Muhammad Moazam Fraz</p>
    <p><b>Summary:</b> Road traffic accidents remain a major public health challenge, particularly
in countries with heterogeneous road conditions, mixed traffic flow, and
variable driving discipline, such as Pakistan. Reliable detection of unsafe
driving behaviours is a prerequisite for improving road safety, enabling
advanced driver assistance systems (ADAS), and supporting data driven decisions
in insurance and fleet management. Most of existing datasets originate from the
developed countries with limited representation of the behavioural diversity
observed in emerging economies and the driver's face recording voilates the
privacy preservation. We present V-SenseDrive, the first privacy-preserving
multimodal driver behaviour dataset collected entirely within the Pakistani
driving environment. V-SenseDrive combines smartphone based inertial and GPS
sensor data with synchronized road facing video to record three target driving
behaviours (normal, aggressive, and risky) on multiple types of roads,
including urban arterials, secondary roads, and motorways. Data was gathered
using a custom Android application designed to capture high frequency
accelerometer, gyroscope, and GPS streams alongside continuous video, with all
sources precisely time aligned to enable multimodal analysis. The focus of this
work is on the data acquisition process, covering participant selection,
driving scenarios, environmental considerations, and sensor video
synchronization techniques. The dataset is structured into raw, processed, and
semantic layers, ensuring adaptability for future research in driver behaviour
classification, traffic safety analysis, and ADAS development. By representing
real world driving in Pakistan, V-SenseDrive fills a critical gap in the global
landscape of driver behaviour datasets and lays the groundwork for context
aware intelligent transportation solutions.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.15047v1">Distributed Batch Matrix Multiplication: Trade-Offs in Download Rate,
  Randomness, and Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2025-09-18T15:10:43Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Amirhosein Morteza, Remi A. Chou</p>
    <p><b>Summary:</b> We study the trade-off between communication rate and privacy for distributed
batch matrix multiplication of two independent sequences of matrices
$\mathbf{A}$ and $\mathbf{B}$ with uniformly distributed entries. In our
setting, $\mathbf{B}$ is publicly accessible by all the servers while
$\mathbf{A}$ must remain private. A user is interested in evaluating the
product $\mathbf{AB}$ with the responses from the $k$ fastest servers. For a
given parameter $\alpha \in [0, 1]$, our privacy constraint must ensure that
any set of $\ell$ colluding servers cannot learn more than a fraction $\alpha$
of $\mathbf{A}$. Additionally, we study the trade-off between the amount of
local randomness needed at the encoder and privacy. Finally, we establish the
optimal trade-offs when the matrices are square and identify a linear
relationship between information leakage and communication rate.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.15278v1">Assessing metadata privacy in neuroimaging</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> 
  <p><b>Published on:</b> 2025-09-18T12:56:03Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Emilie Kibsgaard, Anita Sue Jwa, Christopher J Markiewicz, David Rodriguez Gonzalez, Judith Sainz Pardo, Russell A. Poldrack, Cyril R. Pernet</p>
    <p><b>Summary:</b> The ethical and legal imperative to share research data without causing harm
requires careful attention to privacy risks. While mounting evidence
demonstrates that data sharing benefits science, legitimate concerns persist
regarding the potential leakage of personal information that could lead to
reidentification and subsequent harm. We reviewed metadata accompanying
neuroimaging datasets from six heterogeneous studies openly available on
OpenNeuro, involving participants across the lifespan, from children to older
adults, with and without clinical diagnoses, and including associated clinical
score data. Using metaprivBIDS (https://github.com/CPernet/metaprivBIDS), a
novel tool for the systematic assessment of privacy in tabular data, we found
that privacy is generally well maintained, with serious vulnerabilities being
rare. Nonetheless, minor issues were identified in nearly all datasets and
warrant mitigation. Notably, clinical score data (e.g., neuropsychological
results) posed minimal reidentification risk, whereas demographic variables
(age, sex, race, income, and geolocation) represented the principal privacy
vulnerabilities. We outline practical measures to address these risks, enabling
safer data sharing practices.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.14603v1">Towards Privacy-Preserving and Heterogeneity-aware Split Federated
  Learning via Probabilistic Masking</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-09-18T04:28:08Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xingchen Wang, Feijie Wu, Chenglin Miao, Tianchun Li, Haoyu Hu, Qiming Cao, Jing Gao, Lu Su</p>
    <p><b>Summary:</b> Split Federated Learning (SFL) has emerged as an efficient alternative to
traditional Federated Learning (FL) by reducing client-side computation through
model partitioning. However, exchanging of intermediate activations and model
updates introduces significant privacy risks, especially from data
reconstruction attacks that recover original inputs from intermediate
representations. Existing defenses using noise injection often degrade model
performance. To overcome these challenges, we present PM-SFL, a scalable and
privacy-preserving SFL framework that incorporates Probabilistic Mask training
to add structured randomness without relying on explicit noise. This mitigates
data reconstruction risks while maintaining model utility. To address data
heterogeneity, PM-SFL employs personalized mask learning that tailors submodel
structures to each client's local data. For system heterogeneity, we introduce
a layer-wise knowledge compensation mechanism, enabling clients with varying
resources to participate effectively under adaptive model splitting.
Theoretical analysis confirms its privacy protection, and experiments on image
and wireless sensing tasks demonstrate that PM-SFL consistently improves
accuracy, communication efficiency, and robustness to privacy attacks, with
particularly strong performance under data and system heterogeneity.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.14581v1">Can I Trust This Chatbot? Assessing User Privacy in AI-Healthcare
  Chatbot Applications</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Emerging Technologies-F9C80E">
  <p><b>Published on:</b> 2025-09-18T03:29:43Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ramazan Yener, Guan-Hung Chen, Ece Gumusel, Masooda Bashir</p>
    <p><b>Summary:</b> As Conversational Artificial Intelligence (AI) becomes more integrated into
everyday life, AI-powered chatbot mobile applications are increasingly adopted
across industries, particularly in the healthcare domain. These chatbots offer
accessible and 24/7 support, yet their collection and processing of sensitive
health data present critical privacy concerns. While prior research has
examined chatbot security, privacy issues specific to AI healthcare chatbots
have received limited attention. Our study evaluates the privacy practices of
12 widely downloaded AI healthcare chatbot apps available on the App Store and
Google Play in the United States. We conducted a three-step assessment
analyzing: (1) privacy settings during sign-up, (2) in-app privacy controls,
and (3) the content of privacy policies. The analysis identified significant
gaps in user data protection. Our findings reveal that half of the examined
apps did not present a privacy policy during sign up, and only two provided an
option to disable data sharing at that stage. The majority of apps' privacy
policies failed to address data protection measures. Moreover, users had
minimal control over their personal data. The study provides key insights for
information science researchers, developers, and policymakers to improve
privacy protections in AI healthcare chatbot apps.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.14050v1">AI For Privacy in Smart Homes: Exploring How Leveraging AI-Powered Smart
  Devices Enhances Privacy Protection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2025-09-17T14:53:58Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Wael Albayaydh, Ivan Flechais, Rui Zhao, Jood Albayaydh</p>
    <p><b>Summary:</b> Privacy concerns and fears of unauthorized access in smart home devices often
stem from misunderstandings about how data is collected, used, and protected.
This study explores how AI-powered tools can offer innovative privacy
protections through clear, personalized, and contextual support to users.
Through 23 in-depth interviews with users, AI developers, designers, and
regulators, and using Grounded Theory analysis, we identified two key themes:
Aspirations for AI-Enhanced Privacy - how users perceive AI's potential to
empower them, address power imbalances, and improve ease of use- and AI
Ethical, Security, and Regulatory Considerations-challenges in strengthening
data security, ensuring regulatory compliance, and promoting ethical AI
practices. Our findings contribute to the field by uncovering user aspirations
for AI-driven privacy solutions, identifying key security and ethical
challenges, and providing actionable recommendations for all stakeholders,
particularly targeting smart device designers and AI developers, to guide the
co-design of AI tools that enhance privacy protection in smart home devices. By
bridging the gap between user expectations, AI capabilities, and regulatory
frameworks, this work offers practical insights for shaping the future of
privacy-conscious AI integration in smart homes.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.13987v1">Differential Privacy in Federated Learning: Mitigating Inference Attacks
  with Randomized Response</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-09-17T13:59:38Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ozer Ozturk, Busra Buyuktanir, Gozde Karatas Baydogmus, Kazim Yildiz</p>
    <p><b>Summary:</b> Machine learning models used for distributed architectures consisting of
servers and clients require large amounts of data to achieve high accuracy.
Data obtained from clients are collected on a central server for model
training. However, storing data on a central server raises concerns about
security and privacy. To address this issue, a federated learning architecture
has been proposed. In federated learning, each client trains a local model
using its own data. The trained models are periodically transmitted to the
central server. The server then combines the received models using federated
aggregation algorithms to obtain a global model. This global model is
distributed back to the clients, and the process continues in a cyclical
manner. Although preventing data from leaving the clients enhances security,
certain concerns still remain. Attackers can perform inference attacks on the
obtained models to approximate the training dataset, potentially causing data
leakage. In this study, differential privacy was applied to address the
aforementioned security vulnerability, and a performance analysis was
conducted. The Data-Unaware Classification Based on Association (duCBA)
algorithm was used as the federated aggregation method. Differential privacy
was implemented on the data using the Randomized Response technique, and the
trade-off between security and performance was examined under different epsilon
values. As the epsilon value decreased, the model accuracy declined, and class
prediction imbalances were observed. This indicates that higher levels of
privacy do not always lead to practical outcomes and that the balance between
security and performance must be carefully considered.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.13739v1">ParaAegis: Parallel Protection for Flexible Privacy-preserved Federated
  Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2025-09-17T06:45:13Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zihou Wu, Yuecheng Li, Tianchi Liao, Jian Lou, Chuan Chen</p>
    <p><b>Summary:</b> Federated learning (FL) faces a critical dilemma: existing protection
mechanisms like differential privacy (DP) and homomorphic encryption (HE)
enforce a rigid trade-off, forcing a choice between model utility and
computational efficiency. This lack of flexibility hinders the practical
implementation. To address this, we introduce ParaAegis, a parallel protection
framework designed to give practitioners flexible control over the
privacy-utility-efficiency balance. Our core innovation is a strategic model
partitioning scheme. By applying lightweight DP to the less critical, low norm
portion of the model while protecting the remainder with HE, we create a
tunable system. A distributed voting mechanism ensures consensus on this
partitioning. Theoretical analysis confirms the adjustments between efficiency
and utility with the same privacy. Crucially, the experimental results
demonstrate that by adjusting the hyperparameters, our method enables flexible
prioritization between model accuracy and training time.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.13627v1">Secure, Scalable and Privacy Aware Data Strategy in Cloud</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2025-09-17T01:56:07Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Vijay Kumar Butte, Sujata Butte</p>
    <p><b>Summary:</b> The enterprises today are faced with the tough challenge of processing,
storing large amounts of data in a secure, scalable manner and enabling
decision makers to make quick, informed data driven decisions. This paper
addresses this challenge and develops an effective enterprise data strategy in
the cloud. Various components of an effective data strategy are discussed and
architectures addressing security, scalability and privacy aspects are
provided.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.13625v3">Privacy-Aware In-Context Learning for Large Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-09-17T01:50:32Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Bishnu Bhusal, Manoj Acharya, Ramneet Kaur, Colin Samplawski, Anirban Roy, Adam D. Cobb, Rohit Chadha, Susmit Jha</p>
    <p><b>Summary:</b> Large language models (LLMs) have significantly transformed natural language
understanding and generation, but they raise privacy concerns due to potential
exposure of sensitive information. Studies have highlighted the risk of
information leakage, where adversaries can extract sensitive information
embedded in the prompts. In this work, we introduce a novel private prediction
framework for generating high-quality synthetic text with strong privacy
guarantees. Our approach leverages the Differential Privacy (DP) framework to
ensure worst-case theoretical bounds on information leakage without requiring
any fine-tuning of the underlying models. The proposed method performs
inference on private records and aggregates the resulting per-token output
distributions. This enables the generation of longer and coherent synthetic
text while maintaining privacy guarantees. Additionally, we propose a simple
blending operation that combines private and public inference to further
enhance utility. Empirical evaluations demonstrate that our approach
outperforms previous state-of-the-art methods on in-context-learning (ICL)
tasks, making it a promising direction for privacy-preserving text generation
while maintaining high utility.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.13509v1">Practitioners' Perspectives on a Differential Privacy Deployment
  Registry</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-09-16T20:15:15Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Priyanka Nanayakkara, Elena Ghazi, Salil Vadhan</p>
    <p><b>Summary:</b> Differential privacy (DP) -- a principled approach to producing statistical
data products with strong, mathematically provable privacy guarantees for the
individuals in the underlying dataset -- has seen substantial adoption in
practice over the past decade. Applying DP requires making several
implementation decisions, each with significant impacts on data privacy and/or
utility. Hence, to promote shared learning and accountability around DP
deployments, Dwork, Kohli, and Mulligan (2019) proposed a public-facing
repository ("registry") of DP deployments. The DP community has recently
started to work toward realizing this vision. We contribute to this effort by
(1) developing a holistic, hierarchical schema to describe any given DP
deployment and (2) designing and implementing an interactive interface to act
as a registry where practitioners can access information about past DP
deployments. We (3) populate our interface with 21 real-world DP deployments
and (4) conduct an exploratory user study with DP practitioners ($n=16$) to
understand how they would use the registry, as well as what challenges and
opportunities they foresee around its adoption. We find that participants were
enthusiastic about the registry as a valuable resource for evaluating prior
deployments and making future deployments. They also identified several
opportunities for the registry, including that it can become a "hub" for the
community and support broader communication around DP (e.g., to legal teams).
At the same time, they identified challenges around the registry gaining
adoption, including the effort and risk involved with making implementation
choices public and moderating the quality of entries. Based on our findings, we
offer recommendations for encouraging adoption and increasing the registry's
value not only to DP practitioners, but also to policymakers, data users, and
data subjects.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.14284v1">The Sum Leaks More Than Its Parts: Compositional Privacy Risks and
  Mitigations in Multi-Agent Collaboration</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-09-16T16:57:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Vaidehi Patil, Elias Stengel-Eskin, Mohit Bansal</p>
    <p><b>Summary:</b> As large language models (LLMs) become integral to multi-agent systems, new
privacy risks emerge that extend beyond memorization, direct inference, or
single-turn evaluations. In particular, seemingly innocuous responses, when
composed across interactions, can cumulatively enable adversaries to recover
sensitive information, a phenomenon we term compositional privacy leakage. We
present the first systematic study of such compositional privacy leaks and
possible mitigation methods in multi-agent LLM systems. First, we develop a
framework that models how auxiliary knowledge and agent interactions jointly
amplify privacy risks, even when each response is benign in isolation. Next, to
mitigate this, we propose and evaluate two defense strategies: (1)
Theory-of-Mind defense (ToM), where defender agents infer a questioner's intent
by anticipating how their outputs may be exploited by adversaries, and (2)
Collaborative Consensus Defense (CoDef), where responder agents collaborate
with peers who vote based on a shared aggregated state to restrict sensitive
information spread. Crucially, we balance our evaluation across compositions
that expose sensitive information and compositions that yield benign
inferences. Our experiments quantify how these defense strategies differ in
balancing the privacy-utility trade-off. We find that while chain-of-thought
alone offers limited protection to leakage (~39% sensitive blocking rate), our
ToM defense substantially improves sensitive query blocking (up to 97%) but can
reduce benign task success. CoDef achieves the best balance, yielding the
highest Balanced Outcome (79.8%), highlighting the benefit of combining
explicit reasoning with defender collaboration. Together, our results expose a
new class of risks in collaborative LLM deployments and provide actionable
insights for designing safeguards against compositional, context-driven privacy
leakage.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2509.13051v1">More than Meets the Eye: Understanding the Effect of Individual Objects
  on Perceived Visual Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-09-16T13:10:00Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mete Harun Akcay, Siddharth Prakash Rao, Alexandros Bakas, Buse Gul Atli</p>
    <p><b>Summary:</b> User-generated content, such as photos, comprises the majority of online
media content and drives engagement due to the human ability to process visual
information quickly. Consequently, many online platforms are designed for
sharing visual content, with billions of photos posted daily. However, photos
often reveal more than they intended through visible and contextual cues,
leading to privacy risks. Previous studies typically treat privacy as a
property of the entire image, overlooking individual objects that may carry
varying privacy risks and influence how users perceive it. We address this gap
with a mixed-methods study (n = 92) to understand how users evaluate the
privacy of images containing multiple sensitive objects. Our results reveal
mental models and nuanced patterns that uncover how granular details, such as
photo-capturing context and co-presence of other objects, affect privacy
perceptions. These novel insights could enable personalized, context-aware
privacy protection designs on social media and future technologies.</p>
  </details>
</div>

