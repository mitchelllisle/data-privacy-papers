
<h2>2024-08</h2>

<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.00327v1">Demo: FedCampus: A Real-world Privacy-preserving Mobile Application for
  Smart Campus via Federated Learning & Analytics</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2024-08-31T01:58:36Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jiaxiang Geng, Beilong Tang, Boyan Zhang, Jiaqi Shao, Bing Luo</p>
    <p><b>Summary:</b> In this demo, we introduce FedCampus, a privacy-preserving mobile application
for smart \underline{campus} with \underline{fed}erated learning (FL) and
federated analytics (FA). FedCampus enables cross-platform on-device FL/FA for
both iOS and Android, supporting continuously models and algorithms deployment
(MLOps). Our app integrates privacy-preserving processed data via differential
privacy (DP) from smartwatches, where the processed parameters are used for
FL/FA through the FedCampus backend platform. We distributed 100 smartwatches
to volunteers at Duke Kunshan University and have successfully completed a
series of smart campus tasks featuring capabilities such as sleep tracking,
physical activity monitoring, personalized recommendations, and heavy hitters.
Our project is opensourced at https://github.com/FedCampus/FedCampus_Flutter.
See the FedCampus video at https://youtu.be/k5iu46IjA38.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.17378v1">Empowering Open Data Sharing for Social Good: A Privacy-Aware Approach</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB">
  <p><b>Published on:</b> 2024-08-30T16:14:32Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tânia Carvalho, Luís Antunes, Cristina Costa, Nuno Moniz</p>
    <p><b>Summary:</b> The Covid-19 pandemic has affected the world at multiple levels. Data sharing
was pivotal for advancing research to understand the underlying causes and
implement effective containment strategies. In response, many countries have
promoted the availability of daily cases to support research initiatives,
fostering collaboration between organisations and making such data available to
the public through open data platforms. Despite the several advantages of data
sharing, one of the major concerns before releasing health data is its impact
on individuals' privacy. Such a sharing process should be based on
state-of-the-art methods in Data Protection by Design and by Default. In this
paper, we use a data set related to Covid-19 cases in the second largest
hospital in Portugal to show how it is feasible to ensure data privacy while
improving the quality and maintaining the utility of the data. Our goal is to
demonstrate how knowledge exchange in multidisciplinary teams of healthcare
practitioners, data privacy, and data science experts is crucial to
co-developing strategies that ensure high utility of de-identified data.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.17354v1">Forget to Flourish: Leveraging Machine-Unlearning on Pretrained Language
  Models for Privacy Leakage</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-30T15:35:09Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Md Rafi Ur Rashid, Jing Liu, Toshiaki Koike-Akino, Shagufta Mehnaz, Ye Wang</p>
    <p><b>Summary:</b> Fine-tuning large language models on private data for downstream applications
poses significant privacy risks in potentially exposing sensitive information.
Several popular community platforms now offer convenient distribution of a
large variety of pre-trained models, allowing anyone to publish without
rigorous verification. This scenario creates a privacy threat, as pre-trained
models can be intentionally crafted to compromise the privacy of fine-tuning
datasets. In this study, we introduce a novel poisoning technique that uses
model-unlearning as an attack tool. This approach manipulates a pre-trained
language model to increase the leakage of private data during the fine-tuning
process. Our method enhances both membership inference and data extraction
attacks while preserving model utility. Experimental results across different
models, datasets, and fine-tuning setups demonstrate that our attacks
significantly surpass baseline performance. This work serves as a cautionary
note for users who download pre-trained models from unverified sources,
highlighting the potential risks involved.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.17263v1">Privacy-Preserving Set-Based Estimation Using Differential Privacy and
  Zonotopes</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-30T13:05:38Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mohammed M. Dawoud, Changxin Liu, Karl H. Johansson, Amr Alanwar</p>
    <p><b>Summary:</b> For large-scale cyber-physical systems, the collaboration of spatially
distributed sensors is often needed to perform the state estimation process.
Privacy concerns arise from disclosing sensitive measurements to a cloud
estimator. To solve this issue, we propose a differentially private set-based
estimation protocol that guarantees true state containment in the estimated set
and differential privacy for the sensitive measurements throughout the
set-based state estimation process within the central and local differential
privacy models. Zonotopes are employed in the proposed differentially private
set-based estimator, offering computational advantages in set operations. We
consider a plant of a non-linear discrete-time dynamical system with bounded
modeling uncertainties, sensors that provide sensitive measurements with
bounded measurement uncertainties, and a cloud estimator that predicts the
system's state. The privacy-preserving noise perturbs the centers of
measurement zonotopes, thereby concealing the precise position of these
zonotopes, i.e., ensuring privacy preservation for the sets containing
sensitive measurements. Compared to existing research, our approach achieves
less privacy loss and utility loss through the central and local differential
privacy models by leveraging a numerically optimized truncated noise
distribution. The proposed estimator is perturbed by weaker noise than the
analytical approaches in the literature to guarantee the same level of privacy,
therefore improving the estimation utility. Numerical and comparison
experiments with truncated Laplace noise are presented to support our approach.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.17151v1">Investigating Privacy Leakage in Dimensionality Reduction Methods via
  Reconstruction Attack</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-08-30T09:40:52Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chayadon Lumbut, Donlapark Ponnoprat</p>
    <p><b>Summary:</b> This study investigates privacy leakage in dimensionality reduction methods
through a novel machine learning-based reconstruction attack. Employing an
\emph{informed adversary} threat model, we develop a neural network capable of
reconstructing high-dimensional data from low-dimensional embeddings.
  We evaluate six popular dimensionality reduction techniques: PCA, sparse
random projection (SRP), multidimensional scaling (MDS), Isomap, $t$-SNE, and
UMAP. Using both MNIST and NIH Chest X-ray datasets, we perform a qualitative
analysis to identify key factors affecting reconstruction quality. Furthermore,
we assess the effectiveness of an additive noise mechanism in mitigating these
reconstruction attacks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.17049v1">SPOQchain: Platform for Secure, Scalable, and Privacy-Preserving Supply
  Chain Tracing and Counterfeit Protection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-30T07:15:43Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Moritz Finke, Alexandra Dmitrienko, Jasper Stang</p>
    <p><b>Summary:</b> Product lifecycle tracing is increasingly in the focus of regulators and
producers, as shown with the initiative of the Digital Product Pass. Likewise,
new methods of counterfeit detection are developed that are, e.g., based on
Physical Unclonable Functions (PUFs). In order to ensure trust and integrity of
product lifecycle data, multiple existing supply chain tracing systems are
built on blockchain technology. However, only few solutions employ secure
identifiers such as PUFs. Furthermore, existing systems that publish the data
of individual products, in part fully transparently, have a detrimental impact
on scalability and the privacy of users. This work proposes SPOQchain, a novel
blockchain-based platform that provides comprehensive lifecycle traceability
and originality verification while ensuring high efficiency and user privacy.
The improved efficiency is achieved by a sophisticated batching mechanism that
removes lifecycle redundancies. In addition to the successful evaluation of
SPOQchain's scalability, this work provides a comprehensive analysis of privacy
and security aspects, demonstrating the need and qualification of SPOQchain for
the future of supply chain tracing.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.16913v1">Analyzing Inference Privacy Risks Through Gradients in Machine Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2024-08-29T21:21:53Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhuohang Li, Andrew Lowy, Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, Bradley Malin, Ye Wang</p>
    <p><b>Summary:</b> In distributed learning settings, models are iteratively updated with shared
gradients computed from potentially sensitive user data. While previous work
has studied various privacy risks of sharing gradients, our paper aims to
provide a systematic approach to analyze private information leakage from
gradients. We present a unified game-based framework that encompasses a broad
range of attacks including attribute, property, distributional, and user
disclosures. We investigate how different uncertainties of the adversary affect
their inferential power via extensive experiments on five datasets across
various data modalities. Our results demonstrate the inefficacy of solely
relying on data aggregation to achieve privacy against inference attacks in
distributed learning. We further evaluate five types of defenses, namely,
gradient pruning, signed gradient descent, adversarial perturbations,
variational information bottleneck, and differential privacy, under both static
and adaptive adversary settings. We provide an information-theoretic view for
analyzing the effectiveness of these defenses against inference from gradients.
Finally, we introduce a method for auditing attribute inference privacy,
improving the empirical estimation of worst-case privacy through crafting
adversarial canary records.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.00138v1">PrivacyLens: Evaluating Privacy Norm Awareness of Language Models in
  Action</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-29T17:58:38Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yijia Shao, Tianshi Li, Weiyan Shi, Yanchen Liu, Diyi Yang</p>
    <p><b>Summary:</b> As language models (LMs) are widely utilized in personalized communication
scenarios (e.g., sending emails, writing social media posts) and endowed with a
certain level of agency, ensuring they act in accordance with the contextual
privacy norms becomes increasingly critical. However, quantifying the privacy
norm awareness of LMs and the emerging privacy risk in LM-mediated
communication is challenging due to (1) the contextual and long-tailed nature
of privacy-sensitive cases, and (2) the lack of evaluation approaches that
capture realistic application scenarios. To address these challenges, we
propose PrivacyLens, a novel framework designed to extend privacy-sensitive
seeds into expressive vignettes and further into agent trajectories, enabling
multi-level evaluation of privacy leakage in LM agents' actions. We instantiate
PrivacyLens with a collection of privacy norms grounded in privacy literature
and crowdsourced seeds. Using this dataset, we reveal a discrepancy between LM
performance in answering probing questions and their actual behavior when
executing user instructions in an agent setup. State-of-the-art LMs, like GPT-4
and Llama-3-70B, leak sensitive information in 25.68% and 38.69% of cases, even
when prompted with privacy-enhancing instructions. We also demonstrate the
dynamic nature of PrivacyLens by extending each seed into multiple trajectories
to red-team LM privacy leakage risk. Dataset and code are available at
https://github.com/SALT-NLP/PrivacyLens.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.16304v1">Understanding Privacy Norms through Web Forms</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-29T07:11:09Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Hao Cui, Rahmadi Trimananda, Athina Markopoulou</p>
    <p><b>Summary:</b> Web forms are one of the primary ways to collect personal information online,
yet they are relatively under-studied. Unlike web tracking, data collection
through web forms is explicit and contextualized. Users (i) are asked to input
specific personal information types, and (ii) know the specific context (i.e.,
on which website and for what purpose). For web forms to be trusted by users,
they must meet the common sense standards of appropriate data collection
practices within a particular context (i.e., privacy norms). In this paper, we
extract the privacy norms embedded within web forms through a measurement
study. First, we build a specialized crawler to discover web forms on websites.
We run it on 11,500 popular websites, and we create a dataset of 293K web
forms. Second, to process data of this scale, we develop a cost-efficient way
to annotate web forms with form types and personal information types, using
text classifiers trained with assistance of large language models (LLMs).
Third, by analyzing the annotated dataset, we reveal common patterns of data
collection practices. We find that (i) these patterns are explained by
functional necessities and legal obligations, thus reflecting privacy norms,
and that (ii) deviations from the observed norms often signal unnecessary data
collection. In addition, we analyze the privacy policies that accompany web
forms. We show that, despite their wide adoption and use, there is a disconnect
between privacy policy disclosures and the observed privacy norms.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.15694v1">Protecting Privacy in Federated Time Series Analysis: A Pragmatic
  Technology Review for Application Developers</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-28T10:41:31Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Daniel Bachlechner, Ruben Hetfleisch, Stephan Krenn, Thomas Lorünser, Michael Rader</p>
    <p><b>Summary:</b> The federated analysis of sensitive time series has huge potential in various
domains, such as healthcare or manufacturing. Yet, to fully unlock this
potential, requirements imposed by various stakeholders must be fulfilled,
regarding, e.g., efficiency or trust assumptions. While many of these
requirements can be addressed by deploying advanced secure computation
paradigms such as fully homomorphic encryption, certain aspects require an
integration with additional privacy-preserving technologies.
  In this work, we perform a qualitative requirements elicitation based on
selected real-world use cases. We match the derived requirements categories
against the features and guarantees provided by available technologies. For
each technology, we additionally perform a maturity assessment, including the
state of standardization and availability on the market. Furthermore, we
provide a decision tree supporting application developers in identifying the
most promising technologies available matching their needs. Finally, existing
gaps are identified, highlighting research potential to advance the field.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.15688v1">PDSR: A Privacy-Preserving Diversified Service Recommendation Method on
  Distributed Data</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Retrieval-5BC0EB">
  <p><b>Published on:</b> 2024-08-28T10:25:36Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Lina Wang, Huan Yang, Yiran Shen, Chao Liu, Lianyong Qi, Xiuzhen Cheng, Feng Li</p>
    <p><b>Summary:</b> The last decade has witnessed a tremendous growth of service computing, while
efficient service recommendation methods are desired to recommend high-quality
services to users. It is well known that collaborative filtering is one of the
most popular methods for service recommendation based on QoS, and many existing
proposals focus on improving recommendation accuracy, i.e., recommending
high-quality redundant services. Nevertheless, users may have different
requirements on QoS, and hence diversified recommendation has been attracting
increasing attention in recent years to fulfill users' diverse demands and to
explore potential services. Unfortunately, the recommendation performances
relies on a large volume of data (e.g., QoS data), whereas the data may be
distributed across multiple platforms. Therefore, to enable data sharing across
the different platforms for diversified service recommendation, we propose a
Privacy-preserving Diversified Service Recommendation (PDSR) method.
Specifically, we innovate in leveraging the Locality-Sensitive Hashing (LSH)
mechanism such that privacy-preserved data sharing across different platforms
is enabled to construct a service similarity graph. Based on the similarity
graph, we propose a novel accuracy-diversity metric and design a
$2$-approximation algorithm to select $K$ services to recommend by maximizing
the accuracy-diversity measure. Extensive experiments on real datasets are
conducted to verify the efficacy of our PDSR method.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.15621v1">Convergent Differential Privacy Analysis for General Federated Learning:
  the f-DP Perspective</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-28T08:22:21Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yan Sun, Li Shen, Dacheng Tao</p>
    <p><b>Summary:</b> Federated learning (FL) is an efficient collaborative training paradigm
extensively developed with a focus on local privacy protection, and
differential privacy (DP) is a classical approach to capture and ensure the
reliability of local privacy. The powerful cooperation of FL and DP provides a
promising learning framework for large-scale private clients, juggling both
privacy securing and trustworthy learning. As the predominant algorithm of DP,
the noisy perturbation has been widely studied and incorporated into various
federated algorithms, theoretically proven to offer significant privacy
protections. However, existing analyses in noisy FL-DP mostly rely on the
composition theorem and cannot tightly quantify the privacy leakage challenges,
which is nearly tight for small numbers of communication rounds but yields an
arbitrarily loose and divergent bound under the large communication rounds.
This implies a counterintuitive judgment, suggesting that FL may not provide
adequate privacy protection during long-term training. To further investigate
the convergent privacy and reliability of the FL-DP framework, in this paper,
we comprehensively evaluate the worst privacy of two classical methods under
the non-convex and smooth objectives based on the f-DP analysis, i.e.
Noisy-FedAvg and Noisy-FedProx methods. With the aid of the
shifted-interpolation technique, we successfully prove that the worst privacy
of the Noisy-FedAvg method achieves a tight convergent lower bound. Moreover,
in the Noisy-FedProx method, with the regularization of the proxy term, the
worst privacy has a stable constant lower bound. Our analysis further provides
a solid theoretical foundation for the reliability of privacy protection in
FL-DP. Meanwhile, our conclusions can also be losslessly converted to other
classical DP analytical frameworks, e.g. $(\epsilon,\delta)$-DP and
R$\acute{\text{e}}$nyi-DP (RDP).</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.15391v2">Examining the Interplay Between Privacy and Fairness for Speech
  Processing: A Review and Perspective</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Sound-D91E36">
  <p><b>Published on:</b> 2024-08-27T20:32:01Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Anna Leschanowsky, Sneha Das</p>
    <p><b>Summary:</b> Speech technology has been increasingly deployed in various areas of daily
life including sensitive domains such as healthcare and law enforcement. For
these technologies to be effective, they must work reliably for all users while
preserving individual privacy. Although tradeoffs between privacy and utility,
as well as fairness and utility, have been extensively researched, the specific
interplay between privacy and fairness in speech processing remains
underexplored. This review and position paper offers an overview of emerging
privacy-fairness tradeoffs throughout the entire machine learning lifecycle for
speech processing. By drawing on well-established frameworks on fairness and
privacy, we examine existing biases and sources of privacy harm that coexist
during the development of speech processing models. We then highlight how
corresponding privacy-enhancing technologies have the potential to
inadvertently increase these biases and how bias mitigation strategies may
conversely reduce privacy. By raising open questions, we advocate for a
comprehensive evaluation of privacy-fairness tradeoffs for speech technology
and the development of privacy-enhancing and fairness-aware algorithms in this
domain.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.15077v2">MMASD+: A Novel Dataset for Privacy-Preserving Behavior Analysis of
  Children with Autism Spectrum Disorder</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-08-27T14:05:48Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Pavan Uttej Ravva, Behdokht Kiafar, Pinar Kullu, Jicheng Li, Anjana Bhat, Roghayeh Leila Barmaki</p>
    <p><b>Summary:</b> Autism spectrum disorder (ASD) is characterized by significant challenges in
social interaction and comprehending communication signals. Recently,
therapeutic interventions for ASD have increasingly utilized Deep learning
powered-computer vision techniques to monitor individual progress over time.
These models are trained on private, non-public datasets from the autism
community, creating challenges in comparing results across different models due
to privacy-preserving data-sharing issues. This work introduces MMASD+, an
enhanced version of the novel open-source dataset called Multimodal ASD
(MMASD). MMASD+ consists of diverse data modalities, including 3D-Skeleton, 3D
Body Mesh, and Optical Flow data. It integrates the capabilities of Yolov8 and
Deep SORT algorithms to distinguish between the therapist and children,
addressing a significant barrier in the original dataset. Additionally, a
Multimodal Transformer framework is proposed to predict 11 action types and the
presence of ASD. This framework achieves an accuracy of 95.03% for predicting
action types and 96.42% for predicting ASD presence, demonstrating over a 10%
improvement compared to models trained on single data modalities. These
findings highlight the advantages of integrating multiple data modalities
within the Multimodal Transformer framework.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.14830v1">PolicyLR: A Logic Representation For Privacy Policies</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2024-08-27T07:27:16Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ashish Hooda, Rishabh Khandelwal, Prasad Chalasani, Kassem Fawaz, Somesh Jha</p>
    <p><b>Summary:</b> Privacy policies are crucial in the online ecosystem, defining how services
handle user data and adhere to regulations such as GDPR and CCPA. However,
their complexity and frequent updates often make them difficult for
stakeholders to understand and analyze. Current automated analysis methods,
which utilize natural language processing, have limitations. They typically
focus on individual tasks and fail to capture the full context of the policies.
We propose PolicyLR, a new paradigm that offers a comprehensive
machine-readable representation of privacy policies, serving as an all-in-one
solution for multiple downstream tasks. PolicyLR converts privacy policies into
a machine-readable format using valuations of atomic formulae, allowing for
formal definitions of tasks like compliance and consistency. We have developed
a compiler that transforms unstructured policy text into this format using
off-the-shelf Large Language Models (LLMs). This compiler breaks down the
transformation task into a two-stage translation and entailment procedure. This
procedure considers the full context of the privacy policy to infer a complex
formula, where each formula consists of simpler atomic formulae. The advantage
of this model is that PolicyLR is interpretable by design and grounded in
segments of the privacy policy. We evaluated the compiler using ToS;DR, a
community-annotated privacy policy entailment dataset. Utilizing open-source
LLMs, our compiler achieves precision and recall values of 0.91 and 0.88,
respectively. Finally, we demonstrate the utility of PolicyLR in three privacy
tasks: Policy Compliance, Inconsistency Detection, and Privacy Comparison
Shopping.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.14753v1">CoopASD: Cooperative Machine Anomalous Sound Detection with Privacy
  Concerns</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Sound-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB"> 
  <p><b>Published on:</b> 2024-08-27T03:07:03Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Anbai Jiang, Yuchen Shi, Pingyi Fan, Wei-Qiang Zhang, Jia Liu</p>
    <p><b>Summary:</b> Machine anomalous sound detection (ASD) has emerged as one of the most
promising applications in the Industrial Internet of Things (IIoT) due to its
unprecedented efficacy in mitigating risks of malfunctions and promoting
production efficiency. Previous works mainly investigated the machine ASD task
under centralized settings. However, developing the ASD system under
decentralized settings is crucial in practice, since the machine data are
dispersed in various factories and the data should not be explicitly shared due
to privacy concerns. To enable these factories to cooperatively develop a
scalable ASD model while preserving their privacy, we propose a novel framework
named CoopASD, where each factory trains an ASD model on its local dataset, and
a central server aggregates these local models periodically. We employ a
pre-trained model as the backbone of the ASD model to improve its robustness
and develop specialized techniques to stabilize the model under a completely
non-iid and domain shift setting. Compared with previous state-of-the-art
(SOTA) models trained in centralized settings, CoopASD showcases competitive
results with negligible degradation of 0.08%. We also conduct extensive
ablation studies to demonstrate the effectiveness of CoopASD.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.14735v1">PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework
  with Correlated Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Multimedia-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2024-08-27T02:03:36Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xianzhi Zhang, Yipeng Zhou, Di Wu, Quan Z. Sheng, Miao Hu, Linchang Xiao</p>
    <p><b>Summary:</b> Online video streaming has evolved into an integral component of the
contemporary Internet landscape. Yet, the disclosure of user requests presents
formidable privacy challenges. As users stream their preferred online videos,
their requests are automatically seized by video content providers, potentially
leaking users' privacy.
  Unfortunately, current protection methods are not well-suited to preserving
user request privacy from content providers while maintaining high-quality
online video services. To tackle this challenge, we introduce a novel
Privacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge
devices to pre-fetch and cache videos, ensuring the privacy of users' requests
while optimizing the efficiency of edge caching. More specifically, we design
PPVF with three core components: (1) \textit{Online privacy budget scheduler},
which employs a theoretically guaranteed online algorithm to select
non-requested videos as candidates with assigned privacy budgets. Alternative
videos are chosen by an online algorithm that is theoretically guaranteed to
consider both video utilities and available privacy budgets. (2) \textit{Noisy
video request generator}, which generates redundant video requests (in addition
to original ones) utilizing correlated differential privacy to obfuscate
request privacy. (3) \textit{Online video utility predictor}, which leverages
federated learning to collaboratively evaluate video utility in an online
fashion, aiding in video selection in (1) and noise generation in (2). Finally,
we conduct extensive experiments using real-world video request traces from
Tencent Video. The results demonstrate that PPVF effectively safeguards user
request privacy while upholding high video caching performance.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.14689v1">Federated User Preference Modeling for Privacy-Preserving Cross-Domain
  Recommendation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Retrieval-5BC0EB">
  <p><b>Published on:</b> 2024-08-26T23:29:03Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Li Wang, Shoujin Wang, Quangui Zhang, Qiang Wu, Min Xu</p>
    <p><b>Summary:</b> Cross-domain recommendation (CDR) aims to address the data-sparsity problem
by transferring knowledge across domains. Existing CDR methods generally assume
that the user-item interaction data is shareable between domains, which leads
to privacy leakage. Recently, some privacy-preserving CDR (PPCDR) models have
been proposed to solve this problem. However, they primarily transfer simple
representations learned only from user-item interaction histories, overlooking
other useful side information, leading to inaccurate user preferences.
Additionally, they transfer differentially private user-item interaction
matrices or embeddings across domains to protect privacy. However, these
methods offer limited privacy protection, as attackers may exploit external
information to infer the original data. To address these challenges, we propose
a novel Federated User Preference Modeling (FUPM) framework. In FUPM, first, a
novel comprehensive preference exploration module is proposed to learn users'
comprehensive preferences from both interaction data and additional data
including review texts and potentially positive items. Next, a private
preference transfer module is designed to first learn differentially private
local and global prototypes, and then privately transfer the global prototypes
using a federated learning strategy. These prototypes are generalized
representations of user groups, making it difficult for attackers to infer
individual information. Extensive experiments on four CDR tasks conducted on
the Amazon and Douban datasets validate the superiority of FUPM over SOTA
baselines. Code is available at https://github.com/Lili1013/FUPM.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.14329v1">PHEVA: A Privacy-preserving Human-centric Video Anomaly Detection
  Dataset</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-08-26T14:55:23Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ghazal Alinezhad Noghre, Shanle Yao, Armin Danesh Pazho, Babak Rahimi Ardabili, Vinit Katariya, Hamed Tabkhi</p>
    <p><b>Summary:</b> PHEVA, a Privacy-preserving Human-centric Ethical Video Anomaly detection
dataset. By removing pixel information and providing only de-identified human
annotations, PHEVA safeguards personally identifiable information. The dataset
includes seven indoor/outdoor scenes, featuring one novel, context-specific
camera, and offers over 5x the pose-annotated frames compared to the largest
previous dataset. This study benchmarks state-of-the-art methods on PHEVA using
a comprehensive set of metrics, including the 10% Error Rate (10ER), a metric
used for anomaly detection for the first time providing insights relevant to
real-world deployment. As the first of its kind, PHEVA bridges the gap between
conventional training and real-world deployment by introducing continual
learning benchmarks, with models outperforming traditional methods in 82.14% of
cases. The dataset is publicly available at
https://github.com/TeCSAR-UNCC/PHEVA.git.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.13460v1">DOPPLER: Differentially Private Optimizers with Low-pass Filter for
  Privacy Noise Reduction</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2024-08-24T04:27:07Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xinwei Zhang, Zhiqi Bu, Mingyi Hong, Meisam Razaviyayn</p>
    <p><b>Summary:</b> Privacy is a growing concern in modern deep-learning systems and
applications. Differentially private (DP) training prevents the leakage of
sensitive information in the collected training data from the trained machine
learning models. DP optimizers, including DP stochastic gradient descent
(DPSGD) and its variants, privatize the training procedure by gradient clipping
and DP noise injection. However, in practice, DP models trained using DPSGD and
its variants often suffer from significant model performance degradation. Such
degradation prevents the application of DP optimization in many key tasks, such
as foundation model pretraining. In this paper, we provide a novel signal
processing perspective to the design and analysis of DP optimizers. We show
that a ``frequency domain'' operation called low-pass filtering can be used to
effectively reduce the impact of DP noise. More specifically, by defining the
``frequency domain'' for both the gradient and differential privacy (DP) noise,
we have developed a new component, called DOPPLER. This component is designed
for DP algorithms and works by effectively amplifying the gradient while
suppressing DP noise within this frequency domain. As a result, it maintains
privacy guarantees and enhances the quality of the DP-protected model. Our
experiments show that the proposed DP optimizers with a low-pass filter
outperform their counterparts without the filter by 3%-10% in test accuracy on
various models and datasets. Both theoretical and practical evidence suggest
that the DOPPLER is effective in closing the gap between DP and non-DP
training.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.13424v1">Enabling Humanitarian Applications with Targeted Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-24T01:34:37Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Nitin Kohli, Joshua Blumenstock</p>
    <p><b>Summary:</b> The proliferation of mobile phones in low- and middle-income countries has
suddenly and dramatically increased the extent to which the world's poorest and
most vulnerable populations can be observed and tracked by governments and
corporations. Millions of historically "off the grid" individuals are now
passively generating digital data; these data, in turn, are being used to make
life-altering decisions about those individuals -- including whether or not
they receive government benefits, and whether they qualify for a consumer loan.
  This paper develops an approach to implementing algorithmic decisions based
on personal data, while also providing formal privacy guarantees to data
subjects. The approach adapts differential privacy to applications that require
decisions about individuals, and gives decision makers granular control over
the level of privacy guaranteed to data subjects. We show that stronger privacy
guarantees typically come at some cost, and use data from two real-world
applications -- an anti-poverty program in Togo and a consumer lending platform
in Nigeria -- to illustrate those costs. Our empirical results quantify the
tradeoff between privacy and predictive accuracy, and characterize how
different privacy guarantees impact overall program effectiveness. More
broadly, our results demonstrate a way for humanitarian programs to responsibly
use personal data, and better equip program designers to make informed
decisions about data privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.13038v1">Improving the Classification Effect of Clinical Images of Diseases for
  Multi-Source Privacy Protection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-08-23T12:52:24Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tian Bowen, Xu Zhengyang, Yin Zhihao, Wang Jingying, Yue Yutao</p>
    <p><b>Summary:</b> Privacy data protection in the medical field poses challenges to data
sharing, limiting the ability to integrate data across hospitals for training
high-precision auxiliary diagnostic models. Traditional centralized training
methods are difficult to apply due to violations of privacy protection
principles. Federated learning, as a distributed machine learning framework,
helps address this issue, but it requires multiple hospitals to participate in
training simultaneously, which is hard to achieve in practice. To address these
challenges, we propose a medical privacy data training framework based on data
vectors. This framework allows each hospital to fine-tune pre-trained models on
private data, calculate data vectors (representing the optimization direction
of model parameters in the solution space), and sum them up to generate
synthetic weights that integrate model information from multiple hospitals.
This approach enhances model performance without exchanging private data or
requiring synchronous training. Experimental results demonstrate that this
method effectively utilizes dispersed private data resources while protecting
patient privacy. The auxiliary diagnostic model trained using this approach
significantly outperforms models trained independently by a single hospital,
providing a new perspective for resolving the conflict between medical data
privacy protection and model training and advancing the development of medical
intelligence.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.12787v2">LLM-PBE: Assessing Data Privacy in Large Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-08-23T01:37:29Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Qinbin Li, Junyuan Hong, Chulin Xie, Jeffrey Tan, Rachel Xin, Junyi Hou, Xavier Yin, Zhun Wang, Dan Hendrycks, Zhangyang Wang, Bo Li, Bingsheng He, Dawn Song</p>
    <p><b>Summary:</b> Large Language Models (LLMs) have become integral to numerous domains,
significantly advancing applications in data management, mining, and analysis.
Their profound capabilities in processing and interpreting complex language
data, however, bring to light pressing concerns regarding data privacy,
especially the risk of unintentional training data leakage. Despite the
critical nature of this issue, there has been no existing literature to offer a
comprehensive assessment of data privacy risks in LLMs. Addressing this gap,
our paper introduces LLM-PBE, a toolkit crafted specifically for the systematic
evaluation of data privacy risks in LLMs. LLM-PBE is designed to analyze
privacy across the entire lifecycle of LLMs, incorporating diverse attack and
defense strategies, and handling various data types and metrics. Through
detailed experimentation with multiple LLMs, LLM-PBE facilitates an in-depth
exploration of data privacy concerns, shedding light on influential factors
such as model size, data characteristics, and evolving temporal dimensions.
This study not only enriches the understanding of privacy issues in LLMs but
also serves as a vital resource for future research in the field. Aimed at
enhancing the breadth of knowledge in this area, the findings, resources, and
our full technical report are made available at https://llm-pbe.github.io/,
providing an open platform for academic and practical advancements in LLM
privacy assessment.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.12385v1">Sharper Bounds for Chebyshev Moment Matching with Applications to
  Differential Privacy and Beyond</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-08-22T13:26:41Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Cameron Musco, Christopher Musco, Lucas Rosenblatt, Apoorv Vikram Singh</p>
    <p><b>Summary:</b> We study the problem of approximately recovering a probability distribution
given noisy measurements of its Chebyshev polynomial moments. We sharpen prior
work, proving that accurate recovery in the Wasserstein distance is possible
with more noise than previously known.
  As a main application, our result yields a simple "linear query" algorithm
for constructing a differentially private synthetic data distribution with
Wasserstein-1 error $\tilde{O}(1/n)$ based on a dataset of $n$ points in
$[-1,1]$. This bound is optimal up to log factors and matches a recent
breakthrough of Boedihardjo, Strohmer, and Vershynin [Probab. Theory. Rel.,
2024], which uses a more complex "superregular random walk" method to beat an
$O(1/\sqrt{n})$ accuracy barrier inherent to earlier approaches.
  We illustrate a second application of our new moment-based recovery bound in
numerical linear algebra: by improving an approach of Braverman, Krishnan, and
Musco [STOC 2022], our result yields a faster algorithm for estimating the
spectral density of a symmetric matrix up to small error in the Wasserstein
distance.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.12353v1">Distributed quasi-Newton robust estimation under differential privacy</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Statistics Theory-D91E36"> 
  <p><b>Published on:</b> 2024-08-22T12:51:28Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chuhan Wang, Lixing Zhu, Xuehu Zhu</p>
    <p><b>Summary:</b> For distributed computing with Byzantine machines under Privacy Protection
(PP) constraints, this paper develops a robust PP distributed quasi-Newton
estimation, which only requires the node machines to transmit five vectors to
the central processor with high asymptotic relative efficiency. Compared with
the gradient descent strategy which requires more rounds of transmission and
the Newton iteration strategy which requires the entire Hessian matrix to be
transmitted, the novel quasi-Newton iteration has advantages in reducing
privacy budgeting and transmission cost. Moreover, our PP algorithm does not
depend on the boundedness of gradients and second-order derivatives. When
gradients and second-order derivatives follow sub-exponential distributions, we
offer a mechanism that can ensure PP with a sufficiently high probability.
Furthermore, this novel estimator can achieve the optimal convergence rate and
the asymptotic normality. The numerical studies on synthetic and real data sets
evaluate the performance of the proposed algorithm.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.12010v1">Confounding Privacy and Inverse Composition</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-21T21:45:13Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tao Zhang, Bradley A. Malin, Netanel Raviv, Yevgeniy Vorobeychik</p>
    <p><b>Summary:</b> We introduce a novel privacy notion of ($\epsilon, \delta$)-confounding
privacy that generalizes both differential privacy and Pufferfish privacy. In
differential privacy, sensitive information is contained in the dataset while
in Pufferfish privacy, sensitive information determines data distribution.
Consequently, both assume a chain-rule relationship between the sensitive
information and the output of privacy mechanisms. Confounding privacy, in
contrast, considers general causal relationships between the dataset and
sensitive information. One of the key properties of differential privacy is
that it can be easily composed over multiple interactions with the mechanism
that maps private data to publicly shared information. In contrast, we show
that the quantification of the privacy loss under the composition of
independent ($\epsilon, \delta$)-confounding private mechanisms using the
optimal composition of differential privacy \emph{underestimates} true privacy
loss. To address this, we characterize an inverse composition framework to
tightly implement a target global ($\epsilon_{g}, \delta_{g}$)-confounding
privacy under composition while keeping individual mechanisms independent and
private. In particular, we propose a novel copula-perturbation method which
ensures that (1) each individual mechanism $i$ satisfies a target local
($\epsilon_{i}, \delta_{i}$)-confounding privacy and (2) the target global
($\epsilon_{g}, \delta_{g}$)-confounding privacy is tightly implemented by
solving an optimization problem. Finally, we study inverse composition
empirically on real datasets.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.11649v1">Video-to-Text Pedestrian Monitoring (VTPM): Leveraging Computer Vision
  and Large Language Models for Privacy-Preserve Pedestrian Activity Monitoring
  at Intersections</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-08-21T14:21:53Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ahmed S. Abdelrahman, Mohamed Abdel-Aty, Dongdong Wang</p>
    <p><b>Summary:</b> Computer vision has advanced research methodologies, enhancing system
services across various fields. It is a core component in traffic monitoring
systems for improving road safety; however, these monitoring systems don't
preserve the privacy of pedestrians who appear in the videos, potentially
revealing their identities. Addressing this issue, our paper introduces
Video-to-Text Pedestrian Monitoring (VTPM), which monitors pedestrian movements
at intersections and generates real-time textual reports, including traffic
signal and weather information. VTPM uses computer vision models for pedestrian
detection and tracking, achieving a latency of 0.05 seconds per video frame.
Additionally, it detects crossing violations with 90.2% accuracy by
incorporating traffic signal data. The proposed framework is equipped with
Phi-3 mini-4k to generate real-time textual reports of pedestrian activity
while stating safety concerns like crossing violations, conflicts, and the
impact of weather on their behavior with latency of 0.33 seconds. To enhance
comprehensive analysis of the generated textual reports, Phi-3 medium is
fine-tuned for historical analysis of these generated textual reports. This
fine-tuning enables more reliable analysis about the pedestrian safety at
intersections, effectively detecting patterns and safety critical events. The
proposed VTPM offers a more efficient alternative to video footage by using
textual reports reducing memory usage, saving up to 253 million percent,
eliminating privacy issues, and enabling comprehensive interactive historical
analysis.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.11290v1">Privacy Preservation in Delay-Based Localization Systems: Artificial
  Noise or Artificial Multipath?</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2024-08-21T02:38:44Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yuchen Zhang, Hui Chen, Henk Wymeersch</p>
    <p><b>Summary:</b> Localization plays an increasingly pivotal role in 5G/6G systems, enabling
various applications. This paper focuses on the privacy concerns associated
with delay-based localization, where unauthorized base stations attempt to
infer the location of the end user. We propose a method to disrupt localization
at unauthorized nodes by injecting artificial components into the pilot signal,
exploiting model mismatches inherent in these nodes. Specifically, we
investigate the effectiveness of two techniques, namely artificial multipath
(AM) and artificial noise (AN), in mitigating location leakage. By leveraging
the misspecified Cram\'er-Rao bound framework, we evaluate the impact of these
techniques on unauthorized localization performance. Our results demonstrate
that pilot manipulation significantly degrades the accuracy of unauthorized
localization while minimally affecting legitimate localization. Moreover, we
find that the superiority of AM over AN varies depending on the specific
scenario.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.11263v1">Privacy-Preserving Data Management using Blockchains</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB">
  <p><b>Published on:</b> 2024-08-21T01:10:39Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Michael Mireku Kwakye</p>
    <p><b>Summary:</b> Privacy-preservation policies are guidelines formulated to protect data
providers private data. Previous privacy-preservation methodologies have
addressed privacy in which data are permanently stored in repositories and
disconnected from changing data provider privacy preferences. This occurrence
becomes evident as data moves to another data repository. Hence, the need for
data providers to control and flexibly update their existing privacy
preferences due to changing data usage continues to remain a problem. This
paper proposes a blockchain-based methodology for preserving data providers
private and sensitive data. The research proposes to tightly couple data
providers private attribute data element to privacy preferences and data
accessor data element into a privacy tuple. The implementation presents a
framework of tightly-coupled relational database and blockchains. This delivers
secure, tamper-resistant, and query-efficient platform for data management and
query processing. The evaluation analysis from the implementation validates
efficient query processing of privacy-aware queries on the privacy
infrastructure.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.12387v1">Makeup-Guided Facial Privacy Protection via Untrained Neural Network
  Priors</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-08-20T17:59:39Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Fahad Shamshad, Muzammal Naseer, Karthik Nandakumar</p>
    <p><b>Summary:</b> Deep learning-based face recognition (FR) systems pose significant privacy
risks by tracking users without their consent. While adversarial attacks can
protect privacy, they often produce visible artifacts compromising user
experience. To mitigate this issue, recent facial privacy protection approaches
advocate embedding adversarial noise into the natural looking makeup styles.
However, these methods require training on large-scale makeup datasets that are
not always readily available. In addition, these approaches also suffer from
dataset bias. For instance, training on makeup data that predominantly contains
female faces could compromise protection efficacy for male faces. To handle
these issues, we propose a test-time optimization approach that solely
optimizes an untrained neural network to transfer makeup style from a reference
to a source image in an adversarial manner. We introduce two key modules: a
correspondence module that aligns regions between reference and source images
in latent space, and a decoder with conditional makeup layers. The untrained
decoder, optimized via carefully designed structural and makeup consistency
losses, generates a protected image that resembles the source but incorporates
adversarial makeup to deceive FR models. As our approach does not rely on
training with makeup face datasets, it avoids potential male/female dataset
biases while providing effective protection. We further extend the proposed
approach to videos by leveraging on temporal correlations. Experiments on
benchmark datasets demonstrate superior performance in face verification and
identification tasks and effectiveness against commercial FR systems. Our code
and models will be available at
https://github.com/fahadshamshad/deep-facial-privacy-prior</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.10715v1">Fine-Tuning a Local LLaMA-3 Large Language Model for Automated
  Privacy-Preserving Physician Letter Generation in Radiation Oncology</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-08-20T10:31:36Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yihao Hou, Christoph Bert, Ahmed Gomaa, Godehard Lahmer, Daniel Hoefler, Thomas Weissmann, Raphaela Voigt, Philipp Schubert, Charlotte Schmitter, Alina Depardon, Sabine Semrau, Andreas Maier, Rainer Fietkau, Yixing Huang, Florian Putz</p>
    <p><b>Summary:</b> Generating physician letters is a time-consuming task in daily clinical
practice. This study investigates local fine-tuning of large language models
(LLMs), specifically LLaMA models, for physician letter generation in a
privacy-preserving manner within the field of radiation oncology. Our findings
demonstrate that base LLaMA models, without fine-tuning, are inadequate for
effectively generating physician letters. The QLoRA algorithm provides an
efficient method for local intra-institutional fine-tuning of LLMs with limited
computational resources (i.e., a single 48 GB GPU workstation within the
hospital). The fine-tuned LLM successfully learns radiation oncology-specific
information and generates physician letters in an institution-specific style.
ROUGE scores of the generated summary reports highlight the superiority of the
8B LLaMA-3 model over the 13B LLaMA-2 model. Further multidimensional physician
evaluations of 10 cases reveal that, although the fine-tuned LLaMA-3 model has
limited capacity to generate content beyond the provided input data, it
successfully generates salutations, diagnoses and treatment histories,
recommendations for further treatment, and planned schedules. Overall, clinical
benefit was rated highly by the clinical experts (average score of 3.44 on a
4-point scale). With careful physician review and correction, automated
LLM-based physician letter generation has significant practical value.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.10648v1">Smart Contract Coordinated Privacy Preserving Crowd-Sensing Campaigns</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Networking and Internet Architecture-04E762">
  <p><b>Published on:</b> 2024-08-20T08:41:57Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Luca Bedogni, Stefano Ferretti</p>
    <p><b>Summary:</b> Crowd-sensing has emerged as a powerful data retrieval model, enabling
diverse applications by leveraging active user participation. However, data
availability and privacy concerns pose significant challenges. Traditional
methods like data encryption and anonymization, while essential, may not fully
address these issues. For instance, in sparsely populated areas, anonymized
data can still be traced back to individual users. Additionally, the volume of
data generated by users can reveal their identities. To develop credible
crowd-sensing systems, data must be anonymized, aggregated and separated into
uniformly sized chunks. Furthermore, decentralizing the data management
process, rather than relying on a single server, can enhance security and
trust. This paper proposes a system utilizing smart contracts and blockchain
technologies to manage crowd-sensing campaigns. The smart contract handles user
subscriptions, data encryption, and decentralized storage, creating a secure
data marketplace. Incentive policies within the smart contract encourage user
participation and data diversity. Simulation results confirm the system's
viability, highlighting the importance of user participation for data
credibility and the impact of geographical data scarcity on rewards. This
approach aims to balance data origin and reduce cheating risks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.10647v1">Privacy-preserving Universal Adversarial Defense for Black-box Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2024-08-20T08:40:39Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Qiao Li, Cong Wu, Jing Chen, Zijun Zhang, Kun He, Ruiying Du, Xinxin Wang, Qingchuang Zhao, Yang Liu</p>
    <p><b>Summary:</b> Deep neural networks (DNNs) are increasingly used in critical applications
such as identity authentication and autonomous driving, where robustness
against adversarial attacks is crucial. These attacks can exploit minor
perturbations to cause significant prediction errors, making it essential to
enhance the resilience of DNNs. Traditional defense methods often rely on
access to detailed model information, which raises privacy concerns, as model
owners may be reluctant to share such data. In contrast, existing black-box
defense methods fail to offer a universal defense against various types of
adversarial attacks. To address these challenges, we introduce DUCD, a
universal black-box defense method that does not require access to the target
model's parameters or architecture. Our approach involves distilling the target
model by querying it with data, creating a white-box surrogate while preserving
data privacy. We further enhance this surrogate model using a certified defense
based on randomized smoothing and optimized noise selection, enabling robust
defense against a broad range of adversarial attacks. Comparative evaluations
between the certified defenses of the surrogate and target models demonstrate
the effectiveness of our approach. Experiments on multiple image classification
datasets show that DUCD not only outperforms existing black-box defenses but
also matches the accuracy of white-box defenses, all while enhancing data
privacy and reducing the success rate of membership inference attacks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.10468v4">Tracing Privacy Leakage of Language Models to Training Data via Adjusted
  Influence Functions</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-20T00:40:49Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jinxin Liu, Zao Yang</p>
    <p><b>Summary:</b> The responses generated by Large Language Models (LLMs) can include sensitive
information from individuals and organizations, leading to potential privacy
leakage. This work implements Influence Functions (IFs) to trace privacy
leakage back to the training data, thereby mitigating privacy concerns of
Language Models (LMs). However, we notice that current IFs struggle to
accurately estimate the influence of tokens with large gradient norms,
potentially overestimating their influence. When tracing the most influential
samples, this leads to frequently tracing back to samples with large gradient
norm tokens, overshadowing the actual most influential samples even if their
influences are well estimated. To address this issue, we propose Heuristically
Adjusted IF (HAIF), which reduces the weight of tokens with large gradient
norms, thereby significantly improving the accuracy of tracing the most
influential samples. To establish easily obtained groundtruth for tracing
privacy leakage, we construct two datasets, PII-E and PII-CR, representing two
distinct scenarios: one with identical text in the model outputs and
pre-training data, and the other where models leverage their reasoning
abilities to generate text divergent from pre-training data. HAIF significantly
improves tracing accuracy, enhancing it by 20.96% to 73.71% on the PII-E
dataset and 3.21% to 45.93% on the PII-CR dataset, compared to the best SOTA
IFs against various GPT-2 and QWen-1.5 models. HAIF also outperforms SOTA IFs
on real-world pretraining data CLUECorpus2020, demonstrating strong robustness
regardless prompt and response lengths.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.10442v1">Feasibility of assessing cognitive impairment via distributed camera
  network and privacy-preserving edge computing</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-08-19T22:34:43Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chaitra Hegde, Yashar Kiarashi, Allan I Levey, Amy D Rodriguez, Hyeokhyen Kwon, Gari D Clifford</p>
    <p><b>Summary:</b> INTRODUCTION: Mild cognitive impairment (MCI) is characterized by a decline
in cognitive functions beyond typical age and education-related expectations.
Since, MCI has been linked to reduced social interactions and increased aimless
movements, we aimed to automate the capture of these behaviors to enhance
longitudinal monitoring.
  METHODS: Using a privacy-preserving distributed camera network, we collected
movement and social interaction data from groups of individuals with MCI
undergoing therapy within a 1700$m^2$ space. We developed movement and social
interaction features, which were then used to train a series of machine
learning algorithms to distinguish between higher and lower cognitive
functioning MCI groups.
  RESULTS: A Wilcoxon rank-sum test revealed statistically significant
differences between high and low-functioning cohorts in features such as linear
path length, walking speed, change in direction while walking, entropy of
velocity and direction change, and number of group formations in the indoor
space. Despite lacking individual identifiers to associate with specific levels
of MCI, a machine learning approach using the most significant features
provided a 71% accuracy.
  DISCUSSION: We provide evidence to show that a privacy-preserving low-cost
camera network using edge computing framework has the potential to distinguish
between different levels of cognitive impairment from the movements and social
interactions captured during group activities.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.10053v1">Privacy Checklist: Privacy Violation Detection Grounding on Contextual
  Integrity Theory</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-19T14:48:04Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Haoran Li, Wei Fan, Yulin Chen, Jiayang Cheng, Tianshu Chu, Xuebing Zhou, Peizhao Hu, Yangqiu Song</p>
    <p><b>Summary:</b> Privacy research has attracted wide attention as individuals worry that their
private data can be easily leaked during interactions with smart devices,
social platforms, and AI applications. Computer science researchers, on the
other hand, commonly study privacy issues through privacy attacks and defenses
on segmented fields. Privacy research is conducted on various sub-fields,
including Computer Vision (CV), Natural Language Processing (NLP), and Computer
Networks. Within each field, privacy has its own formulation. Though pioneering
works on attacks and defenses reveal sensitive privacy issues, they are
narrowly trapped and cannot fully cover people's actual privacy concerns.
Consequently, the research on general and human-centric privacy research
remains rather unexplored. In this paper, we formulate the privacy issue as a
reasoning problem rather than simple pattern matching. We ground on the
Contextual Integrity (CI) theory which posits that people's perceptions of
privacy are highly correlated with the corresponding social context. Based on
such an assumption, we develop the first comprehensive checklist that covers
social identities, private attributes, and existing privacy regulations. Unlike
prior works on CI that either cover limited expert annotated norms or model
incomplete social context, our proposed privacy checklist uses the whole Health
Insurance Portability and Accountability Act of 1996 (HIPAA) as an example, to
show that we can resort to large language models (LLMs) to completely cover the
HIPAA's regulations. Additionally, our checklist also gathers expert
annotations across multiple ontologies to determine private information
including but not limited to personally identifiable information (PII). We use
our preliminary results on the HIPAA to shed light on future context-centric
privacy research to cover more privacy regulations, social norms and standards.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.09943v2">Calibrating Noise for Group Privacy in Subsampled Mechanisms</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-19T12:32:50Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yangfan Jiang, Xinjian Luo, Yin Yang, Xiaokui Xiao</p>
    <p><b>Summary:</b> Given a group size m and a sensitive dataset D, group privacy (GP) releases
information about D with the guarantee that the adversary cannot infer with
high confidence whether the underlying data is D or a neighboring dataset D'
that differs from D by m records. GP generalizes the well-established notion of
differential privacy (DP) for protecting individuals' privacy; in particular,
when m=1, GP reduces to DP. Compared to DP, GP is capable of protecting the
sensitive aggregate information of a group of up to m individuals, e.g., the
average annual income among members of a yacht club. Despite its longstanding
presence in the research literature and its promising applications, GP is often
treated as an afterthought, with most approaches first developing a DP
mechanism and then using a generic conversion to adapt it for GP, treating the
DP solution as a black box. As we point out in the paper, this methodology is
suboptimal when the underlying DP solution involves subsampling, e.g., in the
classic DP-SGD method for training deep learning models. In this case, the
DP-to-GP conversion is overly pessimistic in its analysis, leading to low
utility in the published results under GP.
  Motivated by this, we propose a novel analysis framework that provides tight
privacy accounting for subsampled GP mechanisms. Instead of converting a
black-box DP mechanism to GP, our solution carefully analyzes and utilizes the
inherent randomness in subsampled mechanisms, leading to a substantially
improved bound on the privacy loss with respect to GP. The proposed solution
applies to a wide variety of foundational mechanisms with subsampling.
Extensive experiments with real datasets demonstrate that compared to the
baseline convert-from-blackbox-DP approach, our GP mechanisms achieve noise
reductions of over an order of magnitude in several practical settings,
including deep neural network training.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.09935v1">Privacy Technologies for Financial Intelligence</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-19T12:13:53Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yang Li, Thilina Ranbaduge, Kee Siong Ng</p>
    <p><b>Summary:</b> Financial crimes like terrorism financing and money laundering can have real
impacts on society, including the abuse and mismanagement of public funds,
increase in societal problems such as drug trafficking and illicit gambling
with attendant economic costs, and loss of innocent lives in the case of
terrorism activities. Complex financial crimes can be hard to detect primarily
because data related to different pieces of the overall puzzle is usually
distributed across a network of financial institutions, regulators, and
law-enforcement agencies and they cannot be easily shared due to privacy
constraints. Recent advances in Privacy-Preserving Data Matching and Machine
Learning provide an opportunity for regulators and the financial industry to
come together to solve the risk-discovery problem with technology. This paper
provides a survey of the financial intelligence landscape and where
opportunities lie for privacy technologies to improve the state-of-the-art in
financial-crime detection.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.09659v1">An Algorithm for Enhancing Privacy-Utility Tradeoff in the Privacy
  Funnel and Other Lift-based Measures</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2024-08-19T02:43:18Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mohammad Amin Zarrabian, Parastoo Sadeghi</p>
    <p><b>Summary:</b> This paper investigates the privacy funnel, a privacy-utility tradeoff
problem in which mutual information quantifies both privacy and utility. The
objective is to maximize utility while adhering to a specified privacy budget.
However, the privacy funnel represents a non-convex optimization problem,
making it challenging to achieve an optimal solution. An existing proposed
approach to this problem involves substituting the mutual information with the
lift (the exponent of information density) and then solving the optimization.
Since mutual information is the expectation of the information density, this
substitution overestimates the privacy loss and results in a final smaller
bound on the privacy of mutual information than what is allowed in the budget.
This significantly compromises the utility. To overcome this limitation, we
propose using a privacy measure that is more relaxed than the lift but stricter
than mutual information while still allowing the optimization to be efficiently
solved. Instead of directly using information density, our proposed measure is
the average of information density over the sensitive data distribution for
each observed data realization. We then introduce a heuristic algorithm capable
of achieving solutions that produce extreme privacy values, which enhances
utility. The numerical results confirm improved utility at the same privacy
budget compared to existing solutions in the literature. Additionally, we
explore two other privacy measures, $\ell_{1}$-norm and strong
$\chi^2$-divergence, demonstrating the applicability of our algorithm to these
lift-based measures. We evaluate the performance of our method by comparing its
output with previous works. Finally, we validate our heuristic approach with a
theoretical framework that estimates the optimal utility for strong
$\chi^2$-divergence, numerically showing a perfect match.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.08722v1">A Novel Buffered Federated Learning Framework for Privacy-Driven Anomaly
  Detection in IIoT</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-16T13:01:59Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Samira Kamali Poorazad, Chafika Benzaid, Tarik Taleb</p>
    <p><b>Summary:</b> Industrial Internet of Things (IIoT) is highly sensitive to data privacy and
cybersecurity threats. Federated Learning (FL) has emerged as a solution for
preserving privacy, enabling private data to remain on local IIoT clients while
cooperatively training models to detect network anomalies. However, both
synchronous and asynchronous FL architectures exhibit limitations, particularly
when dealing with clients with varying speeds due to data heterogeneity and
resource constraints. Synchronous architecture suffers from straggler effects,
while asynchronous methods encounter communication bottlenecks. Additionally,
FL models are prone to adversarial inference attacks aimed at disclosing
private training data. To address these challenges, we propose a Buffered FL
(BFL) framework empowered by homomorphic encryption for anomaly detection in
heterogeneous IIoT environments. BFL utilizes a novel weighted average time
approach to mitigate both straggler effects and communication bottlenecks,
ensuring fairness between clients with varying processing speeds through
collaboration with a buffer-based server. The performance results, derived from
two datasets, show the superiority of BFL compared to state-of-the-art FL
methods, demonstrating improved accuracy and convergence speed while enhancing
privacy preservation.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.08666v1">A Multivocal Literature Review on Privacy and Fairness in Federated
  Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-08-16T11:15:52Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Beatrice Balbierer, Lukas Heinlein, Domenique Zipperling, Niklas Kühl</p>
    <p><b>Summary:</b> Federated Learning presents a way to revolutionize AI applications by
eliminating the necessity for data sharing. Yet, research has shown that
information can still be extracted during training, making additional
privacy-preserving measures such as differential privacy imperative. To
implement real-world federated learning applications, fairness, ranging from a
fair distribution of performance to non-discriminative behaviour, must be
considered. Particularly in high-risk applications (e.g. healthcare), avoiding
the repetition of past discriminatory errors is paramount. As recent research
has demonstrated an inherent tension between privacy and fairness, we conduct a
multivocal literature review to examine the current methods to integrate
privacy and fairness in federated learning. Our analyses illustrate that the
relationship between privacy and fairness has been neglected, posing a critical
risk for real-world applications. We highlight the need to explore the
relationship between privacy, fairness, and performance, advocating for the
creation of integrated federated learning frameworks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.08642v1">The Power of Bias: Optimizing Client Selection in Federated Learning
  with Heterogeneous Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-08-16T10:19:27Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jiating Ma, Yipeng Zhou, Qi Li, Quan Z. Sheng, Laizhong Cui, Jiangchuan Liu</p>
    <p><b>Summary:</b> To preserve the data privacy, the federated learning (FL) paradigm emerges in
which clients only expose model gradients rather than original data for
conducting model training. To enhance the protection of model gradients in FL,
differentially private federated learning (DPFL) is proposed which incorporates
differentially private (DP) noises to obfuscate gradients before they are
exposed. Yet, an essential but largely overlooked problem in DPFL is the
heterogeneity of clients' privacy requirement, which can vary significantly
between clients and extremely complicates the client selection problem in DPFL.
In other words, both the data quality and the influence of DP noises should be
taken into account when selecting clients. To address this problem, we conduct
convergence analysis of DPFL under heterogeneous privacy, a generic client
selection strategy, popular DP mechanisms and convex loss. Based on convergence
analysis, we formulate the client selection problem to minimize the value of
loss function in DPFL with heterogeneous privacy, which is a convex
optimization problem and can be solved efficiently. Accordingly, we propose the
DPFL-BCS (biased client selection) algorithm. The extensive experiment results
with real datasets under both convex and non-convex loss functions indicate
that DPFL-BCS can remarkably improve model utility compared with the SOTA
baselines.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.08529v1">Privacy-Preserving Vision Transformer Using Images Encrypted with
  Restricted Random Permutation Matrices</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-08-16T04:57:21Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kouki Horio, Kiyoshi Nishikawa, Hitoshi Kiya</p>
    <p><b>Summary:</b> We propose a novel method for privacy-preserving fine-tuning vision
transformers (ViTs) with encrypted images. Conventional methods using encrypted
images degrade model performance compared with that of using plain images due
to the influence of image encryption. In contrast, the proposed encryption
method using restricted random permutation matrices can provide a higher
performance than the conventional ones.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.08475v2">Models Matter: Setting Accurate Privacy Expectations for Local and
  Central Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2024-08-16T01:21:57Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mary Anne Smart, Priyanka Nanayakkara, Rachel Cummings, Gabriel Kaptchuk, Elissa Redmiles</p>
    <p><b>Summary:</b> Differential privacy is a popular privacy-enhancing technology that has been
deployed both in industry and government agencies. Unfortunately, existing
explanations of differential privacy fail to set accurate privacy expectations
for data subjects, which depend on the choice of deployment model. We design
and evaluate new explanations of differential privacy for the local and central
models, drawing inspiration from prior work explaining other privacy-enhancing
technologies. We find that consequences-focused explanations in the style of
privacy nutrition labels that lay out the implications of differential privacy
are a promising approach for setting accurate privacy expectations. Further, we
find that while process-focused explanations are not enough to set accurate
privacy expectations, combining consequences-focused explanations with a brief
description of how differential privacy works leads to greater trust.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.08107v1">Communication-robust and Privacy-safe Distributed Estimation for
  Heterogeneous Community-level Behind-the-meter Solar Power Generation</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36">
  <p><b>Published on:</b> 2024-08-15T12:11:03Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jinglei Feng, Zhengshuo Li</p>
    <p><b>Summary:</b> The rapid growth of behind-the-meter (BTM) solar power generation systems
presents challenges for distribution system planning and scheduling due to
invisible solar power generation. To address the data leakage problem of
centralized machine-learning methods in BTM solar power generation estimation,
the federated learning (FL) method has been investigated for its distributed
learning capability. However, the conventional FL method has encountered
various challenges, including heterogeneity, communication failures, and
malicious privacy attacks. To overcome these challenges, this study proposes a
communication-robust and privacy-safe distributed estimation method for
heterogeneous community-level BTM solar power generation. Specifically, this
study adopts multi-task FL as the main structure and learns the common and
unique features of all communities. Simultaneously, it embeds an updated
parameters estimation method into the multi-task FL, automatically identifies
similarities between any two clients, and estimates the updated parameters for
unavailable clients to mitigate the negative effects of communication failures.
Finally, this study adopts a differential privacy mechanism under the dynamic
privacy budget allocation strategy to combat malicious privacy attacks and
improve model training efficiency. Case studies show that in the presence of
heterogeneity and communication failures, the proposed method exhibits better
estimation accuracy and convergence performance as compared with traditional FL
and localized learning methods, while providing stronger privacy protection.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.08002v1">Practical Privacy-Preserving Identity Verification using Third-Party
  Cloud Services and FHE (Role of Data Encoding in Circuit Depth Management)</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-15T08:12:07Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Deep Inder Mohan, Srinivas Vivek</p>
    <p><b>Summary:</b> National digital identity verification systems have played a critical role in
the effective distribution of goods and services, particularly, in developing
countries. Due to the cost involved in deploying and maintaining such systems,
combined with a lack of in-house technical expertise, governments seek to
outsource this service to third-party cloud service providers to the extent
possible. This leads to increased concerns regarding the privacy of users'
personal data. In this work, we propose a practical privacy-preserving digital
identity (ID) verification protocol where the third-party cloud services
process the identity data encrypted using a (single-key) Fully Homomorphic
Encryption (FHE) scheme such as BFV. Though the role of a trusted entity such
as government is not completely eliminated, our protocol does significantly
reduces the computation load on such parties.
  A challenge in implementing a privacy-preserving ID verification protocol
using FHE is to support various types of queries such as exact and/or fuzzy
demographic and biometric matches including secure age comparisons. From a
cryptographic engineering perspective, our main technical contribution is a
user data encoding scheme that encodes demographic and biometric user data in
only two BFV ciphertexts and yet facilitates us to outsource various types of
ID verification queries to a third-party cloud. Our encoding scheme also
ensures that the only computation done by the trusted entity is a
query-agnostic "extended" decryption. This is in stark contrast with recent
works that outsource all the non-arithmetic operations to a trusted server. We
implement our protocol using the Microsoft SEAL FHE library and demonstrate its
practicality.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.07892v3">Personhood credentials: Artificial intelligence and the value of
  privacy-preserving tools to distinguish who is real online</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2024-08-15T02:41:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Steven Adler, Zoë Hitzig, Shrey Jain, Catherine Brewer, Wayne Chang, Renée DiResta, Eddy Lazzarin, Sean McGregor, Wendy Seltzer, Divya Siddarth, Nouran Soliman, Tobin South, Connor Spelliscy, Manu Sporny, Varya Srivastava, John Bailey, Brian Christian, Andrew Critch, Ronnie Falcon, Heather Flanagan, Kim Hamilton Duffy, Eric Ho, Claire R. Leibowicz, Srikanth Nadhamuni, Alan Z. Rozenshtein, David Schnurr, Evan Shapiro, Lacey Strahm, Andrew Trask, Zoe Weinberg, Cedric Whitney, Tom Zick</p>
    <p><b>Summary:</b> Anonymity is an important principle online. However, malicious actors have
long used misleading identities to conduct fraud, spread disinformation, and
carry out other deceptive schemes. With the advent of increasingly capable AI,
bad actors can amplify the potential scale and effectiveness of their
operations, intensifying the challenge of balancing anonymity and
trustworthiness online. In this paper, we analyze the value of a new tool to
address this challenge: "personhood credentials" (PHCs), digital credentials
that empower users to demonstrate that they are real people -- not AIs -- to
online services, without disclosing any personal information. Such credentials
can be issued by a range of trusted institutions -- governments or otherwise. A
PHC system, according to our definition, could be local or global, and does not
need to be biometrics-based. Two trends in AI contribute to the urgency of the
challenge: AI's increasing indistinguishability from people online (i.e.,
lifelike content and avatars, agentic activity), and AI's increasing
scalability (i.e., cost-effectiveness, accessibility). Drawing on a long
history of research into anonymous credentials and "proof-of-personhood"
systems, personhood credentials give people a way to signal their
trustworthiness on online platforms, and offer service providers new tools for
reducing misuse by bad actors. In contrast, existing countermeasures to
automated deception -- such as CAPTCHAs -- are inadequate against sophisticated
AI, while stringent identity verification solutions are insufficiently private
for many use-cases. After surveying the benefits of personhood credentials, we
also examine deployment risks and design challenges. We conclude with
actionable next steps for policymakers, technologists, and standards bodies to
consider in consultation with the public.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.07614v1">Practical Considerations for Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-14T15:28:28Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kareem Amin, Alex Kulesza, Sergei Vassilvitskii</p>
    <p><b>Summary:</b> Differential privacy is the gold standard for statistical data release. Used
by governments, companies, and academics, its mathematically rigorous
guarantees and worst-case assumptions on the strength and knowledge of
attackers make it a robust and compelling framework for reasoning about
privacy. However, even with landmark successes, differential privacy has not
achieved widespread adoption in everyday data use and data protection. In this
work we examine some of the practical obstacles that stand in the way.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.07021v1">Improved Counting under Continual Observation with Pure Differential
  Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B">
  <p><b>Published on:</b> 2024-08-13T16:36:33Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Joel Daniel Andersson, Rasmus Pagh, Sahel Torkamani</p>
    <p><b>Summary:</b> Counting under continual observation is a well-studied problem in the area of
differential privacy. Given a stream of updates $x_1,x_2,\dots,x_T \in \{0,1\}$
the problem is to continuously release estimates of the prefix sums
$\sum_{i=1}^t x_i$ for $t=1,\dots,T$ while protecting each input $x_i$ in the
stream with differential privacy. Recently, significant leaps have been made in
our understanding of this problem under $\textit{approximate}$ differential
privacy, aka. $(\varepsilon,\delta)$$\textit{-differential privacy}$. However,
for the classical case of $\varepsilon$-differential privacy, we are not aware
of any improvement in mean squared error since the work of Honaker (TPDP 2015).
In this paper we present such an improvement, reducing the mean squared error
by a factor of about 4, asymptotically. The key technique is a new
generalization of the binary tree mechanism that uses a $k$-ary number system
with $\textit{negative digits}$ to improve the privacy-accuracy trade-off. Our
mechanism improves the mean squared error over all 'optimal'
$(\varepsilon,\delta)$-differentially private factorization mechanisms based on
Gaussian noise whenever $\delta$ is sufficiently small. Specifically, using
$k=19$ we get an asymptotic improvement over the bound given in the work by
Henzinger, Upadhyay and Upadhyay (SODA 2023) when $\delta = O(T^{-0.92})$.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.07006v1">The Complexities of Differential Privacy for Survey Data</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-13T16:15:42Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jörg Drechsler, James Bailie</p>
    <p><b>Summary:</b> The concept of differential privacy (DP) has gained substantial attention in
recent years, most notably since the U.S. Census Bureau announced the adoption
of the concept for its 2020 Decennial Census. However, despite its attractive
theoretical properties, implementing DP in practice remains challenging,
especially when it comes to survey data. In this paper we present some results
from an ongoing project funded by the U.S. Census Bureau that is exploring the
possibilities and limitations of DP for survey data. Specifically, we identify
five aspects that need to be considered when adopting DP in the survey context:
the multi-staged nature of data production; the limited privacy amplification
from complex sampling designs; the implications of survey-weighted estimates;
the weighting adjustments for nonresponse and other data deficiencies, and the
imputation of missing values. We summarize the project's key findings with
respect to each of these aspects and also discuss some of the challenges that
still need to be addressed before DP could become the new data protection
standard at statistical agencies.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.07004v1">Casper: Prompt Sanitization for Protecting User Privacy in Web-Based
  Large Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-08-13T16:08:37Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chun Jie Chong, Chenxi Hou, Zhihao Yao, Seyed Mohammadjavad Seyed Talebi</p>
    <p><b>Summary:</b> Web-based Large Language Model (LLM) services have been widely adopted and
have become an integral part of our Internet experience. Third-party plugins
enhance the functionalities of LLM by enabling access to real-world data and
services. However, the privacy consequences associated with these services and
their third-party plugins are not well understood. Sensitive prompt data are
stored, processed, and shared by cloud-based LLM providers and third-party
plugins. In this paper, we propose Casper, a prompt sanitization technique that
aims to protect user privacy by detecting and removing sensitive information
from user inputs before sending them to LLM services. Casper runs entirely on
the user's device as a browser extension and does not require any changes to
the online LLM services. At the core of Casper is a three-layered sanitization
mechanism consisting of a rule-based filter, a Machine Learning (ML)-based
named entity recognizer, and a browser-based local LLM topic identifier. We
evaluate Casper on a dataset of 4000 synthesized prompts and show that it can
effectively filter out Personal Identifiable Information (PII) and
privacy-sensitive topics with high accuracy, at 98.5% and 89.9%, respectively.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.08909v1">An Adaptive Differential Privacy Method Based on Federated Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2024-08-13T13:08:11Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhiqiang Wang, Xinyue Yu, Qianli Huang, Yongguang Gong</p>
    <p><b>Summary:</b> Differential privacy is one of the methods to solve the problem of privacy
protection in federated learning. Setting the same privacy budget for each
round will result in reduced accuracy in training. The existing methods of the
adjustment of privacy budget consider fewer influencing factors and tend to
ignore the boundaries, resulting in unreasonable privacy budgets. Therefore, we
proposed an adaptive differential privacy method based on federated learning.
The method sets the adjustment coefficient and scoring function according to
accuracy, loss, training rounds, and the number of datasets and clients. And
the privacy budget is adjusted based on them. Then the local model update is
processed according to the scaling factor and the noise. Fi-nally, the server
aggregates the noised local model update and distributes the noised global
model. The range of parameters and the privacy of the method are analyzed.
Through the experimental evaluation, it can reduce the privacy budget by about
16%, while the accuracy remains roughly the same.</p>
  </details>
</div>



<h2>2024-09</h2>

<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.07997v1">Privacy-preserving federated prediction of pain intensity change based
  on multi-center survey data</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-09-12T12:41:58Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Supratim Das, Mahdie Rafie, Paula Kammer, Søren T. Skou, Dorte T. Grønne, Ewa M. Roos, André Hajek, Hans-Helmut König, Md Shihab Ullaha, Niklas Probul, Jan Baumbacha, Linda Baumbach</p>
    <p><b>Summary:</b> Background: Patient-reported survey data are used to train prognostic models
aimed at improving healthcare. However, such data are typically available
multi-centric and, for privacy reasons, cannot easily be centralized in one
data repository. Models trained locally are less accurate, robust, and
generalizable. We present and apply privacy-preserving federated machine
learning techniques for prognostic model building, where local survey data
never leaves the legally safe harbors of the medical centers. Methods: We used
centralized, local, and federated learning techniques on two healthcare
datasets (GLA:D data from the five health regions of Denmark and international
SHARE data of 27 countries) to predict two different health outcomes. We
compared linear regression, random forest regression, and random forest
classification models trained on local data with those trained on the entire
data in a centralized and in a federated fashion. Results: In GLA:D data,
federated linear regression (R2 0.34, RMSE 18.2) and federated random forest
regression (R2 0.34, RMSE 18.3) models outperform their local counterparts
(i.e., R2 0.32, RMSE 18.6, R2 0.30, RMSE 18.8) with statistical significance.
We also found that centralized models (R2 0.34, RMSE 18.2, R2 0.32, RMSE 18.5,
respectively) did not perform significantly better than the federated models.
In SHARE, the federated model (AC 0.78, AUROC: 0.71) and centralized model (AC
0.84, AUROC: 0.66) perform significantly better than the local models (AC:
0.74, AUROC: 0.69). Conclusion: Federated learning enables the training of
prognostic models from multi-center surveys without compromising privacy and
with only minimal or no compromise regarding model performance.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.07809v1">Controllable Synthetic Clinical Note Generation with Privacy Guarantees</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-09-12T07:38:34Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tal Baumel, Andre Manoel, Daniel Jones, Shize Su, Huseyin Inan,  Aaron,  Bornstein, Robert Sim</p>
    <p><b>Summary:</b> In the field of machine learning, domain-specific annotated data is an
invaluable resource for training effective models. However, in the medical
domain, this data often includes Personal Health Information (PHI), raising
significant privacy concerns. The stringent regulations surrounding PHI limit
the availability and sharing of medical datasets, which poses a substantial
challenge for researchers and practitioners aiming to develop advanced machine
learning models. In this paper, we introduce a novel method to "clone" datasets
containing PHI. Our approach ensures that the cloned datasets retain the
essential characteristics and utility of the original data without compromising
patient privacy. By leveraging differential-privacy techniques and a novel
fine-tuning task, our method produces datasets that are free from identifiable
information while preserving the statistical properties necessary for model
training. We conduct utility testing to evaluate the performance of machine
learning models trained on the cloned datasets. The results demonstrate that
our cloned datasets not only uphold privacy standards but also enhance model
performance compared to those trained on traditional anonymized datasets. This
work offers a viable solution for the ethical and effective utilization of
sensitive medical data in machine learning, facilitating progress in medical
research and the development of robust predictive models.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.07773v1">PDC-FRS: Privacy-preserving Data Contribution for Federated Recommender
  System</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Retrieval-5BC0EB">
  <p><b>Published on:</b> 2024-09-12T06:13:07Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chaoqun Yang, Wei Yuan, Liang Qu, Thanh Tam Nguyen</p>
    <p><b>Summary:</b> Federated recommender systems (FedRecs) have emerged as a popular research
direction for protecting users' privacy in on-device recommendations. In
FedRecs, users keep their data locally and only contribute their local
collaborative information by uploading model parameters to a central server.
While this rigid framework protects users' raw data during training, it
severely compromises the recommendation model's performance due to the
following reasons: (1) Due to the power law distribution nature of user
behavior data, individual users have few data points to train a recommendation
model, resulting in uploaded model updates that may be far from optimal; (2) As
each user's uploaded parameters are learned from local data, which lacks global
collaborative information, relying solely on parameter aggregation methods such
as FedAvg to fuse global collaborative information may be suboptimal. To bridge
this performance gap, we propose a novel federated recommendation framework,
PDC-FRS. Specifically, we design a privacy-preserving data contribution
mechanism that allows users to share their data with a differential privacy
guarantee. Based on the shared but perturbed data, an auxiliary model is
trained in parallel with the original federated recommendation process. This
auxiliary model enhances FedRec by augmenting each user's local dataset and
integrating global collaborative information. To demonstrate the effectiveness
of PDC-FRS, we conduct extensive experiments on two widely used recommendation
datasets. The empirical results showcase the superiority of PDC-FRS compared to
baseline methods.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.07751v1">Efficient Privacy-Preserving KAN Inference Using Homomorphic Encryption</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-09-12T04:51:27Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhizheng Lai, Yufei Zhou, Peijia Zheng, Lin Chen</p>
    <p><b>Summary:</b> The recently proposed Kolmogorov-Arnold Networks (KANs) offer enhanced
interpretability and greater model expressiveness. However, KANs also present
challenges related to privacy leakage during inference. Homomorphic encryption
(HE) facilitates privacy-preserving inference for deep learning models,
enabling resource-limited users to benefit from deep learning services while
ensuring data security. Yet, the complex structure of KANs, incorporating
nonlinear elements like the SiLU activation function and B-spline functions,
renders existing privacy-preserving inference techniques inadequate. To address
this issue, we propose an accurate and efficient privacy-preserving inference
scheme tailored for KANs. Our approach introduces a task-specific polynomial
approximation for the SiLU activation function, dynamically adjusting the
approximation range to ensure high accuracy on real-world datasets.
Additionally, we develop an efficient method for computing B-spline functions
within the HE domain, leveraging techniques such as repeat packing, lazy
combination, and comparison functions. We evaluate the effectiveness of our
privacy-preserving KAN inference scheme on both symbolic formula evaluation and
image classification. The experimental results show that our model achieves
accuracy comparable to plaintext KANs across various datasets and outperforms
plaintext MLPs. Additionally, on the CIFAR-10 dataset, our inference latency
achieves over 7 times speedup compared to the naive method.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.07444v1">Echoes of Privacy: Uncovering the Profiling Practices of Voice
  Assistants</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Networking and Internet Architecture-04E762">
  <p><b>Published on:</b> 2024-09-11T17:44:41Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tina Khezresmaeilzadeh, Elaine Zhu, Kiersten Grieco, Daniel J. Dubois, Konstantinos Psounis, David Choffnes</p>
    <p><b>Summary:</b> Many companies, including Google, Amazon, and Apple, offer voice assistants
as a convenient solution for answering general voice queries and accessing
their services. These voice assistants have gained popularity and can be easily
accessed through various smart devices such as smartphones, smart speakers,
smartwatches, and an increasing array of other devices. However, this
convenience comes with potential privacy risks. For instance, while companies
vaguely mention in their privacy policies that they may use voice interactions
for user profiling, it remains unclear to what extent this profiling occurs and
whether voice interactions pose greater privacy risks compared to other
interaction modalities.
  In this paper, we conduct 1171 experiments involving a total of 24530 queries
with different personas and interaction modalities over the course of 20 months
to characterize how the three most popular voice assistants profile their
users. We analyze factors such as the labels assigned to users, their accuracy,
the time taken to assign these labels, differences between voice and web
interactions, and the effectiveness of profiling remediation tools offered by
each voice assistant. Our findings reveal that profiling can happen without
interaction, can be incorrect and inconsistent at times, may take several days
to weeks for changes to occur, and can be influenced by the interaction
modality.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.07415v1">SoK: Security and Privacy Risks of Medical AI</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-09-11T16:59:58Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yuanhaur Chang, Han Liu, Evin Jaff, Chenyang Lu, Ning Zhang</p>
    <p><b>Summary:</b> The integration of technology and healthcare has ushered in a new era where
software systems, powered by artificial intelligence and machine learning, have
become essential components of medical products and services. While these
advancements hold great promise for enhancing patient care and healthcare
delivery efficiency, they also expose sensitive medical data and system
integrity to potential cyberattacks. This paper explores the security and
privacy threats posed by AI/ML applications in healthcare. Through a thorough
examination of existing research across a range of medical domains, we have
identified significant gaps in understanding the adversarial attacks targeting
medical AI systems. By outlining specific adversarial threat models for medical
settings and identifying vulnerable application domains, we lay the groundwork
for future research that investigates the security and resilience of AI-driven
medical systems. Through our analysis of different threat models and
feasibility studies on adversarial attacks in different medical domains, we
provide compelling insights into the pressing need for cybersecurity research
in the rapidly evolving field of AI healthcare technology.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.07224v1">Analytic Class Incremental Learning for Sound Source Localization with
  Privacy Protection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Sound-D91E36"> 
  <p><b>Published on:</b> 2024-09-11T12:31:07Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xinyuan Qian, Xianghu Yue, Jiadong Wang, Huiping Zhuang, Haizhou Li</p>
    <p><b>Summary:</b> Sound Source Localization (SSL) enabling technology for applications such as
surveillance and robotics. While traditional Signal Processing (SP)-based SSL
methods provide analytic solutions under specific signal and noise assumptions,
recent Deep Learning (DL)-based methods have significantly outperformed them.
However, their success depends on extensive training data and substantial
computational resources. Moreover, they often rely on large-scale annotated
spatial data and may struggle when adapting to evolving sound classes. To
mitigate these challenges, we propose a novel Class Incremental Learning (CIL)
approach, termed SSL-CIL, which avoids serious accuracy degradation due to
catastrophic forgetting by incrementally updating the DL-based SSL model
through a closed-form analytic solution. In particular, data privacy is ensured
since the learning process does not revisit any historical data
(exemplar-free), which is more suitable for smart home scenarios. Empirical
results in the public SSLR dataset demonstrate the superior performance of our
proposal, achieving a localization accuracy of 90.9%, surpassing other
competitive methods.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.07187v1">A Simple Linear Space Data Structure for ANN with Application in
  Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B">
  <p><b>Published on:</b> 2024-09-11T11:14:33Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Martin Aumüller, Fabrizio Boninsegna, Francesco Silvestri</p>
    <p><b>Summary:</b> Locality Sensitive Filters are known for offering a quasi-linear space data
structure with rigorous guarantees for the Approximate Near Neighbor search
problem. Building on Locality Sensitive Filters, we derive a simple data
structure for the Approximate Near Neighbor Counting problem under differential
privacy. Moreover, we provide a simple analysis leveraging a connection with
concomitant statistics and extreme value theory. Our approach achieves the same
performance as the recent findings of Andoni et al. (NeurIPS 2023) but with a
more straightforward method. As a side result, the paper provides a more
compact description and analysis of Locality Sensitive Filters for Approximate
Near Neighbor Search under inner product similarity, improving a previous
result in Aum\"{u}ller et al. (TODS 2022).</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.06955v1">Privacy-Preserving Federated Learning with Consistency via Knowledge
  Distillation Using Conditional Generator</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2024-09-11T02:36:36Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kangyang Luo, Shuai Wang, Xiang Li, Yunshi Lan, Ming Gao, Jinlong Shu</p>
    <p><b>Summary:</b> Federated Learning (FL) is gaining popularity as a distributed learning
framework that only shares model parameters or gradient updates and keeps
private data locally. However, FL is at risk of privacy leakage caused by
privacy inference attacks. And most existing privacy-preserving mechanisms in
FL conflict with achieving high performance and efficiency. Therefore, we
propose FedMD-CG, a novel FL method with highly competitive performance and
high-level privacy preservation, which decouples each client's local model into
a feature extractor and a classifier, and utilizes a conditional generator
instead of the feature extractor to perform server-side model aggregation. To
ensure the consistency of local generators and classifiers, FedMD-CG leverages
knowledge distillation to train local models and generators at both the latent
feature level and the logit level. Also, we construct additional classification
losses and design new diversity losses to enhance client-side training.
FedMD-CG is robust to data heterogeneity and does not require training extra
discriminators (like cGAN). We conduct extensive experiments on various image
classification tasks to validate the superiority of FedMD-CG.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.06564v1">Advancing Android Privacy Assessments with Automation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-09-10T14:56:51Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mugdha Khedkar, Michael Schlichtig, Eric Bodden</p>
    <p><b>Summary:</b> Android apps collecting data from users must comply with legal frameworks to
ensure data protection. This requirement has become even more important since
the implementation of the General Data Protection Regulation (GDPR) by the
European Union in 2018. Moreover, with the proposed Cyber Resilience Act on the
horizon, stakeholders will soon need to assess software against even more
stringent security and privacy standards. Effective privacy assessments require
collaboration among groups with diverse expertise to function effectively as a
cohesive unit.
  This paper motivates the need for an automated approach that enhances
understanding of data protection in Android apps and improves communication
between the various parties involved in privacy assessments. We propose the
Assessor View, a tool designed to bridge the knowledge gap between these
parties, facilitating more effective privacy assessments of Android
applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.06455v1">Continual Domain Incremental Learning for Privacy-aware Digital
  Pathology</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-09-10T12:21:54Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Pratibha Kumari, Daniel Reisenbüchler, Lucas Luttner, Nadine S. Schaadt, Friedrich Feuerhake, Dorit Merhof</p>
    <p><b>Summary:</b> In recent years, there has been remarkable progress in the field of digital
pathology, driven by the ability to model complex tissue patterns using
advanced deep-learning algorithms. However, the robustness of these models is
often severely compromised in the presence of data shifts (e.g., different
stains, organs, centers, etc.). Alternatively, continual learning (CL)
techniques aim to reduce the forgetting of past data when learning new data
with distributional shift conditions. Specifically, rehearsal-based CL
techniques, which store some past data in a buffer and then replay it with new
data, have proven effective in medical image analysis tasks. However, privacy
concerns arise as these approaches store past data, prompting the development
of our novel Generative Latent Replay-based CL (GLRCL) approach. GLRCL captures
the previous distribution through Gaussian Mixture Models instead of storing
past samples, which are then utilized to generate features and perform latent
replay with new data. We systematically evaluate our proposed framework under
different shift conditions in histopathology data, including stain and organ
shift. Our approach significantly outperforms popular buffer-free CL approaches
and performs similarly to rehearsal-based CL approaches that require large
buffers causing serious privacy violations.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.06422v1">A Pervasive, Efficient and Private Future: Realizing Privacy-Preserving
  Machine Learning Through Hybrid Homomorphic Encryption</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-09-10T11:04:14Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Khoa Nguyen, Mindaugas Budzys, Eugene Frimpong, Tanveer Khan, Antonis Michalas</p>
    <p><b>Summary:</b> Machine Learning (ML) has become one of the most impactful fields of data
science in recent years. However, a significant concern with ML is its privacy
risks due to rising attacks against ML models. Privacy-Preserving Machine
Learning (PPML) methods have been proposed to mitigate the privacy and security
risks of ML models. A popular approach to achieving PPML uses Homomorphic
Encryption (HE). However, the highly publicized inefficiencies of HE make it
unsuitable for highly scalable scenarios with resource-constrained devices.
Hence, Hybrid Homomorphic Encryption (HHE) -- a modern encryption scheme that
combines symmetric cryptography with HE -- has recently been introduced to
overcome these challenges. HHE potentially provides a foundation to build new
efficient and privacy-preserving services that transfer expensive HE operations
to the cloud. This work introduces HHE to the ML field by proposing
resource-friendly PPML protocols for edge devices. More precisely, we utilize
HHE as the primary building block of our PPML protocols. We assess the
performance of our protocols by first extensively evaluating each party's
communication and computational cost on a dummy dataset and show the efficiency
of our protocols by comparing them with similar protocols implemented using
plain BFV. Subsequently, we demonstrate the real-world applicability of our
construction by building an actual PPML application that uses HHE as its
foundation to classify heart disease based on sensitive ECG data.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.06360v1">SoK: Evaluating 5G Protocols Against Legacy and Emerging Privacy and
  Security Attacks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Networking and Internet Architecture-04E762">
  <p><b>Published on:</b> 2024-09-10T09:30:37Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Stavros Eleftherakis, Domenico Giustiniano, Nicolas Kourtellis</p>
    <p><b>Summary:</b> Ensuring user privacy remains a critical concern within mobile cellular
networks, particularly given the proliferation of interconnected devices and
services. In fact, a lot of user privacy issues have been raised in 2G, 3G,
4G/LTE networks. Recognizing this general concern, 3GPP has prioritized
addressing these issues in the development of 5G, implementing numerous
modifications to enhance user privacy since 5G Release 15. In this
systematization of knowledge paper, we first provide a framework for studying
privacy and security related attacks in cellular networks, setting as privacy
objective the User Identity Confidentiality defined in 3GPP standards. Using
this framework, we discuss existing privacy and security attacks in pre-5G
networks, analyzing the weaknesses that lead to these attacks. Furthermore, we
thoroughly study the security characteristics of 5G up to the new Release 19,
and examine mitigation mechanisms of 5G to the identified pre-5G attacks.
Afterwards, we analyze how recent 5G attacks try to overcome these mitigation
mechanisms. Finally, we identify current limitations and open problems in
security of 5G, and propose directions for future work.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.06233v1">VBIT: Towards Enhancing Privacy Control Over IoT Devices</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-09-10T06:00:50Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jad Al Aaraj, Olivia Figueira, Tu Le, Isabela Figueira, Rahmadi Trimananda, Athina Markopoulou</p>
    <p><b>Summary:</b> Internet-of-Things (IoT) devices are increasingly deployed at home, at work,
and in other shared and public spaces. IoT devices collect and share data with
service providers and third parties, which poses privacy concerns. Although
privacy enhancing tools are quite advanced in other applications domains (\eg~
advertising and tracker blockers for browsers), users have currently no
convenient way to know or manage what and how data is collected and shared by
IoT devices. In this paper, we present VBIT, an interactive system combining
Mixed Reality (MR) and web-based applications that allows users to: (1) uncover
and visualize tracking services by IoT devices in an instrumented space and (2)
take action to stop or limit that tracking. We design and implement VBIT to
operate at the network traffic level, and we show that it has negligible
performance overhead, and offers flexibility and good usability. We perform a
mixed-method user study consisting of an online survey and an in-person
interview study. We show that VBIT users appreciate VBIT's transparency,
control, and customization features, and they become significantly more willing
to install an IoT advertising and tracking blocker, after using VBIT. In the
process, we obtain design insights that can be used to further iterate and
improve the design of VBIT and other systems for IoT transparency and control.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.06069v1">Privacy-Preserving Data Linkage Across Private and Public Datasets for
  Collaborative Agriculture Research</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2024-09-09T21:07:13Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Osama Zafar, Rosemarie Santa Gonzalez, Gabriel Wilkins, Alfonso Morales, Erman Ayday</p>
    <p><b>Summary:</b> Digital agriculture leverages technology to enhance crop yield, disease
resilience, and soil health, playing a critical role in agricultural research.
However, it raises privacy concerns such as adverse pricing, price
discrimination, higher insurance costs, and manipulation of resources,
deterring farm operators from sharing data due to potential misuse. This study
introduces a privacy-preserving framework that addresses these risks while
allowing secure data sharing for digital agriculture. Our framework enables
comprehensive data analysis while protecting privacy. It allows stakeholders to
harness research-driven policies that link public and private datasets. The
proposed algorithm achieves this by: (1) identifying similar farmers based on
private datasets, (2) providing aggregate information like time and location,
(3) determining trends in price and product availability, and (4) correlating
trends with public policy data, such as food insecurity statistics. We validate
the framework with real-world Farmer's Market datasets, demonstrating its
efficacy through machine learning models trained on linked privacy-preserved
data. The results support policymakers and researchers in addressing food
insecurity and pricing issues. This work significantly contributes to digital
agriculture by providing a secure method for integrating and analyzing data,
driving advancements in agricultural technology and development.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.05623v1">A Framework for Differential Privacy Against Timing Attacks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-09-09T13:56:04Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zachary Ratliff, Salil Vadhan</p>
    <p><b>Summary:</b> The standard definition of differential privacy (DP) ensures that a
mechanism's output distribution on adjacent datasets is indistinguishable.
However, real-world implementations of DP can, and often do, reveal information
through their runtime distributions, making them susceptible to timing attacks.
In this work, we establish a general framework for ensuring differential
privacy in the presence of timing side channels. We define a new notion of
timing privacy, which captures programs that remain differentially private to
an adversary that observes the program's runtime in addition to the output. Our
framework enables chaining together component programs that are timing-stable
followed by a random delay to obtain DP programs that achieve timing privacy.
Importantly, our definitions allow for measuring timing privacy and output
privacy using different privacy measures. We illustrate how to instantiate our
framework by giving programs for standard DP computations in the RAM and Word
RAM models of computation. Furthermore, we show how our framework can be
realized in code through a natural extension of the OpenDP Programming
Framework.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.05249v1">NetDPSyn: Synthesizing Network Traces under Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Networking and Internet Architecture-04E762">
  <p><b>Published on:</b> 2024-09-08T23:54:00Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Danyu Sun, Joann Qiongna Chen, Chen Gong, Tianhao Wang, Zhou Li</p>
    <p><b>Summary:</b> As the utilization of network traces for the network measurement research
becomes increasingly prevalent, concerns regarding privacy leakage from network
traces have garnered the public's attention. To safeguard network traces,
researchers have proposed the trace synthesis that retains the essential
properties of the raw data. However, previous works also show that synthesis
traces with generative models are vulnerable under linkage attacks.
  This paper introduces NetDPSyn, the first system to synthesize high-fidelity
network traces under privacy guarantees. NetDPSyn is built with the
Differential Privacy (DP) framework as its core, which is significantly
different from prior works that apply DP when training the generative model.
The experiments conducted on three flow and two packet datasets indicate that
NetDPSyn achieves much better data utility in downstream tasks like anomaly
detection. NetDPSyn is also 2.5 times faster than the other methods on average
in data synthesis.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.04877v1">Strong Privacy-Preserving Universally Composable AKA Protocol with
  Seamless Handover Support for Mobile Virtual Network Operator</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-09-07T18:04:54Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Rabiah Alnashwan, Yang Yang, Yilu Dong, Prosanta Gope, Behzad Abdolmaleki, Syed Rafiul Hussain</p>
    <p><b>Summary:</b> Consumers seeking a new mobile plan have many choices in the present mobile
landscape. The Mobile Virtual Network Operator (MVNO) has recently gained
considerable attention among these options. MVNOs offer various benefits,
making them an appealing choice for a majority of consumers. These advantages
encompass flexibility, access to cutting-edge technologies, enhanced coverage,
superior customer service, and substantial cost savings. Even though MVNO
offers several advantages, it also creates some security and privacy concerns
for the customer simultaneously. For instance, in the existing solution, MVNO
needs to hand over all the sensitive details, including the users' identities
and master secret keys of their customers, to a mobile operator (MNO) to
validate the customers while offering any services. This allows MNOs to have
unrestricted access to the MVNO subscribers' location and mobile data,
including voice calls, SMS, and Internet, which the MNOs frequently sell to
third parties (e.g., advertisement companies and surveillance agencies) for
more profit. Although critical for mass users, such privacy loss has been
historically ignored due to the lack of practical and privacy-preserving
solutions for registration and handover procedures in cellular networks. In
this paper, we propose a universally composable authentication and handover
scheme with strong user privacy support, where each MVNO user can validate a
mobile operator (MNO) and vice-versa without compromising user anonymity and
unlinkability support. Here, we anticipate that our proposed solution will most
likely be deployed by the MVNO(s) to ensure enhanced privacy support to their
customer(s).</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.04716v1">Privacy enhanced collaborative inference in the Cox proportional hazards
  model for distributed data</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Statistics Theory-D91E36"> 
  <p><b>Published on:</b> 2024-09-07T05:32:34Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mengtong Hu, Xu Shi, Peter X. -K. Song</p>
    <p><b>Summary:</b> Data sharing barriers are paramount challenges arising from multicenter
clinical studies where multiple data sources are stored in a distributed
fashion at different local study sites. Particularly in the case of
time-to-event analysis when global risk sets are needed for the Cox
proportional hazards model, access to a centralized database is typically
necessary. Merging such data sources into a common data storage for a
centralized statistical analysis requires a data use agreement, which is often
time-consuming. Furthermore, the construction and distribution of risk sets to
participating clinical centers for subsequent calculations may pose a risk of
revealing individual-level information. We propose a new collaborative Cox
model that eliminates the need for accessing the centralized database and
constructing global risk sets but needs only the sharing of summary statistics
with significantly smaller dimensions than risk sets. Thus, the proposed
collaborative inference enjoys maximal protection of data privacy. We show
theoretically and numerically that the new distributed proportional hazards
model approach has little loss of statistical power when compared to the
centralized method that requires merging the entire data. We present a
renewable sieve method to establish large-sample properties for the proposed
method. We illustrate its performance through simulation experiments and a
real-world data example from patients with kidney transplantation in the Organ
Procurement and Transplantation Network (OPTN) to understand the factors
associated with the 5-year death-censored graft failure (DCGF) for patients who
underwent kidney transplants in the US.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.04652v1">Privacy-Preserving Race/Ethnicity Estimation for Algorithmic Bias
  Measurement in the U.S</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-09-06T23:29:18Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Saikrishna Badrinarayanan, Osonde Osoba, Miao Cheng, Ryan Rogers, Sakshi Jain, Rahul Tandra, Natesh S. Pillai</p>
    <p><b>Summary:</b> AI fairness measurements, including tests for equal treatment, often take the
form of disaggregated evaluations of AI systems. Such measurements are an
important part of Responsible AI operations. These measurements compare system
performance across demographic groups or sub-populations and typically require
member-level demographic signals such as gender, race, ethnicity, and location.
However, sensitive member-level demographic attributes like race and ethnicity
can be challenging to obtain and use due to platform choices, legal
constraints, and cultural norms. In this paper, we focus on the task of
enabling AI fairness measurements on race/ethnicity for \emph{U.S. LinkedIn
members} in a privacy-preserving manner. We present the Privacy-Preserving
Probabilistic Race/Ethnicity Estimation (PPRE) method for performing this task.
PPRE combines the Bayesian Improved Surname Geocoding (BISG) model, a sparse
LinkedIn survey sample of self-reported demographics, and privacy-enhancing
technologies like secure two-party computation and differential privacy to
enable meaningful fairness measurements while preserving member privacy. We
provide details of the PPRE method and its privacy guarantees. We then
illustrate sample measurement operations. We conclude with a review of open
research and engineering challenges for expanding our privacy-preserving
fairness measurement capabilities.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.04366v1">Deanonymizing Ethereum Validators: The P2P Network Has a Privacy Issue</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-09-06T15:57:43Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Lioba Heimbach, Yann Vonlanthen, Juan Villacis, Lucianna Kiffer, Roger Wattenhofer</p>
    <p><b>Summary:</b> Many blockchain networks aim to preserve the anonymity of validators in the
peer-to-peer (P2P) network, ensuring that no adversary can link a validator's
identifier to the IP address of a peer due to associated privacy and security
concerns. This work demonstrates that the Ethereum P2P network does not offer
this anonymity. We present a methodology that enables any node in the network
to identify validators hosted on connected peers and empirically verify the
feasibility of our proposed method. Using data collected from four nodes over
three days, we locate more than 15% of Ethereum validators in the P2P network.
The insights gained from our deanonymization technique provide valuable
information on the distribution of validators across peers, their geographic
locations, and hosting organizations. We further discuss the implications and
risks associated with the lack of anonymity in the P2P network and propose
methods to help validators protect their privacy. The Ethereum Foundation has
awarded us a bug bounty, acknowledging the impact of our results.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.04257v1">Privacy risk from synthetic data: practical proposals</a></h3>
  
  <p><b>Published on:</b> 2024-09-06T13:10:40Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Gillian M Raab</p>
    <p><b>Summary:</b> This paper proposes and compares measures of identity and attribute
disclosure risk for synthetic data. Data custodians can use the methods
proposed here to inform the decision as to whether to release synthetic
versions of confidential data. Different measures are evaluated on two data
sets. Insight into the measures is obtained by examining the details of the
records identified as posing a disclosure risk. This leads to methods to
identify, and possibly exclude, apparently risky records where the
identification or attribution would be expected by someone with background
knowledge of the data. The methods described are available as part of the
\textbf{synthpop} package for \textbf{R}.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.04194v1">Towards Privacy-Preserving Relational Data Synthesis via Probabilistic
  Relational Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-09-06T11:24:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Malte Luttermann, Ralf Möller, Mattis Hartwig</p>
    <p><b>Summary:</b> Probabilistic relational models provide a well-established formalism to
combine first-order logic and probabilistic models, thereby allowing to
represent relationships between objects in a relational domain. At the same
time, the field of artificial intelligence requires increasingly large amounts
of relational training data for various machine learning tasks. Collecting
real-world data, however, is often challenging due to privacy concerns, data
protection regulations, high costs, and so on. To mitigate these challenges,
the generation of synthetic data is a promising approach. In this paper, we
solve the problem of generating synthetic relational data via probabilistic
relational models. In particular, we propose a fully-fledged pipeline to go
from relational database to probabilistic relational model, which can then be
used to sample new synthetic relational data points from its underlying
probability distribution. As part of our proposed pipeline, we introduce a
learning algorithm to construct a probabilistic relational model from a given
relational database.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.04173v1">NPU-NTU System for Voice Privacy 2024 Challenge</a></h3>
  
  <p><b>Published on:</b> 2024-09-06T10:32:42Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jixun Yao, Nikita Kuzmin, Qing Wang, Pengcheng Guo, Ziqian Ning, Dake Guo, Kong Aik Lee, Eng-Siong Chng, Lei Xie</p>
    <p><b>Summary:</b> Speaker anonymization is an effective privacy protection solution that
conceals the speaker's identity while preserving the linguistic content and
paralinguistic information of the original speech. To establish a fair
benchmark and facilitate comparison of speaker anonymization systems, the
VoicePrivacy Challenge (VPC) was held in 2020 and 2022, with a new edition
planned for 2024. In this paper, we describe our proposed speaker anonymization
system for VPC 2024. Our system employs a disentangled neural codec
architecture and a serial disentanglement strategy to gradually disentangle the
global speaker identity and time-variant linguistic content and paralinguistic
information. We introduce multiple distillation methods to disentangle
linguistic content, speaker identity, and emotion. These methods include
semantic distillation, supervised speaker distillation, and frame-level emotion
distillation. Based on these distillations, we anonymize the original speaker
identity using a weighted sum of a set of candidate speaker identities and a
randomly generated speaker identity. Our system achieves the best trade-off of
privacy protection and emotion preservation in VPC 2024.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.04167v1">Do Android App Developers Accurately Report Collection of
  Privacy-Related Data?</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-09-06T10:05:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mugdha Khedkar, Ambuj Kumar Mondal, Eric Bodden</p>
    <p><b>Summary:</b> Many Android applications collect data from users. The European Union's
General Data Protection Regulation (GDPR) requires vendors to faithfully
disclose which data their apps collect. This task is complicated because many
apps use third-party code for which the same information is not readily
available. Hence we ask: how accurately do current Android apps fulfill these
requirements?
  In this work, we first expose a multi-layered definition of privacy-related
data to correctly report data collection in Android apps. We further create a
dataset of privacy-sensitive data classes that may be used as input by an
Android app. This dataset takes into account data collected both through the
user interface and system APIs.
  We manually examine the data safety sections of 70 Android apps to observe
how data collection is reported, identifying instances of over- and
under-reporting. Additionally, we develop a prototype to statically extract and
label privacy-related data collected via app source code, user interfaces, and
permissions. Comparing the prototype's results with the data safety sections of
20 apps reveals reporting discrepancies. Using the results from two Messaging
and Social Media apps (Signal and Instagram), we discuss how app developers
under-report and over-report data collection, respectively, and identify
inaccurately reported data categories.
  Our results show that app developers struggle to accurately report data
collection, either due to Google's abstract definition of collected data or
insufficient existing tool support.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.04048v2">Exploring User Privacy Awareness on GitHub: An Empirical Study</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Software Engineering-D91E36">
  <p><b>Published on:</b> 2024-09-06T06:41:46Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Costanza Alfieri, Juri Di Rocco, Paola Inverardi, Phuong T. Nguyen</p>
    <p><b>Summary:</b> GitHub provides developers with a practical way to distribute source code and
collaboratively work on common projects. To enhance account security and
privacy, GitHub allows its users to manage access permissions, review audit
logs, and enable two-factor authentication. However, despite the endless
effort, the platform still faces various issues related to the privacy of its
users. This paper presents an empirical study delving into the GitHub
ecosystem. Our focus is on investigating the utilization of privacy settings on
the platform and identifying various types of sensitive information disclosed
by users. Leveraging a dataset comprising 6,132 developers, we report and
analyze their activities by means of comments on pull requests. Our findings
indicate an active engagement by users with the available privacy settings on
GitHub. Notably, we observe the disclosure of different forms of private
information within pull request comments. This observation has prompted our
exploration into sensitivity detection using a large language model and BERT,
to pave the way for a personalized privacy assistant. Our work provides
insights into the utilization of existing privacy protection tools, such as
privacy settings, along with their inherent limitations. Essentially, we aim to
advance research in this field by providing both the motivation for creating
such privacy protection tools and a proposed methodology for personalizing
them.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.04026v1">Efficient Fault-Tolerant Quantum Protocol for Differential Privacy in
  the Shuffle Model</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-09-06T04:53:19Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Hassan Jameel Asghar, Arghya Mukherjee, Gavin K. Brennen</p>
    <p><b>Summary:</b> We present a quantum protocol which securely and implicitly implements a
random shuffle to realize differential privacy in the shuffle model. The
shuffle model of differential privacy amplifies privacy achievable via local
differential privacy by randomly permuting the tuple of outcomes from data
contributors. In practice, one needs to address how this shuffle is
implemented. Examples include implementing the shuffle via mix-networks, or
shuffling via a trusted third-party. These implementation specific issues raise
non-trivial computational and trust requirements in a classical system. We
propose a quantum version of the protocol using entanglement of quantum states
and show that the shuffle can be implemented without these extra requirements.
Our protocol implements k-ary randomized response, for any value of k > 2, and
furthermore, can be efficiently implemented using fault-tolerant computation.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.03707v1">A Different Level Text Protection Mechanism With Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-09-05T17:13:38Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Qingwen Fu</p>
    <p><b>Summary:</b> The article introduces a method for extracting words of different degrees of
importance based on the BERT pre-training model and proves the effectiveness of
this method. The article also discusses the impact of maintaining the same
perturbation results for words of different importance on the overall text
utility. This method can be applied to long text protection.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.03655v1">Privacy versus Emotion Preservation Trade-offs in Emotion-Preserving
  Speaker Anonymization</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-09-05T16:10:31Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zexin Cai, Henry Li Xinyuan, Ashi Garg, Leibny Paola García-Perera, Kevin Duh, Sanjeev Khudanpur, Nicholas Andrews, Matthew Wiesner</p>
    <p><b>Summary:</b> Advances in speech technology now allow unprecedented access to personally
identifiable information through speech. To protect such information, the
differential privacy field has explored ways to anonymize speech while
preserving its utility, including linguistic and paralinguistic aspects.
However, anonymizing speech while maintaining emotional state remains
challenging. We explore this problem in the context of the VoicePrivacy 2024
challenge. Specifically, we developed various speaker anonymization pipelines
and find that approaches either excel at anonymization or preserving emotion
state, but not both simultaneously. Achieving both would require an in-domain
emotion recognizer. Additionally, we found that it is feasible to train a
semi-effective speaker verification system using only emotion representations,
demonstrating the challenge of separating these two modalities.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.03568v1">Enabling Practical and Privacy-Preserving Image Processing</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2024-09-05T14:22:02Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chao Wang, Shubing Yang, Xiaoyan Sun, Jun Dai, Dongfang Zhao</p>
    <p><b>Summary:</b> Fully Homomorphic Encryption (FHE) enables computations on encrypted data,
preserving confidentiality without the need for decryption. However, FHE is
often hindered by significant performance overhead, particularly for
high-precision and complex data like images. Due to serious efficiency issues,
traditional FHE methods often encrypt images by monolithic data blocks (such as
pixel rows), instead of pixels. However, this strategy compromises the
advantages of homomorphic operations and disables pixel-level image processing.
In this study, we address these challenges by proposing and implementing a
pixel-level homomorphic encryption approach, iCHEETAH, based on the CKKS
scheme. To enhance computational efficiency, we introduce three novel caching
mechanisms to pre-encrypt radix values or frequently occurring pixel values,
substantially reducing redundant encryption operations. Extensive experiments
demonstrate that our approach achieves up to a 19-fold improvement in
encryption speed compared to the original CKKS, while maintaining high image
quality. Additionally, real-world image applications such as mean filtering,
brightness enhancement, image matching and watermarking are tested based on
FHE, showcasing up to a 91.53% speed improvement. We also proved that our
method is IND-CPA (Indistinguishability under Chosen Plaintext Attack) secure,
providing strong encryption security. These results underscore the practicality
and efficiency of iCHEETAH, marking a significant advancement in
privacy-preserving image processing at scale.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.03344v1">Rethinking Improved Privacy-Utility Trade-off with Pre-existing
  Knowledge for DP Training</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-09-05T08:40:54Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yu Zheng, Wenchao Zhang, Yonggang Zhang, Wei Song, Kai Zhou, Bo Han</p>
    <p><b>Summary:</b> Differential privacy (DP) provides a provable framework for protecting
individuals by customizing a random mechanism over a privacy-sensitive dataset.
Deep learning models have demonstrated privacy risks in model exposure as an
established learning model unintentionally records membership-level privacy
leakage. Differentially private stochastic gradient descent (DP- SGD) has been
proposed to safeguard training individuals by adding random Gaussian noise to
gradient updates in the backpropagation. Researchers identify that DP-SGD
typically causes utility loss since the injected homogeneous noise alters the
gradient updates calculated at each iteration. Namely, all elements in the
gradient are contaminated regardless of their importance in updating model
parameters. In this work, we argue that the utility loss mainly results from
the homogeneity of injected noise. Consequently, we propose a generic
differential privacy framework with heterogeneous noise (DP-Hero) by defining a
heterogeneous random mechanism to abstract its property. The insight of DP-Hero
is to leverage the knowledge encoded in the previously trained model to guide
the subsequent allocation of noise heterogeneity, thereby leveraging the
statistical perturbation and achieving enhanced utility. Atop DP-Hero, we
instantiate a heterogeneous version of DP-SGD, where the noise injected into
gradients is heterogeneous and guided by prior-established model parameters. We
conduct comprehensive experiments to verify and explain the effectiveness of
the proposed DP-Hero, showing improved training accuracy compared with
state-of-the-art works. Broadly, we shed light on improving the privacy-utility
space by learning the noise guidance from the pre-existing leaked knowledge
encoded in the previously trained model, showing a different perspective of
understanding the utility-improved DP training.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.03326v1">Enhancing User-Centric Privacy Protection: An Interactive Framework
  through Diffusion Models and Machine Unlearning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-09-05T07:55:55Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Huaxi Huang, Xin Yuan, Qiyu Liao, Dadong Wang, Tongliang Liu</p>
    <p><b>Summary:</b> In the realm of multimedia data analysis, the extensive use of image datasets
has escalated concerns over privacy protection within such data. Current
research predominantly focuses on privacy protection either in data sharing or
upon the release of trained machine learning models. Our study pioneers a
comprehensive privacy protection framework that safeguards image data privacy
concurrently during data sharing and model publication. We propose an
interactive image privacy protection framework that utilizes generative machine
learning models to modify image information at the attribute level and employs
machine unlearning algorithms for the privacy preservation of model parameters.
This user-interactive framework allows for adjustments in privacy protection
intensity based on user feedback on generated images, striking a balance
between maximal privacy safeguarding and maintaining model performance. Within
this framework, we instantiate two modules: a differential privacy diffusion
model for protecting attribute information in images and a feature unlearning
algorithm for efficient updates of the trained model on the revised image
dataset. Our approach demonstrated superiority over existing methods on facial
datasets across various attribute classifications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.03294v1">Federated Prototype-based Contrastive Learning for Privacy-Preserving
  Cross-domain Recommendation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Retrieval-5BC0EB">
  <p><b>Published on:</b> 2024-09-05T06:59:56Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Li Wang, Quangui Zhang, Lei Sang, Qiang Wu, Min Xu</p>
    <p><b>Summary:</b> Cross-domain recommendation (CDR) aims to improve recommendation accuracy in
sparse domains by transferring knowledge from data-rich domains. However,
existing CDR methods often assume the availability of user-item interaction
data across domains, overlooking user privacy concerns. Furthermore, these
methods suffer from performance degradation in scenarios with sparse
overlapping users, as they typically depend on a large number of fully shared
users for effective knowledge transfer. To address these challenges, we propose
a Federated Prototype-based Contrastive Learning (CL) method for
Privacy-Preserving CDR, named FedPCL-CDR. This approach utilizes
non-overlapping user information and prototypes to improve multi-domain
performance while protecting user privacy. FedPCL-CDR comprises two modules:
local domain (client) learning and global server aggregation. In the local
domain, FedPCL-CDR clusters all user data to learn representative prototypes,
effectively utilizing non-overlapping user information and addressing the
sparse overlapping user issue. It then facilitates knowledge transfer by
employing both local and global prototypes returned from the server in a CL
manner. Simultaneously, the global server aggregates representative prototypes
from local domains to learn both local and global prototypes. The combination
of prototypes and federated learning (FL) ensures that sensitive user data
remains decentralized, with only prototypes being shared across domains,
thereby protecting user privacy. Extensive experiments on four CDR tasks using
two real-world datasets demonstrate that FedPCL-CDR outperforms the
state-of-the-art baselines.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.03796v1">Protecting Activity Sensing Data Privacy Using Hierarchical Information
  Dissociation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-09-04T15:38:00Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Guangjing Wang, Hanqing Guo, Yuanda Wang, Bocheng Chen, Ce Zhou, Qiben Yan</p>
    <p><b>Summary:</b> Smartphones and wearable devices have been integrated into our daily lives,
offering personalized services. However, many apps become overprivileged as
their collected sensing data contains unnecessary sensitive information. For
example, mobile sensing data could reveal private attributes (e.g., gender and
age) and unintended sensitive features (e.g., hand gestures when entering
passwords). To prevent sensitive information leakage, existing methods must
obtain private labels and users need to specify privacy policies. However, they
only achieve limited control over information disclosure. In this work, we
present Hippo to dissociate hierarchical information including private metadata
and multi-grained activity information from the sensing data. Hippo achieves
fine-grained control over the disclosure of sensitive information without
requiring private labels. Specifically, we design a latent guidance-based
diffusion model, which generates multi-grained versions of raw sensor data
conditioned on hierarchical latent activity features. Hippo enables users to
control the disclosure of sensitive information in sensing data, ensuring their
privacy while preserving the necessary features to meet the utility
requirements of applications. Hippo is the first unified model that achieves
two goals: perturbing the sensitive attributes and controlling the disclosure
of sensitive information in mobile sensing data. Extensive experiments show
that Hippo can anonymize personal attributes and transform activity information
at various resolutions across different types of sensing data.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.02614v1">Evaluating the Effects of Digital Privacy Regulations on User Trust</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2024-09-04T11:11:41Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mehmet Berk Cetin</p>
    <p><b>Summary:</b> In today's digital society, issues related to digital privacy have become
increasingly important. Issues such as data breaches result in misuse of data,
financial loss, and cyberbullying, which leads to less user trust in digital
services. This research investigates the impact of digital privacy laws on user
trust by comparing the regulations in the Netherlands, Ghana, and Malaysia. The
study employs a comparative case study method, involving interviews with
digital privacy law experts, IT educators, and consumers from each country. The
main findings reveal that while the General Data Protection Regulation (GDPR)
in the Netherlands is strict, its practical impact is limited by enforcement
challenges. In Ghana, the Data Protection Act is underutilized due to low
public awareness and insufficient enforcement, leading to reliance on personal
protective measures. In Malaysia, trust in digital services is largely
dependent on the security practices of individual platforms rather than the
Personal Data Protection Act. The study highlights the importance of public
awareness, effective enforcement, and cultural considerations in shaping the
effectiveness of digital privacy laws. Based on these insights, a
recommendation framework is proposed to enhance digital privacy practices, also
aiming to provide valuable guidance for policymakers, businesses, and citizens
in navigating the challenges of digitalization.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.02404v1">Learning Privacy-Preserving Student Networks via
  Discriminative-Generative Distillation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-09-04T03:06:13Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shiming Ge, Bochao Liu, Pengju Wang, Yong Li, Dan Zeng</p>
    <p><b>Summary:</b> While deep models have proved successful in learning rich knowledge from
massive well-annotated data, they may pose a privacy leakage risk in practical
deployment. It is necessary to find an effective trade-off between high utility
and strong privacy. In this work, we propose a discriminative-generative
distillation approach to learn privacy-preserving deep models. Our key idea is
taking models as bridge to distill knowledge from private data and then
transfer it to learn a student network via two streams. First, discriminative
stream trains a baseline classifier on private data and an ensemble of teachers
on multiple disjoint private subsets, respectively. Then, generative stream
takes the classifier as a fixed discriminator and trains a generator in a
data-free manner. After that, the generator is used to generate massive
synthetic data which are further applied to train a variational autoencoder
(VAE). Among these synthetic data, a few of them are fed into the teacher
ensemble to query labels via differentially private aggregation, while most of
them are embedded to the trained VAE for reconstructing synthetic data.
Finally, a semi-supervised student learning is performed to simultaneously
handle two tasks: knowledge transfer from the teachers with distillation on few
privately labeled synthetic data, and knowledge enhancement with tangent-normal
adversarial regularization on many triples of reconstructed synthetic data. In
this way, our approach can control query cost over private data and mitigate
accuracy degradation in a unified manner, leading to a privacy-preserving
student model. Extensive experiments and analysis clearly show the
effectiveness of the proposed approach.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.02375v1">How Privacy-Savvy Are Large Language Models? A Case Study on Compliance
  and Privacy Technical Review</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2024-09-04T01:51:37Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xichou Zhu, Yang Liu, Zhou Shen, Yi Liu, Min Li, Yujun Chen, Benzi John, Zhenzhen Ma, Tao Hu, Bolong Yang, Manman Wang, Zongxing Xie, Peng Liu, Dan Cai, Junhui Wang</p>
    <p><b>Summary:</b> The recent advances in large language models (LLMs) have significantly
expanded their applications across various fields such as language generation,
summarization, and complex question answering. However, their application to
privacy compliance and technical privacy reviews remains under-explored,
raising critical concerns about their ability to adhere to global privacy
standards and protect sensitive user data. This paper seeks to address this gap
by providing a comprehensive case study evaluating LLMs' performance in
privacy-related tasks such as privacy information extraction (PIE), legal and
regulatory key point detection (KPD), and question answering (QA) with respect
to privacy policies and data protection regulations. We introduce a Privacy
Technical Review (PTR) framework, highlighting its role in mitigating privacy
risks during the software development life-cycle. Through an empirical
assessment, we investigate the capacity of several prominent LLMs, including
BERT, GPT-3.5, GPT-4, and custom models, in executing privacy compliance checks
and technical privacy reviews. Our experiments benchmark the models across
multiple dimensions, focusing on their precision, recall, and F1-scores in
extracting privacy-sensitive information and detecting key regulatory
compliance points. While LLMs show promise in automating privacy reviews and
identifying regulatory discrepancies, significant gaps persist in their ability
to fully comply with evolving legal standards. We provide actionable
recommendations for enhancing LLMs' capabilities in privacy compliance,
emphasizing the need for robust model improvements and better integration with
legal and regulatory requirements. This study underscores the growing
importance of developing privacy-aware LLMs that can both support businesses in
compliance efforts and safeguard user privacy rights.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.02364v1">Examining Caregiving Roles to Differentiate the Effects of Using a
  Mobile App for Community Oversight for Privacy and Security</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2024-09-04T01:21:56Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mamtaj Akter, Jess Kropczynski, Heather Lipford, Pamela Wisniewski</p>
    <p><b>Summary:</b> We conducted a 4-week field study with 101 smartphone users who
self-organized into 22 small groups of family, friends, and neighbors to use
``CO-oPS,'' a mobile app for co-managing mobile privacy and security. We
differentiated between those who provided oversight (i.e., caregivers) and
those who did not (i.e., caregivees) to examine differential effects on their
experiences and behaviors while using CO-oPS. Caregivers reported higher power
use, community trust, belonging, collective efficacy, and self-efficacy than
caregivees. Both groups' self-efficacy and collective efficacy for mobile
privacy and security increased after using CO-oPS. However, this increase was
significantly stronger for caregivees. Our research demonstrates how
community-based approaches can benefit people who need additional help managing
their digital privacy and security. We provide recommendations to support
community-based oversight for managing privacy and security within communities
of different roles and skills.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.02044v1">FedMinds: Privacy-Preserving Personalized Brain Visual Decoding</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB"> 
  <p><b>Published on:</b> 2024-09-03T16:46:29Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Guangyin Bao, Duoqian Miao</p>
    <p><b>Summary:</b> Exploring the mysteries of the human brain is a long-term research topic in
neuroscience. With the help of deep learning, decoding visual information from
human brain activity fMRI has achieved promising performance. However, these
decoding models require centralized storage of fMRI data to conduct training,
leading to potential privacy security issues. In this paper, we focus on
privacy preservation in multi-individual brain visual decoding. To this end, we
introduce a novel framework called FedMinds, which utilizes federated learning
to protect individuals' privacy during model training. In addition, we deploy
individual adapters for each subject, thus allowing personalized visual
decoding. We conduct experiments on the authoritative NSD datasets to evaluate
the performance of the proposed framework. The results demonstrate that our
framework achieves high-precision visual decoding along with privacy
protection.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.01924v1">Privacy-Preserving and Post-Quantum Counter Denial of Service Framework
  for Wireless Networks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-09-03T14:14:41Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Saleh Darzi, Attila Altay Yavuz</p>
    <p><b>Summary:</b> As network services progress and mobile and IoT environments expand, numerous
security concerns have surfaced for spectrum access systems. The omnipresent
risk of Denial-of-Service (DoS) attacks and raising concerns about user privacy
(e.g., location privacy, anonymity) are among such cyber threats. These
security and privacy risks increase due to the threat of quantum computers that
can compromise long-term security by circumventing conventional cryptosystems
and increasing the cost of countermeasures. While some defense mechanisms exist
against these threats in isolation, there is a significant gap in the state of
the art on a holistic solution against DoS attacks with privacy and anonymity
for spectrum management systems, especially when post-quantum (PQ) security is
in mind. In this paper, we propose a new cybersecurity framework PACDoSQ, which
is (to the best of our knowledge) the first to offer location privacy and
anonymity for spectrum management with counter DoS and PQ security
simultaneously. Our solution introduces the private spectrum bastion (database)
concept to exploit existing architectural features of spectrum management
systems and then synergizes them with multi-server private information
retrieval and PQ-secure Tor to guarantee a location-private and anonymous
acquisition of spectrum information together with hash-based client-server
puzzles for counter DoS. We prove that PACDoSQ achieves its security
objectives, and show its feasibility via a comprehensive performance
evaluation.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.01710v1">Privacy-Preserving Multimedia Mobile Cloud Computing Using Protective
  Perturbation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Multimedia-5BC0EB">
  <p><b>Published on:</b> 2024-09-03T08:47:17Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhongze Tang, Mengmei Ye, Yao Liu, Sheng Wei</p>
    <p><b>Summary:</b> Mobile cloud computing has been adopted in many multimedia applications,
where the resource-constrained mobile device sends multimedia data (e.g.,
images) to remote cloud servers to request computation-intensive multimedia
services (e.g., image recognition). While significantly improving the
performance of the mobile applications, the cloud-based mechanism often causes
privacy concerns as the multimedia data and services are offloaded from the
trusted user device to untrusted cloud servers. Several recent studies have
proposed perturbation-based privacy preserving mechanisms, which obfuscate the
offloaded multimedia data to eliminate privacy exposures without affecting the
functionality of the remote multimedia services. However, the existing privacy
protection approaches require the deployment of computation-intensive
perturbation generation on the resource-constrained mobile devices. Also, the
obfuscated images are typically not compliant with the standard image
compression algorithms and suffer from significant bandwidth consumption. In
this paper, we develop a novel privacy-preserving multimedia mobile cloud
computing framework, namely $PMC^2$, to address the resource and bandwidth
challenges. $PMC^2$ employs secure confidential computing in the cloud to
deploy the perturbation generator, which addresses the resource challenge while
maintaining the privacy. Furthermore, we develop a neural compressor
specifically trained to compress the perturbed images in order to address the
bandwidth challenge. We implement $PMC^2$ in an end-to-end mobile cloud
computing system, based on which our evaluations demonstrate superior latency,
power efficiency, and bandwidth consumption achieved by $PMC^2$ while
maintaining high accuracy in the target multimedia service.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.01661v1">$S^2$NeRF: Privacy-preserving Training Framework for NeRF</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-09-03T07:08:30Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Bokang Zhang, Yanglin Zhang, Zhikun Zhang, Jinglan Yang, Lingying Huang, Junfeng Wu</p>
    <p><b>Summary:</b> Neural Radiance Fields (NeRF) have revolutionized 3D computer vision and
graphics, facilitating novel view synthesis and influencing sectors like
extended reality and e-commerce. However, NeRF's dependence on extensive data
collection, including sensitive scene image data, introduces significant
privacy risks when users upload this data for model training. To address this
concern, we first propose SplitNeRF, a training framework that incorporates
split learning (SL) techniques to enable privacy-preserving collaborative model
training between clients and servers without sharing local data. Despite its
benefits, we identify vulnerabilities in SplitNeRF by developing two attack
methods, Surrogate Model Attack and Scene-aided Surrogate Model Attack, which
exploit the shared gradient data and a few leaked scene images to reconstruct
private scene information. To counter these threats, we introduce $S^2$NeRF,
secure SplitNeRF that integrates effective defense mechanisms. By introducing
decaying noise related to the gradient norm into the shared gradient
information, $S^2$NeRF preserves privacy while maintaining a high utility of
the NeRF model. Our extensive evaluations across multiple datasets demonstrate
the effectiveness of $S^2$NeRF against privacy breaches, confirming its
viability for secure NeRF training in sensitive applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.01329v1">Assessing the Impact of Image Dataset Features on Privacy-Preserving
  Machine Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-09-02T15:30:27Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Lucas Lange, Maurice-Maximilian Heykeroth, Erhard Rahm</p>
    <p><b>Summary:</b> Machine Learning (ML) is crucial in many sectors, including computer vision.
However, ML models trained on sensitive data face security challenges, as they
can be attacked and leak information. Privacy-Preserving Machine Learning
(PPML) addresses this by using Differential Privacy (DP) to balance utility and
privacy. This study identifies image dataset characteristics that affect the
utility and vulnerability of private and non-private Convolutional Neural
Network (CNN) models. Through analyzing multiple datasets and privacy budgets,
we find that imbalanced datasets increase vulnerability in minority classes,
but DP mitigates this issue. Datasets with fewer classes improve both model
utility and privacy, while high entropy or low Fisher Discriminant Ratio (FDR)
datasets deteriorate the utility-privacy trade-off. These insights offer
valuable guidance for practitioners and researchers in estimating and
optimizing the utility-privacy trade-off in image datasets, helping to inform
data and privacy modifications for better outcomes based on dataset
characteristics.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.01088v1">Towards Split Learning-based Privacy-Preserving Record Linkage</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-09-02T09:17:05Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Michail Zervas, Alexandros Karakasidis</p>
    <p><b>Summary:</b> Split Learning has been recently introduced to facilitate applications where
user data privacy is a requirement. However, it has not been thoroughly studied
in the context of Privacy-Preserving Record Linkage, a problem in which the
same real-world entity should be identified among databases from different
dataholders, but without disclosing any additional information. In this paper,
we investigate the potentials of Split Learning for Privacy-Preserving Record
Matching, by introducing a novel training method through the utilization of
Reference Sets, which are publicly available data corpora, showcasing minimal
matching impact against a traditional centralized SVM-based technique.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.00974v1">Enhancing Privacy in Federated Learning: Secure Aggregation for
  Real-World Healthcare Applications</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-09-02T06:43:22Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Riccardo Taiello, Sergen Cansiz, Marc Vesin, Francesco Cremonesi, Lucia Innocenti, Melek Önen, Marco Lorenzi</p>
    <p><b>Summary:</b> Deploying federated learning (FL) in real-world scenarios, particularly in
healthcare, poses challenges in communication and security. In particular, with
respect to the federated aggregation procedure, researchers have been focusing
on the study of secure aggregation (SA) schemes to provide privacy guarantees
over the model's parameters transmitted by the clients. Nevertheless, the
practical availability of SA in currently available FL frameworks is currently
limited, due to computational and communication bottlenecks. To fill this gap,
this study explores the implementation of SA within the open-source Fed-BioMed
framework. We implement and compare two SA protocols, Joye-Libert (JL) and Low
Overhead Masking (LOM), by providing extensive benchmarks in a panel of
healthcare data analysis problems. Our theoretical and experimental evaluations
on four datasets demonstrate that SA protocols effectively protect privacy
while maintaining task accuracy. Computational overhead during training is less
than 1% on a CPU and less than 50% on a GPU for large models, with protection
phases taking less than 10 seconds. Incorporating SA into Fed-BioMed impacts
task accuracy by no more than 2% compared to non-SA scenarios. Overall this
study demonstrates the feasibility of SA in real-world healthcare applications
and contributes in reducing the gap towards the adoption of privacy-preserving
technologies in sensitive applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.00740v1">VPVet: Vetting Privacy Policies of Virtual Reality Apps</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-09-01T15:07:11Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yuxia Zhan, Yan Meng, Lu Zhou, Yichang Xiong, Xiaokuan Zhang, Lichuan Ma, Guoxing Chen, Qingqi Pei, Haojin Zhu</p>
    <p><b>Summary:</b> Virtual reality (VR) apps can harvest a wider range of user data than
web/mobile apps running on personal computers or smartphones. Existing law and
privacy regulations emphasize that VR developers should inform users of what
data are collected/used/shared (CUS) through privacy policies. However, privacy
policies in the VR ecosystem are still in their early stages, and many
developers fail to write appropriate privacy policies that comply with
regulations and meet user expectations. In this paper, we propose VPVet to
automatically vet privacy policy compliance issues for VR apps. VPVet first
analyzes the availability and completeness of a VR privacy policy and then
refines its analysis based on three key criteria: granularity, minimization,
and consistency of CUS statements. Our study establishes the first and
currently largest VR privacy policy dataset named VRPP, consisting of privacy
policies of 11,923 different VR apps from 10 mainstream platforms. Our vetting
results reveal severe privacy issues within the VR ecosystem, including the
limited availability and poor quality of privacy policies, along with their
coarse granularity, lack of adaptation to VR traits and the inconsistency
between CUS statements in privacy policies and their actual behaviors. We
open-source VPVet system along with our findings at repository
https://github.com/kalamoo/PPAudit, aiming to raise awareness within the VR
community and pave the way for further research in this field.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.00739v1">Designing and Evaluating Scalable Privacy Awareness and Control User
  Interfaces for Mixed Reality</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2024-09-01T15:06:40Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Marvin Strauss, Viktorija Paneva, Florian Alt, Stefan Schneegass</p>
    <p><b>Summary:</b> As Mixed Reality (MR) devices become increasingly popular across industries,
they raise significant privacy and ethical concerns due to their capacity to
collect extensive data on users and their environments. This paper highlights
the urgent need for privacy-aware user interfaces that educate and empower both
users and bystanders, enabling them to understand, control, and manage data
collection and sharing. Key research questions include improving user awareness
of privacy implications, developing usable privacy controls, and evaluating the
effectiveness of these measures in real-world settings. The proposed research
roadmap aims to embed privacy considerations into the design and development of
MR technologies, promoting responsible innovation that safeguards user privacy
while preserving the functionality and appeal of these emerging technologies.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2409.02715v1">Recoverable Anonymization for Pose Estimation: A Privacy-Enhancing
  Approach</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-09-01T05:58:00Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Wenjun Huang, Yang Ni, Arghavan Rezvani, SungHeon Jeong, Hanning Chen, Yezi Liu, Fei Wen, Mohsen Imani</p>
    <p><b>Summary:</b> Human pose estimation (HPE) is crucial for various applications. However,
deploying HPE algorithms in surveillance contexts raises significant privacy
concerns due to the potential leakage of sensitive personal information (SPI)
such as facial features, and ethnicity. Existing privacy-enhancing methods
often compromise either privacy or performance, or they require costly
additional modalities. We propose a novel privacy-enhancing system that
generates privacy-enhanced portraits while maintaining high HPE performance.
Our key innovations include the reversible recovery of SPI for authorized
personnel and the preservation of contextual information. By jointly optimizing
a privacy-enhancing module, a privacy recovery module, and a pose estimator,
our system ensures robust privacy protection, efficient SPI recovery, and
high-performance HPE. Experimental results demonstrate the system's robust
performance in privacy enhancement, SPI recovery, and HPE.</p>
  </details>
</div>

