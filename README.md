
<h2>2025-04</h2>

<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.21752v1">VDDP: Verifiable Distributed Differential Privacy under the
  Client-Server-Verifier Setup</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB">
  <p><b>Published on:</b> 2025-04-30T15:46:55Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Haochen Sun, Xi He</p>
    <p><b>Summary:</b> Despite differential privacy (DP) often being considered the de facto
standard for data privacy, its realization is vulnerable to unfaithful
execution of its mechanisms by servers, especially in distributed settings.
Specifically, servers may sample noise from incorrect distributions or generate
correlated noise while appearing to follow established protocols. This work
analyzes these malicious behaviors in a general differential privacy framework
within a distributed client-server-verifier setup. To address these adversarial
problems, we propose a novel definition called Verifiable Distributed
Differential Privacy (VDDP) by incorporating additional verification
mechanisms. We also explore the relationship between zero-knowledge proofs
(ZKP) and DP, demonstrating that while ZKPs are sufficient for achieving DP
under verifiability requirements, they are not necessary. Furthermore, we
develop two novel and efficient mechanisms that satisfy VDDP: (1) the
Verifiable Distributed Discrete Laplacian Mechanism (VDDLM), which offers up to
a $4 \times 10^5$x improvement in proof generation efficiency with only
0.1-0.2x error compared to the previous state-of-the-art verifiable
differentially private mechanism; (2) an improved solution to Verifiable
Randomized Response (VRR) under local DP, a special case of VDDP, achieving up
a reduction of up to 5000x in communication costs and the verifier's overhead.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.21646v1">Diffusion-based Adversarial Identity Manipulation for Facial Privacy
  Protection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-04-30T13:49:59Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Liqin Wang, Qianyue Hu, Wei Lu, Xiangyang Luo</p>
    <p><b>Summary:</b> The success of face recognition (FR) systems has led to serious privacy
concerns due to potential unauthorized surveillance and user tracking on social
networks. Existing methods for enhancing privacy fail to generate natural face
images that can protect facial privacy. In this paper, we propose
diffusion-based adversarial identity manipulation (DiffAIM) to generate natural
and highly transferable adversarial faces against malicious FR systems. To be
specific, we manipulate facial identity within the low-dimensional latent space
of a diffusion model. This involves iteratively injecting gradient-based
adversarial identity guidance during the reverse diffusion process,
progressively steering the generation toward the desired adversarial faces. The
guidance is optimized for identity convergence towards a target while promoting
semantic divergence from the source, facilitating effective impersonation while
maintaining visual naturalness. We further incorporate structure-preserving
regularization to preserve facial structure consistency during manipulation.
Extensive experiments on both face verification and identification tasks
demonstrate that compared with the state-of-the-art, DiffAIM achieves stronger
black-box attack transferability while maintaining superior visual quality. We
also demonstrate the effectiveness of the proposed approach for commercial FR
APIs, including Face++ and Aliyun.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.21413v1">An Inversion Theorem for Buffered Linear Toeplitz (BLT) Matrices and
  Applications to Streaming Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> 
  <p><b>Published on:</b> 2025-04-30T08:14:09Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> H. Brendan McMahan, Krishna Pillutla</p>
    <p><b>Summary:</b> Buffered Linear Toeplitz (BLT) matrices are a family of parameterized
lower-triangular matrices that play an important role in streaming differential
privacy with correlated noise. Our main result is a BLT inversion theorem: the
inverse of a BLT matrix is itself a BLT matrix with different parameters. We
also present an efficient and differentiable $O(d^3)$ algorithm to compute the
parameters of the inverse BLT matrix, where $d$ is the degree of the original
BLT (typically $d < 10$). Our characterization enables direct optimization of
BLT parameters for privacy mechanisms through automatic differentiation.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.21297v1">Participatory AI, Public Sector AI, Differential Privacy, Conversational
  Interfaces, Explainable AI, Citizen Engagement in AI</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Emerging Technologies-F9C80E"> 
  <p><b>Published on:</b> 2025-04-30T04:10:50Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Wenjun Yang, Eyhab Al-Masri</p>
    <p><b>Summary:</b> This paper introduces a conversational interface system that enables
participatory design of differentially private AI systems in public sector
applications. Addressing the challenge of balancing mathematical privacy
guarantees with democratic accountability, we propose three key contributions:
(1) an adaptive $\epsilon$-selection protocol leveraging TOPSIS multi-criteria
decision analysis to align citizen preferences with differential privacy (DP)
parameters, (2) an explainable noise-injection framework featuring real-time
Mean Absolute Error (MAE) visualizations and GPT-4-powered impact analysis, and
(3) an integrated legal-compliance mechanism that dynamically modulates privacy
budgets based on evolving regulatory constraints. Our results advance
participatory AI practices by demonstrating how conversational interfaces can
enhance public engagement in algorithmic privacy mechanisms, ensuring that
privacy-preserving AI in public sector governance remains both mathematically
robust and democratically accountable.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.21182v1">Federated One-Shot Learning with Data Privacy and Objective-Hiding</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">  
  <p><b>Published on:</b> 2025-04-29T21:25:34Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Maximilian Egger, RÃ¼diger Urbanke, Rawad Bitar</p>
    <p><b>Summary:</b> Privacy in federated learning is crucial, encompassing two key aspects:
safeguarding the privacy of clients' data and maintaining the privacy of the
federator's objective from the clients. While the first aspect has been
extensively studied, the second has received much less attention.
  We present a novel approach that addresses both concerns simultaneously,
drawing inspiration from techniques in knowledge distillation and private
information retrieval to provide strong information-theoretic privacy
guarantees.
  Traditional private function computation methods could be used here; however,
they are typically limited to linear or polynomial functions. To overcome these
constraints, our approach unfolds in three stages. In stage 0, clients perform
the necessary computations locally. In stage 1, these results are shared among
the clients, and in stage 2, the federator retrieves its desired objective
without compromising the privacy of the clients' data. The crux of the method
is a carefully designed protocol that combines secret-sharing-based multi-party
computation and a graph-based private information retrieval scheme. We show
that our method outperforms existing tools from the literature when properly
adapted to this setting.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.20941v1">Conformal-DP: Differential Privacy on Riemannian Manifolds via Conformal
  Transformation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">  
  <p><b>Published on:</b> 2025-04-29T17:05:55Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Peilin He, Liou Tang, M. Amin Rahimian, James Joshi</p>
    <p><b>Summary:</b> Differential Privacy (DP) has been established as a safeguard for private
data sharing by adding perturbations to information release. Prior research on
DP has extended beyond data in the flat Euclidean space and addressed data on
curved manifolds, e.g., diffusion tensor MRI, social networks, or organ shape
analysis, by adding perturbations along geodesic distances. However, existing
manifold-aware DP methods rely on the assumption that samples are uniformly
distributed across the manifold. In reality, data densities vary, leading to a
biased noise imbalance across manifold regions, weakening the privacy-utility
trade-offs. To address this gap, we propose a novel mechanism: Conformal-DP,
utilizing conformal transformations on the Riemannian manifold to equalize
local sample density and to redefine geodesic distances accordingly while
preserving the intrinsic geometry of the manifold. Our theoretical analysis
yields two main results. First, we prove that the conformal factor computed
from local kernel-density estimates is explicitly data-density-aware; Second,
under the conformal metric, the mechanism satisfies $ \varepsilon
$-differential privacy on any complete Riemannian manifold and admits a
closed-form upper bound on the expected geodesic error that depends only on the
maximal density ratio, not on global curvatureof the manifold. Our experimental
results validate that the mechanism achieves high utility while providing the $
\varepsilon $-DP guarantee for both homogeneous and especially heterogeneous
manifold data.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.20926v1">Bipartite Randomized Response Mechanism for Local Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-04-29T16:39:50Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shun Zhang, Hai Zhu, Zhili Chen, Neal N. Xiong</p>
    <p><b>Summary:</b> With the increasing importance of data privacy, Local Differential Privacy
(LDP) has recently become a strong measure of privacy for protecting each
user's privacy from data analysts without relying on a trusted third party. In
many cases, both data providers and data analysts hope to maximize the utility
of released data. In this paper, we study the fundamental trade-off formulated
as a constrained optimization problem: maximizing data utility subject to the
constraint of LDP budgets. In particular, the Generalized Randomized Response
(GRR) treats all discrete data equally except for the true data. For this, we
introduce an adaptive LDP mechanism called Bipartite Randomized Response (BRR),
which solves the above privacy-utility maximization problem from the global
standpoint. We prove that for any utility function and any privacy level,
solving the maximization problem is equivalent to confirming how many
high-utility data to be treated equally as the true data on release
probability, the outcome of which gives the optimal randomized response.
Further, solving this linear program can be computationally cheap in theory.
Several examples of utility functions defined by distance metrics and
applications in decision trees and deep learning are presented. The results of
various experiments show that our BRR significantly outperforms the
state-of-the-art LDP mechanisms of both continuous and distributed types.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.20827v1">DP-SMOTE: Integrating Differential Privacy and Oversampling Technique to
  Preserve Privacy in Smart Homes</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-04-29T14:50:50Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Amr Tarek Elsayed, Almohammady Sobhi Alsharkawy, Mohamed Sayed Farag, Shaban Ebrahim Abu Yusuf</p>
    <p><b>Summary:</b> Smart homes represent intelligent environments where interconnected devices
gather information, enhancing users living experiences by ensuring comfort,
safety, and efficient energy management. To enhance the quality of life,
companies in the smart device industry collect user data, including activities,
preferences, and power consumption. However, sharing such data necessitates
privacy-preserving practices. This paper introduces a robust method for secure
sharing of data to service providers, grounded in differential privacy (DP).
This empowers smart home residents to contribute usage statistics while
safeguarding their privacy. The approach incorporates the Synthetic Minority
Oversampling technique (SMOTe) and seamlessly integrates Gaussian noise to
generate synthetic data, enabling data and statistics sharing while preserving
individual privacy. The proposed method employs the SMOTe algorithm and applies
Gaussian noise to generate data. Subsequently, it employs a k-anonymity
function to assess reidentification risk before sharing the data. The
simulation outcomes demonstrate that our method delivers strong performance in
safeguarding privacy and in accuracy, recall, and f-measure metrics. This
approach is particularly effective in smart homes, offering substantial utility
in privacy at a reidentification risk of 30%, with Gaussian noise set to 0.3,
SMOTe at 500%, and the application of a k-anonymity function with k = 2.
Additionally, it shows a high classification accuracy, ranging from 90% to 98%,
across various classification techniques.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.20700v1">Building Trust in Healthcare with Privacy Techniques: Blockchain in the
  Cloud</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-04-29T12:31:37Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ferhat Ozgur Catak, Chunming Rong, Ãyvind Meinich-Bache, Sara Brunner, Kjersti Engan</p>
    <p><b>Summary:</b> This study introduces a cutting-edge architecture developed for the
NewbornTime project, which uses advanced AI to analyze video data at birth and
during newborn resuscitation, with the aim of improving newborn care. The
proposed architecture addresses the crucial issues of patient consent, data
security, and investing trust in healthcare by integrating Ethereum blockchain
with cloud computing. Our blockchain-based consent application simplifies
patient consent's secure and transparent management. We explain the smart
contract mechanisms and privacy measures employed, ensuring data protection
while permitting controlled data sharing among authorized parties. This work
demonstrates the potential of combining blockchain and cloud technologies in
healthcare, emphasizing their role in maintaining data integrity, with
implications for computer science and healthcare innovation.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.20639v1">Multi-Message Secure Aggregation with Demand Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2025-04-29T11:11:27Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chenyi Sun, Ziting Zhang, Kai Wan, Giuseppe Caire</p>
    <p><b>Summary:</b> This paper considers a multi-message secure aggregation with privacy problem,
in which a server aims to compute $\sf K_c\geq 1$ linear combinations of local
inputs from $\sf K$ distributed users. The problem addresses two tasks: (1)
security, ensuring that the server can only obtain the desired linear
combinations without any else information about the users' inputs, and (2)
privacy, preventing users from learning about the server's computation task. In
addition, the effect of user dropouts is considered, where at most $\sf{K-U}$
users can drop out and the identity of these users cannot be predicted in
advance. We propose two schemes for $\sf K_c$ is equal to (1) and $\sf 2\leq
K_c\leq U-1$, respectively. For $\sf K_c$ is equal to (1), we introduce
multiplicative encryption of the server's demand using a random variable, where
users share coded keys offline and transmit masked models in the first round,
followed by aggregated coded keys in the second round for task recovery. For
$\sf{2\leq K_c \leq U-1}$, we use robust symmetric private computation to
recover linear combinations of keys in the second round. The objective is to
minimize the number of symbols sent by each user during the two rounds. Our
proposed schemes have achieved the optimal rate region when $ \sf K_c $ is
equal to (1) and the order optimal rate (within 2) when $\sf{2\leq K_c \leq
U-1}$.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.20350v2">SoK: Enhancing Privacy-Preserving Software Development from a
  Developers' Perspective</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Software Engineering-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-04-29T01:38:01Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tharaka Wijesundara, Matthew Warren, Nalin Asanka Gamagedara Arachchilage</p>
    <p><b>Summary:</b> In software development, privacy preservation has become essential with the
rise of privacy concerns and regulations such as GDPR and CCPA. While several
tools, guidelines, methods, methodologies, and frameworks have been proposed to
support developers embedding privacy into software applications, most of them
are proofs-of-concept without empirical evaluations, making their practical
applicability uncertain. These solutions should be evaluated for different
types of scenarios (e.g., industry settings such as rapid software development
environments, teams with different privacy knowledge, etc.) to determine what
their limitations are in various industry settings and what changes are
required to refine current solutions before putting them into industry and
developing new developer-supporting approaches. For that, a thorough review of
empirically evaluated current solutions will be very effective. However, the
existing secondary studies that examine the available developer support provide
broad overviews but do not specifically analyze empirically evaluated solutions
and their limitations. Therefore, this Systematic Literature Review (SLR) aims
to identify and analyze empirically validated solutions that are designed to
help developers in privacy-preserving software development. The findings will
provide valuable insights for researchers to improve current privacy-preserving
solutions and for practitioners looking for effective and validated solutions
to embed privacy into software development.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.20282v1">FedCCL: Federated Clustered Continual Learning Framework for
  Privacy-focused Energy Forecasting</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-04-28T21:51:27Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Michael A. Helcig, Stefan Nastic</p>
    <p><b>Summary:</b> Privacy-preserving distributed model training is crucial for modern machine
learning applications, yet existing Federated Learning approaches struggle with
heterogeneous data distributions and varying computational capabilities.
Traditional solutions either treat all participants uniformly or require costly
dynamic clustering during training, leading to reduced efficiency and delayed
model specialization. We present FedCCL (Federated Clustered Continual
Learning), a framework specifically designed for environments with static
organizational characteristics but dynamic client availability. By combining
static pre-training clustering with an adapted asynchronous FedAvg algorithm,
FedCCL enables new clients to immediately profit from specialized models
without prior exposure to their data distribution, while maintaining reduced
coordination overhead and resilience to client disconnections. Our approach
implements an asynchronous Federated Learning protocol with a three-tier model
topology - global, cluster-specific, and local models - that efficiently
manages knowledge sharing across heterogeneous participants. Evaluation using
photovoltaic installations across central Europe demonstrates that FedCCL's
location-based clustering achieves an energy prediction error of 3.93%
(+-0.21%), while maintaining data privacy and showing that the framework
maintains stability for population-independent deployments, with 0.14
percentage point degradation in performance for new installations. The results
demonstrate that FedCCL offers an effective framework for privacy-preserving
distributed learning, maintaining high accuracy and adaptability even with
dynamic participant populations.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.21036v2">Can Differentially Private Fine-tuning LLMs Protect Against Privacy
  Attacks?</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-04-28T05:34:53Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Hao Du, Shang Liu, Yang Cao</p>
    <p><b>Summary:</b> Fine-tuning large language models (LLMs) has become an essential strategy for
adapting them to specialized tasks; however, this process introduces
significant privacy challenges, as sensitive training data may be inadvertently
memorized and exposed. Although differential privacy (DP) offers strong
theoretical guarantees against such leakage, its empirical privacy
effectiveness on LLMs remains unclear, especially under different fine-tuning
methods. In this paper, we systematically investigate the impact of DP across
fine-tuning methods and privacy budgets, using both data extraction and
membership inference attacks to assess empirical privacy risks. Our main
findings are as follows: (1) Differential privacy reduces model utility, but
its impact varies significantly across different fine-tuning methods. (2)
Without DP, the privacy risks of models fine-tuned with different approaches
differ considerably. (3) When DP is applied, even a relatively high privacy
budget can substantially lower privacy risk. (4) The privacy-utility trade-off
under DP training differs greatly among fine-tuning methods, with some methods
being unsuitable for DP due to severe utility degradation. Our results provide
practical guidance for privacy-conscious deployment of LLMs and pave the way
for future research on optimizing the privacy-utility trade-off in fine-tuning
methodologies.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.21035v2">A False Sense of Privacy: Evaluating Textual Data Sanitization Beyond
  Surface-level Privacy Leakage</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-04-28T01:16:27Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Rui Xin, Niloofar Mireshghallah, Shuyue Stella Li, Michael Duan, Hyunwoo Kim, Yejin Choi, Yulia Tsvetkov, Sewoong Oh, Pang Wei Koh</p>
    <p><b>Summary:</b> Sanitizing sensitive text data typically involves removing personally
identifiable information (PII) or generating synthetic data under the
assumption that these methods adequately protect privacy; however, their
effectiveness is often only assessed by measuring the leakage of explicit
identifiers but ignoring nuanced textual markers that can lead to
re-identification. We challenge the above illusion of privacy by proposing a
new framework that evaluates re-identification attacks to quantify individual
privacy risks upon data release. Our approach shows that seemingly innocuous
auxiliary information -- such as routine social activities -- can be used to
infer sensitive attributes like age or substance use history from sanitized
data. For instance, we demonstrate that Azure's commercial PII removal tool
fails to protect 74\% of information in the MedQA dataset. Although
differential privacy mitigates these risks to some extent, it significantly
reduces the utility of the sanitized text for downstream tasks. Our findings
indicate that current sanitization techniques offer a \textit{false sense of
privacy}, highlighting the need for more robust methods that protect against
semantic-level information leakage.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.19373v2">Doxing via the Lens: Revealing Privacy Leakage in Image Geolocation for
  Agentic Multi-Modal Large Reasoning Model</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-04-27T22:26:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Weidi Luo, Qiming Zhang, Tianyu Lu, Xiaogeng Liu, Yue Zhao, Zhen Xiang, Chaowei Xiao</p>
    <p><b>Summary:</b> The increasing capabilities of agentic multi-modal large reasoning models,
such as ChatGPT o3, have raised critical concerns regarding privacy leakage
through inadvertent image geolocation. In this paper, we conduct the first
systematic and controlled study on the potential privacy risks associated with
visual reasoning abilities of ChatGPT o3. We manually collect and construct a
dataset comprising 50 real-world images that feature individuals alongside
privacy-relevant environmental elements, capturing realistic and sensitive
scenarios for analysis. Our experimental evaluation reveals that ChatGPT o3 can
predict user locations with high precision, achieving street-level accuracy
(within one mile) in 60% of cases. Through analysis, we identify key visual
cues, including street layout and front yard design, that significantly
contribute to the model inference success. Additionally, targeted occlusion
experiments demonstrate that masking critical features effectively mitigates
geolocation accuracy, providing insights into potential defense mechanisms. Our
findings highlight an urgent need for privacy-aware development for agentic
multi-modal large reasoning models, particularly in applications involving
private imagery.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.19274v1">TeleSparse: Practical Privacy-Preserving Verification of Deep Neural
  Networks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-04-27T15:14:09Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mohammad M Maheri, Hamed Haddadi, Alex Davidson</p>
    <p><b>Summary:</b> Verification of the integrity of deep learning inference is crucial for
understanding whether a model is being applied correctly. However, such
verification typically requires access to model weights and (potentially
sensitive or private) training data. So-called Zero-knowledge Succinct
Non-Interactive Arguments of Knowledge (ZK-SNARKs) would appear to provide the
capability to verify model inference without access to such sensitive data.
However, applying ZK-SNARKs to modern neural networks, such as transformers and
large vision models, introduces significant computational overhead.
  We present TeleSparse, a ZK-friendly post-processing mechanisms to produce
practical solutions to this problem. TeleSparse tackles two fundamental
challenges inherent in applying ZK-SNARKs to modern neural networks: (1)
Reducing circuit constraints: Over-parameterized models result in numerous
constraints for ZK-SNARK verification, driving up memory and proof generation
costs. We address this by applying sparsification to neural network models,
enhancing proof efficiency without compromising accuracy or security. (2)
Minimizing the size of lookup tables required for non-linear functions, by
optimizing activation ranges through neural teleportation, a novel adaptation
for narrowing activation functions' range.
  TeleSparse reduces prover memory usage by 67% and proof generation time by
46% on the same model, with an accuracy trade-off of approximately 1%. We
implement our framework using the Halo2 proving system and demonstrate its
effectiveness across multiple architectures (Vision-transformer, ResNet,
MobileNet) and datasets (ImageNet,CIFAR-10,CIFAR-100). This work opens new
directions for ZK-friendly model design, moving toward scalable,
resource-efficient verifiable deep learning.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.19101v1">Privacy-Preserving Federated Embedding Learning for Localized
  Retrieval-Augmented Generation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-04-27T04:26:02Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Qianren Mao, Qili Zhang, Hanwen Hao, Zhentao Han, Runhua Xu, Weifeng Jiang, Qi Hu, Zhijun Chen, Tyler Zhou, Bo Li, Yangqiu Song, Jin Dong, Jianxin Li, Philip S. Yu</p>
    <p><b>Summary:</b> Retrieval-Augmented Generation (RAG) has recently emerged as a promising
solution for enhancing the accuracy and credibility of Large Language Models
(LLMs), particularly in Question & Answer tasks. This is achieved by
incorporating proprietary and private data from integrated databases. However,
private RAG systems face significant challenges due to the scarcity of private
domain data and critical data privacy issues. These obstacles impede the
deployment of private RAG systems, as developing privacy-preserving RAG systems
requires a delicate balance between data security and data availability. To
address these challenges, we regard federated learning (FL) as a highly
promising technology for privacy-preserving RAG services. We propose a novel
framework called Federated Retrieval-Augmented Generation (FedE4RAG). This
framework facilitates collaborative training of client-side RAG retrieval
models. The parameters of these models are aggregated and distributed on a
central-server, ensuring data privacy without direct sharing of raw data. In
FedE4RAG, knowledge distillation is employed for communication between the
server and client models. This technique improves the generalization of local
RAG retrievers during the federated learning process. Additionally, we apply
homomorphic encryption within federated learning to safeguard model parameters
and mitigate concerns related to data leakage. Extensive experiments conducted
on the real-world dataset have validated the effectiveness of FedE4RAG. The
results demonstrate that our proposed framework can markedly enhance the
performance of private RAG systems while maintaining robust data privacy
protection.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.18411v1">Heavy-Tailed Privacy: The Symmetric alpha-Stable Privacy Mechanism</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B">
  <p><b>Published on:</b> 2025-04-25T15:14:02Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Christopher C. Zawacki, Eyad H. Abed</p>
    <p><b>Summary:</b> With the rapid growth of digital platforms, there is increasing apprehension
about how personal data is collected, stored, and used by various entities.
These concerns arise from the increasing frequency of data breaches,
cyber-attacks, and misuse of personal information for targeted advertising and
surveillance. To address these matters, Differential Privacy (DP) has emerged
as a prominent tool for quantifying a digital system's level of protection. The
Gaussian mechanism is commonly used because the Gaussian density is closed
under convolution, and is a common method utilized when aggregating datasets.
However, the Gaussian mechanism only satisfies an approximate form of
Differential Privacy. In this work, we present and analyze of the Symmetric
alpha-Stable (SaS) mechanism. We prove that the mechanism achieves pure
differential privacy while remaining closed under convolution. Additionally, we
study the nuanced relationship between the level of privacy achieved and the
parameters of the density. Lastly, we compare the expected error introduced to
dataset queries by the Gaussian and SaS mechanisms. From our analysis, we
believe the SaS Mechanism is an appealing choice for privacy-focused
applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.18078v1">Privacy-Preserving Personalized Federated Learning for Distributed
  Photovoltaic Disaggregation under Statistical Heterogeneity</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-04-25T05:09:27Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xiaolu Chen, Chenghao Huang, Yanru Zhang, Hao Wang</p>
    <p><b>Summary:</b> The rapid expansion of distributed photovoltaic (PV) installations worldwide,
many being behind-the-meter systems, has significantly challenged energy
management and grid operations, as unobservable PV generation further
complicates the supply-demand balance. Therefore, estimating this generation
from net load, known as PV disaggregation, is critical. Given privacy concerns
and the need for large training datasets, federated learning becomes a
promising approach, but statistical heterogeneity, arising from geographical
and behavioral variations among prosumers, poses new challenges to PV
disaggregation. To overcome these challenges, a privacy-preserving distributed
PV disaggregation framework is proposed using Personalized Federated Learning
(PFL). The proposed method employs a two-level framework that combines local
and global modeling. At the local level, a transformer-based PV disaggregation
model is designed to generate solar irradiance embeddings for representing
local PV conditions. A novel adaptive local aggregation mechanism is adopted to
mitigate the impact of statistical heterogeneity on the local model, extracting
a portion of global information that benefits the local model. At the global
level, a central server aggregates information uploaded from multiple data
centers, preserving privacy while enabling cross-center knowledge sharing.
Experiments on real-world data demonstrate the effectiveness of this proposed
framework, showing improved accuracy and robustness compared to benchmark
methods.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.18032v1">Enhancing Privacy-Utility Trade-offs to Mitigate Memorization in
  Diffusion Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-04-25T02:51:23Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chen Chen, Daochang Liu, Mubarak Shah, Chang Xu</p>
    <p><b>Summary:</b> Text-to-image diffusion models have demonstrated remarkable capabilities in
creating images highly aligned with user prompts, yet their proclivity for
memorizing training set images has sparked concerns about the originality of
the generated images and privacy issues, potentially leading to legal
complications for both model owners and users, particularly when the memorized
images contain proprietary content. Although methods to mitigate these issues
have been suggested, enhancing privacy often results in a significant decrease
in the utility of the outputs, as indicated by text-alignment scores. To bridge
the research gap, we introduce a novel method, PRSS, which refines the
classifier-free guidance approach in diffusion models by integrating prompt
re-anchoring (PR) to improve privacy and incorporating semantic prompt search
(SS) to enhance utility. Extensive experiments across various privacy levels
demonstrate that our approach consistently improves the privacy-utility
trade-off, establishing a new state-of-the-art.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.18007v1">Differential Privacy-Driven Framework for Enhancing Heart Disease
  Prediction</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-04-25T01:27:40Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yazan Otoum, Amiya Nayak</p>
    <p><b>Summary:</b> With the rapid digitalization of healthcare systems, there has been a
substantial increase in the generation and sharing of private health data.
Safeguarding patient information is essential for maintaining consumer trust
and ensuring compliance with legal data protection regulations. Machine
learning is critical in healthcare, supporting personalized treatment, early
disease detection, predictive analytics, image interpretation, drug discovery,
efficient operations, and patient monitoring. It enhances decision-making,
accelerates research, reduces errors, and improves patient outcomes. In this
paper, we utilize machine learning methodologies, including differential
privacy and federated learning, to develop privacy-preserving models that
enable healthcare stakeholders to extract insights without compromising
individual privacy. Differential privacy introduces noise to data to guarantee
statistical privacy, while federated learning enables collaborative model
training across decentralized datasets. We explore applying these technologies
to Heart Disease Data, demonstrating how they preserve privacy while delivering
valuable insights and comprehensive analysis. Our results show that using a
federated learning model with differential privacy achieved a test accuracy of
85%, ensuring patient data remained secure and private throughout the process.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.17703v1">Federated Learning: A Survey on Privacy-Preserving Collaborative
  Intelligence</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-04-24T16:10:29Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Edward Collins, Michel Wang</p>
    <p><b>Summary:</b> Federated Learning (FL) has emerged as a transformative paradigm in the field
of distributed machine learning, enabling multiple clients such as mobile
devices, edge nodes, or organizations to collaboratively train a shared global
model without the need to centralize sensitive data. This decentralized
approach addresses growing concerns around data privacy, security, and
regulatory compliance, making it particularly attractive in domains such as
healthcare, finance, and smart IoT systems. This survey provides a concise yet
comprehensive overview of Federated Learning, beginning with its core
architecture and communication protocol. We discuss the standard FL lifecycle,
including local training, model aggregation, and global updates. A particular
emphasis is placed on key technical challenges such as handling non-IID
(non-independent and identically distributed) data, mitigating system and
hardware heterogeneity, reducing communication overhead, and ensuring privacy
through mechanisms like differential privacy and secure aggregation.
Furthermore, we examine emerging trends in FL research, including personalized
FL, cross-device versus cross-silo settings, and integration with other
paradigms such as reinforcement learning and quantum computing. We also
highlight real-world applications and summarize benchmark datasets and
evaluation metrics commonly used in FL research. Finally, we outline open
research problems and future directions to guide the development of scalable,
efficient, and trustworthy FL systems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.18596v1">Optimizing the Privacy-Utility Balance using Synthetic Data and
  Configurable Perturbation Pipelines</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Probability-5BC0EB">
  <p><b>Published on:</b> 2025-04-24T15:52:53Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Anantha Sharma, Swetha Devabhaktuni, Eklove Mohan</p>
    <p><b>Summary:</b> This paper explores the strategic use of modern synthetic data generation and
advanced data perturbation techniques to enhance security, maintain analytical
utility, and improve operational efficiency when managing large datasets, with
a particular focus on the Banking, Financial Services, and Insurance (BFSI)
sector. We contrast these advanced methods encompassing generative models like
GANs, sophisticated context-aware PII transformation, configurable statistical
perturbation, and differential privacy with traditional anonymization
approaches.
  The goal is to create realistic, privacy-preserving datasets that retain high
utility for complex machine learning tasks and analytics, a critical need in
the data-sensitive industries like BFSI, Healthcare, Retail, and
Telecommunications. We discuss how these modern techniques potentially offer
significant improvements in balancing privacy preservation while maintaining
data utility compared to older methods. Furthermore, we examine the potential
for operational gains, such as reduced overhead and accelerated analytics, by
using these privacy-enhanced datasets. We also explore key use cases where
these methods can mitigate regulatory risks and enable scalable, data-driven
innovation without compromising sensitive customer information.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.17523v1">From Randomized Response to Randomized Index: Answering Subset Counting
  Queries with Local Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-04-24T13:08:11Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Qingqing Ye, Liantong Yu, Kai Huang, Xiaokui Xiao, Weiran Liu, Haibo Hu</p>
    <p><b>Summary:</b> Local Differential Privacy (LDP) is the predominant privacy model for
safeguarding individual data privacy. Existing perturbation mechanisms
typically require perturbing the original values to ensure acceptable privacy,
which inevitably results in value distortion and utility deterioration. In this
work, we propose an alternative approach -- instead of perturbing values, we
apply randomization to indexes of values while ensuring rigorous LDP
guarantees. Inspired by the deniability of randomized indexes, we present CRIAD
for answering subset counting queries on set-value data. By integrating a
multi-dummy, multi-sample, and multi-group strategy, CRIAD serves as a fully
scalable solution that offers flexibility across various privacy requirements
and domain sizes, and achieves more accurate query results than any existing
methods. Through comprehensive theoretical analysis and extensive experimental
evaluations, we validate the effectiveness of CRIAD and demonstrate its
superiority over traditional value-perturbation mechanisms.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.17360v1">PatientDx: Merging Large Language Models for Protecting Data-Privacy in
  Healthcare</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-04-24T08:21:04Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jose G. Moreno, Jesus Lovon, M'Rick Robin-Charlet, Christine Damase-Michel, Lynda Tamine</p>
    <p><b>Summary:</b> Fine-tuning of Large Language Models (LLMs) has become the default practice
for improving model performance on a given task. However, performance
improvement comes at the cost of training on vast amounts of annotated data
which could be sensitive leading to significant data privacy concerns. In
particular, the healthcare domain is one of the most sensitive domains exposed
to data privacy issues. In this paper, we present PatientDx, a framework of
model merging that allows the design of effective LLMs for health-predictive
tasks without requiring fine-tuning nor adaptation on patient data. Our
proposal is based on recently proposed techniques known as merging of LLMs and
aims to optimize a building block merging strategy. PatientDx uses a pivotal
model adapted to numerical reasoning and tunes hyperparameters on examples
based on a performance metric but without training of the LLM on these data.
Experiments using the mortality tasks of the MIMIC-IV dataset show improvements
up to 7% in terms of AUROC when compared to initial models. Additionally, we
confirm that when compared to fine-tuned models, our proposal is less prone to
data leak problems without hurting performance. Finally, we qualitatively show
the capabilities of our proposal through a case study. Our best model is
publicly available at https://huggingface.co/ Jgmorenof/mistral\_merged\_0\_4.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.17274v1">Signal Recovery from Random Dot-Product Graphs Under Local Differential
  Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Statistics Theory-D91E36">   
  <p><b>Published on:</b> 2025-04-24T06:02:02Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Siddharth Vishwanath, Jonathan Hehir</p>
    <p><b>Summary:</b> We consider the problem of recovering latent information from graphs under
$\varepsilon$-edge local differential privacy where the presence of
relationships/edges between two users/vertices remains confidential, even from
the data curator. For the class of generalized random dot-product graphs, we
show that a standard local differential privacy mechanism induces a specific
geometric distortion in the latent positions. Leveraging this insight, we show
that consistent recovery of the latent positions is achievable by appropriately
adjusting the statistical inference procedure for the privatized graph.
Furthermore, we prove that our procedure is nearly minimax-optimal under local
edge differential privacy constraints. Lastly, we show that this framework
allows for consistent recovery of geometric and topological information
underlying the latent positions, as encoded in their persistence diagrams. Our
results extend previous work from the private community detection literature to
a substantially richer class of models and inferential tasks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.16683v1">MCMC for Bayesian estimation of Differential Privacy from Membership
  Inference Attacks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> 
  <p><b>Published on:</b> 2025-04-23T13:10:37Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ceren Yildirim, Kamer Kaya, Sinan Yildirim, Erkay Savas</p>
    <p><b>Summary:</b> We propose a new framework for Bayesian estimation of differential privacy,
incorporating evidence from multiple membership inference attacks (MIA).
Bayesian estimation is carried out via a Markov chain Monte Carlo (MCMC)
algorithm, named MCMC-DP-Est, which provides an estimate of the full posterior
distribution of the privacy parameter (e.g., instead of just credible
intervals). Critically, the proposed method does not assume that privacy
auditing is performed with the most powerful attack on the worst-case (dataset,
challenge point) pair, which is typically unrealistic. Instead, MCMC-DP-Est
jointly estimates the strengths of MIAs used and the privacy of the training
algorithm, yielding a more cautious privacy analysis. We also present an
economical way to generate measurements for the performance of an MIA that is
to be used by the MCMC method to estimate privacy. We present the use of the
methods with numerical examples with both artificial and real data.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.16557v1">Beyond Anonymization: Object Scrubbing for Privacy-Preserving 2D and 3D
  Vision Tasks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-04-23T09:33:10Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Murat Bilgehan Ertan, Ronak Sahu, Phuong Ha Nguyen, Kaleel Mahmood, Marten van Dijk</p>
    <p><b>Summary:</b> We introduce ROAR (Robust Object Removal and Re-annotation), a scalable
framework for privacy-preserving dataset obfuscation that eliminates sensitive
objects instead of modifying them. Our method integrates instance segmentation
with generative inpainting to remove identifiable entities while preserving
scene integrity. Extensive evaluations on 2D COCO-based object detection show
that ROAR achieves 87.5% of the baseline detection average precision (AP),
whereas image dropping achieves only 74.2% of the baseline AP, highlighting the
advantage of scrubbing in preserving dataset utility. The degradation is even
more severe for small objects due to occlusion and loss of fine-grained
details. Furthermore, in NeRF-based 3D reconstruction, our method incurs a PSNR
loss of at most 1.66 dB while maintaining SSIM and improving LPIPS,
demonstrating superior perceptual quality. Our findings establish object
removal as an effective privacy framework, achieving strong privacy guarantees
with minimal performance trade-offs. The results highlight key challenges in
generative inpainting, occlusion-robust segmentation, and task-specific
scrubbing, setting the foundation for future advancements in privacy-preserving
vision systems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.16535v1">Decentralized Quantile Regression for Feature-Distributed Massive
  Datasets with Privacy Guarantees</a></h3>
  
  <p><b>Published on:</b> 2025-04-23T09:04:21Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Peiwen Xiao, Xiaohui Liu, Guangming Pan, Wei Long</p>
    <p><b>Summary:</b> In this paper, we introduce a novel decentralized surrogate gradient-based
algorithm for quantile regression in a feature-distributed setting, where
global features are dispersed across multiple machines within a decentralized
network. The proposed algorithm, \texttt{DSG-cqr}, utilizes a convolution-type
smoothing approach to address the non-smooth nature of the quantile loss
function. \texttt{DSG-cqr} is fully decentralized, conjugate-free, easy to
implement, and achieves linear convergence up to statistical precision. To
ensure privacy, we adopt the Gaussian mechanism to provide
$(\epsilon,\delta)$-differential privacy. To overcome the exact residual
calculation problem, we estimate residuals using auxiliary variables and
develop a confidence interval construction method based on Wald statistics.
Theoretical properties are established, and the practical utility of the
methods is also demonstrated through extensive simulations and a real-world
data application.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.18581v2">Enhancing Privacy in Semantic Communication over Wiretap Channels
  leveraging Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2025-04-23T08:42:44Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Weixuan Chen, Shunpu Tang, Qianqian Yang, Zhiguo Shi, Dusit Niyato</p>
    <p><b>Summary:</b> Semantic communication (SemCom) improves transmission efficiency by focusing
on task-relevant information. However, transmitting semantic-rich data over
insecure channels introduces privacy risks. This paper proposes a novel SemCom
framework that integrates differential privacy (DP) mechanisms to protect
sensitive semantic features. This method employs the generative adversarial
network (GAN) inversion technique to extract disentangled semantic features and
uses neural networks (NNs) to approximate the DP application and removal
processes, effectively mitigating the non-invertibility issue of DP.
Additionally, an NN-based encryption scheme is introduced to strengthen the
security of channel inputs. Simulation results demonstrate that the proposed
approach effectively prevents eavesdroppers from reconstructing sensitive
information by generating chaotic or fake images, while ensuring high-quality
image reconstruction for legitimate users. The system exhibits robust
performance across various privacy budgets and channel conditions, achieving an
optimal balance between privacy protection and reconstruction fidelity.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.16371v1">The Safety-Privacy Tradeoff in Linear Bandits</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Optimization and Control-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-04-23T02:48:02Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Arghavan Zibaie, Spencer Hutchinson, Ramtin Pedarsani, Mahnoosh Alizadeh</p>
    <p><b>Summary:</b> We consider a collection of linear stochastic bandit problems, each modeling
the random response of different agents to proposed interventions, coupled
together by a global safety constraint. We assume a central coordinator must
choose actions to play on each bandit with the objective of regret
minimization, while also ensuring that the expected response of all agents
satisfies the global safety constraints at each round, in spite of uncertainty
about the bandits' parameters. The agents consider their observed responses to
be private and in order to protect their sensitive information, the data
sharing with the central coordinator is performed under local differential
privacy (LDP). However, providing higher level of privacy to different agents
would have consequences in terms of safety and regret. We formalize these
tradeoffs by building on the notion of the sharpness of the safety set - a
measure of how the geometric properties of the safe set affects the growth of
regret - and propose a unilaterally unimprovable vector of privacy levels for
different agents given a maximum regret budget.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.16000v1">How Private is Your Attention? Bridging Privacy with In-Context Learning</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-04-22T16:05:26Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Soham Bonnerjee, Zhen Wei,  Yeon, Anna Asch, Sagnik Nandy, Promit Ghosal</p>
    <p><b>Summary:</b> In-context learning (ICL)-the ability of transformer-based models to perform
new tasks from examples provided at inference time-has emerged as a hallmark of
modern language models. While recent works have investigated the mechanisms
underlying ICL, its feasibility under formal privacy constraints remains
largely unexplored. In this paper, we propose a differentially private
pretraining algorithm for linear attention heads and present the first
theoretical analysis of the privacy-accuracy trade-off for ICL in linear
regression. Our results characterize the fundamental tension between
optimization and privacy-induced noise, formally capturing behaviors observed
in private training via iterative methods. Additionally, we show that our
method is robust to adversarial perturbations of training prompts, unlike
standard ridge regression. All theoretical findings are supported by extensive
simulations across diverse settings.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.15995v1">OPUS-VFL: Incentivizing Optimal Privacy-Utility Tradeoffs in Vertical
  Federated Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-04-22T16:00:11Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sindhuja Madabushi, Ahmad Faraz Khan, Haider Ali, Jin-Hee Cho</p>
    <p><b>Summary:</b> Vertical Federated Learning (VFL) enables organizations with disjoint feature
spaces but shared user bases to collaboratively train models without sharing
raw data. However, existing VFL systems face critical limitations: they often
lack effective incentive mechanisms, struggle to balance privacy-utility
tradeoffs, and fail to accommodate clients with heterogeneous resource
capabilities. These challenges hinder meaningful participation, degrade model
performance, and limit practical deployment. To address these issues, we
propose OPUS-VFL, an Optimal Privacy-Utility tradeoff Strategy for VFL.
OPUS-VFL introduces a novel, privacy-aware incentive mechanism that rewards
clients based on a principled combination of model contribution, privacy
preservation, and resource investment. It employs a lightweight leave-one-out
(LOO) strategy to quantify feature importance per client, and integrates an
adaptive differential privacy mechanism that enables clients to dynamically
calibrate noise levels to optimize their individual utility. Our framework is
designed to be scalable, budget-balanced, and robust to inference and poisoning
attacks. Extensive experiments on benchmark datasets (MNIST, CIFAR-10, and
CIFAR-100) demonstrate that OPUS-VFL significantly outperforms state-of-the-art
VFL baselines in both efficiency and robustness. It reduces label inference
attack success rates by up to 20%, increases feature inference reconstruction
error (MSE) by over 30%, and achieves up to 25% higher incentives for clients
that contribute meaningfully while respecting privacy and cost constraints.
These results highlight the practicality and innovation of OPUS-VFL as a
secure, fair, and performance-driven solution for real-world VFL.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.15580v1">On the Price of Differential Privacy for Hierarchical Clustering</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-04-22T04:39:40Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chengyuan Deng, Jie Gao, Jalaj Upadhyay, Chen Wang, Samson Zhou</p>
    <p><b>Summary:</b> Hierarchical clustering is a fundamental unsupervised machine learning task
with the aim of organizing data into a hierarchy of clusters. Many applications
of hierarchical clustering involve sensitive user information, therefore
motivating recent studies on differentially private hierarchical clustering
under the rigorous framework of Dasgupta's objective. However, it has been
shown that any privacy-preserving algorithm under edge-level differential
privacy necessarily suffers a large error. To capture practical applications of
this problem, we focus on the weight privacy model, where each edge of the
input graph is at least unit weight. We present a novel algorithm in the weight
privacy model that shows significantly better approximation than known
impossibility results in the edge-level DP setting. In particular, our
algorithm achieves $O(\log^{1.5}n/\varepsilon)$ multiplicative error for
$\varepsilon$-DP and runs in polynomial time, where $n$ is the size of the
input graph, and the cost is never worse than the optimal additive error in
existing work. We complement our algorithm by showing if the unit-weight
constraint does not apply, the lower bound for weight-level DP hierarchical
clustering is essentially the same as the edge-level DP, i.e.
$\Omega(n^2/\varepsilon)$ additive error. As a result, we also obtain a new
lower bound of $\tilde{\Omega}(1/\varepsilon)$ additive error for balanced
sparsest cuts in the weight-level DP model, which may be of independent
interest. Finally, we evaluate our algorithm on synthetic and real-world
datasets. Our experimental results show that our algorithm performs well in
terms of extra cost and has good scalability to large graphs.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.18569v1">Large Language Model Empowered Privacy-Protected Framework for PHI
  Annotation in Clinical Notes</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-04-22T03:18:36Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Guanchen Wu, Linzhi Zheng, Han Xie, Zhen Xiang, Jiaying Lu, Darren Liu, Delgersuren Bold, Bo Li, Xiao Hu, Carl Yang</p>
    <p><b>Summary:</b> The de-identification of private information in medical data is a crucial
process to mitigate the risk of confidentiality breaches, particularly when
patient personal details are not adequately removed before the release of
medical records. Although rule-based and learning-based methods have been
proposed, they often struggle with limited generalizability and require
substantial amounts of annotated data for effective performance. Recent
advancements in large language models (LLMs) have shown significant promise in
addressing these issues due to their superior language comprehension
capabilities. However, LLMs present challenges, including potential privacy
risks when using commercial LLM APIs and high computational costs for deploying
open-source LLMs locally. In this work, we introduce LPPA, an LLM-empowered
Privacy-Protected PHI Annotation framework for clinical notes, targeting the
English language. By fine-tuning LLMs locally with synthetic notes, LPPA
ensures strong privacy protection and high PHI annotation accuracy. Extensive
experiments demonstrate LPPA's effectiveness in accurately de-identifying
private information, offering a scalable and efficient solution for enhancing
patient privacy protection.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.15525v1">Federated Latent Factor Learning for Recovering Wireless Sensor Networks
  Signal with Privacy-Preserving</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-04-22T02:01:19Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chengjun Yu, Yixin Ran, Yangyi Xia, Jia Wu, Xiaojing Liu</p>
    <p><b>Summary:</b> Wireless Sensor Networks (WSNs) are a cutting-edge domain in the field of
intelligent sensing. Due to sensor failures and energy-saving strategies, the
collected data often have massive missing data, hindering subsequent analysis
and decision-making. Although Latent Factor Learning (LFL) has been proven
effective in recovering missing data, it fails to sufficiently consider data
privacy protection. To address this issue, this paper innovatively proposes a
federated latent factor learning (FLFL) based spatial signal recovery (SSR)
model, named FLFL-SSR. Its main idea is two-fold: 1) it designs a sensor-level
federated learning framework, where each sensor uploads only gradient updates
instead of raw data to optimize the global model, and 2) it proposes a local
spatial sharing strategy, allowing sensors within the same spatial region to
share their latent feature vectors, capturing spatial correlations and
enhancing recovery accuracy. Experimental results on two real-world WSNs
datasets demonstrate that the proposed model outperforms existing federated
methods in terms of recovery performance.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.15233v1">A Review on Privacy in DAG-Based DLTs</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-04-21T17:08:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mayank Raikwar</p>
    <p><b>Summary:</b> Directed Acyclic Graph (DAG)-based Distributed Ledger Technologies (DLTs)
have emerged as a promising solution to the scalability issues inherent in
traditional blockchains. However, amidst the focus on scalability, the crucial
aspect of privacy within DAG-based DLTs has been largely overlooked. This paper
seeks to address this gap by providing a comprehensive examination of privacy
notions and challenges within DAG-based DLTs. We delve into potential
methodologies to enhance privacy within these systems, while also analyzing the
associated hurdles and real-world implementations within state-of-the-art
DAG-based DLTs. By exploring these methodologies, we not only illuminate the
current landscape of privacy in DAG-based DLTs but also outline future research
directions in this evolving field.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.15090v1">Federated Latent Factor Model for Bias-Aware Recommendation with
  Privacy-Preserving</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-04-21T13:24:30Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Junxiang Gao, Yixin Ran, Jia Chen</p>
    <p><b>Summary:</b> A recommender system (RS) aims to provide users with personalized item
recommendations, enhancing their overall experience. Traditional RSs collect
and process all user data on a central server. However, this centralized
approach raises significant privacy concerns, as it increases the risk of data
breaches and privacy leakages, which are becoming increasingly unacceptable to
privacy-sensitive users. To address these privacy challenges, federated
learning has been integrated into RSs, ensuring that user data remains secure.
In centralized RSs, the issue of rating bias is effectively addressed by
jointly analyzing all users' raw interaction data. However, this becomes a
significant challenge in federated RSs, as raw data is no longer accessible due
to privacy-preserving constraints. To overcome this problem, we propose a
Federated Bias-Aware Latent Factor (FBALF) model. In FBALF, training bias is
explicitly incorporated into every local model's loss function, allowing for
the effective elimination of rating bias without compromising data privacy.
Extensive experiments conducted on three real-world datasets demonstrate that
FBALF achieves significantly higher recommendation accuracy compared to other
state-of-the-art federated RSs.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.14993v1">Dual Utilization of Perturbation for Stream Data Publication under Local
  Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB">
  <p><b>Published on:</b> 2025-04-21T09:51:18Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Rong Du, Qingqing Ye, Yaxin Xiao, Liantong Yu, Yue Fu, Haibo Hu</p>
    <p><b>Summary:</b> Stream data from real-time distributed systems such as IoT, tele-health, and
crowdsourcing has become an important data source. However, the collection and
analysis of user-generated stream data raise privacy concerns due to the
potential exposure of sensitive information. To address these concerns, local
differential privacy (LDP) has emerged as a promising standard. Nevertheless,
applying LDP to stream data presents significant challenges, as stream data
often involves a large or even infinite number of values. Allocating a given
privacy budget across these data points would introduce overwhelming LDP noise
to the original stream data.
  Beyond existing approaches that merely use perturbed values for estimating
statistics, our design leverages them for both perturbation and estimation.
This dual utilization arises from a key observation: each user knows their own
ground truth and perturbed values, enabling a precise computation of the
deviation error caused by perturbation. By incorporating this deviation into
the perturbation process of subsequent values, the previous noise can be
calibrated. Following this insight, we introduce the Iterative Perturbation
Parameterization (IPP) method, which utilizes current perturbed results to
calibrate the subsequent perturbation process. To enhance the robustness of
calibration and reduce sensitivity, two algorithms, namely Accumulated
Perturbation Parameterization (APP) and Clipped Accumulated Perturbation
Parameterization (CAPP) are further developed. We prove that these three
algorithms satisfy $w$-event differential privacy while significantly improving
utility. Experimental results demonstrate that our techniques outperform
state-of-the-art LDP stream publishing solutions in terms of utility, while
retaining the same privacy guarantee.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.14780v1">Delay-Angle Information Spoofing for Channel State Information-Free
  Location-Privacy Enhancement</a></h3>
  
  <p><b>Published on:</b> 2025-04-21T00:40:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jianxiu Li, Urbashi Mitra</p>
    <p><b>Summary:</b> In this paper, a delay-angle information spoofing (DAIS) strategy is proposed
to enhance the location privacy at the physical layer. More precisely, the
location-relevant delays and angles are artificially shifted without the aid of
channel state information (CSI) at the transmitter, such that the location
perceived by the eavesdropper is incorrect and distinct from the true one. By
leveraging the intrinsic structure of the wireless channel, a precoder is
designed to achieve DAIS while the legitimate localizer can remove the
obfuscation via securely receiving a modest amount of information, i.e., the
delay-angle shifts. A lower bound on eavesdropper's localization error is
derived, revealing that location privacy is enhanced not only due to estimation
error, but also by the geometric mismatch introduced by DAIS. Furthermore, the
lower bound is explicitly expressed as a function of the delay-angle shifts,
characterizing performance trends and providing the appropriate design of these
shift parameters. The statistical hardness of maliciously inferring the
delay-angle shifts by a single-antenna eavesdropper as well as the challenges
for a multi-antenna eavesdropper are investigated to assess the robustness of
the proposed DAIS strategy. Numerical results show that the proposed DAIS
strategy results in more than 15 dB performance degradation for the
eavesdropper as compared with that for the legitimate localizer at high
signal-to-noise ratios, and provides more effective location-privacy
enhancement than the prior art.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.14730v1">Optimal Additive Noise Mechanisms for Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2025-04-20T20:04:41Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Atefeh Gilani, Juan Felipe Gomez, Shahab Asoodeh, Flavio P. Calmon, Oliver Kosut, Lalitha Sankar</p>
    <p><b>Summary:</b> We propose a unified optimization framework for designing continuous and
discrete noise distributions that ensure differential privacy (DP) by
minimizing R\'enyi DP, a variant of DP, under a cost constraint. R\'enyi DP has
the advantage that by considering different values of the R\'enyi parameter
$\alpha$, we can tailor our optimization for any number of compositions. To
solve the optimization problem, we reduce it to a finite-dimensional convex
formulation and perform preconditioned gradient descent. The resulting noise
distributions are then compared to their Gaussian and Laplace counterparts.
Numerical results demonstrate that our optimized distributions are consistently
better, with significant improvements in $(\varepsilon, \delta)$-DP guarantees
in the moderate composition regimes, compared to Gaussian and Laplace
distributions with the same variance.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.14368v1">Do You Really Need Public Data? Surrogate Public Data for Differential
  Privacy on Tabular Data</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-04-19T17:55:10Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shlomi Hod, Lucas Rosenblatt, Julia Stoyanovich</p>
    <p><b>Summary:</b> Differentially private (DP) machine learning often relies on the availability
of public data for tasks like privacy-utility trade-off estimation,
hyperparameter tuning, and pretraining. While public data assumptions may be
reasonable in text and image domains, they are less likely to hold for tabular
data due to tabular data heterogeneity across domains. We propose leveraging
powerful priors to address this limitation; specifically, we synthesize
realistic tabular data directly from schema-level specifications - such as
variable names, types, and permissible ranges - without ever accessing
sensitive records. To that end, this work introduces the notion of "surrogate"
public data - datasets generated independently of sensitive data, which consume
no privacy loss budget and are constructed solely from publicly available
schema or metadata. Surrogate public data are intended to encode plausible
statistical assumptions (informed by publicly available information) into a
dataset with many downstream uses in private mechanisms. We automate the
process of generating surrogate public data with large language models (LLMs);
in particular, we propose two methods: direct record generation as CSV files,
and automated structural causal model (SCM) construction for sampling records.
Through extensive experiments, we demonstrate that surrogate public tabular
data can effectively replace traditional public data when pretraining
differentially private tabular classifiers. To a lesser extent, surrogate
public data are also useful for hyperparameter tuning of DP synthetic data
generators, and for estimating the privacy-utility tradeoff.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.14301v1">Balancing Privacy and Action Performance: A Penalty-Driven Approach to
  Image Anonymization</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Multimedia-5BC0EB">
  <p><b>Published on:</b> 2025-04-19T13:52:33Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Nazia Aslam, Kamal Nasrollahi</p>
    <p><b>Summary:</b> The rapid development of video surveillance systems for object detection,
tracking, activity recognition, and anomaly detection has revolutionized our
day-to-day lives while setting alarms for privacy concerns. It isn't easy to
strike a balance between visual privacy and action recognition performance in
most computer vision models. Is it possible to safeguard privacy without
sacrificing performance? It poses a formidable challenge, as even minor privacy
enhancements can lead to substantial performance degradation. To address this
challenge, we propose a privacy-preserving image anonymization technique that
optimizes the anonymizer using penalties from the utility branch, ensuring
improved action recognition performance while minimally affecting privacy
leakage. This approach addresses the trade-off between minimizing privacy
leakage and maintaining high action performance. The proposed approach is
primarily designed to align with the regulatory standards of the EU AI Act and
GDPR, ensuring the protection of personally identifiable information while
maintaining action performance. To the best of our knowledge, we are the first
to introduce a feature-based penalty scheme that exclusively controls the
action features, allowing freedom to anonymize private attributes. Extensive
experiments were conducted to validate the effectiveness of the proposed
method. The results demonstrate that applying a penalty to anonymizer from
utility branch enhances action performance while maintaining nearly consistent
privacy leakage across different penalty settings.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.14208v1">FedCIA: Federated Collaborative Information Aggregation for
  Privacy-Preserving Recommendation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Retrieval-5BC0EB">
  <p><b>Published on:</b> 2025-04-19T06:59:34Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mingzhe Han, Dongsheng Li, Jiafeng Xia, Jiahao Liu, Hansu Gu, Peng Zhang, Ning Gu, Tun Lu</p>
    <p><b>Summary:</b> Recommendation algorithms rely on user historical interactions to deliver
personalized suggestions, which raises significant privacy concerns. Federated
recommendation algorithms tackle this issue by combining local model training
with server-side model aggregation, where most existing algorithms use a
uniform weighted summation to aggregate item embeddings from different client
models. This approach has three major limitations: 1) information loss during
aggregation, 2) failure to retain personalized local features, and 3)
incompatibility with parameter-free recommendation algorithms. To address these
limitations, we first review the development of recommendation algorithms and
recognize that their core function is to share collaborative information,
specifically the global relationship between users and items. With this
understanding, we propose a novel aggregation paradigm named collaborative
information aggregation, which focuses on sharing collaborative information
rather than item parameters. Based on this new paradigm, we introduce the
federated collaborative information aggregation (FedCIA) method for
privacy-preserving recommendation. This method requires each client to upload
item similarity matrices for aggregation, which allows clients to align their
local models without constraining embeddings to a unified vector space. As a
result, it mitigates information loss caused by direct summation, preserves the
personalized embedding distributions of individual clients, and supports the
aggregation of parameter-free models. Theoretical analysis and experimental
results on real-world datasets demonstrate the superior performance of FedCIA
compared with the state-of-the-art federated recommendation algorithms. Code is
available at https://github.com/Mingzhe-Han/FedCIA.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.13526v1">Multi-class Item Mining under Local Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-04-18T07:37:06Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yulian Mao, Qingqing Ye, Rong Du, Qi Wang, Kai Huang, Haibo Hu</p>
    <p><b>Summary:</b> Item mining, a fundamental task for collecting statistical data from users,
has raised increasing privacy concerns. To address these concerns, local
differential privacy (LDP) was proposed as a privacy-preserving technique.
Existing LDP item mining mechanisms primarily concentrate on global statistics,
i.e., those from the entire dataset. Nevertheless, they fall short of
user-tailored tasks such as personalized recommendations, whereas classwise
statistics can improve task accuracy with fine-grained information. Meanwhile,
the introduction of class labels brings new challenges. Label perturbation may
result in invalid items for aggregation. To this end, we propose frameworks for
multi-class item mining, along with two mechanisms: validity perturbation to
reduce the impact of invalid data, and correlated perturbation to preserve the
relationship between labels and items. We also apply these optimized methods to
two multi-class item mining queries: frequency estimation and top-$k$ item
mining. Through theoretical analysis and extensive experiments, we verify the
effectiveness and superiority of these methods.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.13267v1">Leveraging Functional Encryption and Deep Learning for
  Privacy-Preserving Traffic Forecasting</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36"> 
  <p><b>Published on:</b> 2025-04-17T18:21:55Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Isaac Adom, Mohammmad Iqbal Hossain, Hassan Mahmoud, Ahmad Alsharif, Mahmoud Nabil Mahmoud, Yang Xiao</p>
    <p><b>Summary:</b> Over the past few years, traffic congestion has continuously plagued the
nation's transportation system creating several negative impacts including
longer travel times, increased pollution rates, and higher collision risks. To
overcome these challenges, Intelligent Transportation Systems (ITS) aim to
improve mobility and vehicular systems, ensuring higher levels of safety by
utilizing cutting-edge technologies, sophisticated sensing capabilities, and
innovative algorithms. Drivers' participatory sensing, current/future location
reporting, and machine learning algorithms have considerably improved real-time
congestion monitoring and future traffic management. However, each driver's
sensitive spatiotemporal location information can create serious privacy
concerns. To address these challenges, we propose in this paper a secure,
privacy-preserving location reporting and traffic forecasting system that
guarantees privacy protection of driver data while maintaining high traffic
forecasting accuracy. Our novel k-anonymity scheme utilizes functional
encryption to aggregate encrypted location information submitted by drivers
while ensuring the privacy of driver location data. Additionally, using the
aggregated encrypted location information as input, this research proposes a
deep learning model that incorporates a Convolutional-Long Short-Term Memory
(Conv-LSTM) module to capture spatial and short-term temporal features and a
Bidirectional Long Short-Term Memory (Bi-LSTM) module to recover long-term
periodic patterns for traffic forecasting. With extensive evaluation on real
datasets, we demonstrate the effectiveness of the proposed scheme with less
than 10% mean absolute error for a 60-minute forecasting horizon, all while
protecting driver privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.12931v1">Explainable AI in Usable Privacy and Security: Challenges and
  Opportunities</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-04-17T13:28:01Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Vincent Freiberger, Arthur Fleig, Erik Buchmann</p>
    <p><b>Summary:</b> Large Language Models (LLMs) are increasingly being used for automated
evaluations and explaining them. However, concerns about explanation quality,
consistency, and hallucinations remain open research challenges, particularly
in high-stakes contexts like privacy and security, where user trust and
decision-making are at stake. In this paper, we investigate these issues in the
context of PRISMe, an interactive privacy policy assessment tool that leverages
LLMs to evaluate and explain website privacy policies. Based on a prior user
study with 22 participants, we identify key concerns regarding LLM judgment
transparency, consistency, and faithfulness, as well as variations in user
preferences for explanation detail and engagement. We discuss potential
strategies to mitigate these concerns, including structured evaluation
criteria, uncertainty estimation, and retrieval-augmented generation (RAG). We
identify a need for adaptive explanation strategies tailored to different user
profiles for LLM-as-a-judge. Our goal is to showcase the application area of
usable privacy and security to be promising for Human-Centered Explainable AI
(HCXAI) to make an impact.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.12747v1">Privacy Protection Against Personalized Text-to-Image Synthesis via
  Cross-image Consistency Constraints</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-04-17T08:39:32Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Guanyu Wang, Kailong Wang, Yihao Huang, Mingyi Zhou, Zhang Qing cnwatcher, Geguang Pu, Li Li</p>
    <p><b>Summary:</b> The rapid advancement of diffusion models and personalization techniques has
made it possible to recreate individual portraits from just a few publicly
available images. While such capabilities empower various creative
applications, they also introduce serious privacy concerns, as adversaries can
exploit them to generate highly realistic impersonations. To counter these
threats, anti-personalization methods have been proposed, which add adversarial
perturbations to published images to disrupt the training of personalization
models. However, existing approaches largely overlook the intrinsic multi-image
nature of personalization and instead adopt a naive strategy of applying
perturbations independently, as commonly done in single-image settings. This
neglects the opportunity to leverage inter-image relationships for stronger
privacy protection. Therefore, we advocate for a group-level perspective on
privacy protection against personalization. Specifically, we introduce
Cross-image Anti-Personalization (CAP), a novel framework that enhances
resistance to personalization by enforcing style consistency across perturbed
images. Furthermore, we develop a dynamic ratio adjustment strategy that
adaptively balances the impact of the consistency loss throughout the attack
iterations. Extensive experiments on the classical CelebHQ and VGGFace2
benchmarks show that CAP substantially improves existing methods.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.12681v1">GRAIL: Gradient-Based Adaptive Unlearning for Privacy and Copyright in
  LLMs</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-04-17T06:16:32Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kun-Woo Kim, Ji-Hoon Park, Ju-Min Han, Seong-Whan Lee</p>
    <p><b>Summary:</b> Large Language Models (LLMs) trained on extensive datasets often learn
sensitive information, which raises significant social and legal concerns under
principles such as the "Right to be forgotten." Retraining entire models from
scratch to remove undesired information is both costly and impractical.
Furthermore, existing single-domain unlearning methods fail to address
multi-domain scenarios, where knowledge is interwoven across domains such as
privacy and copyright, creating overlapping representations that lead to
excessive knowledge removal or degraded performance. To tackle these issues, we
propose GRAIL (GRadient-based AdaptIve unLearning), a novel multi-domain
unlearning framework. GRAIL leverages gradient information from multiple
domains to precisely distinguish the unlearning scope from the retention scope,
and applies an adaptive parameter-wise localization strategy to selectively
remove targeted knowledge while preserving critical parameters for each domain.
Experimental results on unlearning benchmarks show that GRAIL achieves
unlearning success on par with the existing approaches, while also
demonstrating up to 17% stronger knowledge retention success compared to the
previous state-of-art method. Our findings establish a new paradigm for
effectively managing and regulating sensitive information in large-scale
pre-trained language models.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.16944v1">Burning some myths on privacy properties of social networks against
  active attacks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Social and Information Networks-662E9B"> 
  <p><b>Published on:</b> 2025-04-17T06:03:56Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Serafino Cicerone, Gabriele Di Stefano, Sandi KlavÅ¾ar, Ismael G. Yero</p>
    <p><b>Summary:</b> This work focuses on showing some arguments addressed to dismantle the
extended idea about that social networks completely lacks of privacy
properties. We consider the so-called active attacks to the privacy of social
networks and the counterpart $(k,\ell)$-anonymity measure, which is used to
quantify the privacy satisfied by a social network against active attacks. To
this end, we make use of the graph theoretical concept of $k$-metric
antidimensional graphs for which the case $k=1$ represents those graphs
achieving the worst scenario in privacy whilst considering the
$(k,\ell)$-anonymity measure.
  As a product of our investigation, we present a large number of computational
results stating that social networks might not be as insecure as one often
thinks. In particular, we develop a large number of experiments on random
graphs which show that the number of $1$-metric antidimensional graphs is
indeed ridiculously small with respect to the total number of graphs that can
be considered. Moreover, we search on several real networks in order to check
if they are $1$-metric antidimensional, and obtain that none of them are such.
Along the way, we show some theoretical studies on the mathematical properties
of the $k$-metric antidimensional graphs for any suitable $k\ge 1$. In
addition, we also describe some operations on graphs that are $1$-metric
antidimensional so that they get embedded into another larger graphs that are
not such, in order to obscure their privacy properties against active attacks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.12623v1">Privacy-Preserving CNN Training with Transfer Learning: Two Hidden
  Layers</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-04-17T03:58:23Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> John Chiang</p>
    <p><b>Summary:</b> In this paper, we present the demonstration of training a four-layer neural
network entirely using fully homomorphic encryption (FHE), supporting both
single-output and multi-output classification tasks in a non-interactive
setting. A key contribution of our work is identifying that replacing
\textit{Softmax} with \textit{Sigmoid}, in conjunction with the Binary
Cross-Entropy (BCE) loss function, provides an effective and scalable solution
for homomorphic classification. Moreover, we show that the BCE loss function,
originally designed for multi-output tasks, naturally extends to the
multi-class setting, thereby enabling broader applicability. We also highlight
the limitations of prior loss functions such as the SLE loss and the one
proposed in the 2019 CVPR Workshop, both of which suffer from vanishing
gradients as network depth increases. To address the challenges posed by
large-scale encrypted data, we further introduce an improved version of the
previously proposed data encoding scheme, \textit{Double Volley Revolver},
which achieves a better trade-off between computational and memory efficiency,
making FHE-based neural network training more practical. The complete, runnable
C++ code to implement our work can be found at:
\href{https://github.com/petitioner/ML.NNtraining}{$\texttt{https://github.com/petitioner/ML.NNtraining}$}.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.12552v1">Privacy-Preserving Operating Room Workflow Analysis using Digital Twins</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-04-17T00:46:06Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Alejandra Perez, Han Zhang, Yu-Chun Ku, Lalithkumar Seenivasan, Roger Soberanis, Jose L. Porras, Richard Day, Jeff Jopling, Peter Najjar, Mathias Unberath</p>
    <p><b>Summary:</b> Purpose: The operating room (OR) is a complex environment where optimizing
workflows is critical to reduce costs and improve patient outcomes. The use of
computer vision approaches for the automatic recognition of perioperative
events enables identification of bottlenecks for OR optimization. However,
privacy concerns limit the use of computer vision for automated event detection
from OR videos, which makes privacy-preserving approaches needed for OR
workflow analysis. Methods: We propose a two-stage pipeline for
privacy-preserving OR video analysis and event detection. In the first stage,
we leverage vision foundation models for depth estimation and semantic
segmentation to generate de-identified Digital Twins (DT) of the OR from
conventional RGB videos. In the second stage, we employ the SafeOR model, a
fused two-stream approach that processes segmentation masks and depth maps for
OR event detection. We evaluate this method on an internal dataset of 38
simulated surgical trials with five event classes. Results: Our results
indicate that this DT-based approach to the OR event detection model achieves
performance on par and sometimes even better than raw RGB video-based models on
detecting OR events. Conclusion: DTs enable privacy-preserving OR workflow
analysis, facilitating the sharing of de-identified data across institutions
and they can potentially enhance model generalizability by mitigating
domain-specific appearance differences.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.12520v1">Interpreting Network Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Statistics Theory-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> 
  <p><b>Published on:</b> 2025-04-16T22:45:07Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jonathan Hehir, Xiaoyue Niu, Aleksandra Slavkovic</p>
    <p><b>Summary:</b> How do we interpret the differential privacy (DP) guarantee for network data?
We take a deep dive into a popular form of network DP ($\varepsilon$--edge DP)
to find that many of its common interpretations are flawed. Drawing on prior
work for privacy with correlated data, we interpret DP through the lens of
adversarial hypothesis testing and demonstrate a gap between the pairs of
hypotheses actually protected under DP (tests of complete networks) and the
sorts of hypotheses implied to be protected by common claims (tests of
individual edges). We demonstrate some conditions under which this gap can be
bridged, while leaving some questions open. While some discussion is specific
to edge DP, we offer selected results in terms of abstract DP definitions and
provide discussion of the implications for other forms of network DP.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.12129v2">Anti-Aesthetics: Protecting Facial Privacy against Customized
  Text-to-Image Synthesis</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-04-16T14:44:00Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Songping Wang, Yueming Lyu, Shiqi Liu, Ning Li, Tong Tong, Hao Sun, Caifeng Shan</p>
    <p><b>Summary:</b> The rise of customized diffusion models has spurred a boom in personalized
visual content creation, but also poses risks of malicious misuse, severely
threatening personal privacy and copyright protection. Some studies show that
the aesthetic properties of images are highly positively correlated with human
perception of image quality. Inspired by this, we approach the problem from a
novel and intriguing aesthetic perspective to degrade the generation quality of
maliciously customized models, thereby achieving better protection of facial
identity. Specifically, we propose a Hierarchical Anti-Aesthetic (HAA)
framework to fully explore aesthetic cues, which consists of two key branches:
1) Global Anti-Aesthetics: By establishing a global anti-aesthetic reward
mechanism and a global anti-aesthetic loss, it can degrade the overall
aesthetics of the generated content; 2) Local Anti-Aesthetics: A local
anti-aesthetic reward mechanism and a local anti-aesthetic loss are designed to
guide adversarial perturbations to disrupt local facial identity. By seamlessly
integrating both branches, our HAA effectively achieves the goal of
anti-aesthetics from a global to a local level during customized generation.
Extensive experiments show that HAA outperforms existing SOTA methods largely
in identity removal, providing a powerful tool for protecting facial privacy
and copyright.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.11860v1">From Data Behavior to Code Analysis: A Multimodal Study on Security and
  Privacy Challenges in Blockchain-Based DApp</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-04-16T08:30:43Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Haoyang Sun, Yishun Wang, Xiaoqi Li</p>
    <p><b>Summary:</b> The recent proliferation of blockchain-based decentralized applications
(DApp) has catalyzed transformative advancements in distributed systems, with
extensive deployments observed across financial, entertainment, media, and
cybersecurity domains. These trustless architectures, characterized by their
decentralized nature and elimination of third-party intermediaries, have
garnered substantial institutional attention. Consequently, the escalating
security challenges confronting DApp demand rigorous scholarly investigation.
This study initiates with a systematic analysis of behavioral patterns derived
from empirical DApp datasets, establishing foundational insights for subsequent
methodological developments. The principal security vulnerabilities in
Ethereum-based smart contracts developed via Solidity are then critically
examined. Specifically, reentrancy vulnerability attacks are addressed by
formally representing contract logic using highly expressive code fragments.
This enables precise source code-level detection via bidirectional long
short-term memory networks with attention mechanisms (BLSTM-ATT). Regarding
privacy preservation challenges, contemporary solutions are evaluated through
dual analytical lenses: identity privacy preservation and transaction anonymity
enhancement, while proposing future research trajectories in cryptographic
obfuscation techniques.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.11793v3">Selective Attention Federated Learning: Improving Privacy and Efficiency
  for Clinical Text Classification</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-04-16T05:59:29Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yue Li, Lihong Zhang</p>
    <p><b>Summary:</b> Federated Learning (FL) faces major challenges regarding communication
overhead and model privacy when training large language models (LLMs),
especially in healthcare applications. To address these, we introduce Selective
Attention Federated Learning (SAFL), a novel approach that dynamically
fine-tunes only those transformer layers identified as attention-critical. By
employing attention patterns to determine layer importance, SAFL significantly
reduces communication bandwidth and enhances differential privacy resilience.
Evaluations on clinical NLP benchmarks (i2b2 Clinical Concept Extraction and
MIMIC-III discharge summaries) demonstrate that SAFL achieves competitive
performance with centralized models while substantially improving communication
efficiency and privacy preservation.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.11429v1">Improving Statistical Privacy by Subsampling</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-04-15T17:40:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Dennis Breutigam, RÃ¼diger Reischuk</p>
    <p><b>Summary:</b> Differential privacy (DP) considers a scenario, where an adversary has almost
complete information about the entries of a database This worst-case assumption
is likely to overestimate the privacy thread for an individual in real life.
Statistical privacy (SP) denotes a setting where only the distribution of the
database entries is known to an adversary, but not their exact values. In this
case one has to analyze the interaction between noiseless privacy based on the
entropy of distributions and privacy mechanisms that distort the answers of
queries, which can be quite complex.
  A privacy mechanism often used is to take samples of the data for answering a
query. This paper proves precise bounds how much different methods of sampling
increase privacy in the statistical setting with respect to database size and
sampling rate. They allow us to deduce when and how much sampling provides an
improvement and how far this depends on the privacy parameter {\epsilon}. To
perform these investigations we develop a framework to model sampling
techniques.
  For the DP setting tradeoff functions have been proposed as a finer measure
for privacy compared to ({\epsilon},{\delta})-pairs. We apply these tools to
statistical privacy with subsampling to get a comparable characterization</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.11511v1">Position Paper: Rethinking Privacy in RL for Sequential Decision-making
  in the Age of LLMs</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-04-15T10:45:55Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Flint Xiaofeng Fan, Cheston Tan, Roger Wattenhofer, Yew-Soon Ong</p>
    <p><b>Summary:</b> The rise of reinforcement learning (RL) in critical real-world applications
demands a fundamental rethinking of privacy in AI systems. Traditional privacy
frameworks, designed to protect isolated data points, fall short for sequential
decision-making systems where sensitive information emerges from temporal
patterns, behavioral strategies, and collaborative dynamics. Modern RL
paradigms, such as federated RL (FedRL) and RL with human feedback (RLHF) in
large language models (LLMs), exacerbate these challenges by introducing
complex, interactive, and context-dependent learning environments that
traditional methods do not address. In this position paper, we argue for a new
privacy paradigm built on four core principles: multi-scale protection,
behavioral pattern protection, collaborative privacy preservation, and
context-aware adaptation. These principles expose inherent tensions between
privacy, utility, and interpretability that must be navigated as RL systems
become more pervasive in high-stakes domains like healthcare, autonomous
vehicles, and decision support systems powered by LLMs. To tackle these
challenges, we call for the development of new theoretical frameworks,
practical mechanisms, and rigorous evaluation methodologies that collectively
enable effective privacy protection in sequential decision-making systems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.10698v1">Optimising Intrusion Detection Systems in Cloud-Edge Continuum with
  Knowledge Distillation for Privacy-Preserving and Efficient Communication</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-04-14T20:45:05Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Soad Almabdy, Amjad Ullah</p>
    <p><b>Summary:</b> The growth of the Internet of Things has amplified the need for secure data
interactions in cloud-edge ecosystems, where sensitive information is
constantly processed across various system layers. Intrusion detection systems
are commonly used to protect such environments from malicious attacks.
Recently, Federated Learning has emerged as an effective solution for
implementing intrusion detection systems, owing to its decentralised
architecture that avoids sharing raw data with a central server, thereby
enhancing data privacy. Despite its benefits, Federated Learning faces
criticism for high communication overhead from frequent model updates,
especially in large-scale Cloud-Edge infrastructures. This paper explores
Knowledge Distillation to reduce communication overhead in Cloud-Edge intrusion
detection while preserving accuracy and data privacy. Experiments show
significant improvements over state-of-the-art methods.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.10456v1">Privacy-Preserving Distributed Link Predictions Among Peers in Online
  Classrooms Using Federated Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Social and Information Networks-662E9B">
  <p><b>Published on:</b> 2025-04-14T17:43:11Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Anurata Prabha Hridi, Muntasir Hoq, Zhikai Gao, Collin Lynch, Rajeev Sahay, Seyyedali Hosseinalipour, Bita Akram</p>
    <p><b>Summary:</b> Social interactions among classroom peers, represented as social learning
networks (SLNs), play a crucial role in enhancing learning outcomes. While SLN
analysis has recently garnered attention, most existing approaches rely on
centralized training, where data is aggregated and processed on a local/cloud
server with direct access to raw data. However, in real-world educational
settings, such direct access across multiple classrooms is often restricted due
to privacy concerns. Furthermore, training models on isolated classroom data
prevents the identification of common interaction patterns that exist across
multiple classrooms, thereby limiting model performance. To address these
challenges, we propose one of the first frameworks that integrates Federated
Learning (FL), a distributed and collaborative machine learning (ML) paradigm,
with SLNs derived from students' interactions in multiple classrooms' online
forums to predict future link formations (i.e., interactions) among students.
By leveraging FL, our approach enables collaborative model training across
multiple classrooms while preserving data privacy, as it eliminates the need
for raw data centralization. Recognizing that each classroom may exhibit unique
student interaction dynamics, we further employ model personalization
techniques to adapt the FL model to individual classroom characteristics. Our
results demonstrate the effectiveness of our approach in capturing both shared
and classroom-specific representations of student interactions in SLNs.
Additionally, we utilize explainable AI (XAI) techniques to interpret model
predictions, identifying key factors that influence link formation across
different classrooms. These insights unveil the drivers of social learning
interactions within a privacy-preserving, collaborative, and distributed ML
framework -- an aspect that has not been explored before.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.10267v2">Trade-offs in Privacy-Preserving Eye Tracking through Iris Obfuscation:
  A Benchmarking Study</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-04-14T14:29:38Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mengdi Wang, Efe Bozkir, Enkelejda Kasneci</p>
    <p><b>Summary:</b> Recent developments in hardware, computer graphics, and AI may soon enable
AR/VR head-mounted displays (HMDs) to become everyday devices like smartphones
and tablets. Eye trackers within HMDs provide a special opportunity for such
setups as it is possible to facilitate gaze-based research and interaction.
However, estimating users' gaze information often requires raw eye images and
videos that contain iris textures, which are considered a gold standard
biometric for user authentication, and this raises privacy concerns. Previous
research in the eye-tracking community focused on obfuscating iris textures
while keeping utility tasks such as gaze estimation accurate. Despite these
attempts, there is no comprehensive benchmark that evaluates state-of-the-art
approaches. Considering all, in this paper, we benchmark blurring, noising,
downsampling, rubber sheet model, and iris style transfer to obfuscate user
identity, and compare their impact on image quality, privacy, utility, and risk
of imposter attack on two datasets. We use eye segmentation and gaze estimation
as utility tasks, and reduction in iris recognition accuracy as a measure of
privacy protection, and false acceptance rate to estimate risk of attack. Our
experiments show that canonical image processing methods like blurring and
noising cause a marginal impact on deep learning-based tasks. While
downsampling, rubber sheet model, and iris style transfer are effective in
hiding user identifiers, iris style transfer, with higher computation cost,
outperforms others in both utility tasks, and is more resilient against spoof
attacks. Our analyses indicate that there is no universal optimal approach to
balance privacy, utility, and computation burden. Therefore, we recommend
practitioners consider the strengths and weaknesses of each approach, and
possible combinations of those to reach an optimal privacy-utility trade-off.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.10016v1">Quantifying Privacy Leakage in Split Inference via Fisher-Approximated
  Shannon Information Analysis</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-04-14T09:19:06Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ruijun Deng, Zhihui Lu, Qiang Duan</p>
    <p><b>Summary:</b> Split inference (SI) partitions deep neural networks into distributed
sub-models, enabling privacy-preserving collaborative learning. Nevertheless,
it remains vulnerable to Data Reconstruction Attacks (DRAs), wherein
adversaries exploit exposed smashed data to reconstruct raw inputs. Despite
extensive research on adversarial attack-defense games, a shortfall remains in
the fundamental analysis of privacy risks. This paper establishes a theoretical
framework for privacy leakage quantification using information theory, defining
it as the adversary's certainty and deriving both average-case and worst-case
error bounds. We introduce Fisher-approximated Shannon information (FSInfo), a
novel privacy metric utilizing Fisher Information (FI) for operational privacy
leakage computation. We empirically show that our privacy metric correlates
well with empirical attacks and investigate some of the factors that affect
privacy leakage, namely the data distribution, model size, and overfitting.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.09961v1">Privacy Meets Explainability: Managing Confidential Data and
  Transparency Policies in LLM-Empowered Science</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-04-14T07:58:26Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yashothara Shanmugarasa, Shidong Pan, Ming Ding, Dehai Zhao, Thierry Rakotoarivelo</p>
    <p><b>Summary:</b> As Large Language Models (LLMs) become integral to scientific workflows,
concerns over the confidentiality and ethical handling of confidential data
have emerged. This paper explores data exposure risks through LLM-powered
scientific tools, which can inadvertently leak confidential information,
including intellectual property and proprietary data, from scientists'
perspectives. We propose "DataShield", a framework designed to detect
confidential data leaks, summarize privacy policies, and visualize data flow,
ensuring alignment with organizational policies and procedures. Our approach
aims to inform scientists about data handling practices, enabling them to make
informed decisions and protect sensitive information. Ongoing user studies with
scientists are underway to evaluate the framework's usability, trustworthiness,
and effectiveness in tackling real-world privacy challenges.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.09952v1">Secrecy and Privacy in Multi-Access Combinatorial Topology</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2025-04-14T07:30:03Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mallikharjuna Chinnapadamala, B. Sundar Rajan</p>
    <p><b>Summary:</b> In this work, we consider the multi-access combinatorial topology with $C$
caches where each user accesses a unique set of $r$ caches. For this setup, we
consider secrecy, where each user should not know anything about the files it
did not request, and demand privacy, where each user's demand must be kept
private from other non-colluding users. We propose a scheme satisfying both
conditions and derive a lower bound based on cut-set arguments. Also, we prove
that our scheme is optimal when $r\geq C-1$, and it is order-optimal when the
cache memory size $M$ is greater than or equal to a certain threshold for
$r<C-1$. When $r=1$, in most of the memory region, our scheme achieves the same
rate as the one given by the secretive scheme for the dedicated cache setup by
Ravindrakumar et al. ( 'Private Coded Caching,' in \textit{IEEE Transactions on
Information Forensics and Security}, 2018), while satisfying both secrecy and
demand privacy conditions.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.09517v2">RoboComm: A DID-based scalable and privacy-preserving Robot-to-Robot
  interaction over state channels</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Networking and Internet Architecture-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Robotics-F9C80E">
  <p><b>Published on:</b> 2025-04-13T11:10:04Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Roshan Singh, Sushant Pandey</p>
    <p><b>Summary:</b> In a multi robot system establishing trust amongst untrusted robots from
different organisations while preserving a robot's privacy is a challenge.
Recently decentralized technologies such as smart contract and blockchain are
being explored for applications in robotics. However, the limited transaction
processing and high maintenance cost hinder the widespread adoption of such
approaches. Moreover, blockchain transactions be they on public or private
permissioned blockchain are publically readable which further fails to preserve
the confidentiality of the robot's data and privacy of the robot.
  In this work, we propose RoboComm a Decentralized Identity based approach for
privacy-preserving interaction between robots. With DID a component of
Self-Sovereign Identity; robots can authenticate each other independently
without relying on any third-party service. Verifiable Credentials enable
private data associated with a robot to be stored within the robot's hardware,
unlike existing blockchain based approaches where the data has to be on the
blockchain. We improve throughput by allowing message exchange over state
channels. Being a blockchain backed solution RoboComm provides a trustworthy
system without relying on a single party. Moreover, we implement our proposed
approach to demonstrate the feasibility of our solution.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.09095v1">Privacy Preservation in Gen AI Applications</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-04-12T06:19:37Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Swetha S, Ram Sundhar K Shaju, Rakshana M, Ganesh R, Balavedhaa S, Thiruvaazhi U</p>
    <p><b>Summary:</b> The ability of machines to comprehend and produce language that is similar to
that of humans has revolutionized sectors like customer service, healthcare,
and finance thanks to the quick advances in Natural Language Processing (NLP),
which are fueled by Generative Artificial Intelligence (AI) and Large Language
Models (LLMs). However, because LLMs trained on large datasets may
unintentionally absorb and reveal Personally Identifiable Information (PII)
from user interactions, these capabilities also raise serious privacy concerns.
Deep neural networks' intricacy makes it difficult to track down or stop the
inadvertent storing and release of private information, which raises serious
concerns about the privacy and security of AI-driven data. This study tackles
these issues by detecting Generative AI weaknesses through attacks such as data
extraction, model inversion, and membership inference. A privacy-preserving
Generative AI application that is resistant to these assaults is then
developed. It ensures privacy without sacrificing functionality by using
methods to identify, alter, or remove PII before to dealing with LLMs. In order
to determine how well cloud platforms like Microsoft Azure, Google Cloud, and
AWS provide privacy tools for protecting AI applications, the study also
examines these technologies. In the end, this study offers a fundamental
privacy paradigm for generative AI systems, focusing on data security and moral
AI implementation, and opening the door to a more secure and conscientious use
of these tools.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.08616v1">Preserving Privacy Without Compromising Accuracy: Machine Unlearning for
  Handwritten Text Recognition</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-04-11T15:21:12Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Lei Kang, Xuanshuo Fu, Lluis Gomez, Alicia FornÃ©s, Ernest Valveny, Dimosthenis Karatzas</p>
    <p><b>Summary:</b> Handwritten Text Recognition (HTR) is essential for document analysis and
digitization. However, handwritten data often contains user-identifiable
information, such as unique handwriting styles and personal lexicon choices,
which can compromise privacy and erode trust in AI services. Legislation like
the ``right to be forgotten'' underscores the necessity for methods that can
expunge sensitive information from trained models. Machine unlearning addresses
this by selectively removing specific data from models without necessitating
complete retraining. Yet, it frequently encounters a privacy-accuracy tradeoff,
where safeguarding privacy leads to diminished model performance. In this
paper, we introduce a novel two-stage unlearning strategy for a multi-head
transformer-based HTR model, integrating pruning and random labeling. Our
proposed method utilizes a writer classification head both as an indicator and
a trigger for unlearning, while maintaining the efficacy of the recognition
head. To our knowledge, this represents the first comprehensive exploration of
machine unlearning within HTR tasks. We further employ Membership Inference
Attacks (MIA) to evaluate the effectiveness of unlearning user-identifiable
information. Extensive experiments demonstrate that our approach effectively
preserves privacy while maintaining model accuracy, paving the way for new
research directions in the document analysis community. Our code will be
publicly available upon acceptance.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.08254v2">Understanding the Impact of Data Domain Extraction on Synthetic Data
  Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-04-11T04:35:24Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Georgi Ganev, Meenatchi Sundaram Muthu Selva Annamalai, Sofiane Mahiou, Emiliano De Cristofaro</p>
    <p><b>Summary:</b> Privacy attacks, particularly membership inference attacks (MIAs), are widely
used to assess the privacy of generative models for tabular synthetic data,
including those with Differential Privacy (DP) guarantees. These attacks often
exploit outliers, which are especially vulnerable due to their position at the
boundaries of the data domain (e.g., at the minimum and maximum values).
However, the role of data domain extraction in generative models and its impact
on privacy attacks have been overlooked. In this paper, we examine three
strategies for defining the data domain: assuming it is externally provided
(ideally from public data), extracting it directly from the input data, and
extracting it with DP mechanisms. While common in popular implementations and
libraries, we show that the second approach breaks end-to-end DP guarantees and
leaves models vulnerable. While using a provided domain (if representative) is
preferable, extracting it with DP can also defend against popular MIAs, even at
high privacy budgets.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.07761v1">Exploring a Patch-Wise Approach for Privacy-Preserving Fake ID Detection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-04-10T14:01:22Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Javier MuÃ±oz-Haro, Ruben Tolosana, Ruben Vera-Rodriguez, Aythami Morales, Julian Fierrez</p>
    <p><b>Summary:</b> In an increasingly digitalized world, verifying the authenticity of ID
documents has become a critical challenge for real-life applications such as
digital banking, crypto-exchanges, renting, etc. This study focuses on the
topic of fake ID detection, covering several limitations in the field. In
particular, no publicly available data from real ID documents exists, and most
studies rely on proprietary in-house databases that are not available due to
privacy reasons. In order to shed some light on this critical challenge that
makes difficult to advance in the field, we explore a trade-off between privacy
(i.e., amount of sensitive data available) and performance, proposing a novel
patch-wise approach for privacy-preserving fake ID detection. Our proposed
approach explores how privacy can be enhanced through: i) two levels of
anonymization for an ID document (i.e., fully- and pseudo-anonymized), and ii)
different patch size configurations, varying the amount of sensitive data
visible in the patch image. Also, state-of-the-art methods such as Vision
Transformers and Foundation Models are considered in the analysis. The
experimental framework shows that, on an unseen database (DLC-2021), our
proposal achieves 13.91% and 0% EERs at patch and ID document level, showing a
good generalization to other databases. In addition to this exploration,
another key contribution of our study is the release of the first publicly
available database that contains 48,400 patches from both real and fake ID
documents, along with the experimental framework and models, which will be
available in our GitHub.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.07578v1">Privacy-Preserving Vertical K-Means Clustering</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-04-10T09:20:56Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Federico Mazzone, Trevor Brown, Florian Kerschbaum, Kevin H. Wilson, Maarten Everts, Florian Hahn, Andreas Peter</p>
    <p><b>Summary:</b> Clustering is a fundamental data processing task used for grouping records
based on one or more features. In the vertically partitioned setting, data is
distributed among entities, with each holding only a subset of those features.
A key challenge in this scenario is that computing distances between records
requires access to all distributed features, which may be privacy-sensitive and
cannot be directly shared with other parties. The goal is to compute the joint
clusters while preserving the privacy of each entity's dataset. Existing
solutions using secret sharing or garbled circuits implement privacy-preserving
variants of Lloyd's algorithm but incur high communication costs, scaling as
O(nkt), where n is the number of data points, k the number of clusters, and t
the number of rounds. These methods become impractical for large datasets or
several parties, limiting their use to LAN settings only. On the other hand, a
different line of solutions rely on differential privacy (DP) to outsource the
local features of the parties to a central server. However, they often
significantly degrade the utility of the clustering outcome due to excessive
noise. In this work, we propose a novel solution based on homomorphic
encryption and DP, reducing communication complexity to O(n+kt). In our method,
parties securely outsource their features once, allowing a computing party to
perform clustering operations under encryption. DP is applied only to the
clusters' centroids, ensuring privacy with minimal impact on utility. Our
solution clusters 100,000 two-dimensional points into five clusters using only
73MB of communication, compared to 101GB for existing works, and completes in
just under 3 minutes on a 100Mbps network, whereas existing works take over 1
day. This makes our solution practical even for WAN deployments, all while
maintaining accuracy comparable to plaintext k-means algorithms.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.07414v3">A Unified Framework and Efficient Computation for Privacy Amplification
  via Shuffling</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-04-10T03:11:17Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Pengcheng Su, Haibo Cheng, Ping Wang</p>
    <p><b>Summary:</b> The shuffle model offers significant privacy amplification over local
differential privacy (LDP), enabling improved privacy-utility trade-offs. To
analyze and quantify this amplification effect, two primary frameworks have
been proposed: the \textit{privacy blanket} (Balle et al., CRYPTO 2019) and the
\textit{clone paradigm}, which includes both the \textit{standard clone} and
\textit{stronger clone} (Feldman et al., FOCS 2021; SODA 2023). All of these
approaches are grounded in decomposing the behavior of local randomizers.
  In this work, we present a unified perspective--termed the \textit{general
clone paradigm}--that captures all decomposition-based analyses. We identify
the optimal decomposition within this framework and design a simple yet
efficient algorithm based on the Fast Fourier Transform (FFT) to compute tight
privacy amplification bounds. Empirical results show that our computed upper
bounds nearly match the corresponding lower bounds, demonstrating the accuracy
and tightness of our method.
  Furthermore, we apply our algorithm to derive optimal privacy amplification
bounds for both joint composition and parallel composition of LDP mechanisms in
the shuffle model.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.07362v1">Augmented Shuffle Protocols for Accurate and Robust Frequency Estimation
  under Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-04-10T01:06:05Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Takao Murakami, Yuichi Sei, Reo Eriguchi</p>
    <p><b>Summary:</b> The shuffle model of DP (Differential Privacy) provides high utility by
introducing a shuffler that randomly shuffles noisy data sent from users.
However, recent studies show that existing shuffle protocols suffer from the
following two major drawbacks. First, they are vulnerable to local data
poisoning attacks, which manipulate the statistics about input data by sending
crafted data, especially when the privacy budget epsilon is small. Second, the
actual value of epsilon is increased by collusion attacks by the data collector
and users.
  In this paper, we address these two issues by thoroughly exploring the
potential of the augmented shuffle model, which allows the shuffler to perform
additional operations, such as random sampling and dummy data addition.
Specifically, we propose a generalized framework for local-noise-free protocols
in which users send (encrypted) input data to the shuffler without adding
noise. We show that this generalized protocol provides DP and is robust to the
above two attacks if a simpler mechanism that performs the same process on
binary input data provides DP. Based on this framework, we propose three
concrete protocols providing DP and robustness against the two attacks. Our
first protocol generates the number of dummy values for each item from a
binomial distribution and provides higher utility than several state-of-the-art
existing shuffle protocols. Our second protocol significantly improves the
utility of our first protocol by introducing a novel dummy-count distribution:
asymmetric two-sided geometric distribution. Our third protocol is a special
case of our second protocol and provides pure epsilon-DP. We show the
effectiveness of our protocols through theoretical analysis and comprehensive
experiments.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.07323v1">Prekey Pogo: Investigating Security and Privacy Issues in WhatsApp's
  Handshake Mechanism</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Networking and Internet Architecture-04E762">
  <p><b>Published on:</b> 2025-04-09T22:53:13Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Gabriel K. Gegenhuber, Philipp Ã. Frenzel, Maximilian GÃ¼nther, Aljosha Judmayer</p>
    <p><b>Summary:</b> WhatsApp, the world's largest messaging application, uses a version of the
Signal protocol to provide end-to-end encryption (E2EE) with strong security
guarantees, including Perfect Forward Secrecy (PFS). To ensure PFS right from
the start of a new conversation -- even when the recipient is offline -- a
stash of ephemeral (one-time) prekeys must be stored on a server. While the
critical role of these one-time prekeys in achieving PFS has been outlined in
the Signal specification, we are the first to demonstrate a targeted depletion
attack against them on individual WhatsApp user devices. Our findings not only
reveal an attack that can degrade PFS for certain messages, but also expose
inherent privacy risks and serious availability implications arising from the
refilling and distribution procedure essential for this security mechanism.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.06697v2">"Sorry for bugging you so much." Exploring Developers' Behavior Towards
  Privacy-Compliant Implementation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Software Engineering-D91E36">
  <p><b>Published on:</b> 2025-04-09T08:59:17Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Stefan Albert Horstmann, Sandy Hong, David Klein, Raphael Serafini, Martin Degeling, Martin Johns, Veelasha Moonsamy, Alena Naiakshina</p>
    <p><b>Summary:</b> While protecting user data is essential, software developers often fail to
fulfill privacy requirements. However, the reasons why they struggle with
privacy-compliant implementation remain unclear. Is it due to a lack of
knowledge, or is it because of insufficient support? To provide foundational
insights in this field, we conducted a qualitative 5-hour programming study
with 30 professional software developers implementing 3 privacy-sensitive
programming tasks that were designed with GDPR compliance in mind. To explore
if and how developers implement privacy requirements, participants were divided
into 3 groups: control, privacy prompted, and privacy expert-supported. After
task completion, we conducted follow-up interviews. Alarmingly, almost all
participants submitted non-GDPR-compliant solutions (79/90). In particular,
none of the 3 tasks were solved privacy-compliant by all 30 participants, with
the non-prompted group having the lowest number of 3 out of 30
privacy-compliant solution attempts. Privacy prompting and expert support only
slightly improved participants' submissions, with 6/30 and 8/30
privacy-compliant attempts, respectively. In fact, all participants reported
severe issues addressing common privacy requirements such as purpose
limitation, user consent, or data minimization. Counterintuitively, although
most developers exhibited minimal confidence in their solutions, they rarely
sought online assistance or contacted the privacy expert, with only 4 out of 10
expert-supported participants explicitly asking for compliance confirmation.
Instead, participants often relied on existing implementations and focused on
implementing functionality and security first.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.06552v1">Understanding Users' Security and Privacy Concerns and Attitudes Towards
  Conversational AI Platforms</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2025-04-09T03:22:48Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mutahar Ali, Arjun Arunasalam, Habiba Farrukh</p>
    <p><b>Summary:</b> The widespread adoption of conversational AI platforms has introduced new
security and privacy risks. While these risks and their mitigation strategies
have been extensively researched from a technical perspective, users'
perceptions of these platforms' security and privacy remain largely unexplored.
In this paper, we conduct a large-scale analysis of over 2.5M user posts from
the r/ChatGPT Reddit community to understand users' security and privacy
concerns and attitudes toward conversational AI platforms. Our qualitative
analysis reveals that users are concerned about each stage of the data
lifecycle (i.e., collection, usage, and retention). They seek mitigations for
security vulnerabilities, compliance with privacy regulations, and greater
transparency and control in data handling. We also find that users exhibit
varied behaviors and preferences when interacting with these platforms. Some
users proactively safeguard their data and adjust privacy settings, while
others prioritize convenience over privacy risks, dismissing privacy concerns
in favor of benefits, or feel resigned to inevitable data sharing. Through
qualitative content and regression analysis, we discover that users' concerns
evolve over time with the evolving AI landscape and are influenced by
technological developments and major events. Based on our findings, we provide
recommendations for users, platforms, enterprises, and policymakers to enhance
transparency, improve data controls, and increase user trust and adoption.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.05849v1">On the Importance of Conditioning for Privacy-Preserving Data
  Augmentation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-04-08T09:27:51Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Julian Lorenz, Katja Ludwig, Valentin Haug, Rainer Lienhart</p>
    <p><b>Summary:</b> Latent diffusion models can be used as a powerful augmentation method to
artificially extend datasets for enhanced training. To the human eye, these
augmented images look very different to the originals. Previous work has
suggested to use this data augmentation technique for data anonymization.
However, we show that latent diffusion models that are conditioned on features
like depth maps or edges to guide the diffusion process are not suitable as a
privacy preserving method. We use a contrastive learning approach to train a
model that can correctly identify people out of a pool of candidates. Moreover,
we demonstrate that anonymization using conditioned diffusion models is
susceptible to black box attacks. We attribute the success of the described
methods to the conditioning of the latent diffusion model in the anonymization
process. The diffusion model is instructed to produce similar edges for the
anonymized images. Hence, a model can learn to recognize these patterns for
identification.</p>
  </details>
</div>



<h2>2025-05</h2>

<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.05214v1">Overcoming the hurdle of legal expertise: A reusable model for
  smartwatch privacy policies</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Software Engineering-D91E36">
  <p><b>Published on:</b> 2025-05-08T13:09:12Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Constantin Buschhaus, Arvid Butting, Judith Michael, Verena Nitsch, Sebastian PÃ¼tz, Bernhard Rumpe, Carolin Stellmacher, Sabine Theis</p>
    <p><b>Summary:</b> Regulations for privacy protection aim to protect individuals from the
unauthorized storage, processing, and transfer of their personal data but
oftentimes fail in providing helpful support for understanding these
regulations. To better communicate privacy policies for smartwatches, we need
an in-depth understanding of their concepts and provide better ways to enable
developers to integrate them when engineering systems. Up to now, no conceptual
model exists covering privacy statements from different smartwatch
manufacturers that is reusable for developers. This paper introduces such a
conceptual model for privacy policies of smartwatches and shows its use in a
model-driven software engineering approach to create a platform for data
visualization of wearable privacy policies from different smartwatch
manufacturers. We have analyzed the privacy policies of various manufacturers
and extracted the relevant concepts. Moreover, we have checked the model with
lawyers for its correctness, instantiated it with concrete data, and used it in
a model-driven software engineering approach to create a platform for data
visualization. This reusable privacy policy model can enable developers to
easily represent privacy policies in their systems. This provides a foundation
for more structured and understandable privacy policies which, in the long run,
can increase the data sovereignty of application users.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.05155v1">FedTDP: A Privacy-Preserving and Unified Framework for Trajectory Data
  Preparation via Federated Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-08T11:51:23Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhihao Zeng, Ziquan Fang, Wei Shao, Lu Chen, Yunjun Gao</p>
    <p><b>Summary:</b> Trajectory data, which capture the movement patterns of people and vehicles
over time and space, are crucial for applications like traffic optimization and
urban planning. However, issues such as noise and incompleteness often
compromise data quality, leading to inaccurate trajectory analyses and limiting
the potential of these applications. While Trajectory Data Preparation (TDP)
can enhance data quality, existing methods suffer from two key limitations: (i)
they do not address data privacy concerns, particularly in federated settings
where trajectory data sharing is prohibited, and (ii) they typically design
task-specific models that lack generalizability across diverse TDP scenarios.
To overcome these challenges, we propose FedTDP, a privacy-preserving and
unified framework that leverages the capabilities of Large Language Models
(LLMs) for TDP in federated environments. Specifically, we: (i) design a
trajectory privacy autoencoder to secure data transmission and protect privacy,
(ii) introduce a trajectory knowledge enhancer to improve model learning of
TDP-related knowledge, enabling the development of TDP-oriented LLMs, and (iii)
propose federated parallel optimization to enhance training efficiency by
reducing data transmission and enabling parallel model training. Experiments on
6 real datasets and 10 mainstream TDP tasks demonstrate that FedTDP
consistently outperforms 13 state-of-the-art baselines.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.05031v1">LSRP: A Leader-Subordinate Retrieval Framework for Privacy-Preserving
  Cloud-Device Collaboration</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Retrieval-5BC0EB">
  <p><b>Published on:</b> 2025-05-08T08:06:34Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yingyi Zhang, Pengyue Jia, Xianneng Li, Derong Xu, Maolin Wang, Yichao Wang, Zhaocheng Du, Huifeng Guo, Yong Liu, Ruiming Tang, Xiangyu Zhao</p>
    <p><b>Summary:</b> Cloud-device collaboration leverages on-cloud Large Language Models (LLMs)
for handling public user queries and on-device Small Language Models (SLMs) for
processing private user data, collectively forming a powerful and
privacy-preserving solution. However, existing approaches often fail to fully
leverage the scalable problem-solving capabilities of on-cloud LLMs while
underutilizing the advantage of on-device SLMs in accessing and processing
personalized data. This leads to two interconnected issues: 1) Limited
utilization of the problem-solving capabilities of on-cloud LLMs, which fail to
align with personalized user-task needs, and 2) Inadequate integration of user
data into on-device SLM responses, resulting in mismatches in contextual user
information.
  In this paper, we propose a Leader-Subordinate Retrieval framework for
Privacy-preserving cloud-device collaboration (LSRP), a novel solution that
bridges these gaps by: 1) enhancing on-cloud LLM guidance to on-device SLM
through a dynamic selection of task-specific leader strategies named as
user-to-user retrieval-augmented generation (U-U-RAG), and 2) integrating the
data advantages of on-device SLMs through small model feedback Direct
Preference Optimization (SMFB-DPO) for aligning the on-cloud LLM with the
on-device SLM. Experiments on two datasets demonstrate that LSRP consistently
outperforms state-of-the-art baselines, significantly improving question-answer
relevance and personalization, while preserving user privacy through efficient
on-device retrieval. Our code is available at:
https://github.com/Zhang-Yingyi/LSRP.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.04889v1">FedRE: Robust and Effective Federated Learning with Privacy Preference</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-08T01:50:27Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tianzhe Xiao, Yichen Li, Yu Zhou, Yining Qi, Yi Liu, Wei Wang, Haozhao Wang, Yi Wang, Ruixuan Li</p>
    <p><b>Summary:</b> Despite Federated Learning (FL) employing gradient aggregation at the server
for distributed training to prevent the privacy leakage of raw data, private
information can still be divulged through the analysis of uploaded gradients
from clients. Substantial efforts have been made to integrate local
differential privacy (LDP) into the system to achieve a strict privacy
guarantee. However, existing methods fail to take practical issues into account
by merely perturbing each sample with the same mechanism while each client may
have their own privacy preferences on privacy-sensitive information (PSI),
which is not uniformly distributed across the raw data. In such a case,
excessive privacy protection from private-insensitive information can
additionally introduce unnecessary noise, which may degrade the model
performance. In this work, we study the PSI within data and develop FedRE, that
can simultaneously achieve robustness and effectiveness benefits with LDP
protection. More specifically, we first define PSI with regard to the privacy
preferences of each client. Then, we optimize the LDP by allocating less
privacy budget to gradients with higher PSI in a layer-wise manner, thus
providing a stricter privacy guarantee for PSI. Furthermore, to mitigate the
performance degradation caused by LDP, we design a parameter aggregation
mechanism based on the distribution of the perturbed information. We conducted
experiments with text tamper detection on T-SROIE and DocTamper datasets, and
FedRE achieves competitive performance compared to state-of-the-art methods.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.04799v1">Safeguard-by-Development: A Privacy-Enhanced Development Paradigm for
  Multi-Agent Collaboration Systems</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-07T20:54:43Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jian Cui, Zichuan Li, Luyi Xing, Xiaojing Liao</p>
    <p><b>Summary:</b> Multi-agent collaboration systems (MACS), powered by large language models
(LLMs), solve complex problems efficiently by leveraging each agent's
specialization and communication between agents. However, the inherent exchange
of information between agents and their interaction with external environments,
such as LLM, tools, and users, inevitably introduces significant risks of
sensitive data leakage, including vulnerabilities to attacks like prompt
injection and reconnaissance. Existing MACS fail to enable privacy controls,
making it challenging to manage sensitive information securely. In this paper,
we take the first step to address the MACS's data leakage threat at the system
development level through a privacy-enhanced development paradigm, Maris. Maris
enables rigorous message flow control within MACS by embedding reference
monitors into key multi-agent conversation components. We implemented Maris as
an integral part of AutoGen, a widely adopted open-source multi-agent
development framework. Then, we evaluate Maris for its effectiveness and
performance overhead on privacy-critical MACS use cases, including healthcare,
supply chain optimization, and personalized recommendation system. The result
shows that Maris achieves satisfactory effectiveness, performance overhead and
practicability for adoption.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.04570v1">Privacy-preserving neutral atom-based quantum classifier towards real
  healthcare applications</a></h3>
    
  <p><b>Published on:</b> 2025-05-07T17:03:35Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ettore Canonici, Filippo Caruso</p>
    <p><b>Summary:</b> Technological advances in Artificial Intelligence (AI) and Machine Learning
(ML) for the healthcare domain are rapidly arising, with a growing discussion
regarding the ethical management of their development. In general, ML
healthcare applications crucially require performance, interpretability of
data, and respect for data privacy. The latter is an increasingly debated topic
as commercial cloud computing services become more and more widespread.
Recently, dedicated methods are starting to be developed aiming to protect data
privacy. However, these generally result in a trade-off forcing one to balance
the level of data privacy and the algorithm performance. Here, a Support Vector
Machine (SVM) classifier model is proposed whose training is reformulated into
a Quadratic Unconstrained Binary Optimization (QUBO) problem, and adapted to a
neutral atom-based Quantum Processing Unit (QPU). Our final model does not
require anonymization techniques to protect data privacy since the sensitive
data are not needed to be transferred to the cloud-available QPU. Indeed, the
latter is used only during the training phase, hence allowing a future concrete
application in a real-world scenario. Finally, performance and scaling analyses
on a publicly available breast cancer dataset are discussed, both using ideal
and noisy simulations for the training process, and also successfully tested on
a currently available real neutral-atom QPU.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.04361v1">RDPP-TD: Reputation and Data Privacy-Preserving based Truth Discovery
  Scheme in Mobile Crowdsensing</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computational Engineering, Finance, and Science-5BC0EB">
  <p><b>Published on:</b> 2025-05-07T12:20:55Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Lijian Wu, Weikun Xie, Wei Tan, Tian Wang, Houbing Herbert Song, Anfeng Liu</p>
    <p><b>Summary:</b> Truth discovery (TD) plays an important role in Mobile Crowdsensing (MCS).
However, existing TD methods, including privacy-preserving TD approaches,
estimate the truth by weighting only the data submitted in the current round,
which often results in low data quality. Moreover, there is a lack of effective
TD methods that preserve both reputation and data privacy. To address these
issues, a Reputation and Data Privacy-Preserving based Truth Discovery
(RDPP-TD) scheme is proposed to obtain high-quality data for MCS. The RDPP-TD
scheme consists of two key approaches: a Reputation-based Truth Discovery (RTD)
approach, which integrates the weight of current-round data with workers'
reputation values to estimate the truth, thereby achieving more accurate
results, and a Reputation and Data Privacy-Preserving (RDPP) approach, which
ensures privacy preservation for sensing data and reputation values. First, the
RDPP approach, when seamlessly integrated with RTD, can effectively evaluate
the reliability of workers and their sensing data in a privacy-preserving
manner. Second, the RDPP scheme supports reputation-based worker recruitment
and rewards, ensuring high-quality data collection while incentivizing workers
to provide accurate information. Comprehensive theoretical analysis and
extensive experiments based on real-world datasets demonstrate that the
proposed RDPP-TD scheme provides strong privacy protection and improves data
quality by up to 33.3%.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.04181v1">Privacy Challenges In Image Processing Applications</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-07T07:28:03Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b>  Maneesha, Bharat Gupta, Rishabh Sethi, Charvi Adita Das</p>
    <p><b>Summary:</b> As image processing systems proliferate, privacy concerns intensify given the
sensitive personal information contained in images. This paper examines privacy
challenges in image processing and surveys emerging privacy-preserving
techniques including differential privacy, secure multiparty computation,
homomorphic encryption, and anonymization. Key applications with heightened
privacy risks include healthcare, where medical images contain patient health
data, and surveillance systems that can enable unwarranted tracking.
Differential privacy offers rigorous privacy guarantees by injecting controlled
noise, while MPC facilitates collaborative analytics without exposing raw data
inputs. Homomorphic encryption enables computations on encrypted data and
anonymization directly removes identifying elements. However, balancing privacy
protections and utility remains an open challenge. Promising future directions
identified include quantum-resilient cryptography, federated learning,
dedicated hardware, and conceptual innovations like privacy by design.
Ultimately, a holistic effort combining technological innovations, ethical
considerations, and policy frameworks is necessary to uphold the fundamental
right to privacy as image processing capabilities continue advancing rapidly.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.04034v1">Izhikevich-Inspired Temporal Dynamics for Enhancing Privacy, Efficiency,
  and Transferability in Spiking Neural Networks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Neural and Evolutionary Computing-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-05-07T00:27:00Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ayana Moshruba, Hamed Poursiami, Maryam Parsa</p>
    <p><b>Summary:</b> Biological neurons exhibit diverse temporal spike patterns, which are
believed to support efficient, robust, and adaptive neural information
processing. While models such as Izhikevich can replicate a wide range of these
firing dynamics, their complexity poses challenges for directly integrating
them into scalable spiking neural networks (SNN) training pipelines. In this
work, we propose two probabilistically driven, input-level temporal spike
transformations: Poisson-Burst and Delayed-Burst that introduce biologically
inspired temporal variability directly into standard Leaky Integrate-and-Fire
(LIF) neurons. This enables scalable training and systematic evaluation of how
spike timing dynamics affect privacy, generalization, and learning performance.
Poisson-Burst modulates burst occurrence based on input intensity, while
Delayed-Burst encodes input strength through burst onset timing. Through
extensive experiments across multiple benchmarks, we demonstrate that
Poisson-Burst maintains competitive accuracy and lower resource overhead while
exhibiting enhanced privacy robustness against membership inference attacks,
whereas Delayed-Burst provides stronger privacy protection at a modest accuracy
trade-off. These findings highlight the potential of biologically grounded
temporal spike dynamics in improving the privacy, generalization and biological
plausibility of neuromorphic learning systems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.03639v1">Differential Privacy for Network Assortativity</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-06T15:40:47Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Fei Ma, Jinzhi Ouyang, Xincheng Hu</p>
    <p><b>Summary:</b> The analysis of network assortativity is of great importance for
understanding the structural characteristics of and dynamics upon networks.
Often, network assortativity is quantified using the assortativity coefficient
that is defined based on the Pearson correlation coefficient between vertex
degrees. It is well known that a network may contain sensitive information,
such as the number of friends of an individual in a social network (which is
abstracted as the degree of vertex.). So, the computation of the assortativity
coefficient leads to privacy leakage, which increases the urgent need for
privacy-preserving protocol. However, there has been no scheme addressing the
concern above.
  To bridge this gap, in this work, we are the first to propose approaches
based on differential privacy (DP for short). Specifically, we design three
DP-based algorithms: $Local_{ru}$, $Shuffle_{ru}$, and $Decentral_{ru}$. The
first two algorithms, based on Local DP (LDP) and Shuffle DP respectively, are
designed for settings where each individual only knows his/her direct friends.
In contrast, the third algorithm, based on Decentralized DP (DDP), targets
scenarios where each individual has a broader view, i.e., also knowing his/her
friends' friends. Theoretically, we prove that each algorithm enables an
unbiased estimation of the assortativity coefficient of the network. We further
evaluate the performance of the proposed algorithms using mean squared error
(MSE), showing that $Shuffle_{ru}$ achieves the best performance, followed by
$Decentral_{ru}$, with $Local_{ru}$ performing the worst. Note that these three
algorithms have different assumptions, so each has its applicability scenario.
Lastly, we conduct extensive numerical simulations, which demonstrate that the
presented approaches are adequate to achieve the estimation of network
assortativity under the demand for privacy protection.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.02975v1">Navigating Privacy and Trust: AI Assistants as Social Support for Older
  Adults</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-05-05T19:00:14Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Karina LaRubbio, Malcolm Grba, Diana Freed</p>
    <p><b>Summary:</b> AI assistants are increasingly integrated into older adults' daily lives,
offering new opportunities for social support and accessibility while raising
important questions about privacy, autonomy, and trust. As these systems become
embedded in caregiving and social networks, older adults must navigate
trade-offs between usability, data privacy, and personal agency across
different interaction contexts. Although prior work has explored AI assistants'
potential benefits, further research is needed to understand how perceived
usefulness and risk shape adoption and engagement. This paper examines these
dynamics and advocates for participatory design approaches that position older
adults as active decision makers in shaping AI assistant functionality. By
advancing a framework for privacy-aware, user-centered AI design, this work
contributes to ongoing discussions on developing ethical and transparent AI
systems that enhance well-being without compromising user control.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.02828v1">Privacy Risks and Preservation Methods in Explainable Artificial
  Intelligence: A Scoping Review</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Emerging Technologies-F9C80E">
  <p><b>Published on:</b> 2025-05-05T17:53:28Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sonal Allana, Mohan Kankanhalli, Rozita Dara</p>
    <p><b>Summary:</b> Explainable Artificial Intelligence (XAI) has emerged as a pillar of
Trustworthy AI and aims to bring transparency in complex models that are opaque
by nature. Despite the benefits of incorporating explanations in models, an
urgent need is found in addressing the privacy concerns of providing this
additional information to end users. In this article, we conduct a scoping
review of existing literature to elicit details on the conflict between privacy
and explainability. Using the standard methodology for scoping review, we
extracted 57 articles from 1,943 studies published from January 2019 to
December 2024. The review addresses 3 research questions to present readers
with more understanding of the topic: (1) what are the privacy risks of
releasing explanations in AI systems? (2) what current methods have researchers
employed to achieve privacy preservation in XAI systems? (3) what constitutes a
privacy preserving explanation? Based on the knowledge synthesized from the
selected studies, we categorize the privacy risks and preservation methods in
XAI and propose the characteristics of privacy preserving explanations to aid
researchers and practitioners in understanding the requirements of XAI that is
privacy compliant. Lastly, we identify the challenges in balancing privacy with
other system desiderata and provide recommendations for achieving privacy
preserving XAI. We expect that this review will shed light on the complex
relationship of privacy and explainability, both being the fundamental
principles of Trustworthy AI.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.02798v1">Unifying Laplace Mechanism with Instance Optimality in Differential
  Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B">
  <p><b>Published on:</b> 2025-05-05T17:20:28Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> David Durfee</p>
    <p><b>Summary:</b> We adapt the canonical Laplace mechanism, widely used in differentially
private data analysis, to achieve near instance optimality with respect to the
hardness of the underlying dataset. In particular, we construct a piecewise
Laplace distribution whereby we defy traditional assumptions and show that
Laplace noise can in fact be drawn proportional to the local sensitivity when
done in a piecewise manner. While it may initially seem counterintuitive that
this satisfies (pure) differential privacy and can be sampled, we provide both
through a simple connection to the exponential mechanism and inverse
sensitivity along with the fact that the Laplace distribution is a two-sided
exponential distribution. As a result, we prove that in the continuous setting
our \textit{piecewise Laplace mechanism} strictly dominates the inverse
sensitivity mechanism, which was previously shown to both be nearly instance
optimal and uniformly outperform the smooth sensitivity framework. Furthermore,
in the worst-case where all local sensitivities equal the global sensitivity,
our method simply reduces to a Laplace mechanism. We also complement this with
an approximate local sensitivity variant to potentially ease the computational
cost, which can also extend to higher dimensions.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.02513v1">Trustworthy Inter-Provider Agreements in 6G Using a Privacy-Enabled
  Hybrid Blockchain Framework</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Networking and Internet Architecture-04E762">
  <p><b>Published on:</b> 2025-05-05T09:46:30Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Farhana Javed, Josep Mangues-Bafalluy</p>
    <p><b>Summary:</b> Inter-provider agreements are central to 6G networks, where administrative
domains must securely and dynamically share services. To address the dual need
for transparency and confidentiality, we propose a privacy-enabled hybrid
blockchain setup using Hyperledger Besu, integrating both public and private
transaction workflows. The system enables decentralized service registration,
selection, and SLA breach reporting through role-based smart contracts and
privacy groups. We design and deploy a proof-of-concept implementation,
evaluating performance using end-to-end latency as a key metric within privacy
groups. Results show that public interactions maintain stable latency, while
private transactions incur additional overhead due to off-chain coordination.
The block production rate governed by IBFT 2.0 had limited impact on private
transaction latency, due to encryption and peer synchronization. Lessons
learned highlight design considerations for smart contract structure, validator
management, and scalability patterns suitable for dynamic inter-domain
collaboration. Our findings offer practical insights for deploying trustworthy
agreement systems in 6G networks using privacy-enabled hybrid blockchains.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.02392v1">Moneros Decentralized P2P Exchanges: Functionality, Adoption, and
  Privacy Risks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-05T06:27:37Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yannik Kopyciok, Friedhelm Victor, Stefan Schmid</p>
    <p><b>Summary:</b> Privacy-focused cryptocurrencies like Monero remain popular, despite
increasing regulatory scrutiny that has led to their delisting from major
centralized exchanges. The latter also explains the recent popularity of
decentralized exchanges (DEXs) with no centralized ownership structures. These
platforms typically leverage peer-to-peer (P2P) networks, promising secure and
anonymous asset trading. However, questions of liability remain, and the
academic literature lacks comprehensive insights into the functionality,
trading activity, and privacy claims of these P2P platforms. In this paper, we
provide an early systematization of the current landscape of decentralized
peer-to-peer exchanges within the Monero ecosystem. We examine several recently
developed DEX platforms, analyzing their popularity, functionality,
architectural choices, and potential weaknesses. We further identify and report
on a privacy vulnerability in the recently popularized Haveno exchange,
demonstrating that certain Haveno trades could be detected, allowing
transactions to be linked across the Monero and Bitcoin blockchains. We hope
that our findings can nourish the discussion in the research community about
more secure designs, and provide insights for regulators.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.02383v1">Connecting Thompson Sampling and UCB: Towards More Efficient Trade-offs
  Between Privacy and Regret</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-05-05T05:48:52Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Bingshan Hu, Zhiming Huang, Tianyue H. Zhang, Mathias LÃ©cuyer, Nidhi Hegde</p>
    <p><b>Summary:</b> We address differentially private stochastic bandit problems from the angles
of exploring the deep connections among Thompson Sampling with Gaussian priors,
Gaussian mechanisms, and Gaussian differential privacy (GDP). We propose
DP-TS-UCB, a novel parametrized private bandit algorithm that enables to trade
off privacy and regret. DP-TS-UCB satisfies $ \tilde{O}
\left(T^{0.25(1-\alpha)}\right)$-GDP and enjoys an $O
\left(K\ln^{\alpha+1}(T)/\Delta \right)$ regret bound, where $\alpha \in [0,1]$
controls the trade-off between privacy and regret. Theoretically, our DP-TS-UCB
relies on anti-concentration bounds of Gaussian distributions and links
exploration mechanisms in Thompson Sampling-based algorithms and Upper
Confidence Bound-based algorithms, which may be of independent interest.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.01976v1">A Survey on Privacy Risks and Protection in Large Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-04T03:04:07Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kang Chen, Xiuze Zhou, Yuanguo Lin, Shibo Feng, Li Shen, Pengcheng Wu</p>
    <p><b>Summary:</b> Although Large Language Models (LLMs) have become increasingly integral to
diverse applications, their capabilities raise significant privacy concerns.
This survey offers a comprehensive overview of privacy risks associated with
LLMs and examines current solutions to mitigate these challenges. First, we
analyze privacy leakage and attacks in LLMs, focusing on how these models
unintentionally expose sensitive information through techniques such as model
inversion, training data extraction, and membership inference. We investigate
the mechanisms of privacy leakage, including the unauthorized extraction of
training data and the potential exploitation of these vulnerabilities by
malicious actors. Next, we review existing privacy protection against such
risks, such as inference detection, federated learning, backdoor mitigation,
and confidential computing, and assess their effectiveness in preventing
privacy leakage. Furthermore, we highlight key practical challenges and propose
future research directions to develop secure and privacy-preserving LLMs,
emphasizing privacy risk assessment, secure knowledge transfer between models,
and interdisciplinary frameworks for privacy governance. Ultimately, this
survey aims to establish a roadmap for addressing escalating privacy challenges
in the LLMs domain.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.01879v1">What to Do When Privacy Is Gone</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2025-05-03T17:51:36Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> James Brusseau</p>
    <p><b>Summary:</b> Today's ethics of privacy is largely dedicated to defending personal
information from big data technologies. This essay goes in the other direction.
It considers the struggle to be lost, and explores two strategies for living
after privacy is gone. First, total exposure embraces privacy's decline, and
then contributes to the process with transparency. All personal information is
shared without reservation. The resulting ethics is explored through a big data
version of Robert Nozick's Experience Machine thought experiment. Second,
transient existence responds to privacy's loss by ceaselessly generating new
personal identities, which translates into constantly producing temporarily
unviolated private information. The ethics is explored through Gilles Deleuze's
metaphysics of difference applied in linguistic terms to the formation of the
self. Comparing the exposure and transience alternatives leads to the
conclusion that today's big data reality splits the traditional ethical link
between authenticity and freedom. Exposure provides authenticity, but negates
human freedom. Transience provides freedom, but disdains authenticity.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.01788v1">Privacy Preserving Machine Learning Model Personalization through
  Federated Personalized Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2025-05-03T11:31:38Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Md. Tanzib Hosain, Asif Zaman, Md. Shahriar Sajid, Shadman Sakeeb Khan, Shanjida Akter</p>
    <p><b>Summary:</b> The widespread adoption of Artificial Intelligence (AI) has been driven by
significant advances in intelligent system research. However, this progress has
raised concerns about data privacy, leading to a growing awareness of the need
for privacy-preserving AI. In response, there has been a seismic shift in
interest towards the leading paradigm for training Machine Learning (ML) models
on decentralized data silos while maintaining data privacy, Federated Learning
(FL). This research paper presents a comprehensive performance analysis of a
cutting-edge approach to personalize ML model while preserving privacy achieved
through Privacy Preserving Machine Learning with the innovative framework of
Federated Personalized Learning (PPMLFPL). Regarding the increasing concerns
about data privacy, this study evaluates the effectiveness of PPMLFPL
addressing the critical balance between personalized model refinement and
maintaining the confidentiality of individual user data. According to our
analysis, Adaptive Personalized Cross-Silo Federated Learning with Differential
Privacy (APPLE+DP) offering efficient execution whereas overall, the use of the
Adaptive Personalized Cross-Silo Federated Learning with Homomorphic Encryption
(APPLE+HE) algorithm for privacy-preserving machine learning tasks in federated
personalized learning settings is strongly suggested. The results offer
valuable insights creating it a promising scope for future advancements in the
field of privacy-conscious data-driven technologies.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.01524v1">The DCR Delusion: Measuring the Privacy Risk of Synthetic Data</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-05-02T18:21:14Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zexi Yao, NataÅ¡a KrÄo, Georgi Ganev, Yves-Alexandre de Montjoye</p>
    <p><b>Summary:</b> Synthetic data has become an increasingly popular way to share data without
revealing sensitive information. Though Membership Inference Attacks (MIAs) are
widely considered the gold standard for empirically assessing the privacy of a
synthetic dataset, practitioners and researchers often rely on simpler proxy
metrics such as Distance to Closest Record (DCR). These metrics estimate
privacy by measuring the similarity between the training data and generated
synthetic data. This similarity is also compared against that between the
training data and a disjoint holdout set of real records to construct a binary
privacy test. If the synthetic data is not more similar to the training data
than the holdout set is, it passes the test and is considered private. In this
work we show that, while computationally inexpensive, DCR and other
distance-based metrics fail to identify privacy leakage. Across multiple
datasets and both classical models such as Baynet and CTGAN and more recent
diffusion models, we show that datasets deemed private by proxy metrics are
highly vulnerable to MIAs. We similarly find both the binary privacy test and
the continuous measure based on these metrics to be uninformative of actual
membership inference risk. We further show that these failures are consistent
across different metric hyperparameter settings and record selection methods.
Finally, we argue DCR and other distance-based metrics to be flawed by design
and show a example of a simple leakage they miss in practice. With this work,
we hope to motivate practitioners to move away from proxy metrics to MIAs as
the rigorous, comprehensive standard of evaluating privacy of synthetic data,
in particular to make claims of datasets being legally anonymous.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.01292v1">Fine-grained Manipulation Attacks to Local Differential Privacy
  Protocols for Data Streams</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-02T14:09:56Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xinyu Li, Xuebin Ren, Shusen Yang, Liang Shi, Chia-Mu Yu</p>
    <p><b>Summary:</b> Local Differential Privacy (LDP) enables massive data collection and analysis
while protecting end users' privacy against untrusted aggregators. It has been
applied to various data types (e.g., categorical, numerical, and graph data)
and application settings (e.g., static and streaming). Recent findings indicate
that LDP protocols can be easily disrupted by poisoning or manipulation
attacks, which leverage injected/corrupted fake users to send crafted data
conforming to the LDP reports. However, current attacks primarily target static
protocols, neglecting the security of LDP protocols in the streaming settings.
Our research fills the gap by developing novel fine-grained manipulation
attacks to LDP protocols for data streams. By reviewing the attack surfaces in
existing algorithms, We introduce a unified attack framework with composable
modules, which can manipulate the LDP estimated stream toward a target stream.
Our attack framework can adapt to state-of-the-art streaming LDP algorithms
with different analytic tasks (e.g., frequency and mean) and LDP models
(event-level, user-level, w-event level). We validate our attacks theoretically
and through extensive experiments on real-world datasets, and finally explore a
possible defense mechanism for mitigating these attacks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.00951v1">Preserving Privacy and Utility in LLM-Based Product Recommendations</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Retrieval-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-05-02T01:54:08Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tina Khezresmaeilzadeh, Jiang Zhang, Dimitrios Andreadis, Konstantinos Psounis</p>
    <p><b>Summary:</b> Large Language Model (LLM)-based recommendation systems leverage powerful
language models to generate personalized suggestions by processing user
interactions and preferences. Unlike traditional recommendation systems that
rely on structured data and collaborative filtering, LLM-based models process
textual and contextual information, often using cloud-based infrastructure.
This raises privacy concerns, as user data is transmitted to remote servers,
increasing the risk of exposure and reducing control over personal information.
To address this, we propose a hybrid privacy-preserving recommendation
framework which separates sensitive from nonsensitive data and only shares the
latter with the cloud to harness LLM-powered recommendations. To restore lost
recommendations related to obfuscated sensitive data, we design a
de-obfuscation module that reconstructs sensitive recommendations locally.
Experiments on real-world e-commerce datasets show that our framework achieves
almost the same recommendation utility with a system which shares all data with
an LLM, while preserving privacy to a large extend. Compared to
obfuscation-only techniques, our approach improves HR@10 scores and category
distribution alignment, offering a better balance between privacy and
recommendation quality. Furthermore, our method runs efficiently on
consumer-grade hardware, making privacy-aware LLM-based recommendation systems
practical for real-world use.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.00593v1">A Novel Feature-Aware Chaotic Image Encryption Scheme For Data Security
  and Privacy in IoT and Edge Networks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-01T15:26:48Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Muhammad Shahbaz Khan, Ahmed Al-Dubai, Jawad Ahmad, Nikolaos Pitropakis, Baraq Ghaleb</p>
    <p><b>Summary:</b> The security of image data in the Internet of Things (IoT) and edge networks
is crucial due to the increasing deployment of intelligent systems for
real-time decision-making. Traditional encryption algorithms such as AES and
RSA are computationally expensive for resource-constrained IoT devices and
ineffective for large-volume image data, leading to inefficiencies in
privacy-preserving distributed learning applications. To address these
concerns, this paper proposes a novel Feature-Aware Chaotic Image Encryption
scheme that integrates Feature-Aware Pixel Segmentation (FAPS) with Chaotic
Chain Permutation and Confusion mechanisms to enhance security while
maintaining efficiency. The proposed scheme consists of three stages: (1) FAPS,
which extracts and reorganizes pixels based on high and low edge intensity
features for correlation disruption; (2) Chaotic Chain Permutation, which
employs a logistic chaotic map with SHA-256-based dynamically updated keys for
block-wise permutation; and (3) Chaotic chain Confusion, which utilises
dynamically generated chaotic seed matrices for bitwise XOR operations.
Extensive security and performance evaluations demonstrate that the proposed
scheme significantly reduces pixel correlation -- almost zero, achieves high
entropy values close to 8, and resists differential cryptographic attacks. The
optimum design of the proposed scheme makes it suitable for real-time
deployment in resource-constrained environments.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.00257v1">Graph Privacy: A Heterogeneous Federated GNN for Trans-Border Financial
  Data Circulation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-01T02:47:43Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhizhong Tan, Jiexin Zheng, Kevin Qi Zhang, Wenyong Wang</p>
    <p><b>Summary:</b> The sharing of external data has become a strong demand of financial
institutions, but the privacy issue has led to the difficulty of
interconnecting different platforms and the low degree of data openness. To
effectively solve the privacy problem of financial data in trans-border flow
and sharing, to ensure that the data is available but not visible, to realize
the joint portrait of all kinds of heterogeneous data of business organizations
in different industries, we propose a Heterogeneous Federated Graph Neural
Network (HFGNN) approach. In this method, the distribution of heterogeneous
business data of trans-border organizations is taken as subgraphs, and the
sharing and circulation process among subgraphs is constructed as a
statistically heterogeneous global graph through a central server. Each
subgraph learns the corresponding personalized service model through local
training to select and update the relevant subset of subgraphs with aggregated
parameters, and effectively separates and combines topological and feature
information among subgraphs. Finally, our simulation experimental results show
that the proposed method has higher accuracy performance and faster convergence
speed than existing methods.</p>
  </details>
</div>

