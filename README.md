
<h2>2025-02</h2>

<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.20629v1">Towards Privacy-Preserving Split Learning: Destabilizing Adversarial
  Inference and Reconstruction Attacks in the Cloud</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-02-28T01:24:33Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Griffin Higgins, Roozbeh Razavi-Far, Xichen Zhang, Amir David, Ali Ghorbani, Tongyu Ge</p>
    <p><b>Summary:</b> This work aims to provide both privacy and utility within a split learning
framework while considering both forward attribute inference and backward
reconstruction attacks. To address this, a novel approach has been proposed,
which makes use of class activation maps and autoencoders as a plug-in strategy
aiming to increase the user's privacy and destabilize an adversary. The
proposed approach is compared with a dimensionality-reduction-based plug-in
strategy, which makes use of principal component analysis to transform the
feature map onto a lower-dimensional feature space. Our work shows that our
proposed autoencoder-based approach is preferred as it can provide protection
at an earlier split position over the tested architectures in our setting, and,
hence, better utility for resource-constrained devices in edge-cloud
collaborative inference (EC) systems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.20621v1">EPhishCADE: A Privacy-Aware Multi-Dimensional Framework for Email
  Phishing Campaign Detection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-02-28T00:58:42Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Wei Kang, Nan Wang, Jang Seung, Shuo Wang, Alsharif Abuadbba</p>
    <p><b>Summary:</b> Phishing attacks, typically carried out by email, remain a significant
cybersecurity threat with attackers creating legitimate-looking websites to
deceive recipients into revealing sensitive information or executing harmful
actions. In this paper, we propose {\bf EPhishCADE}, the first {\em
privacy-aware}, {\em multi-dimensional} framework for {\bf E}mail {\bf
Phish}ing {\bf CA}mpaign {\bf DE}tection to automatically identify email
phishing campaigns by clustering seemingly unrelated attacks. Our framework
employs a hierarchical architecture combining a structural layer and a
contextual layer, offering a comprehensive analysis of phishing attacks by
thoroughly examining both structural and contextual elements. Specifically, we
implement a graph-based contextual layer to reveal hidden similarities across
multiple dimensions, including textual, numeric, temporal, and spatial
features, among attacks that may initially appear unrelated. Our framework
streamlines the handling of security threat reports, reducing analysts' fatigue
and workload while enhancing protection against these threats. Another key
feature of our framework lies in its sole reliance on phishing URLs in emails
without the need for private information, including senders, recipients,
content, etc. This feature enables a collaborative identification of phishing
campaigns and attacks among multiple organizations without compromising
privacy. Finally, we benchmark our framework against an established
structure-based study (WWW \textquotesingle 17) to demonstrate its
effectiveness.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.20477v1">HELENE: An Open-Source High-Security Privacy-Preserving Blockchain Based
  System for Automating and Managing Laboratory Health Tests</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2025-02-27T19:28:29Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Gabriel Fernández-Blanco, Pedro García-Cereijo, David Lema-Núñez, Diego Ramil-López, Paula Fraga-Lamas, Leire Egia-Mendikute, Asís Palazón, Tiago M. Fernández-Caramés</p>
    <p><b>Summary:</b> In the last years, especially since the COVID-19 pandemic, precision medicine
platforms emerged as useful tools for supporting new tests like the ones that
detect the presence of antibodies and antigens with better sensitivity and
specificity than traditional methods. In addition, the pandemic has also
influenced the way people interact (decentralization), behave (digital world)
and purchase health services (online). Moreover, there is a growing concern in
the way health data are managed, especially in terms of privacy. To tackle such
issues, this article presents a sustainable direct-to-consumer health-service
open-source platform called HELENE that is supported by blockchain and by a
novel decentralized oracle that protects patient data privacy. Specifically,
HELENE enables health test providers to compete through auctions, allowing
patients to bid for their services and to keep the control over their health
test results. Moreover, data exchanges among the involved stakeholders can be
performed in a trustworthy, transparent and standardized way to ease software
integration and to avoid incompatibilities. After providing a thorough
description of the platform, the proposed health platform is assessed in terms
of smart contract performance. In addition, the response time of the developed
oracle is evaluated and NIST SP 800-22 tests are executed to demonstrate the
adequacy of the devised random number generator. Thus, this article shows the
capabilities and novel propositions of HELENE for delivering health services
providing an open-source platform for future researchers, who can enhance it
and adapt it to their needs.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.19912v1">Model-Free Privacy Preserving Power Flow Analysis in Distribution
  Networks</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36">
  <p><b>Published on:</b> 2025-02-27T09:31:53Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Dong Liu, Juan S. Giraldo, Peter Palensky, Pedro P. Vergara</p>
    <p><b>Summary:</b> Model-free power flow calculation, driven by the rise of smart meter (SM)
data and the lack of network topology, often relies on artificial intelligence
neural networks (ANNs). However, training ANNs require vast amounts of SM data,
posing privacy risks for households in distribution networks. To ensure
customers' privacy during the SM data gathering and online sharing, we
introduce a privacy preserving PF calculation framework, composed of two local
strategies: a local randomisation strategy (LRS) and a local zero-knowledge
proof (ZKP)-based data collection strategy. First, the LRS is used to achieve
irreversible transformation and robust privacy protection for active and
reactive power data, thereby ensuring that personal data remains confidential.
Subsequently, the ZKP-based data collecting strategy is adopted to securely
gather the training dataset for the ANN, enabling SMs to interact with the
distribution system operator without revealing the actual voltage magnitude.
Moreover, to mitigate the accuracy loss induced by the seasonal variations in
load profiles, an incremental learning strategy is incorporated into the online
application. The results across three datasets with varying measurement errors
demonstrate that the proposed framework efficiently collects one month of SM
data within one hour. Furthermore, it robustly maintains mean errors of 0.005
p.u. and 0.014 p.u. under multiple measurement errors and seasonal variations
in load profiles, respectively.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.00062v1">CRFU: Compressive Representation Forgetting Against Privacy Leakage on
  Machine Unlearning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-02-27T05:59:02Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Weiqi Wang, Chenhan Zhang, Zhiyi Tian, Shushu Liu, Shui Yu</p>
    <p><b>Summary:</b> Machine unlearning allows data owners to erase the impact of their specified
data from trained models. Unfortunately, recent studies have shown that
adversaries can recover the erased data, posing serious threats to user
privacy. An effective unlearning method removes the information of the
specified data from the trained model, resulting in different outputs for the
same input before and after unlearning. Adversaries can exploit these output
differences to conduct privacy leakage attacks, such as reconstruction and
membership inference attacks. However, directly applying traditional defenses
to unlearning leads to significant model utility degradation. In this paper, we
introduce a Compressive Representation Forgetting Unlearning scheme (CRFU),
designed to safeguard against privacy leakage on unlearning. CRFU achieves data
erasure by minimizing the mutual information between the trained compressive
representation (learned through information bottleneck theory) and the erased
data, thereby maximizing the distortion of data. This ensures that the model's
output contains less information that adversaries can exploit. Furthermore, we
introduce a remembering constraint and an unlearning rate to balance the
forgetting of erased data with the preservation of previously learned
knowledge, thereby reducing accuracy degradation. Theoretical analysis
demonstrates that CRFU can effectively defend against privacy leakage attacks.
Our experimental results show that CRFU significantly increases the
reconstruction mean square error (MSE), achieving a defense effect improvement
of approximately $200\%$ against privacy reconstruction attacks with only
$1.5\%$ accuracy degradation on MNIST.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.19154v1">Towards Privacy-Preserving Anomaly-Based Intrusion Detection in Energy
  Communities</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-02-26T14:13:02Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zeeshan Afzal, Giovanni Gaggero, Mikael Asplund</p>
    <p><b>Summary:</b> Energy communities consist of decentralized energy production, storage,
consumption, and distribution and are gaining traction in modern power systems.
However, these communities may increase the vulnerability of the grid to cyber
threats. We propose an anomaly-based intrusion detection system to enhance the
security of energy communities. The system leverages deep autoencoders to
detect deviations from normal operational patterns in order to identify
anomalies induced by malicious activities and attacks. Operational data for
training and evaluation are derived from a Simulink model of an energy
community. The results show that the autoencoder-based intrusion detection
system achieves good detection performance across multiple attack scenarios. We
also demonstrate potential for real-world application of the system by training
a federated model that enables distributed intrusion detection while preserving
data privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.19119v1">Chemical knowledge-informed framework for privacy-aware retrosynthesis
  learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-02-26T13:13:24Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Guikun Chen, Xu Zhang, Yi Yang, Wenguan Wang</p>
    <p><b>Summary:</b> Chemical reaction data is a pivotal asset, driving advances in competitive
fields such as pharmaceuticals, materials science, and industrial chemistry.
Its proprietary nature renders it sensitive, as it often includes confidential
insights and competitive advantages organizations strive to protect. However,
in contrast to this need for confidentiality, the current standard training
paradigm for machine learning-based retrosynthesis gathers reaction data from
multiple sources into one single edge to train prediction models. This paradigm
poses considerable privacy risks as it necessitates broad data availability
across organizational boundaries and frequent data transmission between
entities, potentially exposing proprietary information to unauthorized access
or interception during storage and transfer. In the present study, we introduce
the chemical knowledge-informed framework (CKIF), a privacy-preserving approach
for learning retrosynthesis models. CKIF enables distributed training across
multiple chemical organizations without compromising the confidentiality of
proprietary reaction data. Instead of gathering raw reaction data, CKIF learns
retrosynthesis models through iterative, chemical knowledge-informed
aggregation of model parameters. In particular, the chemical properties of
predicted reactants are leveraged to quantitatively assess the observable
behaviors of individual models, which in turn determines the adaptive weights
used for model aggregation. On a variety of reaction datasets, CKIF outperforms
several strong baselines by a clear margin (e.g., ~20% performance improvement
over FedAvg on USPTO-50K), showing its feasibility and superiority to stimulate
further research on privacy-preserving retrosynthesis.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.19082v2">Trust-Enabled Privacy: Social Media Designs to Support Adolescent User
  Boundary Regulation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-02-26T12:19:53Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> JaeWon Kim, Robert Wolfe, Ramya Bhagirathi Subramanian, Mei-Hsuan Lee, Jessica Colnago, Alexis Hiniker</p>
    <p><b>Summary:</b> Through a three-part co-design study involving 19 teens aged 13-18, we
identify key barriers to effective boundary regulation on social media,
including ambiguous audience expectations, social risks associated with
oversharing, and the lack of design affordances that facilitate trust-building.
Our findings reveal that while adolescents seek casual, frequent sharing to
strengthen relationships, existing platform norms and designs often discourage
such interactions, leading to withdrawal. To address these challenges, we
introduce trust-enabled privacy as a design framework that recognizes trust,
whether building or eroding, as central to boundary regulation. When trust is
supported, boundary regulation becomes more adaptive and empowering; when it
erodes, users default to self-censorship or withdrawal. We propose concrete
design affordances, including guided disclosure, contextual audience
segmentation, intentional engagement signaling, and trust-centered norms, to
help platforms foster a more dynamic and nuanced privacy experience for teen
social media users. By reframing privacy as a trust-driven process rather than
a rigid control-based trade-off, this work provides empirical insights and
actionable guidelines for designing social media environments that empower
teens to manage their online presence while fostering meaningful social
connections.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.18974v1">Distributed Transition System with Tags and Value-wise Metric, for
  Privacy Analysis</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Logic in Computer Science-662E9B">
  <p><b>Published on:</b> 2025-02-26T09:35:48Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Siva Anantharaman, Sabine Frittella, Benjamin Nguyen</p>
    <p><b>Summary:</b> We introduce a logical framework named Distributed Labeled Tagged Transition
System (DLTTS), using concepts from Probabilistic Automata, Probabilistic
Concurrent Systems, and Probabilistic labelled transition systems. We show that
DLTTS can be used to formally model how a given piece of private information P
(e.g., a set of tuples) stored in a given database D can get captured
progressively by an adversary A repeatedly querying D, enhancing the knowledge
acquired from the answers to these queries with relational deductions using
certain additional non-private data. The database D is assumed protected with
generalization mechanisms. We also show that, on a large class of databases,
metrics can be defined 'value-wise', and more general notions of adjacency
between data bases can be defined, based on these metrics. These notions can
also play a role in differentially private protection mechanisms.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.18706v1">Differentially Private Federated Learning With Time-Adaptive Privacy
  Spending</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2025-02-25T23:56:23Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shahrzad Kiani, Nupur Kulkarni, Adam Dziedzic, Stark Draper, Franziska Boenisch</p>
    <p><b>Summary:</b> Federated learning (FL) with differential privacy (DP) provides a framework
for collaborative machine learning, enabling clients to train a shared model
while adhering to strict privacy constraints. The framework allows each client
to have an individual privacy guarantee, e.g., by adding different amounts of
noise to each client's model updates. One underlying assumption is that all
clients spend their privacy budgets uniformly over time (learning rounds).
However, it has been shown in the literature that learning in early rounds
typically focuses on more coarse-grained features that can be learned at lower
signal-to-noise ratios while later rounds learn fine-grained features that
benefit from higher signal-to-noise ratios. Building on this intuition, we
propose a time-adaptive DP-FL framework that expends the privacy budget
non-uniformly across both time and clients. Our framework enables each client
to save privacy budget in early rounds so as to be able to spend more in later
rounds when additional accuracy is beneficial in learning more fine-grained
features. We theoretically prove utility improvements in the case that clients
with stricter privacy budgets spend budgets unevenly across rounds, compared to
clients with more relaxed budgets, who have sufficient budgets to distribute
their spend more evenly. Our practical experiments on standard benchmark
datasets support our theoretical results and show that, in practice, our
algorithms improve the privacy-utility trade-offs compared to baseline schemes.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.18697v1">H-FLTN: A Privacy-Preserving Hierarchical Framework for Electric Vehicle
  Spatio-Temporal Charge Prediction</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2025-02-25T23:20:53Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Robert Marlin, Raja Jurdak, Alsharif Abuadbba</p>
    <p><b>Summary:</b> The widespread adoption of Electric Vehicles (EVs) poses critical challenges
for energy providers, particularly in predicting charging time (temporal
prediction), ensuring user privacy, and managing resources efficiently in
mobility-driven networks. This paper introduces the Hierarchical Federated
Learning Transformer Network (H-FLTN) framework to address these challenges.
H-FLTN employs a three-tier hierarchical architecture comprising EVs, community
Distributed Energy Resource Management Systems (DERMS), and the Energy Provider
Data Centre (EPDC) to enable accurate spatio-temporal predictions of EV
charging needs while preserving privacy. Temporal prediction is enhanced using
Transformer-based learning, capturing complex dependencies in charging
behavior. Privacy is ensured through Secure Aggregation, Additive Secret
Sharing, and Peer-to-Peer (P2P) Sharing with Augmentation, which allow only
secret shares of model weights to be exchanged while securing all
transmissions. To improve training efficiency and resource management, H-FLTN
integrates Dynamic Client Capping Mechanism (DCCM) and Client Rotation
Management (CRM), ensuring that training remains both computationally and
temporally efficient as the number of participating EVs increases. DCCM
optimises client participation by limiting excessive computational loads, while
CRM balances training contributions across epochs, preventing imbalanced
participation. Our simulation results based on large-scale empirical vehicle
mobility data reveal that DCCM and CRM reduce the training time complexity with
increasing EVs from linear to constant. Its integration into real-world smart
city infrastructure enhances energy demand forecasting, resource allocation,
and grid stability, ensuring reliability and sustainability in future mobility
ecosystems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.18623v1">On the Privacy-Preserving Properties of Spiking Neural Networks with
  Unique Surrogate Gradients and Quantization Levels</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Neural and Evolutionary Computing-5BC0EB">
  <p><b>Published on:</b> 2025-02-25T20:14:14Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ayana Moshruba, Shay Snyder, Hamed Poursiami, Maryam Parsa</p>
    <p><b>Summary:</b> As machine learning models increasingly process sensitive data, understanding
their vulnerability to privacy attacks is vital. Membership inference attacks
(MIAs) exploit model responses to infer whether specific data points were used
during training, posing a significant privacy risk. Prior research suggests
that spiking neural networks (SNNs), which rely on event-driven computation and
discrete spike-based encoding, exhibit greater resilience to MIAs than
artificial neural networks (ANNs). This resilience stems from their
non-differentiable activations and inherent stochasticity, which obscure the
correlation between model responses and individual training samples. To enhance
privacy in SNNs, we explore two techniques: quantization and surrogate
gradients. Quantization, which reduces precision to limit information leakage,
has improved privacy in ANNs. Given SNNs' sparse and irregular activations,
quantization may further disrupt the activation patterns exploited by MIAs. We
assess the vulnerability of SNNs and ANNs under weight and activation
quantization across multiple datasets, using the attack model's receiver
operating characteristic (ROC) curve area under the curve (AUC) metric, where
lower values indicate stronger privacy, and evaluate the privacy-accuracy
trade-off. Our findings show that quantization enhances privacy in both
architectures with minimal performance loss, though full-precision SNNs remain
more resilient than quantized ANNs. Additionally, we examine the impact of
surrogate gradients on privacy in SNNs. Among five evaluated gradients, spike
rate escape provides the best privacy-accuracy trade-off, while arctangent
increases vulnerability to MIAs. These results reinforce SNNs' inherent privacy
advantages and demonstrate that quantization and surrogate gradient selection
significantly influence privacy-accuracy trade-offs in SNNs.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.18545v1">PII-Bench: Evaluating Query-Aware Privacy Protection Systems</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Retrieval-5BC0EB">
  <p><b>Published on:</b> 2025-02-25T14:49:08Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Hao Shen, Zhouhong Gu, Haokai Hong, Weili Han</p>
    <p><b>Summary:</b> The widespread adoption of Large Language Models (LLMs) has raised
significant privacy concerns regarding the exposure of personally identifiable
information (PII) in user prompts. To address this challenge, we propose a
query-unrelated PII masking strategy and introduce PII-Bench, the first
comprehensive evaluation framework for assessing privacy protection systems.
PII-Bench comprises 2,842 test samples across 55 fine-grained PII categories,
featuring diverse scenarios from single-subject descriptions to complex
multi-party interactions. Each sample is carefully crafted with a user query,
context description, and standard answer indicating query-relevant PII. Our
empirical evaluation reveals that while current models perform adequately in
basic PII detection, they show significant limitations in determining PII query
relevance. Even state-of-the-art LLMs struggle with this task, particularly in
handling complex multi-subject scenarios, indicating substantial room for
improvement in achieving intelligent PII masking.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.18227v1">TLDP: An Algorithm of Local Differential Privacy for Tensors</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-02-25T14:11:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yachao Yuan, Xiao Tang, Yu Huang, Jin Wang</p>
    <p><b>Summary:</b> Tensor-valued data, increasingly common in applications like spatiotemporal
modeling and social networks, pose unique challenges for privacy protection due
to their multidimensional structure and the risk of losing critical structural
information. Traditional local differential privacy (LDP) methods, designed for
scalars and matrices, are insufficient for tensors, as they fail to preserve
essential relationships among tensor elements. We introduce TLDP, a novel
\emph{LDP} algorithm for \emph{T}ensors, which employs a randomized response
mechanism to perturb tensor components while maintaining structural integrity.
To strike a better balance between utility and privacy, we incorporate a weight
matrix that selectively protects sensitive regions. Both theoretical analysis
and empirical findings from real-world datasets show that TLDP achieves
superior utility while preserving privacy, making it a robust solution for
high-dimensional data.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.17772v2">An Improved Privacy and Utility Analysis of Differentially Private SGD
  with Bounded Domain and Smooth Losses</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2025-02-25T02:05:41Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Hao Liang, Wanrong Zhang, Xinlei He, Kaishun Wu, Hong Xing</p>
    <p><b>Summary:</b> Differentially Private Stochastic Gradient Descent (DPSGD) is widely used to
protect sensitive data during the training of machine learning models, but its
privacy guarantees often come at the cost of model performance, largely due to
the inherent challenge of accurately quantifying privacy loss. While recent
efforts have strengthened privacy guarantees by focusing solely on the final
output and bounded domain cases, they still impose restrictive assumptions,
such as convexity and other parameter limitations, and often lack a thorough
analysis of utility. In this paper, we provide rigorous privacy and utility
characterization for DPSGD for smooth loss functions in both bounded and
unbounded domains. We track the privacy loss over multiple iterations by
exploiting the noisy smooth-reduction property and establish the utility
analysis by leveraging the projection's non-expansiveness and clipped SGD
properties. In particular, we show that for DPSGD with a bounded domain, (i)
the privacy loss can still converge without the convexity assumption, and (ii)
a smaller bounded diameter can improve both privacy and utility simultaneously
under certain conditions. Numerical results validate our results.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.17748v1">FinP: Fairness-in-Privacy in Federated Learning by Addressing
  Disparities in Privacy Risk</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-02-25T00:56:47Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tianyu Zhao, Mahmoud Srewa, Salma Elmalaki</p>
    <p><b>Summary:</b> Ensuring fairness in machine learning, particularly in human-centric
applications, extends beyond algorithmic bias to encompass fairness in privacy,
specifically the equitable distribution of privacy risk. This is critical in
federated learning (FL), where decentralized data necessitates balanced privacy
preservation across clients. We introduce FinP, a framework designed to achieve
fairness in privacy by mitigating disproportionate exposure to source inference
attacks (SIA). FinP employs a dual approach: (1) server-side adaptive
aggregation to address unfairness in client contributions in global model, and
(2) client-side regularization to reduce client vulnerability. This
comprehensive strategy targets both the symptoms and root causes of privacy
unfairness. Evaluated on the Human Activity Recognition (HAR) and CIFAR-10
datasets, FinP demonstrates ~20% improvement in fairness in privacy on HAR with
minimal impact on model utility, and effectively mitigates SIA risks on
CIFAR-10, showcasing its ability to provide fairness in privacy in FL systems
without compromising performance.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.18527v2">GOD model: Privacy Preserved AI School for Personal Assistant</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-02-24T20:30:17Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b>  PIN AI Team, Bill Sun, Gavin Guo, Regan Peng, Boliang Zhang, Shouqiao Wang, Laura Florescu, Xi Wang, Davide Crapis, Ben Wu</p>
    <p><b>Summary:</b> Personal AI assistants (e.g., Apple Intelligence, Meta AI) offer proactive
recommendations that simplify everyday tasks, but their reliance on sensitive
user data raises concerns about privacy and trust. To address these challenges,
we introduce the Guardian of Data (GOD), a secure, privacy-preserving framework
for training and evaluating AI assistants directly on-device. Unlike
traditional benchmarks, the GOD model measures how well assistants can
anticipate user needs-such as suggesting gifts-while protecting user data and
autonomy. Functioning like an AI school, it addresses the cold start problem by
simulating user queries and employing a curriculum-based approach to refine the
performance of each assistant. Running within a Trusted Execution Environment
(TEE), it safeguards user data while applying reinforcement and imitation
learning to refine AI recommendations. A token-based incentive system
encourages users to share data securely, creating a data flywheel that drives
continuous improvement. Specifically, users mine with their data, and the
mining rate is determined by GOD's evaluation of how well their AI assistant
understands them across categories such as shopping, social interactions,
productivity, trading, and Web3. By integrating privacy, personalization, and
trust, the GOD model provides a scalable, responsible path for advancing
personal AI assistants. For community collaboration, part of the framework is
open-sourced at https://github.com/PIN-AI/God-Model.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.17591v1">Proactive Privacy Amnesia for Large Language Models: Safeguarding PII
  with Negligible Impact on Model Utility</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-02-24T19:16:39Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Martin Kuo, Jingyang Zhang, Jianyi Zhang, Minxue Tang, Louis DiValentin, Aolin Ding, Jingwei Sun, William Chen, Amin Hass, Tianlong Chen, Yiran Chen, Hai Li</p>
    <p><b>Summary:</b> With the rise of large language models (LLMs), increasing research has
recognized their risk of leaking personally identifiable information (PII)
under malicious attacks. Although efforts have been made to protect PII in
LLMs, existing methods struggle to balance privacy protection with maintaining
model utility. In this paper, inspired by studies of amnesia in cognitive
science, we propose a novel approach, Proactive Privacy Amnesia (PPA), to
safeguard PII in LLMs while preserving their utility. This mechanism works by
actively identifying and forgetting key memories most closely associated with
PII in sequences, followed by a memory implanting using suitable substitute
memories to maintain the LLM's functionality. We conduct evaluations across
multiple models to protect common PII, such as phone numbers and physical
addresses, against prevalent PII-targeted attacks, demonstrating the
superiority of our method compared with other existing defensive techniques.
The results show that our PPA method completely eliminates the risk of phone
number exposure by 100% and significantly reduces the risk of physical address
exposure by 9.8% - 87.6%, all while maintaining comparable model utility
performance.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.17384v1">On the Dichotomy Between Privacy and Traceability in $\ell_p$ Stochastic
  Convex Optimization</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-02-24T18:10:06Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sasha Voitovych, Mahdi Haghifam, Idan Attias, Gintare Karolina Dziugaite, Roi Livni, Daniel M. Roy</p>
    <p><b>Summary:</b> In this paper, we investigate the necessity of memorization in stochastic
convex optimization (SCO) under $\ell_p$ geometries. Informally, we say a
learning algorithm memorizes $m$ samples (or is $m$-traceable) if, by analyzing
its output, it is possible to identify at least $m$ of its training samples.
Our main results uncover a fundamental tradeoff between traceability and excess
risk in SCO. For every $p\in [1,\infty)$, we establish the existence of a risk
threshold below which any sample-efficient learner must memorize a \em{constant
fraction} of its sample. For $p\in [1,2]$, this threshold coincides with best
risk of differentially private (DP) algorithms, i.e., above this threshold,
there are algorithms that do not memorize even a single sample. This
establishes a sharp dichotomy between privacy and traceability for $p \in
[1,2]$. For $p \in (2,\infty)$, this threshold instead gives novel lower bounds
for DP learning, partially closing an open problem in this setup. En route of
proving these results, we introduce a complexity notion we term \em{trace
value} of a problem, which unifies privacy lower bounds and traceability
results, and prove a sparse variant of the fingerprinting lemma.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.17150v1">Differential privacy guarantees of Markov chain Monte Carlo algorithms</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> 
  <p><b>Published on:</b> 2025-02-24T13:40:46Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Andrea Bertazzi, Tim Johnston, Gareth O. Roberts, Alain Durmus</p>
    <p><b>Summary:</b> This paper aims to provide differential privacy (DP) guarantees for Markov
chain Monte Carlo (MCMC) algorithms. In a first part, we establish DP
guarantees on samples output by MCMC algorithms as well as Monte Carlo
estimators associated with these methods under assumptions on the convergence
properties of the underlying Markov chain. In particular, our results highlight
the critical condition of ensuring the target distribution is differentially
private itself. In a second part, we specialise our analysis to the unadjusted
Langevin algorithm and stochastic gradient Langevin dynamics and establish
guarantees on their (R\'enyi) DP. To this end, we develop a novel methodology
based on Girsanov's theorem combined with a perturbation trick to obtain bounds
for an unbounded domain and in a non-convex setting. We establish: (i) uniform
in $n$ privacy guarantees when the state of the chain after $n$ iterations is
released, (ii) bounds on the privacy of the entire chain trajectory. These
findings provide concrete guidelines for privacy-preserving MCMC.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.17041v1">PrivaCI-Bench: Evaluating Privacy with Contextual Integrity and Legal
  Compliance</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-02-24T10:49:34Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Haoran Li, Wenbin Hu, Huihao Jing, Yulin Chen, Qi Hu, Sirui Han, Tianshu Chu, Peizhao Hu, Yangqiu Song</p>
    <p><b>Summary:</b> Recent advancements in generative large language models (LLMs) have enabled
wider applicability, accessibility, and flexibility. However, their reliability
and trustworthiness are still in doubt, especially for concerns regarding
individuals' data privacy. Great efforts have been made on privacy by building
various evaluation benchmarks to study LLMs' privacy awareness and robustness
from their generated outputs to their hidden representations. Unfortunately,
most of these works adopt a narrow formulation of privacy and only investigate
personally identifiable information (PII). In this paper, we follow the merit
of the Contextual Integrity (CI) theory, which posits that privacy evaluation
should not only cover the transmitted attributes but also encompass the whole
relevant social context through private information flows. We present
PrivaCI-Bench, a comprehensive contextual privacy evaluation benchmark targeted
at legal compliance to cover well-annotated privacy and safety regulations,
real court cases, privacy policies, and synthetic data built from the official
toolkit to study LLMs' privacy and safety compliance. We evaluate the latest
LLMs, including the recent reasoner models QwQ-32B and Deepseek R1. Our
experimental results suggest that though LLMs can effectively capture key CI
parameters inside a given context, they still require further advancements for
privacy compliance.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.16877v1">APINT: A Full-Stack Framework for Acceleration of Privacy-Preserving
  Inference of Transformers based on Garbled Circuits</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Hardware Architecture-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-02-24T06:26:42Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Hyunjun Cho, Jaeho Jeon, Jaehoon Heo, Joo-Young Kim</p>
    <p><b>Summary:</b> As the importance of Privacy-Preserving Inference of Transformers (PiT)
increases, a hybrid protocol that integrates Garbled Circuits (GC) and
Homomorphic Encryption (HE) is emerging for its implementation. While this
protocol is preferred for its ability to maintain accuracy, it has a severe
drawback of excessive latency. To address this, existing protocols primarily
focused on reducing HE latency, thus making GC the new latency bottleneck.
Furthermore, previous studies only focused on individual computing layers, such
as protocol or hardware accelerator, lacking a comprehensive solution at the
system level. This paper presents APINT, a full-stack framework designed to
reduce PiT's overall latency by addressing the latency problem of GC through
both software and hardware solutions. APINT features a novel protocol that
reallocates possible GC workloads to alternative methods (i.e., HE or standard
matrix operation), substantially decreasing the GC workload. It also suggests
GC-friendly circuit generation that reduces the number of AND gates at the
most, which is the expensive operator in GC. Furthermore, APINT proposes an
innovative netlist scheduling that combines coarse-grained operation mapping
and fine-grained scheduling for maximal data reuse and minimal dependency.
Finally, APINT's hardware accelerator, combined with its compiler speculation,
effectively resolves the memory stall issue. Putting it all together, APINT
achieves a remarkable end-to-end reduction in latency, outperforming the
existing protocol on CPU platform by 12.2x online and 2.2x offline. Meanwhile,
the APINT accelerator not only reduces its latency by 3.3x but also saves
energy consumption by 4.6x while operating PiT compared to the state-of-the-art
GC accelerator.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.16739v1">Investigating the Security & Privacy Risks from Unsanctioned Technology
  Use by Educators</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2025-02-23T22:52:58Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Easton Kelso, Ananta Soneji, Syed Zami-Ul-Haque Navid, Yan Soshitaishvili, Sazzadur Rahaman, Rakibul Hasan</p>
    <p><b>Summary:</b> Educational technologies are revolutionizing how educational institutions
operate. Consequently, it makes them a lucrative target for breach and abuse as
they often serve as centralized hubs for diverse types of sensitive data, from
academic records to health information. Existing studies looked into how
existing stakeholders perceive the security and privacy risks of educational
technologies and how those risks are affecting institutional policies for
acquiring new technologies. However, outside of institutional vetting and
approval, there is a pervasive practice of using applications and devices
acquired personally. It is unclear how these applications and devices affect
the dynamics of the overall institutional ecosystem.
  This study aims to address this gap by understanding why instructors use
unsanctioned applications, how instructors perceive the associated risks, and
how it affects institutional security and privacy postures. We designed and
conducted an online survey-based study targeting instructors and administrators
from K-12 and higher education institutions.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.16519v2">Guarding the Privacy of Label-Only Access to Neural Network Classifiers
  via iDP Verification</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Programming Languages-D91E36">
  <p><b>Published on:</b> 2025-02-23T09:50:26Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Anan Kabaha, Dana Drachsler-Cohen</p>
    <p><b>Summary:</b> Neural networks are susceptible to privacy attacks that can extract private
information of the training set. To cope, several training algorithms guarantee
differential privacy (DP) by adding noise to their computation. However, DP
requires to add noise considering every possible training set. This leads to a
significant decrease in the network's accuracy. Individual DP (iDP) restricts
DP to a given training set. We observe that some inputs deterministically
satisfy iDP without any noise. By identifying them, we can provide iDP
label-only access to the network with a minor decrease to its accuracy.
However, identifying the inputs that satisfy iDP without any noise is highly
challenging. Our key idea is to compute the iDP deterministic bound (iDP-DB),
which overapproximates the set of inputs that do not satisfy iDP, and add noise
only to their predicted labels. To compute the tightest iDP-DB, which enables
to guard the label-only access with minimal accuracy decrease, we propose
LUCID, which leverages several formal verification techniques. First, it
encodes the problem as a mixed-integer linear program, defined over a network
and over every network trained identically but without a unique data point.
Second, it abstracts a set of networks using a hyper-network. Third, it
eliminates the overapproximation error via a novel branch-and-bound technique.
Fourth, it bounds the differences of matching neurons in the network and the
hyper-network and employs linear relaxation if they are small. We show that
LUCID can provide classifiers with a perfect individuals' privacy guarantee
(0-iDP) -- which is infeasible for DP training algorithms -- with an accuracy
decrease of 1.4%. For more relaxed $\varepsilon$-iDP guarantees, LUCID has an
accuracy decrease of 1.2%. In contrast, existing DP training algorithms reduce
the accuracy by 12.7%.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.18517v1">RewardDS: Privacy-Preserving Fine-Tuning for Large Language Models via
  Reward Driven Data Synthesis</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-02-23T02:52:23Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jianwei Wang, Junyao Yang, Haoran Li, Huiping Zhuang, Cen Chen, Ziqian Zeng</p>
    <p><b>Summary:</b> The success of large language models (LLMs) has attracted many individuals to
fine-tune them for domain-specific tasks by uploading their data. However, in
sensitive areas like healthcare and finance, privacy concerns often arise. One
promising solution is to sample synthetic data with Differential Privacy (DP)
guarantees to replace private data. However, these synthetic data contain
significant flawed data, which are considered as noise. Existing solutions
typically rely on naive filtering by comparing ROUGE-L scores or embedding
similarities, which are ineffective in addressing the noise. To address this
issue, we propose RewardDS, a novel privacy-preserving framework that
fine-tunes a reward proxy model and uses reward signals to guide the synthetic
data generation. Our RewardDS introduces two key modules, Reward Guided
Filtering and Self-Optimizing Refinement, to both filter and refine the
synthetic data, effectively mitigating the noise. Extensive experiments across
medical, financial, and code generation domains demonstrate the effectiveness
of our method.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.18509v1">Protecting Users From Themselves: Safeguarding Contextual Privacy in
  Interactions with Conversational Agents</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-02-22T09:05:39Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ivoline Ngong, Swanand Kadhe, Hao Wang, Keerthiram Murugesan, Justin D. Weisz, Amit Dhurandhar, Karthikeyan Natesan Ramamurthy</p>
    <p><b>Summary:</b> Conversational agents are increasingly woven into individuals' personal
lives, yet users often underestimate the privacy risks involved. The moment
users share information with these agents (e.g., LLMs), their private
information becomes vulnerable to exposure. In this paper, we characterize the
notion of contextual privacy for user interactions with LLMs. It aims to
minimize privacy risks by ensuring that users (sender) disclose only
information that is both relevant and necessary for achieving their intended
goals when interacting with LLMs (untrusted receivers). Through a formative
design user study, we observe how even "privacy-conscious" users inadvertently
reveal sensitive information through indirect disclosures. Based on insights
from this study, we propose a locally-deployable framework that operates
between users and LLMs, and identifies and reformulates out-of-context
information in user prompts. Our evaluation using examples from ShareGPT shows
that lightweight models can effectively implement this framework, achieving
strong gains in contextual privacy while preserving the user's intended
interaction goals through different approaches to classify information relevant
to the intended goals.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.16091v2">Privacy-Aware Joint DNN Model Deployment and Partition Optimization for
  Delay-Efficient Collaborative Edge Inference</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Networking and Internet Architecture-04E762">
  <p><b>Published on:</b> 2025-02-22T05:27:24Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhipeng Cheng, Xiaoyu Xia, Hong Wang, Minghui Liwang, Ning Chen, Xuwei Fan, Xianbin Wang</p>
    <p><b>Summary:</b> Edge inference (EI) is a key solution to address the growing challenges of
delayed response times, limited scalability, and privacy concerns in
cloud-based Deep Neural Network (DNN) inference. However, deploying DNN models
on resource-constrained edge devices faces more severe challenges, such as
model storage limitations, dynamic service requests, and privacy risks. This
paper proposes a novel framework for privacy-aware joint DNN model deployment
and partition optimization to minimize long-term average inference delay under
resource and privacy constraints. Specifically, the problem is formulated as a
complex optimization problem considering model deployment, user-server
association, and model partition strategies. To handle the NP-hardness and
future uncertainties, a Lyapunov-based approach is introduced to transform the
long-term optimization into a single-time-slot problem, ensuring system
performance. Additionally, a coalition formation game model is proposed for
edge server association, and a greedy-based algorithm is developed for model
deployment within each coalition to efficiently solve the problem. Extensive
simulations show that the proposed algorithms effectively reduce inference
delay while satisfying privacy constraints, outperforming baseline approaches
in various scenarios.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.15929v1">Approximate Differential Privacy of the $\ell_2$ Mechanism</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-02-21T20:56:34Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Matthew Joseph, Alex Kulesza, Alexander Yu</p>
    <p><b>Summary:</b> We study the $\ell_2$ mechanism for computing a $d$-dimensional statistic
with bounded $\ell_2$ sensitivity under approximate differential privacy.
Across a range of privacy parameters, we find that the $\ell_2$ mechanism
obtains lower error than the Laplace and Gaussian mechanisms, matching the
former at $d=1$ and approaching the latter as $d \to \infty$.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.15680v1">Privacy Ripple Effects from Adding or Removing Personal Information in
  Language Model Training</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-02-21T18:59:14Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jaydeep Borkar, Matthew Jagielski, Katherine Lee, Niloofar Mireshghallah, David A. Smith, Christopher A. Choquette-Choo</p>
    <p><b>Summary:</b> Due to the sensitive nature of personally identifiable information (PII), its
owners may have the authority to control its inclusion or request its removal
from large-language model (LLM) training. Beyond this, PII may be added or
removed from training datasets due to evolving dataset curation techniques,
because they were newly scraped for retraining, or because they were included
in a new downstream fine-tuning stage. We find that the amount and ease of PII
memorization is a dynamic property of a model that evolves throughout training
pipelines and depends on commonly altered design choices. We characterize three
such novel phenomena: (1) similar-appearing PII seen later in training can
elicit memorization of earlier-seen sequences in what we call assisted
memorization, and this is a significant factor (in our settings, up to 1/3);
(2) adding PII can increase memorization of other PII significantly (in our
settings, as much as $\approx\!7.5\times$); and (3) removing PII can lead to
other PII being memorized. Model creators should consider these first- and
second-order privacy risks when training models to avoid the risk of new PII
regurgitation.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.15567v1">Model Privacy: A Unified Framework to Understand Model Stealing Attacks
  and Defenses</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> 
  <p><b>Published on:</b> 2025-02-21T16:29:11Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ganghua Wang, Yuhong Yang, Jie Ding</p>
    <p><b>Summary:</b> The use of machine learning (ML) has become increasingly prevalent in various
domains, highlighting the importance of understanding and ensuring its safety.
One pressing concern is the vulnerability of ML applications to model stealing
attacks. These attacks involve adversaries attempting to recover a learned
model through limited query-response interactions, such as those found in
cloud-based services or on-chip artificial intelligence interfaces. While
existing literature proposes various attack and defense strategies, these often
lack a theoretical foundation and standardized evaluation criteria. In
response, this work presents a framework called ``Model Privacy'', providing a
foundation for comprehensively analyzing model stealing attacks and defenses.
We establish a rigorous formulation for the threat model and objectives,
propose methods to quantify the goodness of attack and defense strategies, and
analyze the fundamental tradeoffs between utility and privacy in ML models. Our
developed theory offers valuable insights into enhancing the security of ML
models, especially highlighting the importance of the attack-specific structure
of perturbations for effective defenses. We demonstrate the application of
model privacy from the defender's perspective through various learning
scenarios. Extensive experiments corroborate the insights and the effectiveness
of defense mechanisms developed under the proposed framework.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.15233v1">A General Pseudonymization Framework for Cloud-Based LLMs: Replacing
  Privacy Information in Controlled Text Generation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-02-21T06:15:53Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shilong Hou, Ruilin Shang, Zi Long, Xianghua Fu, Yin Chen</p>
    <p><b>Summary:</b> An increasing number of companies have begun providing services that leverage
cloud-based large language models (LLMs), such as ChatGPT. However, this
development raises substantial privacy concerns, as users' prompts are
transmitted to and processed by the model providers. Among the various privacy
protection methods for LLMs, those implemented during the pre-training and
fine-tuning phrases fail to mitigate the privacy risks associated with the
remote use of cloud-based LLMs by users. On the other hand, methods applied
during the inference phrase are primarily effective in scenarios where the
LLM's inference does not rely on privacy-sensitive information. In this paper,
we outline the process of remote user interaction with LLMs and, for the first
time, propose a detailed definition of a general pseudonymization framework
applicable to cloud-based LLMs. The experimental results demonstrate that the
proposed framework strikes an optimal balance between privacy protection and
utility. The code for our method is available to the public at
https://github.com/Mebymeby/Pseudonymization-Framework.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.14780v1">ReVision: A Dataset and Baseline VLM for Privacy-Preserving
  Task-Oriented Visual Instruction Rewriting</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-02-20T18:01:41Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Abhijit Mishra, Richard Noh, Hsiang Fu, Mingda Li, Minji Kim</p>
    <p><b>Summary:</b> Efficient and privacy-preserving multimodal interaction is essential as AR,
VR, and modern smartphones with powerful cameras become primary interfaces for
human-computer communication. Existing powerful large vision-language models
(VLMs) enabling multimodal interaction often rely on cloud-based processing,
raising significant concerns about (1) visual privacy by transmitting sensitive
vision data to servers, and (2) their limited real-time, on-device usability.
This paper explores Visual Instruction Rewriting, a novel approach that
transforms multimodal instructions into text-only commands, allowing seamless
integration of lightweight on-device instruction rewriter VLMs (250M
parameters) with existing conversational AI systems, enhancing vision data
privacy. To achieve this, we present a dataset of over 39,000 examples across
14 domains and develop a compact VLM, pretrained on image captioning datasets
and fine-tuned for instruction rewriting. Experimental results, evaluated
through NLG metrics such as BLEU, METEOR, and ROUGE, along with semantic
parsing analysis, demonstrate that even a quantized version of the model
(<500MB storage footprint) can achieve effective instruction rewriting, thus
enabling privacy-focused, multimodal AI applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.14761v1">User Awareness and Perspectives Survey on Privacy, Security and
  Usability of Auditory Prostheses</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-02-20T17:36:19Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sohini Saha, Leslie M. Collins, Sherri L. Smith, Boyla O. Mainsah</p>
    <p><b>Summary:</b> According to the World Health Organization, over 466 million people worldwide
suffer from disabling hearing loss, with approximately 34 million of these
being children. Hearing aids (HA) and cochlear implants (CI) have become
indispensable tools for restoring hearing and enhancing the quality of life for
individuals with hearing impairments. Clinical research and consumer studies
indicate that users of HAs and CIs report significant improvements in their
daily lives, including enhanced communication abilities and social engagement
and reduced psychological stress. Modern auditory prosthetic devices are more
advanced and interconnected with digital networks to add functionality, such as
streaming audio directly from smartphones and other devices, remote adjustments
by audiologists, integration with smart home systems, and access to artificial
intelligence-driven sound enhancement features. With this interconnectivity,
issues surrounding data privacy and security have become increasingly
pertinent. There is limited research on the usability perceptions of current HA
and CI models from the perspective of end-users. In addition, no studies have
investigated consumer mental models during the purchasing process, particularly
which factors they prioritize when selecting a device. In this study, we
assessed participants' satisfaction levels with various features of their
auditory prostheses. This work contributes to the field by addressing gaps in
user perceptions of HA and CI usability, identifying key factors in consumer
purchasing decisions, and highlighting the need for improved privacy and
security awareness and education among users.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.14309v2">On Theoretical Limits of Learning with Label Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2025-02-20T06:51:42Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Puning Zhao, Chuan Ma, Li Shen, Shaowei Wang, Rongfei Fan</p>
    <p><b>Summary:</b> Label differential privacy (DP) is designed for learning problems involving
private labels and public features. While various methods have been proposed
for learning under label DP, the theoretical limits remain largely unexplored.
In this paper, we investigate the fundamental limits of learning with label DP
in both local and central models for both classification and regression tasks,
characterized by minimax convergence rates. We establish lower bounds by
converting each task into a multiple hypothesis testing problem and bounding
the test error. Additionally, we develop algorithms that yield matching upper
bounds. Our results demonstrate that under label local DP (LDP), the risk has a
significantly faster convergence rate than that under full LDP, i.e. protecting
both features and labels, indicating the advantages of relaxing the DP
definition to focus solely on labels. In contrast, under the label central DP
(CDP), the risk is only reduced by a constant factor compared to full DP,
indicating that the relaxation of CDP only has limited benefits on the
performance.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.14291v1">A Note on Efficient Privacy-Preserving Similarity Search for Encrypted
  Vectors</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-02-20T06:07:04Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Dongfang Zhao</p>
    <p><b>Summary:</b> Traditional approaches to vector similarity search over encrypted data rely
on fully homomorphic encryption (FHE) to enable computation without decryption.
However, the substantial computational overhead of FHE makes it impractical for
large-scale real-time applications. This work explores a more efficient
alternative: using additively homomorphic encryption (AHE) for
privacy-preserving similarity search. We consider scenarios where either the
query vector or the database vectors remain encrypted, a setting that
frequently arises in applications such as confidential recommender systems and
secure federated learning. While AHE only supports addition and scalar
multiplication, we show that it is sufficient to compute inner product
similarity--one of the most widely used similarity measures in vector
retrieval. Compared to FHE-based solutions, our approach significantly reduces
computational overhead by avoiding ciphertext-ciphertext multiplications and
bootstrapping, while still preserving correctness and privacy. We present an
efficient algorithm for encrypted similarity search under AHE and analyze its
error growth and security implications. Our method provides a scalable and
practical solution for privacy-preserving vector search in real-world machine
learning applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.14087v1">Learning from End User Data with Shuffled Differential Privacy over
  Kernel Densities</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B">
  <p><b>Published on:</b> 2025-02-19T20:27:01Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tal Wagner</p>
    <p><b>Summary:</b> We study a setting of collecting and learning from private data distributed
across end users. In the shuffled model of differential privacy, the end users
partially protect their data locally before sharing it, and their data is also
anonymized during its collection to enhance privacy. This model has recently
become a prominent alternative to central DP, which requires full trust in a
central data curator, and local DP, where fully local data protection takes a
steep toll on downstream accuracy.
  Our main technical result is a shuffled DP protocol for privately estimating
the kernel density function of a distributed dataset, with accuracy essentially
matching central DP. We use it to privately learn a classifier from the end
user data, by learning a private density function per class. Moreover, we show
that the density function itself can recover the semantic content of its class,
despite having been learned in the absence of any unprotected data. Our
experiments show the favorable downstream performance of our approach, and
highlight key downstream considerations and trade-offs in a practical ML
deployment of shuffled DP.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.13833v1">Contrastive Learning-Based privacy metrics in Tabular Synthetic Datasets</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-02-19T15:52:23Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Milton Nicolás Plasencia Palacios, Sebastiano Saccani, Gabriele Sgroi, Alexander Boudewijn, Luca Bortolussi</p>
    <p><b>Summary:</b> Synthetic data has garnered attention as a Privacy Enhancing Technology (PET)
in sectors such as healthcare and finance. When using synthetic data in
practical applications, it is important to provide protection guarantees. In
the literature, two family of approaches are proposed for tabular data: on the
one hand, Similarity-based methods aim at finding the level of similarity
between training and synthetic data. Indeed, a privacy breach can occur if the
generated data is consistently too similar or even identical to the train data.
On the other hand, Attack-based methods conduce deliberate attacks on synthetic
datasets. The success rates of these attacks reveal how secure the synthetic
datasets are.
  In this paper, we introduce a contrastive method that improves privacy
assessment of synthetic datasets by embedding the data in a more representative
space. This overcomes obstacles surrounding the multitude of data types and
attributes. It also makes the use of intuitive distance metrics possible for
similarity measurements and as an attack vector. In a series of experiments
with publicly available datasets, we compare the performances of
similarity-based and attack-based methods, both with and without use of the
contrastive learning-based embeddings. Our results show that relatively
efficient, easy to implement privacy metrics can perform equally well as more
advanced metrics explicitly modeling conditions for privacy referred to by the
GDPR.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.14921v1">The Canary's Echo: Auditing Privacy Risks of LLM-Generated Synthetic
  Text</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-02-19T15:30:30Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Matthieu Meeus, Lukas Wutschitz, Santiago Zanella-Béguelin, Shruti Tople, Reza Shokri</p>
    <p><b>Summary:</b> How much information about training samples can be gleaned from synthetic
data generated by Large Language Models (LLMs)? Overlooking the subtleties of
information flow in synthetic data generation pipelines can lead to a false
sense of privacy. In this paper, we design membership inference attacks (MIAs)
that target data used to fine-tune pre-trained LLMs that are then used to
synthesize data, particularly when the adversary does not have access to the
fine-tuned model but only to the synthetic data. We show that such data-based
MIAs do significantly better than a random guess, meaning that synthetic data
leaks information about the training data. Further, we find that canaries
crafted to maximize vulnerability to model-based MIAs are sub-optimal for
privacy auditing when only synthetic data is released. Such out-of-distribution
canaries have limited influence on the model's output when prompted to generate
useful, in-distribution synthetic data, which drastically reduces their
vulnerability. To tackle this problem, we leverage the mechanics of
auto-regressive models to design canaries with an in-distribution prefix and a
high-perplexity suffix that leave detectable traces in synthetic data. This
enhances the power of data-based MIAs and provides a better assessment of the
privacy risks of releasing synthetic data generated by LLMs.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.13564v1">PRIV-QA: Privacy-Preserving Question Answering for Cloud Large Language
  Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-02-19T09:17:07Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Guangwei Li, Yuansen Zhang, Yinggui Wang, Shoumeng Yan, Lei Wang, Tao Wei</p>
    <p><b>Summary:</b> The rapid development of large language models (LLMs) is redefining the
landscape of human-computer interaction, and their integration into various
user-service applications is becoming increasingly prevalent. However,
transmitting user data to cloud-based LLMs presents significant risks of data
breaches and unauthorized access to personal identification information. In
this paper, we propose a privacy preservation pipeline for protecting privacy
and sensitive information during interactions between users and LLMs in
practical LLM usage scenarios. We construct SensitiveQA, the first privacy
open-ended question-answering dataset. It comprises 57k interactions in Chinese
and English, encompassing a diverse range of user-sensitive information within
the conversations. Our proposed solution employs a multi-stage strategy aimed
at preemptively securing user information while simultaneously preserving the
response quality of cloud-based LLMs. Experimental validation underscores our
method's efficacy in balancing privacy protection with maintaining robust
interaction quality. The code and dataset are available at
https://github.com/ligw1998/PRIV-QA.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.13415v1">Indifferential Privacy: A New Paradigm and Its Applications to Optimal
  Matching in Dark Pool Auctions</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-02-19T04:19:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Antigoni Polychroniadou, T. -H. Hubert Chan, Adya Agrawal</p>
    <p><b>Summary:</b> Public exchanges like the New York Stock Exchange and NASDAQ act as
auctioneers in a public double auction system, where buyers submit their
highest bids and sellers offer their lowest asking prices, along with the
number of shares (volume) they wish to trade. The auctioneer matches compatible
orders and executes the trades when a match is found. However, auctioneers
involved in high-volume exchanges, such as dark pools, may not always be
reliable. They could exploit their position by engaging in practices like
front-running or face significant conflicts of interest, i.e., ethical breaches
that have frequently resulted in hefty fines and regulatory scrutiny within the
financial industry.
  Previous solutions, based on the use of fully homomorphic encryption (Asharov
et al., AAMAS 2020), encrypt orders ensuring that information is revealed only
when a match occurs. However, this approach introduces significant
computational overhead, making it impractical for high-frequency trading
environments such as dark pools.
  In this work, we propose a new system based on differential privacy combined
with lightweight encryption, offering an efficient and practical solution that
mitigates the risks of an untrustworthy auctioneer. Specifically, we introduce
a new concept called Indifferential Privacy, which can be of independent
interest, where a user is indifferent to whether certain information is
revealed after some special event, unlike standard differential privacy. For
example, in an auction, it's reasonable to disclose the true volume of a trade
once all of it has been matched. Moreover, our new concept of Indifferential
Privacy allows for maximum matching, which is impossible with conventional
differential privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.13313v1">Revisiting Privacy, Utility, and Efficiency Trade-offs when Fine-Tuning
  Large Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-02-18T22:16:03Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Soumi Das, Camila Kolling, Mohammad Aflah Khan, Mahsa Amani, Bishwamittra Ghosh, Qinyuan Wu, Till Speicher, Krishna P. Gummadi</p>
    <p><b>Summary:</b> We study the inherent trade-offs in minimizing privacy risks and maximizing
utility, while maintaining high computational efficiency, when fine-tuning
large language models (LLMs). A number of recent works in privacy research have
attempted to mitigate privacy risks posed by memorizing fine-tuning data by
using differentially private training methods (e.g., DP), albeit at a
significantly higher computational cost (inefficiency). In parallel, several
works in systems research have focussed on developing (parameter) efficient
fine-tuning methods (e.g., LoRA), but few works, if any, investigated whether
such efficient methods enhance or diminish privacy risks. In this paper, we
investigate this gap and arrive at a surprising conclusion: efficient
fine-tuning methods like LoRA mitigate privacy risks similar to private
fine-tuning methods like DP. Our empirical finding directly contradicts
prevailing wisdom that privacy and efficiency objectives are at odds during
fine-tuning. Our finding is established by (a) carefully defining measures of
privacy and utility that distinguish between memorizing sensitive and
non-sensitive tokens in training and test datasets used in fine-tuning and (b)
extensive evaluations using multiple open-source language models from Pythia,
Gemma, and Llama families and different domain-specific datasets.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.12976v1">Does Training with Synthetic Data Truly Protect Privacy?</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-02-18T15:56:52Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yunpeng Zhao, Jie Zhang</p>
    <p><b>Summary:</b> As synthetic data becomes increasingly popular in machine learning tasks,
numerous methods--without formal differential privacy guarantees--use synthetic
data for training. These methods often claim, either explicitly or implicitly,
to protect the privacy of the original training data. In this work, we explore
four different training paradigms: coreset selection, dataset distillation,
data-free knowledge distillation, and synthetic data generated from diffusion
models. While all these methods utilize synthetic data for training, they lead
to vastly different conclusions regarding privacy preservation. We caution that
empirical approaches to preserving data privacy require careful and rigorous
evaluation; otherwise, they risk providing a false sense of privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.13191v1">On the Privacy Risks of Spiking Neural Networks: A Membership Inference
  Analysis</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-02-18T15:19:20Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Junyi Guan, Abhijith Sharma, Chong Tian, Salem Lahlou</p>
    <p><b>Summary:</b> Spiking Neural Networks (SNNs) are increasingly explored for their energy
efficiency and robustness in real-world applications, yet their privacy risks
remain largely unexamined. In this work, we investigate the susceptibility of
SNNs to Membership Inference Attacks (MIAs) -- a major privacy threat where an
adversary attempts to determine whether a given sample was part of the training
dataset. While prior work suggests that SNNs may offer inherent robustness due
to their discrete, event-driven nature, we find that its resilience diminishes
as latency (T) increases. Furthermore, we introduce an input dropout strategy
under black box setting, that significantly enhances membership inference in
SNNs. Our findings challenge the assumption that SNNs are inherently more
secure, and even though they are expected to be better, our results reveal that
SNNs exhibit privacy vulnerabilities that are equally comparable to Artificial
Neural Networks (ANNs). Our code is available at
https://anonymous.4open.science/r/MIA_SNN-3610.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.17485v1">Decentralized and Robust Privacy-Preserving Model Using
  Blockchain-Enabled Federated Deep Learning in Intelligent Enterprises</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-02-18T15:17:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Reza Fotohi, Fereidoon Shams Aliee, Bahar Farahani</p>
    <p><b>Summary:</b> In Federated Deep Learning (FDL), multiple local enterprises are allowed to
train a model jointly. Then, they submit their local updates to the central
server, and the server aggregates the updates to create a global model.
However, trained models usually perform worse than centralized models,
especially when the training data distribution is non-independent and
identically distributed (nonIID). NonIID data harms the accuracy and
performance of the model. Additionally, due to the centrality of federated
learning (FL) and the untrustworthiness of enterprises, traditional FL
solutions are vulnerable to security and privacy attacks. To tackle this issue,
we propose FedAnil, a secure blockchain enabled Federated Deep Learning Model
that improves enterprise models decentralization, performance, and tamper proof
properties, incorporating two main phases. The first phase addresses the nonIID
challenge (label and feature distribution skew). The second phase addresses
security and privacy concerns against poisoning and inference attacks through
three steps. Extensive experiments were conducted using the Sent140,
FashionMNIST, FEMNIST, and CIFAR10 new real world datasets to evaluate FedAnils
robustness and performance. The simulation results demonstrate that FedAnil
satisfies FDL privacy preserving requirements. In terms of convergence
analysis, the model parameter obtained with FedAnil converges to the optimum of
the model parameter. In addition, it performs better in terms of accuracy (more
than 11, 15, and 24%) and computation overhead (less than 8, 10, and 15%)
compared with baseline approaches, namely ShieldFL, RVPFL, and RFA.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.12658v1">R.R.: Unveiling LLM Training Privacy through Recollection and Ranking</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-02-18T09:05:59Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Wenlong Meng, Zhenyuan Guo, Lenan Wu, Chen Gong, Wenyan Liu, Weixian Li, Chengkun Wei, Wenzhi Chen</p>
    <p><b>Summary:</b> Large Language Models (LLMs) pose significant privacy risks, potentially
leaking training data due to implicit memorization. Existing privacy attacks
primarily focus on membership inference attacks (MIAs) or data extraction
attacks, but reconstructing specific personally identifiable information (PII)
in LLM's training data remains challenging. In this paper, we propose R.R.
(Recollect and Rank), a novel two-step privacy stealing attack that enables
attackers to reconstruct PII entities from scrubbed training data where the PII
entities have been masked. In the first stage, we introduce a prompt paradigm
named recollection, which instructs the LLM to repeat a masked text but fill in
masks. Then we can use PII identifiers to extract recollected PII candidates.
In the second stage, we design a new criterion to score each PII candidate and
rank them. Motivated by membership inference, we leverage the reference model
as a calibration to our criterion. Experiments across three popular PII
datasets demonstrate that the R.R. achieves better PII identical performance
compared to baselines. These results highlight the vulnerability of LLMs to PII
leakage even when training data has been scrubbed. We release the replicate
package of R.R. at a link.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.13172v1">Unveiling Privacy Risks in LLM Agent Memory</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-02-17T19:55:53Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Bo Wang, Weiyi He, Pengfei He, Shenglai Zeng, Zhen Xiang, Yue Xing, Jiliang Tang</p>
    <p><b>Summary:</b> Large Language Model (LLM) agents have become increasingly prevalent across
various real-world applications. They enhance decision-making by storing
private user-agent interactions in the memory module for demonstrations,
introducing new privacy risks for LLM agents. In this work, we systematically
investigate the vulnerability of LLM agents to our proposed Memory EXTRaction
Attack (MEXTRA) under a black-box setting. To extract private information from
memory, we propose an effective attacking prompt design and an automated prompt
generation method based on different levels of knowledge about the LLM agent.
Experiments on two representative agents demonstrate the effectiveness of
MEXTRA. Moreover, we explore key factors influencing memory leakage from both
the agent's and the attacker's perspectives. Our findings highlight the urgent
need for effective memory safeguards in LLM agent design and deployment.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.11682v1">Double Momentum and Error Feedback for Clipping with Fast Rates and
  Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Optimization and Control-F9C80E"> 
  <p><b>Published on:</b> 2025-02-17T11:16:21Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Rustem Islamov, Samuel Horvath, Aurelien Lucchi, Peter Richtarik, Eduard Gorbunov</p>
    <p><b>Summary:</b> Strong Differential Privacy (DP) and Optimization guarantees are two
desirable properties for a method in Federated Learning (FL). However, existing
algorithms do not achieve both properties at once: they either have optimal DP
guarantees but rely on restrictive assumptions such as bounded
gradients/bounded data heterogeneity, or they ensure strong optimization
performance but lack DP guarantees. To address this gap in the literature, we
propose and analyze a new method called Clip21-SGD2M based on a novel
combination of clipping, heavy-ball momentum, and Error Feedback. In
particular, for non-convex smooth distributed problems with clients having
arbitrarily heterogeneous data, we prove that Clip21-SGD2M has optimal
convergence rate and also near optimal (local-)DP neighborhood. Our numerical
experiments on non-convex logistic regression and training of neural networks
highlight the superiority of Clip21-SGD2M over baselines in terms of the
optimization performance for a given DP-budget.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.11658v2">"I'm not for sale" -- Perceptions and limited awareness of privacy risks
  by digital natives about location data</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-02-17T10:49:23Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Antoine Boutet, Victor Morel</p>
    <p><b>Summary:</b> Although mobile devices benefit users in their daily lives in numerous ways,
they also raise several privacy concerns. For instance, they can reveal
sensitive information that can be inferred from location data. This location
data is shared through service providers as well as mobile applications.
Understanding how and with whom users share their location data -- as well as
users' perception of the underlying privacy risks --, are important notions to
grasp in order to design usable privacy-enhancing technologies. In this work,
we perform a quantitative and qualitative analysis of smartphone users'
awareness, perception and self-reported behavior towards location data-sharing
through a survey of n=99 young adult participants (i.e., digital natives). We
compare stated practices with actual behaviors to better understand their
mental models, and survey participants' understanding of privacy risks before
and after the inspection of location traces and the information that can be
inferred therefrom.
  Our empirical results show that participants have risky privacy practices:
about 54% of participants underestimate the number of mobile applications to
which they have granted access to their data, and 33% forget or do not think of
revoking access to their data. Also, by using a demonstrator to perform
inferences from location data, we observe that slightly more than half of
participants (57%) are surprised by the extent of potentially inferred
information, and that 47% intend to reduce access to their data via permissions
as a result of using the demonstrator. Last, a majority of participants have
little knowledge of the tools to better protect themselves, but are nonetheless
willing to follow suggestions to improve privacy (51%). Educating people,
including digital natives, about privacy risks through transparency tools seems
a promising approach.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.11533v1">Be Cautious When Merging Unfamiliar LLMs: A Phishing Model Capable of
  Stealing Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-02-17T08:04:52Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhenyuan Guo, Yi Shi, Wenlong Meng, Chen Gong, Chengkun Wei, Wenzhi Chen</p>
    <p><b>Summary:</b> Model merging is a widespread technology in large language models (LLMs) that
integrates multiple task-specific LLMs into a unified one, enabling the merged
model to inherit the specialized capabilities of these LLMs. Most task-specific
LLMs are sourced from open-source communities and have not undergone rigorous
auditing, potentially imposing risks in model merging. This paper highlights an
overlooked privacy risk: \textit{an unsafe model could compromise the privacy
of other LLMs involved in the model merging.} Specifically, we propose PhiMM, a
privacy attack approach that trains a phishing model capable of stealing
privacy using a crafted privacy phishing instruction dataset. Furthermore, we
introduce a novel model cloaking method that mimics a specialized capability to
conceal attack intent, luring users into merging the phishing model. Once
victims merge the phishing model, the attacker can extract personally
identifiable information (PII) or infer membership information (MI) by querying
the merged model with the phishing instruction. Experimental results show that
merging a phishing model increases the risk of privacy breaches. Compared to
the results before merging, PII leakage increased by 3.9\% and MI leakage
increased by 17.4\% on average. We release the code of PhiMM through a link.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.11163v1">VLMs as GeoGuessr Masters: Exceptional Performance, Hidden Biases, and
  Privacy Risks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-02-16T15:28:34Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jingyuan Huang, Jen-tse Huang, Ziyi Liu, Xiaoyuan Liu, Wenxuan Wang, Jieyu Zhao</p>
    <p><b>Summary:</b> Visual-Language Models (VLMs) have shown remarkable performance across
various tasks, particularly in recognizing geographic information from images.
However, significant challenges remain, including biases and privacy concerns.
To systematically address these issues in the context of geographic information
recognition, we introduce a benchmark dataset consisting of 1,200 images paired
with detailed geographic metadata. Evaluating four VLMs, we find that while
these models demonstrate the ability to recognize geographic information from
images, achieving up to $53.8\%$ accuracy in city prediction, they exhibit
significant regional biases. Specifically, performance is substantially higher
for economically developed and densely populated regions compared to less
developed ($-12.5\%$) and sparsely populated ($-17.0\%$) areas. Moreover, the
models exhibit regional biases, frequently overpredicting certain locations;
for instance, they consistently predict Sydney for images taken in Australia.
The strong performance of VLMs also raises privacy concerns, particularly for
users who share images online without the intent of being identified. Our code
and dataset are publicly available at
https://github.com/uscnlp-lime/FairLocator.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.11009v2">Computing Inconsistency Measures Under Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB">
  <p><b>Published on:</b> 2025-02-16T06:23:11Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shubhankar Mohapatra, Amir Gilad, Xi He, Benny Kimelfeld</p>
    <p><b>Summary:</b> Assessing data quality is crucial to knowing whether and how to use the data
for different purposes. Specifically, given a collection of integrity
constraints, various ways have been proposed to quantify the inconsistency of a
database. Inconsistency measures are particularly important when we wish to
assess the quality of private data without revealing sensitive information. We
study the estimation of inconsistency measures for a database protected under
Differential Privacy (DP). Such estimation is nontrivial since some measures
intrinsically query sensitive information, and the computation of others
involves functions on underlying sensitive data. Among five inconsistency
measures that have been proposed in recent work, we identify that two are
intractable in the DP setting. The major challenge for the other three is high
sensitivity: adding or removing one tuple from the dataset may significantly
affect the outcome. To mitigate that, we model the dataset using a conflict
graph and investigate private graph statistics to estimate these measures. The
proposed machinery includes adapting graph-projection techniques with parameter
selection optimizations on the conflict graph and a DP variant of approximate
vertex cover size. We experimentally show that we can effectively compute DP
estimates of the three measures on five real-world datasets with denial
constraints, where the density of the conflict graphs highly varies.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.10997v1">New Rates in Stochastic Decision-Theoretic Online Learning under
  Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B">
  <p><b>Published on:</b> 2025-02-16T05:13:51Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ruihan Wu, Yu-Xiang Wang</p>
    <p><b>Summary:</b> Hu and Mehta (2024) posed an open problem: what is the optimal
instance-dependent rate for the stochastic decision-theoretic online learning
(with $K$ actions and $T$ rounds) under $\varepsilon$-differential privacy?
Before, the best known upper bound and lower bound are $O\left(\frac{\log
K}{\Delta_{\min}} + \frac{\log K\log T}{\varepsilon}\right)$ and
$\Omega\left(\frac{\log K}{\Delta_{\min}} + \frac{\log K}{\varepsilon}\right)$
(where $\Delta_{\min}$ is the gap between the optimal and the second actions).
In this paper, we partially address this open problem by having two new
results. First, we provide an improved upper bound for this problem
$O\left(\frac{\log K}{\Delta_{\min}} + \frac{\log^2K}{\varepsilon}\right)$,
where the $T$-dependency has been removed. Second, we introduce the
deterministic setting, a weaker setting of this open problem, where the
received loss vector is deterministic and we can focus on the analysis for
$\varepsilon$ regardless of the sampling error. At the deterministic setting,
we prove upper and lower bounds that match at $\Theta\left(\frac{\log
K}{\varepsilon}\right)$, while a direct application of the analysis and
algorithms from the original setting still leads to an extra log factor.
Technically, we introduce the Bernoulli resampling trick, which enforces a
monotonic property for the output from report-noisy-max mechanism that enables
a tighter analysis. Moreover, by replacing the Laplace noise with Gumbel noise,
we derived explicit integral form that gives a tight characterization of the
regret in the deterministic case.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.10801v1">FaceSwapGuard: Safeguarding Facial Privacy from DeepFake Threats through
  Identity Obfuscation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-02-15T13:45:19Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Li Wang, Zheng Li, Xuhong Zhang, Shouling Ji, Shanqing Guo</p>
    <p><b>Summary:</b> DeepFakes pose a significant threat to our society. One representative
DeepFake application is face-swapping, which replaces the identity in a facial
image with that of a victim. Although existing methods partially mitigate these
risks by degrading the quality of swapped images, they often fail to disrupt
the identity transformation effectively. To fill this gap, we propose
FaceSwapGuard (FSG), a novel black-box defense mechanism against deepfake
face-swapping threats. Specifically, FSG introduces imperceptible perturbations
to a user's facial image, disrupting the features extracted by identity
encoders. When shared online, these perturbed images mislead face-swapping
techniques, causing them to generate facial images with identities
significantly different from the original user. Extensive experiments
demonstrate the effectiveness of FSG against multiple face-swapping techniques,
reducing the face match rate from 90\% (without defense) to below 10\%. Both
qualitative and quantitative studies further confirm its ability to confuse
human perception, highlighting its practical utility. Additionally, we
investigate key factors that may influence FSG and evaluate its robustness
against various adaptive adversaries.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.10788v2">Analyzing Privacy Dynamics within Groups using Gamified Auctions</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-02-15T12:48:30Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Hüseyin Aydın, Onuralp Ulusoy, Ilaria Liccardi, Pınar Yolum</p>
    <p><b>Summary:</b> Online shared content, such as group pictures, often contains information
about multiple users. Developing technical solutions to manage the privacy of
such "co-owned" content is challenging because each co-owner may have different
preferences. Recent technical approaches advocate group-decision mechanisms,
including auctions, to decide as how best to resolve these differences.
However, it is not clear if users would participate in such mechanisms and if
they do, whether they would act altruistically. Understanding the privacy
dynamics is crucial to develop effective mechanisms for privacy-respecting
collaborative systems. Accordingly, this work develops RESOLVE, a privacy
auction game to understand the sharing behavior of users in groups. Our results
of users' playing the game show that i) the users' understanding of individual
vs. group privacy differs significantly; ii) often users fight for their
preferences even at the cost of others' privacy; and iii) at times users
collaborate to fight for the privacy of others.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.10701v1">Unpacking the Layers: Exploring Self-Disclosure Norms, Engagement
  Dynamics, and Privacy Implications</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Social and Information Networks-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-02-15T07:15:09Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ehsan-Ul Haq, Shalini Jangra, Suparna De, Nishanth Sastry, Gareth Tyson</p>
    <p><b>Summary:</b> This paper characterizes the self-disclosure behavior of Reddit users across
11 different types of self-disclosure. We find that at least half of the users
share some type of disclosure in at least 10% of their posts, with half of
these posts having more than one type of disclosure. We show that different
types of self-disclosure are likely to receive varying levels of engagement.
For instance, a Sexual Orientation disclosure garners more comments than other
self-disclosures. We also explore confounding factors that affect future
self-disclosure. We show that users who receive interactions from
(self-disclosure) specific subreddit members are more likely to disclose in the
future. We also show that privacy risks due to self-disclosure extend beyond
Reddit users themselves to include their close contacts, such as family and
friends, as their information is also revealed. We develop a browser plugin for
end-users to flag self-disclosure in their content.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.10635v2">Privacy Preservation through Practical Machine Unlearning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-02-15T02:25:27Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Robert Dilworth</p>
    <p><b>Summary:</b> Machine Learning models thrive on vast datasets, continuously adapting to
provide accurate predictions and recommendations. However, in an era dominated
by privacy concerns, Machine Unlearning emerges as a transformative approach,
enabling the selective removal of data from trained models. This paper examines
methods such as Naive Retraining and Exact Unlearning via the SISA framework,
evaluating their Computational Costs, Consistency, and feasibility using the
$\texttt{HSpam14}$ dataset. We explore the potential of integrating unlearning
principles into Positive Unlabeled (PU) Learning to address challenges posed by
partially labeled datasets. Our findings highlight the promise of unlearning
frameworks like $\textit{DaRE}$ for ensuring privacy compliance while
maintaining model performance, albeit with significant computational
trade-offs. This study underscores the importance of Machine Unlearning in
achieving ethical AI and fostering trust in data-driven systems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.10599v1">Federated Learning-Driven Cybersecurity Framework for IoT Networks with
  Privacy-Preserving and Real-Time Threat Detection Capabilities</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Networking and Internet Architecture-04E762">
  <p><b>Published on:</b> 2025-02-14T23:11:51Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Milad Rahmati</p>
    <p><b>Summary:</b> The rapid expansion of the Internet of Things (IoT) ecosystem has transformed
various sectors but has also introduced significant cybersecurity challenges.
Traditional centralized security methods often struggle to balance privacy
preservation and real-time threat detection in IoT networks. To address these
issues, this study proposes a Federated Learning-Driven Cybersecurity Framework
designed specifically for IoT environments. The framework enables decentralized
data processing by training models locally on edge devices, ensuring data
privacy. Secure aggregation of these locally trained models is achieved using
homomorphic encryption, allowing collaborative learning without exposing
sensitive information.
  The proposed framework utilizes recurrent neural networks (RNNs) for anomaly
detection, optimized for resource-constrained IoT networks. Experimental
results demonstrate that the system effectively detects complex cyber threats,
including distributed denial-of-service (DDoS) attacks, with over 98% accuracy.
Additionally, it improves energy efficiency by reducing resource consumption by
20% compared to centralized approaches.
  This research addresses critical gaps in IoT cybersecurity by integrating
federated learning with advanced threat detection techniques. The framework
offers a scalable and privacy-preserving solution adaptable to various IoT
applications. Future work will explore the integration of blockchain for
transparent model aggregation and quantum-resistant cryptographic methods to
further enhance security in evolving technological landscapes.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.09744v1">Fine-Tuning Foundation Models with Federated Learning for Privacy
  Preserving Medical Time Series Forecasting</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-02-13T20:01:15Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mahad Ali, Curtis Lisle, Patrick W. Moore, Tammer Barkouki, Brian J. Kirkwood, Laura J. Brattain</p>
    <p><b>Summary:</b> Federated Learning (FL) provides a decentralized machine learning approach,
where multiple devices or servers collaboratively train a model without sharing
their raw data, thus enabling data privacy. This approach has gained
significant interest in academia and industry due to its privacy-preserving
properties, which are particularly valuable in the medical domain where data
availability is often protected under strict regulations. A relatively
unexplored area is the use of FL to fine-tune Foundation Models (FMs) for time
series forecasting, potentially enhancing model efficacy by overcoming data
limitation while maintaining privacy. In this paper, we fine-tuned time series
FMs with Electrocardiogram (ECG) and Impedance Cardiography (ICG) data using
different FL techniques. We then examined various scenarios and discussed the
challenges FL faces under different data heterogeneity configurations. Our
empirical results demonstrated that while FL can be effective for fine-tuning
FMs on time series forecasting tasks, its benefits depend on the data
distribution across clients. We highlighted the trade-offs in applying FL to FM
fine-tuning.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.09716v1">Genetic Data Governance in Crisis: Policy Recommendations for
  Safeguarding Privacy and Preventing Discrimination</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> 
  <p><b>Published on:</b> 2025-02-13T19:05:10Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Vivek Ramanan, Ria Vinod, Cole Williams, Sohini Ramachandran, Suresh Venkatasubramanian</p>
    <p><b>Summary:</b> Genetic data collection has become ubiquitous today. The ability to
meaningfully interpret genetic data has motivated its widespread use, providing
crucial insights into human health and ancestry while driving important public
health initiatives. Easy access to genetic testing has fueled a rapid expansion
of recreational direct-to-consumer offerings. However, the growth of genetic
datasets and their applications has created significant privacy and
discrimination risks, as our understanding of the scientific basis for genetic
traits continues to evolve. In this paper, we organize the uses of genetic data
along four distinct "pillars": clinical practice, research, forensic and
government use, and recreational use. Using our scientific understanding of
genetics, genetic inference methods and their associated risks, and current
public protections, we build a risk assessment framework that identifies key
values that any governance system must preserve. We analyze case studies using
this framework to assess how well existing regulatory frameworks preserve
desired values. Our investigation reveals critical gaps in these frameworks and
identifies specific threats to privacy and personal liberties, particularly
through genetic discrimination. We propose comprehensive policy reforms to: (1)
update the legal definition of genetic data to protect against modern
technological capabilities, (2) expand the Genetic Information
Nondiscrimination Act (GINA) to cover currently unprotected domains, and (3)
establish a unified regulatory framework under a single governing body to
oversee all applications of genetic data. We conclude with three open questions
about genetic data: the challenges posed by its relational nature, including
consent for relatives and minors; the complexities of international data
transfer; and its potential integration into large language models.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.09001v1">Privacy-Preserving Hybrid Ensemble Model for Network Anomaly Detection:
  Balancing Security and Data Protection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-02-13T06:33:16Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shaobo Liu, Zihao Zhao, Weijie He, Jiren Wang, Jing Peng, Haoyuan Ma</p>
    <p><b>Summary:</b> Privacy-preserving network anomaly detection has become an essential area of
research due to growing concerns over the protection of sensitive data.
Traditional anomaly detection models often prioritize accuracy while neglecting
the critical aspect of privacy. In this work, we propose a hybrid ensemble
model that incorporates privacy-preserving techniques to address both detection
accuracy and data protection. Our model combines the strengths of several
machine learning algorithms, including K-Nearest Neighbors (KNN), Support
Vector Machines (SVM), XGBoost, and Artificial Neural Networks (ANN), to create
a robust system capable of identifying network anomalies while ensuring
privacy. The proposed approach integrates advanced preprocessing techniques
that enhance data quality and address the challenges of small sample sizes and
imbalanced datasets. By embedding privacy measures into the model design, our
solution offers a significant advancement over existing methods, ensuring both
enhanced detection performance and strong privacy safeguards.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.08989v1">RLSA-PFL: Robust Lightweight Secure Aggregation with Model Inconsistency
  Detection in Privacy-Preserving Federated Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">  
  <p><b>Published on:</b> 2025-02-13T06:01:09Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Nazatul H. Sultan, Yan Bo, Yansong Gao, Seyit Camtepe, Arash Mahboubi, Hang Thanh Bui, Aufeef Chauhan, Hamed Aboutorab, Michael Bewong, Praveen Gauravaram, Rafiqul Islam, Sharif Abuadbba</p>
    <p><b>Summary:</b> Federated Learning (FL) allows users to collaboratively train a global
machine learning model by sharing local model only, without exposing their
private data to a central server. This distributed learning is particularly
appealing in scenarios where data privacy is crucial, and it has garnered
substantial attention from both industry and academia. However, studies have
revealed privacy vulnerabilities in FL, where adversaries can potentially infer
sensitive information from the shared model parameters. In this paper, we
present an efficient masking-based secure aggregation scheme utilizing
lightweight cryptographic primitives to mitigate privacy risks. Our scheme
offers several advantages over existing methods. First, it requires only a
single setup phase for the entire FL training session, significantly reducing
communication overhead. Second, it minimizes user-side overhead by eliminating
the need for user-to-user interactions, utilizing an intermediate server layer
and a lightweight key negotiation method. Third, the scheme is highly resilient
to user dropouts, and the users can join at any FL round. Fourth, it can detect
and defend against malicious server activities, including recently discovered
model inconsistency attacks. Finally, our scheme ensures security in both
semi-honest and malicious settings. We provide security analysis to formally
prove the robustness of our approach. Furthermore, we implemented an end-to-end
prototype of our scheme. We conducted comprehensive experiments and
comparisons, which show that it outperforms existing solutions in terms of
communication and computation overhead, functionality, and security.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.08970v1">A Decade of Metric Differential Privacy: Advancements and Applications</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-02-13T05:18:24Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xinpeng Xie, Chenyang Yu, Yan Huang, Yang Cao, Chenxi Qiu</p>
    <p><b>Summary:</b> Metric Differential Privacy (mDP) builds upon the core principles of
Differential Privacy (DP) by incorporating various distance metrics, which
offer adaptable and context-sensitive privacy guarantees for a wide range of
applications, such as location-based services, text analysis, and image
processing. Since its inception in 2013, mDP has garnered substantial research
attention, advancing theoretical foundations, algorithm design, and practical
implementations. Despite this progress, existing surveys mainly focus on
traditional DP and local DP, and they provide limited coverage of mDP. This
paper provides a comprehensive survey of mDP research from 2013 to 2024,
tracing its development from the foundations of DP. We categorize essential
mechanisms, including Laplace, Exponential, and optimization-based approaches,
and assess their strengths, limitations, and application domains. Additionally,
we highlight key challenges and outline future research directions to encourage
innovation and real-world adoption of mDP. This survey is designed to be a
valuable resource for researchers and practitioners aiming to deepen their
understanding and drive progress in mDP within the broader privacy ecosystem.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.08966v2">RTBAS: Defending LLM Agents Against Prompt Injection and Privacy Leakage</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-02-13T05:06:22Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Peter Yong Zhong, Siyuan Chen, Ruiqi Wang, McKenna McCall, Ben L. Titzer, Heather Miller, Phillip B. Gibbons</p>
    <p><b>Summary:</b> Tool-Based Agent Systems (TBAS) allow Language Models (LMs) to use external
tools for tasks beyond their standalone capabilities, such as searching
websites, booking flights, or making financial transactions. However, these
tools greatly increase the risks of prompt injection attacks, where malicious
content hijacks the LM agent to leak confidential data or trigger harmful
actions. Existing defenses (OpenAI GPTs) require user confirmation before every
tool call, placing onerous burdens on users. We introduce Robust TBAS (RTBAS),
which automatically detects and executes tool calls that preserve integrity and
confidentiality, requiring user confirmation only when these safeguards cannot
be ensured. RTBAS adapts Information Flow Control to the unique challenges
presented by TBAS. We present two novel dependency screeners, using
LM-as-a-judge and attention-based saliency, to overcome these challenges.
Experimental results on the AgentDojo Prompt Injection benchmark show RTBAS
prevents all targeted attacks with only a 2% loss of task utility when under
attack, and further tests confirm its ability to obtain near-oracle performance
on detecting both subtle and direct privacy leaks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.08202v1">Privacy amplification by random allocation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-02-12T08:32:10Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Vitaly Feldman, Moshe Shenfeld</p>
    <p><b>Summary:</b> We consider the privacy guarantees of an algorithm in which a user's data is
used in $k$ steps randomly and uniformly chosen from a sequence (or set) of $t$
differentially private steps. We demonstrate that the privacy guarantees of
this sampling scheme can be upper bound by the privacy guarantees of the
well-studied independent (or Poisson) subsampling in which each step uses the
user's data with probability $(1+ o(1))k/t $. Further, we provide two
additional analysis techniques that lead to numerical improvements in some
parameter regimes. The case of $k=1$ has been previously studied in the context
of DP-SGD in Balle et al. (2020) and very recently in Chua et al. (2024).
Privacy analysis of Balle et al. (2020) relies on privacy amplification by
shuffling which leads to overly conservative bounds. Privacy analysis of Chua
et al. (2024a) relies on Monte Carlo simulations that are computationally
prohibitive in many practical scenarios and have additional inherent
limitations.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.08151v1">Local Differential Privacy is Not Enough: A Sample Reconstruction Attack
  against Federated Learning with Local Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-02-12T06:37:26Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhichao You, Xuewen Dong, Shujun Li, Ximeng Liu, Siqi Ma, Yulong Shen</p>
    <p><b>Summary:</b> Reconstruction attacks against federated learning (FL) aim to reconstruct
users' samples through users' uploaded gradients. Local differential privacy
(LDP) is regarded as an effective defense against various attacks, including
sample reconstruction in FL, where gradients are clipped and perturbed.
Existing attacks are ineffective in FL with LDP since clipped and perturbed
gradients obliterate most sample information for reconstruction. Besides,
existing attacks embed additional sample information into gradients to improve
the attack effect and cause gradient expansion, leading to a more severe
gradient clipping in FL with LDP. In this paper, we propose a sample
reconstruction attack against LDP-based FL with any target models to
reconstruct victims' sensitive samples to illustrate that FL with LDP is not
flawless. Considering gradient expansion in reconstruction attacks and noise in
LDP, the core of the proposed attack is gradient compression and reconstructed
sample denoising. For gradient compression, an inference structure based on
sample characteristics is presented to reduce redundant gradients against LDP.
For reconstructed sample denoising, we artificially introduce zero gradients to
observe noise distribution and scale confidence interval to filter the noise.
Theoretical proof guarantees the effectiveness of the proposed attack.
Evaluations show that the proposed attack is the only attack that reconstructs
victims' training samples in LDP-based FL and has little impact on the target
model's accuracy. We conclude that LDP-based FL needs further improvements to
defend against sample reconstruction attacks effectively.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.08008v2">An Interactive Framework for Implementing Privacy-Preserving Federated
  Learning: Experiments on Large Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-02-11T23:07:14Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kasra Ahmadi, Rouzbeh Behnia, Reza Ebrahimi, Mehran Mozaffari Kermani, Jeremiah Birrell, Jason Pacheco, Attila A Yavuz</p>
    <p><b>Summary:</b> Federated learning (FL) enhances privacy by keeping user data on local
devices. However, emerging attacks have demonstrated that the updates shared by
users during training can reveal significant information about their data. This
has greatly thwart the adoption of FL methods for training robust AI models in
sensitive applications. Differential Privacy (DP) is considered the gold
standard for safeguarding user data. However, DP guarantees are highly
conservative, providing worst-case privacy guarantees. This can result in
overestimating privacy needs, which may compromise the model's accuracy.
Additionally, interpretations of these privacy guarantees have proven to be
challenging in different contexts. This is further exacerbated when other
factors, such as the number of training iterations, data distribution, and
specific application requirements, can add further complexity to this problem.
In this work, we proposed a framework that integrates a human entity as a
privacy practitioner to determine an optimal trade-off between the model's
privacy and utility. Our framework is the first to address the variable memory
requirement of existing DP methods in FL settings, where resource-limited
devices (e.g., cell phones) can participate. To support such settings, we adopt
a recent DP method with fixed memory usage to ensure scalable private FL. We
evaluated our proposed framework by fine-tuning a BERT-based LLM model using
the GLUE dataset (a common approach in literature), leveraging the new
accountant, and employing diverse data partitioning strategies to mimic
real-world conditions. As a result, we achieved stable memory usage, with an
average accuracy reduction of 1.33% for $\epsilon = 10$ and 1.9% for $\epsilon
= 6$, when compared to the state-of-the-art DP accountant which does not
support fixed memory usage.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.08001v1">Unveiling Client Privacy Leakage from Public Dataset Usage in Federated
  Distillation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-02-11T22:48:49Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Haonan Shi, Tu Ouyang, An Wang</p>
    <p><b>Summary:</b> Federated Distillation (FD) has emerged as a popular federated training
framework, enabling clients to collaboratively train models without sharing
private data. Public Dataset-Assisted Federated Distillation (PDA-FD), which
leverages public datasets for knowledge sharing, has become widely adopted.
Although PDA-FD enhances privacy compared to traditional Federated Learning, we
demonstrate that the use of public datasets still poses significant privacy
risks to clients' private training data. This paper presents the first
comprehensive privacy analysis of PDA-FD in presence of an honest-but-curious
server. We show that the server can exploit clients' inference results on
public datasets to extract two critical types of private information: label
distributions and membership information of the private training dataset. To
quantify these vulnerabilities, we introduce two novel attacks specifically
designed for the PDA-FD setting: a label distribution inference attack and
innovative membership inference methods based on Likelihood Ratio Attack
(LiRA). Through extensive evaluation of three representative PDA-FD frameworks
(FedMD, DS-FL, and Cronus), our attacks achieve state-of-the-art performance,
with label distribution attacks reaching minimal KL-divergence and membership
inference attacks maintaining high True Positive Rates under low False Positive
Rate constraints. Our findings reveal significant privacy risks in current
PDA-FD frameworks and emphasize the need for more robust privacy protection
mechanisms in collaborative learning systems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.10450v1">Trustworthy AI on Safety, Bias, and Privacy: A Survey</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-02-11T20:08:42Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xingli Fang, Jianwei Li, Varun Mulchandani, Jung-Eun Kim</p>
    <p><b>Summary:</b> The capabilities of artificial intelligence systems have been advancing to a
great extent, but these systems still struggle with failure modes,
vulnerabilities, and biases. In this paper, we study the current state of the
field, and present promising insights and perspectives regarding concerns that
challenge the trustworthiness of AI models. In particular, this paper
investigates the issues regarding three thrusts: safety, privacy, and bias,
which hurt models' trustworthiness. For safety, we discuss safety alignment in
the context of large language models, preventing them from generating toxic or
harmful content. For bias, we focus on spurious biases that can mislead a
network. Lastly, for privacy, we cover membership inference attacks in deep
neural networks. The discussions addressed in this paper reflect our own
experiments and observations.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.07693v2">SoK: A Classification for AI-driven Personalized Privacy Assistants</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-02-11T16:46:56Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Victor Morel, Leonardo Iwaya, Simone Fischer-Hübner</p>
    <p><b>Summary:</b> To help users make privacy-related decisions, personalized privacy assistants
based on AI technology have been developed in recent years. These AI-driven
Personalized Privacy Assistants (AI-driven PPAs) can reap significant benefits
for users, who may otherwise struggle to make decisions regarding their
personal data in environments saturated with privacy-related decision requests.
However, no study systematically inquired about the features of these AI-driven
PPAs, their underlying technologies, or the accuracy of their decisions. To
fill this gap, we present a Systematization of Knowledge (SoK) to map the
existing solutions found in the scientific literature. We screened 1697 unique
research papers over the last decade (2013-2023), constructing a classification
from 39 included papers. As a result, this SoK reviews several aspects of
existing research on AI-driven PPAs in terms of types of publications,
contributions, methodological quality, and other quantitative insights.
Furthermore, we provide a comprehensive classification for AI-driven PPAs,
delving into their architectural choices, system contexts, types of AI used,
data sources, types of decisions, and control over decisions, among other
facets. Based on our SoK, we further underline the research gaps and challenges
and formulate recommendations for the design and development of AI-driven PPAs
as well as avenues for future research.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.06652v1">Transparent NLP: Using RAG and LLM Alignment for Privacy Q&A</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-02-10T16:42:00Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Anna Leschanowsky, Zahra Kolagar, Erion Çano, Ivan Habernal, Dara Hallinan, Emanuël A. P. Habets, Birgit Popp</p>
    <p><b>Summary:</b> The transparency principle of the General Data Protection Regulation (GDPR)
requires data processing information to be clear, precise, and accessible.
While language models show promise in this context, their probabilistic nature
complicates truthfulness and comprehensibility.
  This paper examines state-of-the-art Retrieval Augmented Generation (RAG)
systems enhanced with alignment techniques to fulfill GDPR obligations. We
evaluate RAG systems incorporating an alignment module like Rewindable
Auto-regressive Inference (RAIN) and our proposed multidimensional extension,
MultiRAIN, using a Privacy Q&A dataset. Responses are optimized for preciseness
and comprehensibility and are assessed through 21 metrics, including
deterministic and large language model-based evaluations.
  Our results show that RAG systems with an alignment module outperform
baseline RAG systems on most metrics, though none fully match human answers.
Principal component analysis of the results reveals complex interactions
between metrics, highlighting the need to refine metrics. This study provides a
foundation for integrating advanced natural language processing systems into
legal compliance frameworks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.06597v1">Continual Release Moment Estimation with Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> 
  <p><b>Published on:</b> 2025-02-10T15:58:26Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Nikita P. Kalinin, Jalaj Upadhyay, Christoph H. Lampert</p>
    <p><b>Summary:</b> We propose Joint Moment Estimation (JME), a method for continually and
privately estimating both the first and second moments of data with reduced
noise compared to naive approaches. JME uses the matrix mechanism and a joint
sensitivity analysis to allow the second moment estimation with no additional
privacy cost, thereby improving accuracy while maintaining privacy. We
demonstrate JME's effectiveness in two applications: estimating the running
mean and covariance matrix for Gaussian density estimation, and model training
with DP-Adam on CIFAR-10.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.06425v1">Generating Privacy-Preserving Personalized Advice with Zero-Knowledge
  Proofs and LLMs</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-02-10T13:02:00Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Hiroki Watanabe, Motonobu Uchikoshi</p>
    <p><b>Summary:</b> Large language models (LLMs) are increasingly utilized in domains such as
finance, healthcare, and interpersonal relationships to provide advice tailored
to user traits and contexts. However, this personalization often relies on
sensitive data, raising critical privacy concerns and necessitating data
minimization. To address these challenges, we propose a framework that
integrates zero-knowledge proof (ZKP) technology, specifically zkVM, with
LLM-based chatbots. This integration enables privacy-preserving data sharing by
verifying user traits without disclosing sensitive information. Our research
introduces both an architecture and a prompting strategy for this approach.
Through empirical evaluation, we clarify the current constraints and
performance limitations of both zkVM and the proposed prompting strategy,
thereby demonstrating their practical feasibility in real-world scenarios.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.05765v2">Privacy-Preserving Dataset Combination</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2025-02-09T03:54:17Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Keren Fuentes, Mimee Xu, Irene Chen</p>
    <p><b>Summary:</b> Access to diverse, high-quality datasets is crucial for machine learning
model performance, yet data sharing remains limited by privacy concerns and
competitive interests, particularly in regulated domains like healthcare. This
dynamic especially disadvantages smaller organizations that lack resources to
purchase data or negotiate favorable sharing agreements. We present SecureKL, a
privacy-preserving framework that enables organizations to identify beneficial
data partnerships without exposing sensitive information. Building on recent
advances in dataset combination methods, we develop a secure multiparty
computation protocol that maintains strong privacy guarantees while achieving
>90\% correlation with plaintext evaluations. In experiments with real-world
hospital data, SecureKL successfully identifies beneficial data partnerships
that improve model performance for intensive care unit mortality prediction
while preserving data privacy. Our framework provides a practical solution for
organizations seeking to leverage collective data resources while maintaining
privacy and competitive advantages. These results demonstrate the potential for
privacy-preserving data collaboration to advance machine learning applications
in high-stakes domains while promoting more equitable access to data resources.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.05547v1">Dual Defense: Enhancing Privacy and Mitigating Poisoning Attacks in
  Federated Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-02-08T12:28:20Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Runhua Xu, Shiqi Gao, Chao Li, James Joshi, Jianxin Li</p>
    <p><b>Summary:</b> Federated learning (FL) is inherently susceptible to privacy breaches and
poisoning attacks. To tackle these challenges, researchers have separately
devised secure aggregation mechanisms to protect data privacy and robust
aggregation methods that withstand poisoning attacks. However, simultaneously
addressing both concerns is challenging; secure aggregation facilitates
poisoning attacks as most anomaly detection techniques require access to
unencrypted local model updates, which are obscured by secure aggregation. Few
recent efforts to simultaneously tackle both challenges offen depend on
impractical assumption of non-colluding two-server setups that disrupt FL's
topology, or three-party computation which introduces scalability issues,
complicating deployment and application. To overcome this dilemma, this paper
introduce a Dual Defense Federated learning (DDFed) framework. DDFed
simultaneously boosts privacy protection and mitigates poisoning attacks,
without introducing new participant roles or disrupting the existing FL
topology. DDFed initially leverages cutting-edge fully homomorphic encryption
(FHE) to securely aggregate model updates, without the impractical requirement
for non-colluding two-server setups and ensures strong privacy protection.
Additionally, we proposes a unique two-phase anomaly detection mechanism for
encrypted model updates, featuring secure similarity computation and
feedback-driven collaborative selection, with additional measures to prevent
potential privacy breaches from Byzantine clients incorporated into the
detection process. We conducted extensive experiments on various model
poisoning attacks and FL scenarios, including both cross-device and cross-silo
FL. Experiments on publicly available datasets demonstrate that DDFed
successfully protects model privacy and effectively defends against model
poisoning threats.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.05516v1">Evaluating Differential Privacy on Correlated Datasets Using Pointwise
  Maximal Leakage</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2025-02-08T10:30:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sara Saeidian, Tobias J. Oechtering, Mikael Skoglund</p>
    <p><b>Summary:</b> Data-driven advancements significantly contribute to societal progress, yet
they also pose substantial risks to privacy. In this landscape, differential
privacy (DP) has become a cornerstone in privacy preservation efforts. However,
the adequacy of DP in scenarios involving correlated datasets has sometimes
been questioned and multiple studies have hinted at potential vulnerabilities.
In this work, we delve into the nuances of applying DP to correlated datasets
by leveraging the concept of pointwise maximal leakage (PML) for a quantitative
assessment of information leakage. Our investigation reveals that DP's
guarantees can be arbitrarily weak for correlated databases when assessed
through the lens of PML. More precisely, we prove the existence of a pure DP
mechanism with PML levels arbitrarily close to that of a mechanism which
releases individual entries from a database without any perturbation. By
shedding light on the limitations of DP on correlated datasets, our work aims
to foster a deeper understanding of subtle privacy risks and highlight the need
for the development of more effective privacy-preserving mechanisms tailored to
diverse scenarios.</p>
  </details>
</div>



<h2>2025-03</h2>

<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.03652v1">Token-Level Privacy in Large Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-05T16:27:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Re'em Harel, Niv Gilboa, Yuval Pinter</p>
    <p><b>Summary:</b> The use of language models as remote services requires transmitting private
information to external providers, raising significant privacy concerns. This
process not only risks exposing sensitive data to untrusted service providers
but also leaves it vulnerable to interception by eavesdroppers. Existing
privacy-preserving methods for natural language processing (NLP) interactions
primarily rely on semantic similarity, overlooking the role of contextual
information. In this work, we introduce dchi-stencil, a novel token-level
privacy-preserving mechanism that integrates contextual and semantic
information while ensuring strong privacy guarantees under the dchi
differential privacy framework, achieving 2epsilon-dchi-privacy. By
incorporating both semantic and contextual nuances, dchi-stencil achieves a
robust balance between privacy and utility. We evaluate dchi-stencil using
state-of-the-art language models and diverse datasets, achieving comparable and
even better trade-off between utility and privacy compared to existing methods.
This work highlights the potential of dchi-stencil to set a new standard for
privacy-preserving NLP in modern, high-risk applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.03587v1">"You don't need a university degree to comprehend data protection this
  way": LLM-Powered Interactive Privacy Policy Assessment</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-03-05T15:22:35Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Vincent Freiberger, Arthur Fleig, Erik Buchmann</p>
    <p><b>Summary:</b> Protecting online privacy requires users to engage with and comprehend
website privacy policies, but many policies are difficult and tedious to read.
We present the first qualitative user study on Large Language Model
(LLM)-driven privacy policy assessment. To this end, we build and evaluate an
LLM-based privacy policy assessment browser extension, which helps users
understand the essence of a lengthy, complex privacy policy while browsing. The
tool integrates a dashboard and an LLM chat. In our qualitative user study
(N=22), we evaluate usability, understandability of the information our tool
provides, and its impacts on awareness. While providing a comprehensible quick
overview and a chat for in-depth discussion improves privacy awareness, users
note issues with building trust in the tool. From our insights, we derive
important design implications to guide future policy analysis tools.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.03539v1">Data Sharing, Privacy and Security Considerations in the Energy Sector:
  A Review from Technical Landscape to Regulatory Specifications</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-05T14:23:56Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shiliang Zhang, Sabita Maharjan, Lee Andrew Bygrave, Shui Yu</p>
    <p><b>Summary:</b> Decarbonization, decentralization and digitalization are the three key
elements driving the twin energy transition. The energy system is evolving to a
more data driven ecosystem, leading to the need of communication and storage of
large amount of data of different resolution from the prosumers and other
stakeholders in the energy ecosystem. While the energy system is certainly
advancing, this paradigm shift is bringing in new privacy and security issues
related to collection, processing and storage of data - not only from the
technical dimension, but also from the regulatory perspective. Understanding
data privacy and security in the evolving energy system, regarding regulatory
compliance, is an immature field of research. Contextualized knowledge of how
related issues are regulated is still in its infancy, and the practical and
technical basis for the regulatory framework for data privacy and security is
not clear. To fill this gap, this paper conducts a comprehensive review of the
data-related issues for the energy system by integrating both technical and
regulatory dimensions. We start by reviewing open-access data, data
communication and data-processing techniques for the energy system, and use it
as the basis to connect the analysis of data-related issues from the integrated
perspective. We classify the issues into three categories: (i) data-sharing
among energy end users and stakeholders (ii) privacy of end users, and (iii)
cyber security, and then explore these issues from a regulatory perspective. We
analyze the evolution of related regulations, and introduce the relevant
regulatory initiatives for the categorized issues in terms of regulatory
definitions, concepts, principles, rights and obligations in the context of
energy systems. Finally, we provide reflections on the gaps that still exist,
and guidelines for regulatory frameworks for a truly participatory energy
system.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.03506v1">Rethinking Synthetic Data definitions: A privacy driven approach</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-03-05T13:54:13Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Vibeke Binz Vallevik, Serena Elizabeth Marshall, Aleksandar Babic, Jan Franz Nygaard</p>
    <p><b>Summary:</b> Synthetic data is gaining traction as a cost-effective solution for the
increasing data demands of AI development and can be generated either from
existing knowledge or derived data captured from real-world events. The source
of the synthetic data generation and the technique used significantly impacts
its residual privacy risk and therefore its opportunity for sharing.
Traditional classification of synthetic data types no longer fit the newer
generation techniques and there is a need to better align the classification
with practical needs. We suggest a new way of grouping synthetic data types
that better supports privacy evaluations to aid regulatory policymaking. Our
novel classification provides flexibility to new advancements like deep
generative methods and offers a more practical framework for future
applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.03428v1">Privacy is All You Need: Revolutionizing Wearable Health Data with
  Advanced PETs</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Emerging Technologies-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-03-05T12:01:22Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Karthik Barma, Seshu Babu Barma</p>
    <p><b>Summary:</b> In a world where data is the new currency, wearable health devices offer
unprecedented insights into daily life, continuously monitoring vital signs and
metrics. However, this convenience raises privacy concerns, as these devices
collect sensitive data that can be misused or breached. Traditional measures
often fail due to real-time data processing needs and limited device power.
Users also lack awareness and control over data sharing and usage. We propose a
Privacy-Enhancing Technology (PET) framework for wearable devices, integrating
federated learning, lightweight cryptographic methods, and selectively deployed
blockchain technology. The blockchain acts as a secure ledger triggered only
upon data transfer requests, granting users real-time notifications and
control. By dismantling data monopolies, this approach returns data sovereignty
to individuals. Through real-world applications like secure medical data
sharing, privacy-preserving fitness tracking, and continuous health monitoring,
our framework reduces privacy risks by up to 70 percent while preserving data
utility and performance. This innovation sets a new benchmark for wearable
privacy and can scale to broader IoT ecosystems, including smart homes and
industry. As data continues to shape our digital landscape, our research
underscores the critical need to maintain privacy and user control at the
forefront of technological progress.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.03267v1">Quantum-Inspired Privacy-Preserving Federated Learning Framework for
  Secure Dementia Classification</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2025-03-05T08:49:31Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Gazi Tanbhir, Md. Farhan Shahriyar</p>
    <p><b>Summary:</b> Dementia, a neurological disorder impacting millions globally, presents
significant challenges in diagnosis and patient care. With the rise of privacy
concerns and security threats in healthcare, federated learning (FL) has
emerged as a promising approach to enable collaborative model training across
decentralized datasets without exposing sensitive patient information. However,
FL remains vulnerable to advanced security breaches such as gradient inversion
and eavesdropping attacks. This paper introduces a novel framework that
integrates federated learning with quantum-inspired encryption techniques for
dementia classification, emphasizing privacy preservation and security.
Leveraging quantum key distribution (QKD), the framework ensures secure
transmission of model weights, protecting against unauthorized access and
interception during training. The methodology utilizes a convolutional neural
network (CNN) for dementia classification, with federated training conducted
across distributed healthcare nodes, incorporating QKD-encrypted weight sharing
to secure the aggregation process. Experimental evaluations conducted on MRI
data from the OASIS dataset demonstrate that the proposed framework achieves
identical accuracy levels to a baseline model while enhancing data security and
reducing loss by almost 1% compared to the classical baseline model. The
framework offers significant implications for democratizing access to AI-driven
dementia diagnostics in low- and middle-income countries, addressing critical
resource and privacy constraints. This work contributes a robust, scalable, and
secure federated learning solution for healthcare applications, paving the way
for broader adoption of quantum-inspired techniques in AI-driven medical
research.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.03146v1">PriFFT: Privacy-preserving Federated Fine-tuning of Large Language
  Models via Function Secret Sharing</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-05T03:41:57Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhichao You, Xuewen Dong, Ke Cheng, Xutong Mu, Jiaxuan Fu, Shiyang Ma, Qiang Qu, Yulong Shen</p>
    <p><b>Summary:</b> Fine-tuning large language models (LLMs) raises privacy concerns due to the
risk of exposing sensitive training data. Federated learning (FL) mitigates
this risk by keeping training samples on local devices, but recent studies show
that adversaries can still infer private information from model updates in FL.
Additionally, LLM parameters are typically shared publicly during federated
fine-tuning, while developers are often reluctant to disclose these parameters,
posing further security challenges. Inspired by the above problems, we propose
PriFFT, a privacy-preserving federated fine-tuning mechanism, to protect both
the model updates and parameters. In PriFFT, clients and the server share model
inputs and parameters by secret sharing, performing secure fine-tuning on
shared values without accessing plaintext data. Due to considerable LLM
parameters, privacy-preserving federated fine-tuning invokes complex secure
calculations and requires substantial communication and computation resources.
To optimize the efficiency of privacy-preserving federated fine-tuning of LLMs,
we introduce function secret-sharing protocols for various operations,
including reciprocal calculation, tensor products, natural exponentiation,
softmax, hyperbolic tangent, and dropout. The proposed protocols achieve up to
4.02X speed improvement and reduce 7.19X communication overhead compared to the
implementation based on existing secret sharing methods. Besides, PriFFT
achieves a 2.23X speed improvement and reduces 4.08X communication overhead in
privacy-preserving fine-tuning without accuracy drop compared to the existing
secret sharing methods.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.03087v1">"Watch My Health, Not My Data": Understanding Perceptions, Barriers,
  Emotional Impact, & Coping Strategies Pertaining to IoT Privacy and Security
  in Health Monitoring for Older Adults</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-03-05T01:04:13Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Suleiman Saka, Sanchari Das</p>
    <p><b>Summary:</b> The proliferation of "Internet of Things (IoT)" provides older adults with
critical support for "health monitoring" and independent living, yet
significant concerns about security and privacy persist. In this paper, we
report on these issues through a two-phase user study, including a survey (N =
22) and semi-structured interviews (n = 9) with adults aged 65+. We found that
while 81.82% of our participants are aware of security features like
"two-factor authentication (2FA)" and encryption, 63.64% express serious
concerns about unauthorized access to sensitive health data. Only 13.64% feel
confident in existing protections, citing confusion over "data sharing
policies" and frustration with "complex security settings" which lead to
distrust and anxiety. To cope, our participants adopt various strategies, such
as relying on family or professional support and limiting feature usage leading
to disengagement. Thus, we recommend "adaptive security mechanisms," simplified
interfaces, and real-time transparency notifications to foster trust and ensure
"privacy and security by design" in IoT health systems for older adults.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.03043v1">Leveraging Randomness in Model and Data Partitioning for Privacy
  Amplification</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-04T22:49:59Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Andy Dong, Wei-Ning Chen, Ayfer Ozgur</p>
    <p><b>Summary:</b> We study how inherent randomness in the training process -- where each sample
(or client in federated learning) contributes only to a randomly selected
portion of training -- can be leveraged for privacy amplification. This
includes (1) data partitioning, where a sample participates in only a subset of
training iterations, and (2) model partitioning, where a sample updates only a
subset of the model parameters. We apply our framework to model parallelism in
federated learning, where each client updates a randomly selected subnetwork to
reduce memory and computational overhead, and show that existing methods, e.g.
model splitting or dropout, provide a significant privacy amplification gain
not captured by previous privacy analysis techniques. Additionally, we
introduce Balanced Iteration Subsampling, a new data partitioning method where
each sample (or client) participates in a fixed number of training iterations.
We show that this method yields stronger privacy amplification than Poisson
(i.i.d.) sampling of data (or clients). Our results demonstrate that randomness
in the training process, which is structured rather than i.i.d. and interacts
with data in complex ways, can be systematically leveraged for significant
privacy amplification.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.02968v1">Privacy-Preserving Fair Synthetic Tabular Data</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-04T19:51:00Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Fatima J. Sarmin, Atiquer R. Rahman, Christopher J. Henry, Noman Mohammed</p>
    <p><b>Summary:</b> Sharing of tabular data containing valuable but private information is
limited due to legal and ethical issues. Synthetic data could be an alternative
solution to this sharing problem, as it is artificially generated by machine
learning algorithms and tries to capture the underlying data distribution.
However, machine learning models are not free from memorization and may
introduce biases, as they rely on training data. Producing synthetic data that
preserves privacy and fairness while maintaining utility close to the real data
is a challenging task. This research simultaneously addresses both the privacy
and fairness aspects of synthetic data, an area not explored by other studies.
In this work, we present PF-WGAN, a privacy-preserving, fair synthetic tabular
data generator based on the WGAN-GP model. We have modified the original
WGAN-GP by adding privacy and fairness constraints forcing it to produce
privacy-preserving fair data. This approach will enable the publication of
datasets that protect individual's privacy and remain unbiased toward any
particular group. We compared the results with three state-of-the-art synthetic
data generator models in terms of utility, privacy, and fairness across four
different datasets. We found that the proposed model exhibits a more balanced
trade-off among utility, privacy, and fairness.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.02862v1">Privacy and Accuracy-Aware AI/ML Model Deduplication</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB">
  <p><b>Published on:</b> 2025-03-04T18:40:38Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Hong Guan, Lei Yu, Lixi Zhou, Li Xiong, Kanchan Chowdhury, Lulu Xie, Xusheng Xiao, Jia Zou</p>
    <p><b>Summary:</b> With the growing adoption of privacy-preserving machine learning algorithms,
such as Differentially Private Stochastic Gradient Descent (DP-SGD), training
or fine-tuning models on private datasets has become increasingly prevalent.
This shift has led to the need for models offering varying privacy guarantees
and utility levels to satisfy diverse user requirements. However, managing
numerous versions of large models introduces significant operational
challenges, including increased inference latency, higher resource consumption,
and elevated costs. Model deduplication is a technique widely used by many
model serving and database systems to support high-performance and low-cost
inference queries and model diagnosis queries. However, none of the existing
model deduplication works has considered privacy, leading to unbounded
aggregation of privacy costs for certain deduplicated models and inefficiencies
when applied to deduplicate DP-trained models. We formalize the problems of
deduplicating DP-trained models for the first time and propose a novel privacy-
and accuracy-aware deduplication mechanism to address the problems. We
developed a greedy strategy to select and assign base models to target models
to minimize storage and privacy costs. When deduplicating a target model, we
dynamically schedule accuracy validations and apply the Sparse Vector Technique
to reduce the privacy costs associated with private validation data. Compared
to baselines that do not provide privacy guarantees, our approach improved the
compression ratio by up to $35\times$ for individual models (including large
language models and vision transformers). We also observed up to $43\times$
inference speedup due to the reduction of I/O operations.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.02693v1">Federated Learning for Privacy-Preserving Feedforward Control in
  Multi-Agent Systems</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Multiagent Systems-662E9B">
  <p><b>Published on:</b> 2025-03-04T15:07:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jakob Weber, Markus Gurtner, Benedikt Alt, Adrian Trachte, Andreas Kugi</p>
    <p><b>Summary:</b> Feedforward control (FF) is often combined with feedback control (FB) in many
control systems, improving tracking performance, efficiency, and stability.
However, designing effective data-driven FF controllers in multi-agent systems
requires significant data collection, including transferring private or
proprietary data, which raises privacy concerns and incurs high communication
costs. Therefore, we propose a novel approach integrating Federated Learning
(FL) into FF control to address these challenges. This approach enables
privacy-preserving, communication-efficient, and decentralized continuous
improvement of FF controllers across multiple agents without sharing personal
or proprietary data. By leveraging FL, each agent learns a local, neural FF
controller using its data and contributes only model updates to a global
aggregation process, ensuring data privacy and scalability. We demonstrate the
effectiveness of our method in an autonomous driving use case. Therein,
vehicles equipped with a trajectory-tracking feedback controller are enhanced
by FL-based neural FF control. Simulations highlight significant improvements
in tracking performance compared to pure FB control, analogous to model-based
FF control. We achieve comparable tracking performance without exchanging
private vehicle-specific data compared to a centralized neural FF control. Our
results underscore the potential of FL-based neural FF control to enable
privacy-preserving learning in multi-agent control systems, paving the way for
scalable and efficient autonomous systems applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.02549v1">Federated nnU-Net for Privacy-Preserving Medical Image Segmentation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-03-04T12:20:06Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Grzegorz Skorupko, Fotios Avgoustidis, Carlos Martín-Isla, Lidia Garrucho, Dimitri A. Kessler, Esmeralda Ruiz Pujadas, Oliver Díaz, Maciej Bobowicz, Katarzyna Gwoździewicz, Xavier Bargalló, Paulius Jaruševičius, Kaisar Kushibar, Karim Lekadir</p>
    <p><b>Summary:</b> The nnU-Net framework has played a crucial role in medical image segmentation
and has become the gold standard in multitudes of applications targeting
different diseases, organs, and modalities. However, so far it has been used
primarily in a centralized approach where the data collected from hospitals are
stored in one center and used to train the nnU-Net. This centralized approach
has various limitations, such as leakage of sensitive patient information and
violation of patient privacy. Federated learning is one of the approaches to
train a segmentation model in a decentralized manner that helps preserve
patient privacy. In this paper, we propose FednnU-Net, a federated learning
extension of nnU-Net. We introduce two novel federated learning methods to the
nnU-Net framework - Federated Fingerprint Extraction (FFE) and Asymmetric
Federated Averaging (AsymFedAvg) - and experimentally show their consistent
performance for breast, cardiac and fetal segmentation using 6 datasets
representing samples from 18 institutions. Additionally, to further promote
research and deployment of decentralized training in privacy constrained
institutions, we make our plug-n-play framework public. The source-code is
available at https://github.com/faildeny/FednnUNet .</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.02455v1">Privacy Preservation Techniques (PPTs) in IoT Systems: A Scoping Review
  and Future Directions</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-04T10:03:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Emmanuel Alalade, Ashraf Matrawy</p>
    <p><b>Summary:</b> Privacy preservation in Internet of Things (IoT) systems requires the use of
privacy-enhancing technologies (PETs) built from innovative technologies such
as cryptography and artificial intelligence (AI) to create techniques called
privacy preservation techniques (PPTs). These PPTs achieve various privacy
goals and address different privacy concerns by mitigating potential privacy
threats within IoT systems. This study carried out a scoping review of
different types of PPTs used in previous research works on IoT systems between
2010 and early 2023 to further explore the advantages of privacy preservation
in these systems. This scoping review looks at privacy goals, possible
technologies used for building PET, the integration of PPTs into the computing
layer of the IoT architecture, different IoT applications in which PPTs are
deployed, and the different privacy types addressed by these techniques within
IoT systems. Key findings, such as the prominent privacy goal and privacy type
in IoT, are discussed in this survey, along with identified research gaps that
could inform future endeavors in privacy research and benefit the privacy
research community and other stakeholders in IoT systems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.02132v1">Video-DPRP: A Differentially Private Approach for Visual
  Privacy-Preserving Video Human Activity Recognition</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-03-03T23:43:12Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Allassan Tchangmena A Nken, Susan Mckeever, Peter Corcoran, Ihsan Ullah</p>
    <p><b>Summary:</b> Considerable effort has been made in privacy-preserving video human activity
recognition (HAR). Two primary approaches to ensure privacy preservation in
Video HAR are differential privacy (DP) and visual privacy. Techniques
enforcing DP during training provide strong theoretical privacy guarantees but
offer limited capabilities for visual privacy assessment. Conversely methods,
such as low-resolution transformations, data obfuscation and adversarial
networks, emphasize visual privacy but lack clear theoretical privacy
assurances. In this work, we focus on two main objectives: (1) leveraging DP
properties to develop a model-free approach for visual privacy in videos and
(2) evaluating our proposed technique using both differential privacy and
visual privacy assessments on HAR tasks. To achieve goal (1), we introduce
Video-DPRP: a Video-sample-wise Differentially Private Random Projection
framework for privacy-preserved video reconstruction for HAR. By using random
projections, noise matrices and right singular vectors derived from the
singular value decomposition of videos, Video-DPRP reconstructs DP videos using
privacy parameters ($\epsilon,\delta$) while enabling visual privacy
assessment. For goal (2), using UCF101 and HMDB51 datasets, we compare
Video-DPRP's performance on activity recognition with traditional DP methods,
and state-of-the-art (SOTA) visual privacy-preserving techniques. Additionally,
we assess its effectiveness in preserving privacy-related attributes such as
facial features, gender, and skin color, using the PA-HMDB and VISPR datasets.
Video-DPRP combines privacy-preservation from both a DP and visual privacy
perspective unlike SOTA methods that typically address only one of these
aspects.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.02114v1">Fairness and/or Privacy on Social Graphs</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Social and Information Networks-662E9B">
  <p><b>Published on:</b> 2025-03-03T22:56:32Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Bartlomiej Surma, Michael Backes, Yang Zhang</p>
    <p><b>Summary:</b> Graph Neural Networks (GNNs) have shown remarkable success in various
graph-based learning tasks. However, recent studies have raised concerns about
fairness and privacy issues in GNNs, highlighting the potential for biased or
discriminatory outcomes and the vulnerability of sensitive information. This
paper presents a comprehensive investigation of fairness and privacy in GNNs,
exploring the impact of various fairness-preserving measures on model
performance. We conduct experiments across diverse datasets and evaluate the
effectiveness of different fairness interventions. Our analysis considers the
trade-offs between fairness, privacy, and accuracy, providing insights into the
challenges and opportunities in achieving both fair and private graph learning.
The results highlight the importance of carefully selecting and combining
fairness-preserving measures based on the specific characteristics of the data
and the desired fairness objectives. This study contributes to a deeper
understanding of the complex interplay between fairness, privacy, and accuracy
in GNNs, paving the way for the development of more robust and ethical graph
learning models.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.02091v1">Which Code Statements Implement Privacy Behaviors in Android
  Applications?</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Software Engineering-D91E36">
  <p><b>Published on:</b> 2025-03-03T22:20:01Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chia-Yi Su, Aakash Bansal, Vijayanta Jain, Sepideh Ghanavati, Sai Teja Peddinti, Collin McMillan</p>
    <p><b>Summary:</b> A "privacy behavior" in software is an action where the software uses
personal information for a service or a feature, such as a website using
location to provide content relevant to a user. Programmers are required by
regulations or application stores to provide privacy notices and labels
describing these privacy behaviors. Although many tools and research prototypes
have been developed to help programmers generate these notices by analyzing the
source code, these approaches are often fairly coarse-grained (i.e., at the
level of whole methods or files, rather than at the statement level). But this
is not necessarily how privacy behaviors exist in code. Privacy behaviors are
embedded in specific statements in code. Current literature does not examine
what statements programmers see as most important, how consistent these views
are, or how to detect them. In this paper, we conduct an empirical study to
examine which statements programmers view as most-related to privacy behaviors.
We find that expression statements that make function calls are most associated
with privacy behaviors, while the type of privacy label has little effect on
the attributes of the selected statements. We then propose an approach to
automatically detect these privacy-relevant statements by fine-tuning three
large language models with the data from the study. We observe that the
agreement between our approach and participants is comparable to or higher than
an agreement between two participants. Our study and detection approach can
help programmers understand which statements in code affect privacy in mobile
applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.02019v1">SLAP: Secure Location-proof and Anonymous Privacy-preserving Spectrum
  Access</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-03T19:52:56Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Saleh Darzi, Attila A. Yavuz</p>
    <p><b>Summary:</b> The rapid advancements in wireless technology have significantly increased
the demand for communication resources, leading to the development of Spectrum
Access Systems (SAS). However, network regulations require disclosing sensitive
user information, such as location coordinates and transmission details,
raising critical privacy concerns. Moreover, as a database-driven architecture
reliant on user-provided data, SAS necessitates robust location verification to
counter identity and location spoofing attacks and remains a primary target for
denial-of-service (DoS) attacks. Addressing these security challenges while
adhering to regulatory requirements is essential. In this paper, we propose
SLAP, a novel framework that ensures location privacy and anonymity during
spectrum queries, usage notifications, and location-proof acquisition. Our
solution includes an adaptive dual-scenario location verification mechanism
with architectural flexibility and a fallback option, along with a counter-DoS
approach using time-lock puzzles. We prove the security of SLAP and demonstrate
its advantages over existing solutions through comprehensive performance
evaluations.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.02017v1">A Lightweight and Secure Deep Learning Model for Privacy-Preserving
  Federated Learning in Intelligent Enterprises</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-03-03T19:51:13Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Reza Fotohi, Fereidoon Shams Aliee, Bahar Farahani</p>
    <p><b>Summary:</b> The ever growing Internet of Things (IoT) connections drive a new type of
organization, the Intelligent Enterprise. In intelligent enterprises, machine
learning based models are adopted to extract insights from data. Due to the
efficiency and privacy challenges of these traditional models, a new federated
learning (FL) paradigm has emerged. In FL, multiple enterprises can jointly
train a model to update a final model. However, firstly, FL trained models
usually perform worse than centralized models, especially when enterprises
training data is non-IID (Independent and Identically Distributed). Second, due
to the centrality of FL and the untrustworthiness of local enterprises,
traditional FL solutions are vulnerable to poisoning and inference attacks and
violate privacy. Thirdly, the continuous transfer of parameters between
enterprises and servers increases communication costs. To this end, the
FedAnil+ model is proposed, a novel, lightweight, and secure Federated Deep
Learning Model that includes three main phases. In the first phase, the goal is
to solve the data type distribution skew challenge. Addressing privacy concerns
against poisoning and inference attacks is covered in the second phase.
Finally, to alleviate the communication overhead, a novel compression approach
is proposed that significantly reduces the size of the updates. The experiment
results validate that FedAnil+ is secure against inference and poisoning
attacks with better accuracy. In addition, it shows improvements over existing
approaches in terms of model accuracy (13%, 16%, and 26%), communication cost
(17%, 21%, and 25%), and computation cost (7%, 9%, and 11%).</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.01482v1">Revisiting Locally Differentially Private Protocols: Towards Better
  Trade-offs in Privacy, Utility, and Attack Resistance</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-03T12:41:01Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Héber H. Arcolezi, Sébastien Gambs</p>
    <p><b>Summary:</b> Local Differential Privacy (LDP) offers strong privacy protection, especially
in settings in which the server collecting the data is untrusted. However,
designing LDP mechanisms that achieve an optimal trade-off between privacy,
utility, and robustness to adversarial inference attacks remains challenging.
In this work, we introduce a general multi-objective optimization framework for
refining LDP protocols, enabling the joint optimization of privacy and utility
under various adversarial settings. While our framework is flexible enough to
accommodate multiple privacy and security attacks as well as utility metrics,
in this paper we specifically optimize for Attacker Success Rate (ASR) under
distinguishability attack as a measure of privacy and Mean Squared Error (MSE)
as a measure of utility. We systematically revisit these trade-offs by
analyzing eight state-of-the-art LDP protocols and proposing refined
counterparts that leverage tailored optimization techniques. Experimental
results demonstrate that our proposed adaptive mechanisms consistently
outperform their non-adaptive counterparts, reducing ASR by up to five orders
of magnitude while maintaining competitive utility. Analytical derivations also
confirm the effectiveness of our mechanisms, moving them closer to the ASR-MSE
Pareto frontier.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.01470v1">Position: Ensuring mutual privacy is necessary for effective external
  evaluation of proprietary AI systems</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-03T12:24:59Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ben Bucknall, Robert F. Trager, Michael A. Osborne</p>
    <p><b>Summary:</b> The external evaluation of AI systems is increasingly recognised as a crucial
approach for understanding their potential risks. However, facilitating
external evaluation in practice faces significant challenges in balancing
evaluators' need for system access with AI developers' privacy and security
concerns. Additionally, evaluators have reason to protect their own privacy -
for example, in order to maintain the integrity of held-out test sets. We refer
to the challenge of ensuring both developers' and evaluators' privacy as one of
providing mutual privacy. In this position paper, we argue that (i) addressing
this mutual privacy challenge is essential for effective external evaluation of
AI systems, and (ii) current methods for facilitating external evaluation
inadequately address this challenge, particularly when it comes to preserving
evaluators' privacy. In making these arguments, we formalise the mutual privacy
problem; examine the privacy and access requirements of both model owners and
evaluators; and explore potential solutions to this challenge, including
through the application of cryptographic and hardware-based approaches.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.01208v1">Watch Out Your Album! On the Inadvertent Privacy Memorization in
  Multi-Modal Large Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-03-03T06:10:27Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tianjie Ju, Yi Hua, Hao Fei, Zhenyu Shao, Yubin Zheng, Haodong Zhao, Mong-Li Lee, Wynne Hsu, Zhuosheng Zhang, Gongshen Liu</p>
    <p><b>Summary:</b> Multi-Modal Large Language Models (MLLMs) have exhibited remarkable
performance on various vision-language tasks such as Visual Question Answering
(VQA). Despite accumulating evidence of privacy concerns associated with
task-relevant content, it remains unclear whether MLLMs inadvertently memorize
private content that is entirely irrelevant to the training tasks. In this
paper, we investigate how randomly generated task-irrelevant private content
can become spuriously correlated with downstream objectives due to partial
mini-batch training dynamics, thus causing inadvertent memorization.
Concretely, we randomly generate task-irrelevant watermarks into VQA
fine-tuning images at varying probabilities and propose a novel probing
framework to determine whether MLLMs have inadvertently encoded such content.
Our experiments reveal that MLLMs exhibit notably different training behaviors
in partial mini-batch settings with task-irrelevant watermarks embedded.
Furthermore, through layer-wise probing, we demonstrate that MLLMs trigger
distinct representational patterns when encountering previously seen
task-irrelevant knowledge, even if this knowledge does not influence their
output during prompting. Our code is available at
https://github.com/illusionhi/ProbingPrivacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.01089v1">Privacy-preserving Machine Learning in Internet of Vehicle Applications:
  Fundamentals, Recent Advances, and Future Direction</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-03T01:24:04Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Nazmul Islam, Mohammad Zulkernine</p>
    <p><b>Summary:</b> Machine learning (ML) has revolutionized Internet of Vehicles (IoV)
applications by enhancing intelligent transportation, autonomous driving
capabilities, and various connected services within a large, heterogeneous
network. However, the increased connectivity and massive data exchange for ML
applications introduce significant privacy challenges. Privacy-preserving
machine learning (PPML) offers potential solutions to address these challenges
by preserving privacy at various stages of the ML pipeline. Despite the rapid
development of ML-based IoV applications and the growing data privacy concerns,
there are limited comprehensive studies on the adoption of PPML within this
domain. Therefore, this study provides a comprehensive review of the
fundamentals, recent advancements, and the challenges of integrating PPML into
IoV applications. To conduct an extensive study, we first review existing
surveys of various PPML techniques and their integration into IoV across
different scopes. We then discuss the fundamentals of IoV and propose a
four-layer IoV architecture. Additionally, we categorize IoV applications into
three key domains and analyze the privacy challenges in leveraging ML for these
application domains. Next, we provide an overview of various PPML techniques,
highlighting their applicability and performance to address the privacy
challenges. Building on these fundamentals, we thoroughly review recent
advancements in integrating various PPML techniques within IoV applications,
discussing their frameworks, key features, and performance evaluation in terms
of privacy, utility, and efficiency. Finally, we identify current challenges
and propose future research directions to enhance privacy and reliability in
IoV applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.01000v1">Privacy vs. Profit: The Impact of Google's Manifest Version 3 (MV3)
  Update on Ad Blocker Effectiveness</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">    
  <p><b>Published on:</b> 2025-03-02T19:41:34Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Karlo Lukic, Lazaros Papadopoulos</p>
    <p><b>Summary:</b> Google's recent update to the manifest file for Chrome browser
extensions-transitioning from manifest version 2 (MV2) to manifest version 3
(MV3)-has raised concerns among users and ad blocker providers, who worry that
the new restrictions, notably the shift from the powerful WebRequest API to the
more restrictive DeclarativeNetRequest API, might reduce ad blocker
effectiveness. Because ad blockers play a vital role for millions of users
seeking a more private and ad-free browsing experience, this study empirically
investigates how the MV3 update affects their ability to block ads and
trackers. Through a browser-based experiment conducted across multiple samples
of ad-supported websites, we compare the MV3 to MV2 instances of four widely
used ad blockers. Our results reveal no statistically significant reduction in
ad-blocking or anti-tracking effectiveness for MV3 ad blockers compared to
their MV2 counterparts, and in some cases, MV3 instances even exhibit slight
improvements in blocking trackers. These findings are reassuring for users,
indicating that the MV3 instances of popular ad blockers continue to provide
effective protection against intrusive ads and privacy-infringing trackers.
While some uncertainties remain, ad blocker providers appear to have
successfully navigated the MV3 update, finding solutions that maintain the core
functionality of their ad blockers.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.00703v1">Towards hyperparameter-free optimization with differential privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-03-02T02:59:52Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhiqi Bu, Ruixuan Liu</p>
    <p><b>Summary:</b> Differential privacy (DP) is a privacy-preserving paradigm that protects the
training data when training deep learning models. Critically, the performance
of models is determined by the training hyperparameters, especially those of
the learning rate schedule, thus requiring fine-grained hyperparameter tuning
on the data. In practice, it is common to tune the learning rate
hyperparameters through the grid search that (1) is computationally expensive
as multiple runs are needed, and (2) increases the risk of data leakage as the
selection of hyperparameters is data-dependent. In this work, we adapt the
automatic learning rate schedule to DP optimization for any models and
optimizers, so as to significantly mitigate or even eliminate the cost of
hyperparameter tuning when applied together with automatic per-sample gradient
clipping. Our hyperparameter-free DP optimization is almost as computationally
efficient as the standard non-DP optimization, and achieves state-of-the-art DP
performance on various language and vision tasks.</p>
  </details>
</div>

