
<h2>2024-01</h2>

<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.00205v1">Decentralised, Collaborative, and Privacy-preserving Machine Learning
  for Multi-Hospital Data</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-01-31T22:06:10Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Congyu Fang, Adam Dziedzic, Lin Zhang, Laura Oliva, Amol Verma, Fahad Razak, Nicolas Papernot, Bo Wang</p>
    <p><b>Summary:</b> Machine Learning (ML) has demonstrated its great potential on medical data
analysis. Large datasets collected from diverse sources and settings are
essential for ML models in healthcare to achieve better accuracy and
generalizability. Sharing data across different healthcare institutions is
challenging because of complex and varying privacy and regulatory requirements.
Hence, it is hard but crucial to allow multiple parties to collaboratively
train an ML model leveraging the private datasets available at each party
without the need for direct sharing of those datasets or compromising the
privacy of the datasets through collaboration. In this paper, we address this
challenge by proposing Decentralized, Collaborative, and Privacy-preserving ML
for Multi-Hospital Data (DeCaPH). It offers the following key benefits: (1) it
allows different parties to collaboratively train an ML model without
transferring their private datasets; (2) it safeguards patient privacy by
limiting the potential privacy leakage arising from any contents shared across
the parties during the training process; and (3) it facilitates the ML model
training without relying on a centralized server. We demonstrate the
generalizability and power of DeCaPH on three distinct tasks using real-world
distributed medical datasets: patient mortality prediction using electronic
health records, cell-type classification using single-cell human genomes, and
pathology identification using chest radiology images. We demonstrate that the
ML models trained with DeCaPH framework have an improved utility-privacy
trade-off, showing it enables the models to have good performance while
preserving the privacy of the training data points. In addition, the ML models
trained with DeCaPH framework in general outperform those trained solely with
the private datasets from individual parties, showing that DeCaPH enhances the
model generalizability.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.17829v1">Evolving privacy: drift parameter estimation for discretely observed
  i.i.d. diffusion processes under LDP</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Statistics Theory-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36">   
  <p><b>Published on:</b> 2024-01-31T13:41:23Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chiara Amorino, Arnaud Gloter, Hélène Halconruy</p>
    <p><b>Summary:</b> The problem of estimating a parameter in the drift coefficient is addressed
for $N$ discretely observed independent and identically distributed stochastic
differential equations (SDEs). This is done considering additional constraints,
wherein only public data can be published and used for inference. The concept
of local differential privacy (LDP) is formally introduced for a system of
stochastic differential equations. The objective is to estimate the drift
parameter by proposing a contrast function based on a pseudo-likelihood
approach. A suitably scaled Laplace noise is incorporated to meet the privacy
requirements. Our key findings encompass the derivation of explicit conditions
tied to the privacy level. Under these conditions, we establish the consistency
and asymptotic normality of the associated estimator. Notably, the convergence
rate is intricately linked to the privacy level, and is some situations may be
completely different from the case where privacy constraints are ignored. Our
results hold true as the discretization step approaches zero and the number of
processes $N$ tends to infinity.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.00896v1">Privacy and Security Implications of Cloud-Based AI Services : A Survey</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-01-31T13:30:20Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Alka Luqman, Riya Mahesh, Anupam Chattopadhyay</p>
    <p><b>Summary:</b> This paper details the privacy and security landscape in today's cloud
ecosystem and identifies that there is a gap in addressing the risks introduced
by machine learning models. As machine learning algorithms continue to evolve
and find applications across diverse domains, the need to categorize and
quantify privacy and security risks becomes increasingly critical. With the
emerging trend of AI-as-a-Service (AIaaS), machine learned AI models (or ML
models) are deployed on the cloud by model providers and used by model
consumers. We first survey the AIaaS landscape to document the various kinds of
liabilities that ML models, especially Deep Neural Networks pose and then
introduce a taxonomy to bridge this gap by holistically examining the risks
that creators and consumers of ML models are exposed to and their known
defences till date. Such a structured approach will be beneficial for ML model
providers to create robust solutions. Likewise, ML model consumers will find it
valuable to evaluate such solutions and understand the implications of their
engagement with such services. The proposed taxonomies provide a foundational
basis for solutions in private, secure and robust ML, paving the way for more
transparent and resilient AI systems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.17823v1">Privacy-preserving data release leveraging optimal transport and
  particle gradient descent</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-01-31T13:28:07Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Konstantin Donhauser, Javier Abad, Neha Hulkund, Fanny Yang</p>
    <p><b>Summary:</b> We present a novel approach for differentially private data synthesis of
protected tabular datasets, a relevant task in highly sensitive domains such as
healthcare and government. Current state-of-the-art methods predominantly use
marginal-based approaches, where a dataset is generated from private estimates
of the marginals. In this paper, we introduce PrivPGD, a new generation method
for marginal-based private data synthesis, leveraging tools from optimal
transport and particle gradient descent. Our algorithm outperforms existing
methods on a large range of datasets while being highly scalable and offering
the flexibility to incorporate additional domain-specific constraints.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.17630v1">Towards Personalized Privacy: User-Governed Data Contribution for
  Federated Recommendation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Retrieval-5BC0EB">
  <p><b>Published on:</b> 2024-01-31T07:20:56Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Liang Qu, Wei Yuan, Ruiqi Zheng, Lizhen Cui, Yuhui Shi, Hongzhi Yin</p>
    <p><b>Summary:</b> Federated recommender systems (FedRecs) have gained significant attention for
their potential to protect user's privacy by keeping user privacy data locally
and only communicating model parameters/gradients to the server. Nevertheless,
the currently existing architecture of FedRecs assumes that all users have the
same 0-privacy budget, i.e., they do not upload any data to the server, thus
overlooking those users who are less concerned about privacy and are willing to
upload data to get a better recommendation service. To bridge this gap, this
paper explores a user-governed data contribution federated recommendation
architecture where users are free to take control of whether they share data
and the proportion of data they share to the server. To this end, this paper
presents a cloud-device collaborative graph neural network federated
recommendation model, named CDCGNNFed. It trains user-centric ego graphs
locally, and high-order graphs based on user-shared data in the server in a
collaborative manner via contrastive learning. Furthermore, a graph mending
strategy is utilized to predict missing links in the graph on the server, thus
leveraging the capabilities of graph neural networks over high-order graphs.
Extensive experiments were conducted on two public datasets, and the results
demonstrate the effectiveness of the proposed method.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.17628v1">Elephants Do Not Forget: Differential Privacy with State Continuity for
  Privacy Budget</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-01-31T07:08:14Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jiankai Jin, Chitchanok Chuengsatiansup, Toby Murray, Benjamin I. P. Rubinstein, Yuval Yarom, Olga Ohrimenko</p>
    <p><b>Summary:</b> Current implementations of differentially-private (DP) systems either lack
support to track the global privacy budget consumed on a dataset, or fail to
faithfully maintain the state continuity of this budget. We show that failure
to maintain a privacy budget enables an adversary to mount replay, rollback and
fork attacks - obtaining answers to many more queries than what a secure system
would allow. As a result the attacker can reconstruct secret data that DP aims
to protect - even if DP code runs in a Trusted Execution Environment (TEE). We
propose ElephantDP, a system that aims to provide the same guarantees as a
trusted curator in the global DP model would, albeit set in an untrusted
environment. Our system relies on a state continuity module to provide
protection for the privacy budget and a TEE to faithfully execute DP code and
update the budget. To provide security, our protocol makes several design
choices including the content of the persistent state and the order between
budget updates and query answers. We prove that ElephantDP provides liveness
(i.e., the protocol can restart from a correct state and respond to queries as
long as the budget is not exceeded) and DP confidentiality (i.e., an attacker
learns about a dataset as much as it would from interacting with a trusted
curator). Our implementation and evaluation of the protocol use Intel SGX as a
TEE to run the DP code and a network of TEEs to maintain state continuity.
Compared to an insecure baseline, we observe only 1.1-2$\times$ overheads and
lower relative overheads for larger datasets and complex DP queries.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.17127v1">Personalized Differential Privacy for Ridge Regression</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2024-01-30T16:00:14Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Krishna Acharya, Franziska Boenisch, Rakshit Naidu, Juba Ziani</p>
    <p><b>Summary:</b> The increased application of machine learning (ML) in sensitive domains
requires protecting the training data through privacy frameworks, such as
differential privacy (DP). DP requires to specify a uniform privacy level
$\varepsilon$ that expresses the maximum privacy loss that each data point in
the entire dataset is willing to tolerate. Yet, in practice, different data
points often have different privacy requirements. Having to set one uniform
privacy level is usually too restrictive, often forcing a learner to guarantee
the stringent privacy requirement, at a large cost to accuracy. To overcome
this limitation, we introduce our novel Personalized-DP Output Perturbation
method (PDP-OP) that enables to train Ridge regression models with individual
per data point privacy levels. We provide rigorous privacy proofs for our
PDP-OP as well as accuracy guarantees for the resulting model. This work is the
first to provide such theoretical accuracy guarantees when it comes to
personalized DP in machine learning, whereas previous work only provided
empirical evaluations. We empirically evaluate PDP-OP on synthetic and real
datasets and with diverse privacy distributions. We show that by enabling each
data point to specify their own privacy requirement, we can significantly
improve the privacy-accuracy trade-offs in DP. We also show that PDP-OP
outperforms the personalized privacy techniques of Jorgensen et al. (2015).</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.00888v1">Security and Privacy Challenges of Large Language Models: A Survey</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-01-30T04:00:54Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Badhan Chandra Das, M. Hadi Amini, Yanzhao Wu</p>
    <p><b>Summary:</b> Large Language Models (LLMs) have demonstrated extraordinary capabilities and
contributed to multiple fields, such as generating and summarizing text,
language translation, and question-answering. Nowadays, LLM is becoming a very
popular tool in computerized language processing tasks, with the capability to
analyze complicated linguistic patterns and provide relevant and appropriate
responses depending on the context. While offering significant advantages,
these models are also vulnerable to security and privacy attacks, such as
jailbreaking attacks, data poisoning attacks, and Personally Identifiable
Information (PII) leakage attacks. This survey provides a thorough review of
the security and privacy challenges of LLMs for both training data and users,
along with the application-based risks in various domains, such as
transportation, education, and healthcare. We assess the extent of LLM
vulnerabilities, investigate emerging security and privacy attacks for LLMs,
and review the potential defense mechanisms. Additionally, the survey outlines
existing research gaps in this domain and highlights future research
directions.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.16596v1">PrIsing: Privacy-Preserving Peer Effect Estimation via Ising Model</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Social and Information Networks-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Statistics Theory-D91E36">  
  <p><b>Published on:</b> 2024-01-29T21:56:39Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Abhinav Chakraborty, Anirban Chatterjee, Abhinandan Dalal</p>
    <p><b>Summary:</b> The Ising model, originally developed as a spin-glass model for ferromagnetic
elements, has gained popularity as a network-based model for capturing
dependencies in agents' outputs. Its increasing adoption in healthcare and the
social sciences has raised privacy concerns regarding the confidentiality of
agents' responses. In this paper, we present a novel
$(\varepsilon,\delta)$-differentially private algorithm specifically designed
to protect the privacy of individual agents' outcomes. Our algorithm allows for
precise estimation of the natural parameter using a single network through an
objective perturbation technique. Furthermore, we establish regret bounds for
this algorithm and assess its performance on synthetic datasets and two
real-world networks: one involving HIV status in a social network and the other
concerning the political leaning of online blogs.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.16251v2">Cross-silo Federated Learning with Record-level Personalized
  Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-01-29T16:01:46Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Junxu Liu, Jian Lou, Li Xiong, Jinfei Liu, Xiaofeng Meng</p>
    <p><b>Summary:</b> Federated learning enhanced by differential privacy has emerged as a popular
approach to better safeguard the privacy of client-side data by protecting
clients' contributions during the training process. Existing solutions
typically assume a uniform privacy budget for all records and provide
one-size-fits-all solutions that may not be adequate to meet each record's
privacy requirement. In this paper, we explore the uncharted territory of
cross-silo FL with record-level personalized differential privacy. We devise a
novel framework named rPDP-FL, employing a two-stage hybrid sampling scheme
with both client-level sampling and non-uniform record-level sampling to
accommodate varying privacy requirements. A critical and non-trivial problem is
to select the ideal per-record sampling probability q given the personalized
privacy budget {\epsilon}. We introduce a versatile solution named
Simulation-CurveFitting, allowing us to uncover a significant insight into the
nonlinear correlation between q and {\epsilon} and derive an elegant
mathematical model to tackle the problem. Our evaluation demonstrates that our
solution can provide significant performance gains over the baselines that do
not consider personalized privacy preservation.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.16170v1">A Privacy-preserving key transmission protocol to distribute QRNG keys
  using zk-SNARKs</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-01-29T14:00:37Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> David Soler, Carlos Dafonte, Manuel Fernández-Veiga, Ana Fernández Vilas, Francisco J. Nóvoa</p>
    <p><b>Summary:</b> High-entropy random numbers are an essential part of cryptography, and
Quantum Random Number Generators (QRNG) are an emergent technology that can
provide high-quality keys for cryptographic algorithms but unfortunately are
currently difficult to access. Existing Entropy-as-a-Service solutions require
users to trust the central authority distributing the key material, which is
not desirable in a high-privacy environment. In this paper, we present a novel
key transmission protocol that allows users to obtain cryptographic material
generated by a QRNG in such a way that the server is unable to identify which
user is receiving each key. This is achieved with the inclusion of Zero
Knowledge Succinct Non-interactive Arguments of Knowledge (zk-SNARK), a
cryptographic primitive that allow users to prove knowledge of some value
without needing to reveal it. The security analysis of the protocol proves that
it satisfies the properties of Anonymity, Unforgeability and Confidentiality,
as defined in this document. We also provide an implementation of the protocol
demonstrating its functionality and performance, using NFC as the transmission
channel for the QRNG key.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.16094v1">Federated unsupervised random forest for privacy-preserving patient
  stratification</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2024-01-29T12:04:14Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Bastian Pfeifer, Christel Sirocchi, Marcus D. Bloice, Markus Kreuzthaler, Martin Urschler</p>
    <p><b>Summary:</b> In the realm of precision medicine, effective patient stratification and
disease subtyping demand innovative methodologies tailored for multi-omics
data. Clustering techniques applied to multi-omics data have become
instrumental in identifying distinct subgroups of patients, enabling a
finer-grained understanding of disease variability. This work establishes a
powerful framework for advancing precision medicine through unsupervised
random-forest-based clustering and federated computing. We introduce a novel
multi-omics clustering approach utilizing unsupervised random-forests. The
unsupervised nature of the random forest enables the determination of
cluster-specific feature importance, unraveling key molecular contributors to
distinct patient groups. Moreover, our methodology is designed for federated
execution, a crucial aspect in the medical domain where privacy concerns are
paramount. We have validated our approach on machine learning benchmark data
sets as well as on cancer data from The Cancer Genome Atlas (TCGA). Our method
is competitive with the state-of-the-art in terms of disease subtyping, but at
the same time substantially improves the cluster interpretability. Experiments
indicate that local clustering performance can be improved through federated
computing.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.15906v2">Mean Estimation with User-Level Privacy for Spatio-Temporal IoT Datasets</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36">  
  <p><b>Published on:</b> 2024-01-29T06:21:29Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> V. Arvind Rameshwar, Anshoo Tandon, Prajjwal Gupta, Novoneel Chakraborty, Abhay Sharma</p>
    <p><b>Summary:</b> This paper considers the problem of the private release of sample means of
speed values from traffic datasets. Our key contribution is the development of
user-level differentially private algorithms that incorporate carefully chosen
parameter values to ensure low estimation errors on real-world datasets, while
ensuring privacy. We test our algorithms on ITMS (Intelligent Traffic
Management System) data from an Indian city, where the speeds of different
buses are drawn in a potentially non-i.i.d. manner from an unknown
distribution, and where the number of speed samples contributed by different
buses is potentially different. We then apply our algorithms to a synthetic
dataset, generated based on the ITMS data, having either a large number of
users or a large number of samples per user. Here, we provide recommendations
for the choices of parameters and algorithm subroutines that result in low
estimation errors, while guaranteeing user-level privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.15774v1">Integrating Differential Privacy and Contextual Integrity</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2024-01-28T21:28:07Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sebastian Benthall, Rachel Cummings</p>
    <p><b>Summary:</b> In this work, we propose the first framework for integrating Differential
Privacy (DP) and Contextual Integrity (CI). DP is a property of an algorithm
that injects statistical noise to obscure information about individuals
represented within a database. CI defines privacy as information flow that is
appropriate to social context. Analyzed together, these paradigms outline two
dimensions on which to analyze privacy of information flows: descriptive and
normative properties. We show that our new integrated framework provides
benefits to both CI and DP that cannot be attained when each definition is
considered in isolation: it enables contextually-guided tuning of the epsilon
parameter in DP, and it enables CI to be applied to a broader set of
information flows occurring in real-world systems, such as those involving PETs
and machine learning. We conclude with a case study based on the use of DP in
the U.S. Census Bureau.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.15561v1">A Parameter Privacy-Preserving Strategy for Mixed-Autonomy Platoon
  Control</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Robotics-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36">
  <p><b>Published on:</b> 2024-01-28T04:00:00Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jingyuan Zhou, Kaidi Yang</p>
    <p><b>Summary:</b> It has been demonstrated that leading cruise control (LCC) can improve the
operation of mixed-autonomy platoons by allowing connected and automated
vehicles (CAVs) to make longitudinal control decisions based on the information
provided by surrounding vehicles. However, LCC generally requires surrounding
human-driven vehicles (HDVs) to share their real-time states, which can be used
by adversaries to infer drivers' car-following behavior, potentially leading to
financial losses or safety concerns. This paper aims to address such privacy
concerns and protect the behavioral characteristics of HDVs by devising a
parameter privacy-preserving approach for mixed-autonomy platoon control.
First, we integrate a parameter privacy filter into LCC to protect sensitive
car-following parameters. The privacy filter allows each vehicle to generate
seemingly realistic pseudo states by distorting the true parameters to pseudo
parameters, which can protect drivers' privacy in behavioral parameters without
significantly influencing the control performance. Second, to enhance the
practicality and reliability of the privacy filter within LCC, we first extend
the current approach to accommodate continuous parameter spaces through a
neural network estimator. Subsequently, we introduce an individual-level
parameter privacy preservation constraint, focusing on the privacy level of
each individual parameter pair, further enhancing the approach's reliability.
Third, analysis of head-to-tail string stability reveals the potential impact
of privacy filters in degrading mixed traffic flow performance. Simulation
shows that this approach can effectively trade off privacy and control
performance in LCC. We further demonstrate the benefit of such an approach in
networked systems, i.e., by applying the privacy filter to a proceeding
vehicle, one can also achieve a certain level of privacy for the following
vehicle.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.15491v1">General Inferential Limits Under Differential and Pufferfish Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Statistics Theory-D91E36"> 
  <p><b>Published on:</b> 2024-01-27T19:44:46Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> James Bailie, Ruobin Gong</p>
    <p><b>Summary:</b> Differential privacy (DP) is a class of mathematical standards for assessing
the privacy provided by a data-release mechanism. This work concerns two
important flavors of DP that are related yet conceptually distinct: pure
$\epsilon$-differential privacy ($\epsilon$-DP) and Pufferfish privacy. We
restate $\epsilon$-DP and Pufferfish privacy as Lipschitz continuity conditions
and provide their formulations in terms of an object from the imprecise
probability literature: the interval of measures. We use these formulations to
derive limits on key quantities in frequentist hypothesis testing and in
Bayesian inference using data that are sanitised according to either of these
two privacy standards. Under very mild conditions, the results in this work are
valid for arbitrary parameters, priors and data generating models. These bounds
are weaker than those attainable when analysing specific data generating models
or data-release mechanisms. However, they provide generally applicable limits
on the ability to learn from differentially private data - even when the
analyst's knowledge of the model or mechanism is limited. They also shed light
on the semantic interpretations of the two DP flavors under examination, a
subject of contention in the current literature.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.15369v1">Privacy-Preserving Cross-Domain Sequential Recommendation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Retrieval-5BC0EB">
  <p><b>Published on:</b> 2024-01-27T10:14:21Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhaohao Lin, Weike Pan, Zhong Ming</p>
    <p><b>Summary:</b> Cross-domain sequential recommendation is an important development direction
of recommender systems. It combines the characteristics of sequential
recommender systems and cross-domain recommender systems, which can capture the
dynamic preferences of users and alleviate the problem of cold-start users.
However, in recent years, people pay more and more attention to their privacy.
They do not want other people to know what they just bought, what videos they
just watched, and where they just came from. How to protect the users' privacy
has become an urgent problem to be solved. In this paper, we propose a novel
privacy-preserving cross-domain sequential recommender system (PriCDSR), which
can provide users with recommendation services while preserving their privacy
at the same time. Specifically, we define a new differential privacy on the
data, taking into account both the ID information and the order information.
Then, we design a random mechanism that satisfies this differential privacy and
provide its theoretical proof. Our PriCDSR is a non-invasive method that can
adopt any cross-domain sequential recommender system as a base model without
any modification to it. To the best of our knowledge, our PriCDSR is the first
work to investigate privacy issues in cross-domain sequential recommender
systems. We conduct experiments on three domains, and the results demonstrate
that our PriCDSR, despite introducing noise, still outperforms recommender
systems that only use data from a single domain.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.15221v1">Designing and Testing a Mobile Application for Collecting WhatsApp Chat
  Data While Preserving Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2024-01-26T22:18:28Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Brennan Schaffner, Archie Brohn, Jason Chee, K. J. Feng, Marshini Chetty</p>
    <p><b>Summary:</b> It is common practice for researchers to join public WhatsApp chats and
scrape their contents for analysis. However, research shows collecting data
this way contradicts user expectations and preferences, even if the data is
effectively public. To overcome these issues, we outline design considerations
for collecting WhatsApp chat data with improved user privacy by heightening
user control and oversight of data collection and taking care to minimize the
data researchers collect and process off a user's device. We refer to these
design principles as User-Centered Data Sharing (UCDS). To evaluate our UCDS
principles, we implemented a mobile application representing one possible
instance of these improved data collection techniques and evaluated the
viability of using the app to collect WhatsApp chat data. Second, we surveyed
WhatsApp users to gather user perceptions on common existing WhatsApp data
collection methods as well as UCDS methods. Our results show that we were able
to glean similar informative insights into WhatsApp chats using UCDS principles
in our prototype app to common, less privacy-preserving methods. Our survey
showed that methods following the UCDS principles are preferred by users
because they offered users more control over the data collection process.
Future user studies could further expand upon UCDS principles to overcome
complications of researcher-to-group communication in research on WhatsApp
chats and evaluate these principles in other data sharing contexts.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.14884v1">P3LS: Partial Least Squares under Privacy Preservation</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-01-26T14:08:43Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Du Nguyen Duy, Ramin Nikzad-Langerodi</p>
    <p><b>Summary:</b> Modern manufacturing value chains require intelligent orchestration of
processes across company borders in order to maximize profits while fostering
social and environmental sustainability. However, the implementation of
integrated, systems-level approaches for data-informed decision-making along
value chains is currently hampered by privacy concerns associated with
cross-organizational data exchange and integration. We here propose
Privacy-Preserving Partial Least Squares (P3LS) regression, a novel federated
learning technique that enables cross-organizational data integration and
process modeling with privacy guarantees. P3LS involves a singular value
decomposition (SVD) based PLS algorithm and employs removable, random masks
generated by a trusted authority in order to protect the privacy of the data
contributed by each data holder. We demonstrate the capability of P3LS to
vertically integrate process data along a hypothetical value chain consisting
of three parties and to improve the prediction performance on several
process-related key performance indicators. Furthermore, we show the numerical
equivalence of P3LS and PLS model components on simulated data and provide a
thorough privacy analysis of the former. Moreover, we propose a mechanism for
determining the relevance of the contributed data to the problem being
addressed, thus creating a basis for quantifying the contribution of
participants.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.14840v1">GuardML: Efficient Privacy-Preserving Machine Learning Services Through
  Hybrid Homomorphic Encryption</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-01-26T13:12:52Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Eugene Frimpong, Khoa Nguyen, Mindaugas Budzys, Tanveer Khan, Antonis Michalas</p>
    <p><b>Summary:</b> Machine Learning (ML) has emerged as one of data science's most
transformative and influential domains. However, the widespread adoption of ML
introduces privacy-related concerns owing to the increasing number of malicious
attacks targeting ML models. To address these concerns, Privacy-Preserving
Machine Learning (PPML) methods have been introduced to safeguard the privacy
and security of ML models. One such approach is the use of Homomorphic
Encryption (HE). However, the significant drawbacks and inefficiencies of
traditional HE render it impractical for highly scalable scenarios.
Fortunately, a modern cryptographic scheme, Hybrid Homomorphic Encryption
(HHE), has recently emerged, combining the strengths of symmetric cryptography
and HE to surmount these challenges. Our work seeks to introduce HHE to ML by
designing a PPML scheme tailored for end devices. We leverage HHE as the
fundamental building block to enable secure learning of classification outcomes
over encrypted data, all while preserving the privacy of the input data and ML
model. We demonstrate the real-world applicability of our construction by
developing and evaluating an HHE-based PPML application for classifying heart
disease based on sensitive ECG data. Notably, our evaluations revealed a slight
reduction in accuracy compared to inference on plaintext data. Additionally,
both the analyst and end devices experience minimal communication and
computation costs, underscoring the practical viability of our approach. The
successful integration of HHE into PPML provides a glimpse into a more secure
and privacy-conscious future for machine learning on relatively constrained end
devices.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.14792v1">Deep Variational Privacy Funnel: General Modeling with Applications in
  Face Recognition</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> 
  <p><b>Published on:</b> 2024-01-26T11:32:53Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Behrooz Razeghi, Parsa Rahimi, Sébastien Marcel</p>
    <p><b>Summary:</b> In this study, we harness the information-theoretic Privacy Funnel (PF) model
to develop a method for privacy-preserving representation learning using an
end-to-end training framework. We rigorously address the trade-off between
obfuscation and utility. Both are quantified through the logarithmic loss, a
measure also recognized as self-information loss. This exploration deepens the
interplay between information-theoretic privacy and representation learning,
offering substantive insights into data protection mechanisms for both
discriminative and generative models. Importantly, we apply our model to
state-of-the-art face recognition systems. The model demonstrates adaptability
across diverse inputs, from raw facial images to both derived or refined
embeddings, and is competent in tasks such as classification, reconstruction,
and generation.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.17319v1">Decentralized Federated Learning: A Survey on Security and Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> 
  <p><b>Published on:</b> 2024-01-25T23:35:47Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ehsan Hallaji, Roozbeh Razavi-Far, Mehrdad Saif, Boyu Wang, Qiang Yang</p>
    <p><b>Summary:</b> Federated learning has been rapidly evolving and gaining popularity in recent
years due to its privacy-preserving features, among other advantages.
Nevertheless, the exchange of model updates and gradients in this architecture
provides new attack surfaces for malicious users of the network which may
jeopardize the model performance and user and data privacy. For this reason,
one of the main motivations for decentralized federated learning is to
eliminate server-related threats by removing the server from the network and
compensating for it through technologies such as blockchain. However, this
advantage comes at the cost of challenging the system with new privacy threats.
Thus, performing a thorough security analysis in this new paradigm is
necessary. This survey studies possible variations of threats and adversaries
in decentralized federated learning and overviews the potential defense
mechanisms. Trustability and verifiability of decentralized federated learning
are also considered in this study.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.14549v1">Privacy-preserving Quantile Treatment Effect Estimation for Randomized
  Controlled Trials</a></h3>
  
  <p><b>Published on:</b> 2024-01-25T22:35:33Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Leon Yao, Paul Yiming Li, Jiannan Lu</p>
    <p><b>Summary:</b> In accordance with the principle of "data minimization", many internet
companies are opting to record less data. However, this is often at odds with
A/B testing efficacy. For experiments with units with multiple observations,
one popular data minimizing technique is to aggregate data for each unit.
However, exact quantile estimation requires the full observation-level data. In
this paper, we develop a method for approximate Quantile Treatment Effect (QTE)
analysis using histogram aggregation. In addition, we can also achieve formal
privacy guarantees using differential privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.14436v1">Trust model of privacy-concerned, emotionally-aware agents in a
  cooperative logistics problem</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Multiagent Systems-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-01-25T13:31:43Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> J. Carbo, J. M. Molina</p>
    <p><b>Summary:</b> In this paper we propose a trust model to be used into a hypothetical mixed
environment where humans and unmanned vehicles cooperate. We address the
inclusion of emotions inside a trust model in a coherent way to the practical
approaches to the current psychology theories. The most innovative contribution
is how privacy issues play a role in the cooperation decisions of the emotional
trust model. Both, emotions and trust have been cognitively modeled and managed
with the Beliefs, Desires and Intentions (BDI) paradigm into autonomous agents
implemented in GAML (the programming language of GAMA agent platform) that
communicates using the IEEE FIPA standard. The trusting behaviour of these
emotional agents is tested in a cooperative logistics problem where: agents
have to move objects to destinations and some of the objects and places have
privacy issues. The execution of simulations of this logistic problem shows how
emotions and trust contribute to improve the performance of agents in terms of
both, time savings and privacy protection</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.13952v1">Randomized Response with Gradual Release of Privacy Budget</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-01-25T05:18:47Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mingen Pan</p>
    <p><b>Summary:</b> An algorithm is developed to gradually relax the Differential Privacy (DP)
guarantee of a randomized response. The output from each relaxation maintains
the same probability distribution as a standard randomized response with the
equivalent DP guarantee, ensuring identical utility as the standard approach.
The entire relaxation process is proven to have the same DP guarantee as the
most recent relaxed guarantee.
  The DP relaxation algorithm is adaptable to any Local Differential Privacy
(LDP) mechanisms relying on randomized response. It has been seamlessly
integrated into RAPPOR, an LDP crowdsourcing string-collecting tool, to
optimize the utility of estimating the frequency of collected data.
Additionally, it facilitates the relaxation of the DP guarantee for mean
estimation based on randomized response. Finally, numerical experiments have
been conducted to validate the utility and DP guarantee of the algorithm.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.13848v1">A V2X-based Privacy Preserving Federated Measuring and Learning System</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">   
  <p><b>Published on:</b> 2024-01-24T23:11:11Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Levente Alekszejenkó, Tadeusz Dobrowiecki</p>
    <p><b>Summary:</b> Future autonomous vehicles (AVs) will use a variety of sensors that generate
a vast amount of data. Naturally, this data not only serves self-driving
algorithms; but can also assist other vehicles or the infrastructure in
real-time decision-making. Consequently, vehicles shall exchange their
measurement data over Vehicle-to-Everything (V2X) technologies. Moreover,
predicting the state of the road network might be beneficial too. With such a
prediction, we might mitigate road congestion, balance parking lot usage, or
optimize the traffic flow. That would decrease transportation costs as well as
reduce its environmental impact.
  In this paper, we propose a federated measurement and learning system that
provides real-time data to fellow vehicles over Vehicle-to-Vehicle (V2V)
communication while also operating a federated learning (FL) scheme over the
Vehicle-to-Network (V2N) link to create a predictive model of the
transportation network. As we are yet to have real-world AV data, we model it
with a non-IID (independent and identically distributed) dataset to evaluate
the capabilities of the proposed system in terms of performance and privacy.
Results indicate that the proposed FL scheme improves learning performance and
prevents eavesdropping at the aggregator server side.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.13386v1">Privacy-Preserving Face Recognition in Hybrid Frequency-Color Domain</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-01-24T11:27:32Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Dong Han, Yong Li, Joachim Denzler</p>
    <p><b>Summary:</b> Face recognition technology has been deployed in various real-life
applications. The most sophisticated deep learning-based face recognition
systems rely on training millions of face images through complex deep neural
networks to achieve high accuracy. It is quite common for clients to upload
face images to the service provider in order to access the model inference.
However, the face image is a type of sensitive biometric attribute tied to the
identity information of each user. Directly exposing the raw face image to the
service provider poses a threat to the user's privacy. Current
privacy-preserving approaches to face recognition focus on either concealing
visual information on model input or protecting model output face embedding.
The noticeable drop in recognition accuracy is a pitfall for most methods. This
paper proposes a hybrid frequency-color fusion approach to reduce the input
dimensionality of face recognition in the frequency domain. Moreover, sparse
color information is also introduced to alleviate significant accuracy
degradation after adding differential privacy noise. Besides, an
identity-specific embedding mapping scheme is applied to protect original face
embedding by enlarging the distance among identities. Lastly, secure multiparty
computation is implemented for safely computing the embedding distance during
model inference. The proposed method performs well on multiple widely used
verification datasets. Moreover, it has around 2.6% to 4.2% higher accuracy
than the state-of-the-art in the 1:N verification scenario.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.13327v1">Generating Synthetic Health Sensor Data for Privacy-Preserving Wearable
  Stress Detection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-01-24T09:44:57Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Lucas Lange, Nils Wenzlitschke, Erhard Rahm</p>
    <p><b>Summary:</b> Smartwatch health sensor data is increasingly utilized in smart health
applications and patient monitoring, including stress detection. However, such
medical data often comprises sensitive personal information and is
resource-intensive to acquire for research purposes. In response to this
challenge, we introduce the privacy-aware synthetization of multi-sensor
smartwatch health readings related to moments of stress. Our method involves
the generation of synthetic sequence data through Generative Adversarial
Networks (GANs), coupled with the implementation of Differential Privacy (DP)
safeguards for protecting patient information during model training. To ensure
the integrity of our synthetic data, we employ a range of quality assessments
and monitor the plausibility between synthetic and original data. To test the
usefulness, we create private machine learning models on a commonly used,
albeit small, stress detection dataset, exploring strategies for enhancing the
existing data foundation with our synthetic data. Through our GAN-based
augmentation methods, we observe improvements in model performance, both in
non-private (0.45% F1) and private (11.90-15.48% F1) training scenarios. We
underline the potential of differentially private synthetic data in optimizing
utility-privacy trade-offs, especially with limited availability of real
training samples.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.12436v1">Wasserstein Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-01-23T02:08:20Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chengyi Yang, Jiayin Qi, Aimin Zhou</p>
    <p><b>Summary:</b> Differential privacy (DP) has achieved remarkable results in the field of
privacy-preserving machine learning. However, existing DP frameworks do not
satisfy all the conditions for becoming metrics, which prevents them from
deriving better basic private properties and leads to exaggerated values on
privacy budgets. We propose Wasserstein differential privacy (WDP), an
alternative DP framework to measure the risk of privacy leakage, which
satisfies the properties of symmetry and triangle inequality. We show and prove
that WDP has 13 excellent properties, which can be theoretical supports for the
better performance of WDP than other DP frameworks. In addition, we derive a
general privacy accounting method called Wasserstein accountant, which enables
WDP to be applied in stochastic gradient descent (SGD) scenarios containing
sub-sampling. Experiments on basic mechanisms, compositions and deep learning
show that the privacy budgets obtained by Wasserstein accountant are relatively
stable and less influenced by order. Moreover, the overestimation on privacy
budgets can be effectively alleviated. The code is available at
https://github.com/Hifipsysta/WDP.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.12393v1">A Learning-based Declarative Privacy-Preserving Framework for Federated
  Data Management</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-01-22T22:50:59Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Hong Guan, Summer Gautier, Deepti Gupta, Rajan Hari Ambrish, Yancheng Wang, Harsha Lakamsani, Dhanush Giriyan, Saajan Maslanka, Chaowei Xiao, Yingzhen Yang, Jia Zou</p>
    <p><b>Summary:</b> It is challenging to balance the privacy and accuracy for federated query
processing over multiple private data silos. In this work, we will demonstrate
an end-to-end workflow for automating an emerging privacy-preserving technique
that uses a deep learning model trained using the Differentially-Private
Stochastic Gradient Descent (DP-SGD) algorithm to replace portions of actual
data to answer a query. Our proposed novel declarative privacy-preserving
workflow allows users to specify "what private information to protect" rather
than "how to protect". Under the hood, the system automatically chooses
query-model transformation plans as well as hyper-parameters. At the same time,
the proposed workflow also allows human experts to review and tune the selected
privacy-preserving mechanism for audit/compliance, and optimization purposes.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.12391v1">Approximation of Pufferfish Privacy for Gaussian Priors</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2024-01-22T22:43:38Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ni Ding</p>
    <p><b>Summary:</b> This paper studies how to approximate pufferfish privacy when the adversary's
prior belief of the published data is Gaussian distributed. Using Monge's
optimal transport plan, we show that $(\epsilon, \delta)$-pufferfish privacy is
attained if the additive Laplace noise is calibrated to the differences in mean
and variance of the Gaussian distributions conditioned on every discriminative
secret pair. A typical application is the private release of the summation (or
average) query, for which sufficient conditions are derived for approximating
$\epsilon$-statistical indistinguishability in individual's sensitive data. The
result is then extended to arbitrary prior beliefs trained by Gaussian mixture
models (GMMs): calibrating Laplace noise to a convex combination of differences
in mean and variance between Gaussian components attains
$(\epsilon,\delta)$-pufferfish privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.11983v1">Lightweight Protection for Privacy in Offloaded Speech Understanding</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Sound-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2024-01-22T14:36:01Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Dongqi Cai</p>
    <p><b>Summary:</b> Speech is a common input method for mobile embedded devices, but cloud-based
speech recognition systems pose privacy risks. Disentanglement-based encoders,
designed to safeguard user privacy by filtering sensitive information from
speech signals, unfortunately require substantial memory and computational
resources, which limits their use in less powerful devices. To overcome this,
we introduce a novel system, XXX, optimized for such devices. XXX is built on
the insight that speech understanding primarily relies on understanding the
entire utterance's long-term dependencies, while privacy concerns are often
linked to short-term details. Therefore, XXX focuses on selectively masking
these short-term elements, preserving the quality of long-term speech
understanding. The core of XXX is an innovative differential mask generator,
grounded in interpretable learning, which fine-tunes the masking process. We
tested XXX on the STM32H7 microcontroller, assessing its performance in various
potential attack scenarios. The results show that XXX maintains speech
understanding accuracy and privacy at levels comparable to existing encoders,
but with a significant improvement in efficiency, achieving up to 53.3$\times$
faster processing and a 134.1$\times$ smaller memory footprint.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.11857v1">Adversarial speech for voice privacy protection from Personalized Speech
  generation</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Sound-D91E36">
  <p><b>Published on:</b> 2024-01-22T11:26:59Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shihao Chen, Liping Chen, Jie Zhang, KongAik Lee, Zhenhua Ling, Lirong Dai</p>
    <p><b>Summary:</b> The rapid progress in personalized speech generation technology, including
personalized text-to-speech (TTS) and voice conversion (VC), poses a challenge
in distinguishing between generated and real speech for human listeners,
resulting in an urgent demand in protecting speakers' voices from malicious
misuse. In this regard, we propose a speaker protection method based on
adversarial attacks. The proposed method perturbs speech signals by minimally
altering the original speech while rendering downstream speech generation
models unable to accurately generate the voice of the target speaker. For
validation, we employ the open-source pre-trained YourTTS model for speech
generation and protect the target speaker's speech in the white-box scenario.
Automatic speaker verification (ASV) evaluations were carried out on the
generated speech as the assessment of the voice protection capability. Our
experimental results show that we successfully perturbed the speaker encoder of
the YourTTS model using the gradient-based I-FGSM adversarial perturbation
method. Furthermore, the adversarial perturbation is effective in preventing
the YourTTS model from generating the speech of the target speaker. Audio
samples can be found in
https://voiceprivacy.github.io/Adeversarial-Speech-with-YourTTS.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.11836v1">Privacy-Preserving Data Fusion for Traffic State Estimation: A Vertical
  Federated Learning Approach</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36"> 
  <p><b>Published on:</b> 2024-01-22T10:52:22Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Qiqing Wang, Kaidi Yang</p>
    <p><b>Summary:</b> This paper proposes a privacy-preserving data fusion method for traffic state
estimation (TSE). Unlike existing works that assume all data sources to be
accessible by a single trusted party, we explicitly address data privacy
concerns that arise in the collaboration and data sharing between multiple data
owners, such as municipal authorities (MAs) and mobility providers (MPs). To
this end, we propose a novel vertical federated learning (FL) approach, FedTSE,
that enables multiple data owners to collaboratively train and apply a TSE
model without having to exchange their private data. To enhance the
applicability of the proposed FedTSE in common TSE scenarios with limited
availability of ground-truth data, we further propose a privacy-preserving
physics-informed FL approach, i.e., FedTSE-PI, that integrates traffic models
into FL. Real-world data validation shows that the proposed methods can protect
privacy while yielding similar accuracy to the oracle method without privacy
considerations.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.11735v1">zkLogin: Privacy-Preserving Blockchain Authentication with Existing
  Credentials</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-01-22T07:23:58Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Foteini Baldimtsi, Konstantinos Kryptos Chalkias, Yan Ji, Jonas Lindstrøm, Deepak Maram, Ben Riva, Arnab Roy, Mahdi Sedaghat, Joy Wang</p>
    <p><b>Summary:</b> For many users, a private key based wallet serves as the primary entry point
to blockchains. Commonly recommended wallet authentication methods, such as
mnemonics or hardware wallets, can be cumbersome. This difficulty in user
onboarding has significantly hindered the adoption of blockchain-based
applications.
  We develop zkLogin, a novel technique that leverages identity tokens issued
by popular platforms (any OpenID Connect enabled platform e.g. Google,
Facebook, etc.) to authenticate transactions. At the heart of zkLogin lies a
signature scheme allowing the signer to \textit{sign using their existing
OpenID accounts} and nothing else. This improves the user experience
significantly as users do not need to remember a new secret and can reuse their
existing accounts.
  zkLogin provides strong security and privacy guarantees. By design, zkLogin
builds on top of the underlying platform's authentication mechanisms, and
derives its security from there. Unlike prior related works however, zkLogin
avoids the use of additional trusted parties (e.g., trusted hardware or
oracles) for its security guarantees. zkLogin leverages zero-knowledge proofs
(ZKP) to ensure that the link between a user's off-chain and on-chain
identities is hidden, even from the platform itself.
  We have implemented and deployed zkLogin on the Sui blockchain as an
alternative to traditional digital signature-based addresses. Due to the ease
of web3 on-boarding just with social login, without requiring mnemonics, many
hundreds of thousands zkLogin accounts have already been generated in various
industries such as gaming, DeFi, direct payments, NFT collections, ride
sharing, sports racing and many more.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.11592v1">Differential Privacy in Hierarchical Federated Learning: A Formal
  Analysis and Evaluation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2024-01-21T20:46:21Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Frank Po-Chen Lin, Christopher Brinton</p>
    <p><b>Summary:</b> While federated learning (FL) eliminates the transmission of raw data over a
network, it is still vulnerable to privacy breaches from the communicated model
parameters. In this work, we formalize Differentially Private Hierarchical
Federated Learning (DP-HFL), a DP-enhanced FL methodology that seeks to improve
the privacy-utility tradeoff inherent in FL. Building upon recent proposals for
Hierarchical Differential Privacy (HDP), one of the key concepts of DP-HFL is
adapting DP noise injection at different layers of an established FL hierarchy
-- edge devices, edge servers, and cloud servers -- according to the trust
models within particular subnetworks. We conduct a comprehensive analysis of
the convergence behavior of DP-HFL, revealing conditions on parameter tuning
under which the model training process converges sublinearly to a stationarity
gap, with this gap depending on the network hierarchy, trust model, and target
privacy level. Subsequent numerical evaluations demonstrate that DP-HFL obtains
substantial improvements in convergence speed over baselines for different
privacy budgets, and validate the impact of network configuration on training.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.11305v1">Progress in Privacy Protection: A Review of Privacy Preserving
  Techniques in Recommender Systems, Edge Computing, and Cloud Computing</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2024-01-20T19:32:56Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Syed Raza Bashir, Shaina Raza, Vojislav Misic</p>
    <p><b>Summary:</b> As digital technology evolves, the increasing use of connected devices brings
both challenges and opportunities in the areas of mobile crowdsourcing, edge
computing, and recommender systems. This survey focuses on these dynamic
fields, emphasizing the critical need for privacy protection in our
increasingly data-oriented world. It explores the latest trends in these
interconnected areas, with a special emphasis on privacy and data security. Our
method involves an in-depth analysis of various academic works, which helps us
to gain a comprehensive understanding of these sectors and their shifting focus
towards privacy concerns. We present new insights and marks a significant
advancement in addressing privacy issues within these technologies. The survey
is a valuable resource for researchers, industry practitioners, and policy
makers, offering an extensive overview of these fields and their related
privacy challenges, catering to a wide audience in the modern digital era.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.11249v1">Evaluating if trust and personal information privacy concerns are
  barriers to using health insurance that explicitly utilizes AI</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> 
  <p><b>Published on:</b> 2024-01-20T15:02:56Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Alex Zarifis, Peter Kawalek, Aida Azadegan</p>
    <p><b>Summary:</b> Trust and privacy have emerged as significant concerns in online
transactions. Sharing information on health is especially sensitive but it is
necessary for purchasing and utilizing health insurance. Evidence shows that
consumers are increasingly comfortable with technology in place of humans, but
the expanding use of AI potentially changes this. This research explores
whether trust and privacy concern are barriers to the adoption of AI in health
insurance. Two scenarios are compared: The first scenario has limited AI that
is not in the interface and its presence is not explicitly revealed to the
consumer. In the second scenario there is an AI interface and AI evaluation,
and this is explicitly revealed to the consumer. The two scenarios were modeled
and compared using SEM PLS-MGA. The findings show that trust is significantly
lower in the second scenario where AI is visible. Privacy concerns are higher
with AI but the difference is not statistically significant within the model.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.11225v1">Protecting Personalized Trajectory with Differential Privacy under
  Temporal Correlations</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-01-20T12:59:08Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mingge Cao, Haopeng Zhu, Minghui Min, Yulu Li, Shiyin Li, Hongliang Zhang, Zhu Han</p>
    <p><b>Summary:</b> Location-based services (LBSs) in vehicular ad hoc networks (VANETs) offer
users numerous conveniences. However, the extensive use of LBSs raises concerns
about the privacy of users' trajectories, as adversaries can exploit temporal
correlations between different locations to extract personal information.
Additionally, users have varying privacy requirements depending on the time and
location. To address these issues, this paper proposes a personalized
trajectory privacy protection mechanism (PTPPM). This mechanism first uses the
temporal correlation between trajectory locations to determine the possible
location set for each time instant. We identify a protection location set (PLS)
for each location by employing the Hilbert curve-based minimum distance search
algorithm. This approach incorporates the complementary features of
geo-indistinguishability and distortion privacy. We put forth a novel
Permute-and-Flip mechanism for location perturbation, which maps its initial
application in data publishing privacy protection to a location perturbation
mechanism. This mechanism generates fake locations with smaller perturbation
distances while improving the balance between privacy and quality of service
(QoS). Simulation results show that our mechanism outperforms the benchmark by
providing enhanced privacy protection while meeting user's QoS requirements.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.11089v1">FedRKG: A Privacy-preserving Federated Recommendation Framework via
  Knowledge Graph Enhancement</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Retrieval-5BC0EB">
  <p><b>Published on:</b> 2024-01-20T02:38:21Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Dezhong Yao, Tongtong Liu, Qi Cao, Hai Jin</p>
    <p><b>Summary:</b> Federated Learning (FL) has emerged as a promising approach for preserving
data privacy in recommendation systems by training models locally. Recently,
Graph Neural Networks (GNN) have gained popularity in recommendation tasks due
to their ability to capture high-order interactions between users and items.
However, privacy concerns prevent the global sharing of the entire user-item
graph. To address this limitation, some methods create pseudo-interacted items
or users in the graph to compensate for missing information for each client.
Unfortunately, these methods introduce random noise and raise privacy concerns.
In this paper, we propose FedRKG, a novel federated recommendation system,
where a global knowledge graph (KG) is constructed and maintained on the server
using publicly available item information, enabling higher-order user-item
interactions. On the client side, a relation-aware GNN model leverages diverse
KG relationships. To protect local interaction items and obscure gradients, we
employ pseudo-labeling and Local Differential Privacy (LDP). Extensive
experiments conducted on three real-world datasets demonstrate the competitive
performance of our approach compared to centralized algorithms while ensuring
privacy preservation. Moreover, FedRKG achieves an average accuracy improvement
of 4% compared to existing federated learning baselines.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.10765v2">Starlit: Privacy-Preserving Federated Learning to Enhance Financial
  Fraud Detection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-01-19T15:37:11Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Aydin Abadi, Bradley Doyle, Francesco Gini, Kieron Guinamard, Sasi Kumar Murakonda, Jack Liddell, Paul Mellor, Steven J. Murdoch, Mohammad Naseri, Hector Page, George Theodorakopoulos, Suzanne Weller</p>
    <p><b>Summary:</b> Federated Learning (FL) is a data-minimization approach enabling
collaborative model training across diverse clients with local data, avoiding
direct data exchange. However, state-of-the-art FL solutions to identify
fraudulent financial transactions exhibit a subset of the following
limitations. They (1) lack a formal security definition and proof, (2) assume
prior freezing of suspicious customers' accounts by financial institutions
(limiting the solutions' adoption), (3) scale poorly, involving either $O(n^2)$
computationally expensive modular exponentiation (where $n$ is the total number
of financial institutions) or highly inefficient fully homomorphic encryption,
(4) assume the parties have already completed the identity alignment phase,
hence excluding it from the implementation, performance evaluation, and
security analysis, and (5) struggle to resist clients' dropouts. This work
introduces Starlit, a novel scalable privacy-preserving FL mechanism that
overcomes these limitations. It has various applications, such as enhancing
financial fraud detection, mitigating terrorism, and enhancing digital health.
We implemented Starlit and conducted a thorough performance analysis using
synthetic data from a key player in global financial transactions. The
evaluation indicates Starlit's scalability, efficiency, and accuracy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.09604v1">MedBlindTuner: Towards Privacy-preserving Fine-tuning on Biomedical
  Images with Transformers and Fully Homomorphic Encryption</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-01-17T21:30:22Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Prajwal Panzade, Daniel Takabi, Zhipeng Cai</p>
    <p><b>Summary:</b> Advancements in machine learning (ML) have significantly revolutionized
medical image analysis, prompting hospitals to rely on external ML services.
However, the exchange of sensitive patient data, such as chest X-rays, poses
inherent privacy risks when shared with third parties. Addressing this concern,
we propose MedBlindTuner, a privacy-preserving framework leveraging fully
homomorphic encryption (FHE) and a data-efficient image transformer (DEiT).
MedBlindTuner enables the training of ML models exclusively on FHE-encrypted
medical images. Our experimental evaluation demonstrates that MedBlindTuner
achieves comparable accuracy to models trained on non-encrypted images,
offering a secure solution for outsourcing ML computations while preserving
patient data privacy. To the best of our knowledge, this is the first work that
uses data-efficient image transformers and fully homomorphic encryption in this
domain.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.09519v1">Privacy Engineering in Smart Home (SH) Systems: A Comprehensive Privacy
  Threat Analysis and Risk Management Approach</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-01-17T17:34:52Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Emmanuel Dare Alalade, Mohammed Mahyoub, Ashraf Matrawy</p>
    <p><b>Summary:</b> Addressing trust concerns in Smart Home (SH) systems is imperative due to the
limited study on preservation approaches that focus on analyzing and evaluating
privacy threats for effective risk management. While most research focuses
primarily on user privacy, device data privacy, especially identity privacy, is
almost neglected, which can significantly impact overall user privacy within
the SH system. To this end, our study incorporates privacy engineering (PE)
principles in the SH system that consider user and device data privacy. We
start with a comprehensive reference model for a typical SH system. Based on
the initial stage of LINDDUN PRO for the PE framework, we present a data flow
diagram (DFD) based on a typical SH reference model to better understand SH
system operations. To identify potential areas of privacy threat and perform a
privacy threat analysis (PTA), we employ the LINDDUN PRO threat model. Then, a
privacy impact assessment (PIA) was carried out to implement privacy risk
management by prioritizing privacy threats based on their likelihood of
occurrence and potential consequences. Finally, we suggest possible privacy
enhancement techniques (PETs) that can mitigate some of these threats. The
study aims to elucidate the main threats to privacy, associated risks, and
effective prioritization of privacy control in SH systems. The outcomes of this
study are expected to benefit SH stakeholders, including vendors, cloud
providers, users, researchers, and regulatory bodies in the SH systems domain.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.08935v1">Privacy Protected Contactless Cardio-respiratory Monitoring using
  Defocused Cameras during Sleep</a></h3>
  
  <p><b>Published on:</b> 2024-01-17T03:05:24Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yingen Zhu, Jia Huang, Hongzhou Lu, Wenjin Wang</p>
    <p><b>Summary:</b> The monitoring of vital signs such as heart rate (HR) and respiratory rate
(RR) during sleep is important for the assessment of sleep quality and
detection of sleep disorders. Camera-based HR and RR monitoring gained
popularity in sleep monitoring in recent years. However, they are all facing
with serious privacy issues when using a video camera in the sleeping scenario.
In this paper, we propose to use the defocused camera to measure vital signs
from optically blurred images, which can fundamentally eliminate the privacy
invasion as face is difficult to be identified in obtained blurry images. A
spatial-redundant framework involving living-skin detection is used to extract
HR and RR from the defocused camera in NIR, and a motion metric is designed to
exclude outliers caused by body motions. In the benchmark, the overall Mean
Absolute Error (MAE) for HR measurement is 4.4 bpm, for RR measurement is 5.9
bpm. Both have quality drops as compared to the measurement using a focused
camera, but the degradation in HR is much less, i.e. HR measurement has strong
correlation with the reference ($R \geq 0.90$). Preliminary experiments suggest
that it is feasible to use a defocused camera for cardio-respiratory monitoring
while protecting the privacy. Further improvement is needed for robust RR
measurement, such as by PPG-modulation based RR extraction.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.08458v1">Security and Privacy Issues and Solutions in Federated Learning for
  Digital Healthcare</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-01-16T16:07:53Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Hyejun Jeong, Tai-Myoung Chung</p>
    <p><b>Summary:</b> The advent of Federated Learning has enabled the creation of a
high-performing model as if it had been trained on a considerable amount of
data. A multitude of participants and a server cooperatively train a model
without the need for data disclosure or collection. The healthcare industry,
where security and privacy are paramount, can substantially benefit from this
new learning paradigm, as data collection is no longer feasible due to
stringent data policies. Nonetheless, unaddressed challenges and insufficient
attack mitigation are hampering its adoption. Attack surfaces differ from
traditional centralized learning in that the server and clients communicate
between each round of training. In this paper, we thus present vulnerabilities,
attacks, and defenses based on the widened attack surfaces, as well as suggest
promising new research directions toward a more robust FL.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.08723v1">HierSFL: Local Differential Privacy-aided Split Federated Learning in
  Mobile Edge Computing</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-01-16T09:34:10Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Minh K. Quan, Dinh C. Nguyen, Van-Dinh Nguyen, Mayuri Wijayasundara, Sujeeva Setunge, Pubudu N. Pathirana</p>
    <p><b>Summary:</b> Federated Learning is a promising approach for learning from user data while
preserving data privacy. However, the high requirements of the model training
process make it difficult for clients with limited memory or bandwidth to
participate. To tackle this problem, Split Federated Learning is utilized,
where clients upload their intermediate model training outcomes to a cloud
server for collaborative server-client model training. This methodology
facilitates resource-constrained clients' participation in model training but
also increases the training time and communication overhead. To overcome these
limitations, we propose a novel algorithm, called Hierarchical Split Federated
Learning (HierSFL), that amalgamates models at the edge and cloud phases,
presenting qualitative directives for determining the best aggregation
timeframes to reduce computation and communication expenses. By implementing
local differential privacy at the client and edge server levels, we enhance
privacy during local model parameter updates. Our experiments using CIFAR-10
and MNIST datasets show that HierSFL outperforms standard FL approaches with
better training accuracy, training time, and communication-computing
trade-offs. HierSFL offers a promising solution to mobile edge computing's
challenges, ultimately leading to faster content delivery and improved mobile
service quality.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.08224v4">Privacy Preserving Adaptive Experiment Design</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-01-16T09:22:12Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jiachun Li, Kaining Shi, David Simchi-Levi</p>
    <p><b>Summary:</b> Adaptive experiment is widely adopted to estimate conditional average
treatment effect (CATE) in clinical trials and many other scenarios. While the
primary goal in experiment is to maximize estimation accuracy, due to the
imperative of social welfare, it's also crucial to provide treatment with
superior outcomes to patients, which is measured by regret in contextual bandit
framework. These two objectives often lead to contrast optimal allocation
mechanism. Furthermore, privacy concerns arise in clinical scenarios containing
sensitive data like patients health records. Therefore, it's essential for the
treatment allocation mechanism to incorporate robust privacy protection
measures. In this paper, we investigate the tradeoff between loss of social
welfare and statistical power in contextual bandit experiment. We propose a
matched upper and lower bound for the multi-objective optimization problem, and
then adopt the concept of Pareto optimality to mathematically characterize the
optimality condition. Furthermore, we propose differentially private algorithms
which still matches the lower bound, showing that privacy is "almost free".
Additionally, we derive the asymptotic normality of the estimator, which is
essential in statistical inference and hypothesis testing.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.08038v1">Calpric: Inclusive and Fine-grain Labeling of Privacy Policies with
  Crowdsourcing and Active Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-01-16T01:27:26Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Wenjun Qiu, David Lie, Lisa Austin</p>
    <p><b>Summary:</b> A significant challenge to training accurate deep learning models on privacy
policies is the cost and difficulty of obtaining a large and comprehensive set
of training data. To address these challenges, we present Calpric , which
combines automatic text selection and segmentation, active learning and the use
of crowdsourced annotators to generate a large, balanced training set for
privacy policies at low cost. Automated text selection and segmentation
simplifies the labeling task, enabling untrained annotators from crowdsourcing
platforms, like Amazon's Mechanical Turk, to be competitive with trained
annotators, such as law students, and also reduces inter-annotator agreement,
which decreases labeling cost. Having reliable labels for training enables the
use of active learning, which uses fewer training samples to efficiently cover
the input space, further reducing cost and improving class and data category
balance in the data set. The combination of these techniques allows Calpric to
produce models that are accurate over a wider range of data categories, and
provide more detailed, fine-grain labels than previous work. Our crowdsourcing
process enables Calpric to attain reliable labeled data at a cost of roughly
$0.92-$1.71 per labeled text segment. Calpric 's training process also
generates a labeled data set of 16K privacy policy text segments across 9 Data
categories with balanced positive and negative samples.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.08037v1">Understanding factors behind IoT privacy -- A user's perspective on RF
  sensors</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2024-01-16T01:13:14Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Akash Deep Singh, Brian Wang, Luis Garcia, Xiang Chen, Mani Srivastava</p>
    <p><b>Summary:</b> While IoT sensors in physical spaces have provided utility and comfort in our
lives, their instrumentation in private and personal spaces has led to growing
concerns regarding privacy. The existing notion behind IoT privacy is that the
sensors whose data can easily be understood and interpreted by humans (such as
cameras) are more privacy-invasive than sensors that are not
human-understandable, such as RF (radio-frequency) sensors. However, given
recent advancements in machine learning, we can not only make sensitive
inferences on RF data but also translate between modalities. Thus, the existing
notions of privacy for IoT sensors need to be revisited. In this paper, our
goal is to understand what factors affect the privacy notions of a non-expert
user (someone who is not well-versed in privacy concepts). To this regard, we
conduct an online study of 162 participants from the USA to find out what
factors affect the privacy perception of a user regarding an RF-based device or
a sensor. Our findings show that a user's perception of privacy not only
depends upon the data collected by the sensor but also on the inferences that
can be made on that data, familiarity with the device and its form factor as
well as the control a user has over the device design and its data policies.
When the data collected by the sensor is not human-interpretable, it is the
inferences that can be made on the data and not the data itself that users care
about when making informed decisions regarding device privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.07691v1">Privacy-Aware Single-Nucleotide Polymorphisms (SNPs) using Bilinear
  Group Accumulators in Batch Mode</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-01-15T13:59:51Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> William J Buchanan, Sam Grierson, Daniel Uribe</p>
    <p><b>Summary:</b> Biometric data is often highly sensitive, and a leak of this data can lead to
serious privacy breaches. Some of the most sensitive of this type of data
relates to the usage of DNA data on individuals. A leak of this type of data
without consent could lead to privacy breaches of data protection laws. Along
with this, there have been several recent data breaches related to the leak of
DNA information, including from 23andMe and Ancestry. It is thus fundamental
that a citizen should have the right to know if their DNA data is contained
within a DNA database and ask for it to be removed if they are concerned about
its usage. This paper outlines a method of hashing the core information
contained within the data stores - known as Single-Nucleotide Polymorphisms
(SNPs) - into a bilinear group accumulator in batch mode, which can then be
searched by a trusted entity for matches. The time to create the witness proof
and to verify were measured at 0.86 ms and 10.90 ms, respectively.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.10158v1">DISTINQT: A Distributed Privacy Aware Learning Framework for QoS
  Prediction for Future Mobile and Wireless Networks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Networking and Internet Architecture-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-01-15T13:00:48Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Nikolaos Koursioumpas, Lina Magoula, Ioannis Stavrakakis, Nancy Alonistioti, M. A. Gutierrez-Estevez, Ramin Khalili</p>
    <p><b>Summary:</b> Beyond 5G and 6G networks are expected to support new and challenging use
cases and applications that depend on a certain level of Quality of Service
(QoS) to operate smoothly. Predicting the QoS in a timely manner is of high
importance, especially for safety-critical applications as in the case of
vehicular communications. Although until recent years the QoS prediction has
been carried out by centralized Artificial Intelligence (AI) solutions, a
number of privacy, computational, and operational concerns have emerged.
Alternative solutions have been surfaced (e.g. Split Learning, Federated
Learning), distributing AI tasks of reduced complexity across nodes, while
preserving the privacy of the data. However, new challenges rise when it comes
to scalable distributed learning approaches, taking into account the
heterogeneous nature of future wireless networks. The current work proposes
DISTINQT, a privacy-aware distributed learning framework for QoS prediction.
Our framework supports multiple heterogeneous nodes, in terms of data types and
model architectures, by sharing computations across them. This, enables the
incorporation of diverse knowledge into a sole learning process that will
enhance the robustness and generalization capabilities of the final QoS
prediction model. DISTINQT also contributes to data privacy preservation by
encoding any raw input data into a non-linear latent representation before any
transmission. Evaluation results showcase that our framework achieves a
statistically identical performance compared to its centralized version and an
average performance improvement of up to 65% against six state-of-the-art
centralized baseline solutions in the Tele-Operated Driving use case.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.07464v1">Quantum Privacy Aggregation of Teacher Ensembles (QPATE) for
  Privacy-preserving Quantum Machine Learning</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-01-15T04:38:06Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> William Watkins, Heehwan Wang, Sangyoon Bae, Huan-Hsin Tseng, Jiook Cha, Samuel Yen-Chi Chen, Shinjae Yoo</p>
    <p><b>Summary:</b> The utility of machine learning has rapidly expanded in the last two decades
and presents an ethical challenge. Papernot et. al. developed a technique,
known as Private Aggregation of Teacher Ensembles (PATE) to enable federated
learning in which multiple teacher models are trained on disjoint datasets.
This study is the first to apply PATE to an ensemble of quantum neural networks
(QNN) to pave a new way of ensuring privacy in quantum machine learning (QML)
models.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.07348v1">Generative AI in EU Law: Liability, Privacy, Intellectual Property, and
  Cybersecurity</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-01-14T19:16:29Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Claudio Novelli, Federico Casolari, Philipp Hacker, Giorgio Spedicato, Luciano Floridi</p>
    <p><b>Summary:</b> The advent of Generative AI, particularly through Large Language Models
(LLMs) like ChatGPT and its successors, marks a paradigm shift in the AI
landscape. Advanced LLMs exhibit multimodality, handling diverse data formats,
thereby broadening their application scope. However, the complexity and
emergent autonomy of these models introduce challenges in predictability and
legal compliance. This paper delves into the legal and regulatory implications
of Generative AI and LLMs in the European Union context, analyzing aspects of
liability, privacy, intellectual property, and cybersecurity. It critically
examines the adequacy of the existing and proposed EU legislation, including
the Artificial Intelligence Act (AIA) draft, in addressing the unique
challenges posed by Generative AI in general and LLMs in particular. The paper
identifies potential gaps and shortcomings in the legislative framework and
proposes recommendations to ensure the safe and compliant deployment of
generative models, ensuring they align with the EU's evolving digital landscape
and legal standards.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.07343v2">Privacy-Preserving Intrusion Detection in Software-defined VANET using
  Federated Learning with BERT</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-01-14T18:32:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shakil Ibne Ahsan, Phil Legg, S M Iftekharul Alam</p>
    <p><b>Summary:</b> The absence of robust security protocols renders the VANET (Vehicle ad-hoc
Networks) network open to cyber threats by compromising passengers and road
safety. Intrusion Detection Systems (IDS) are widely employed to detect network
security threats. With vehicles' high mobility on the road and diverse
environments, VANETs devise ever-changing network topologies, lack privacy and
security, and have limited bandwidth efficiency. The absence of privacy
precautions, End-to-End Encryption methods, and Local Data Processing systems
in VANET also present many privacy and security difficulties. So, assessing
whether a novel real-time processing IDS approach can be utilized for this
emerging technology is crucial. The present study introduces a novel approach
for intrusion detection using Federated Learning (FL) capabilities in
conjunction with the BERT model for sequence classification (FL-BERT). The
significance of data privacy is duly recognized. According to FL methodology,
each client has its own local model and dataset. They train their models
locally and then send the model's weights to the server. After aggregation, the
server aggregates the weights from all clients to update a global model. After
aggregation, the global model's weights are shared with the clients. This
practice guarantees the secure storage of sensitive raw data on individual
clients' devices, effectively protecting privacy. After conducting the
federated learning procedure, we assessed our models' performance using a
separate test dataset. The FL-BERT technique has yielded promising results,
opening avenues for further investigation in this particular area of research.
We reached the result of our approaches by comparing existing research works
and found that FL-BERT is more effective for privacy and security concerns. Our
results suggest that FL-BERT is a promising technique for enhancing attack
detection.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.07316v1">Finding Privacy-relevant Source Code</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Software Engineering-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-01-14T15:38:29Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Feiyang Tang, Bjarte M. Østvold</p>
    <p><b>Summary:</b> Privacy code review is a critical process that enables developers and legal
experts to ensure compliance with data protection regulations. However, the
task is challenging due to resource constraints. To address this, we introduce
the concept of privacy-relevant methods - specific methods in code that are
directly involved in the processing of personal data. We then present an
automated approach to assist in code review by identifying and categorizing
these privacy-relevant methods in source code.
  Using static analysis, we identify a set of methods based on their
occurrences in 50 commonly used libraries. We then rank these methods according
to their frequency of invocation with actual personal data in the top 30 GitHub
applications. The highest-ranked methods are the ones we designate as
privacy-relevant in practice. For our evaluation, we examined 100 open-source
applications and found that our approach identifies fewer than 5% of the
methods as privacy-relevant for personal data processing. This reduces the time
required for code reviews. Case studies on Signal Desktop and Cal.com further
validate the effectiveness of our approach in aiding code reviewers to produce
enhanced reports that facilitate compliance with privacy regulations.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.06894v1">On Coded Caching Systems with Offline Users, with and without Demand
  Privacy against Colluding Users</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2024-01-12T21:06:42Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yinbin Ma, Daniela Tuninetti</p>
    <p><b>Summary:</b> Coded caching is a technique that leverages locally cached contents at the
end users to reduce the network's peak-time communication load. Coded caching
has been shown to achieve significant performance gains compared to uncoded
schemes and is thus considered a promising technique to boost performance in
future networks by effectively trading off bandwidth for storage. The original
coded caching model introduced by Maddah-Ali and Niesen does not consider the
case where some users involved in the placement phase, may be offline during
the delivery phase. If so, the delivery may not start or it may be wasteful to
perform the delivery with fictitious demands for the offline users. In
addition, the active users may require their demand to be kept private. This
paper formally defines a coded caching system where some users are offline, and
investigates the optimal performance with and without demand privacy against
colluding users. For this novel coded caching model with offline users,
achievable and converse bounds are proposed. These bounds are shown to meet
under certain conditions, and otherwise to be to within a constant
multiplicative gap of one another. In addition, the proposed achievable schemes
have lower subpacketization and lower load compared to baseline schemes (that
trivially extend known schemes so as to accommodate for privacy) in some memory
regimes.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.06883v1">Scaling While Privacy Preserving: A Comprehensive Synthetic Tabular Data
  Generation and Evaluation in Learning Analytics</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-01-12T20:27:55Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Qinyi Liu, Mohammad Khalil, Ronas Shakya, Jelena Jovanovic</p>
    <p><b>Summary:</b> Privacy poses a significant obstacle to the progress of learning analytics
(LA), presenting challenges like inadequate anonymization and data misuse that
current solutions struggle to address. Synthetic data emerges as a potential
remedy, offering robust privacy protection. However, prior LA research on
synthetic data lacks thorough evaluation, essential for assessing the delicate
balance between privacy and data utility. Synthetic data must not only enhance
privacy but also remain practical for data analytics. Moreover, diverse LA
scenarios come with varying privacy and utility needs, making the selection of
an appropriate synthetic data approach a pressing challenge. To address these
gaps, we propose a comprehensive evaluation of synthetic data, which
encompasses three dimensions of synthetic data quality, namely resemblance,
utility, and privacy. We apply this evaluation to three distinct LA datasets,
using three different synthetic data generation methods. Our results show that
synthetic data can maintain similar utility (i.e., predictive performance) as
real data, while preserving privacy. Furthermore, considering different privacy
and data utility requirements in different LA scenarios, we make customized
recommendations for synthetic data generation. This paper not only presents a
comprehensive evaluation of synthetic data but also illustrates its potential
in mitigating privacy concerns within the field of LA, thus contributing to a
wider application of synthetic data in LA and promoting a better practice for
open science.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.06657v2">Accelerating Tactile Internet with QUIC: A Security and Privacy
  Perspective</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Networking and Internet Architecture-04E762">
  <p><b>Published on:</b> 2024-01-12T16:05:13Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jayasree Sengupta, Debasmita Dey, Simone Ferlin, Nirnay Ghosh, Vaibhav Bajpai</p>
    <p><b>Summary:</b> The Tactile Internet paradigm is set to revolutionize human society by
enabling skill-set delivery and haptic communication over ultra-reliable,
low-latency networks. The emerging sixth-generation (6G) mobile communication
systems are envisioned to underpin this Tactile Internet ecosystem at the
network edge by providing ubiquitous global connectivity. However, apart from a
multitude of opportunities of the Tactile Internet, security and privacy
challenges emerge at the forefront. We believe that the recently standardized
QUIC protocol, characterized by end-to-end encryption and reduced round-trip
delay would serve as the backbone of Tactile Internet. In this article, we
envision a futuristic scenario where a QUIC-enabled network uses the underlying
6G communication infrastructure to achieve the requirements for Tactile
Internet. Interestingly this requires a deeper investigation of a wide range of
security and privacy challenges in QUIC, that need to be mitigated for its
adoption in Tactile Internet. Henceforth, this article reviews the existing
security and privacy attacks in QUIC and their implication on users. Followed
by that, we discuss state-of-the-art attack mitigation strategies and
investigate some of their drawbacks with possible directions for future work</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.06601v1">A proposal to increase data utility on Global Differential Privacy data
  based on data use predictions</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB">
  <p><b>Published on:</b> 2024-01-12T14:34:30Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Henry C. Nunes, Marlon P. da Silva, Charles V. Neu, Avelino F. Zorzo</p>
    <p><b>Summary:</b> This paper presents ongoing research focused on improving the utility of data
protected by Global Differential Privacy(DP) in the scenario of summary
statistics. Our approach is based on predictions on how an analyst will use
statistics released under DP protection, so that a developer can optimise data
utility on further usage of the data in the privacy budget allocation. This
novel approach can potentially improve the utility of data without compromising
privacy constraints. We also propose a metric that can be used by the developer
to optimise the budget allocation process.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.05835v1">Privacy Analysis of Affine Transformations in Cloud-based MPC:
  Vulnerability to Side-knowledge</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36">
  <p><b>Published on:</b> 2024-01-11T11:08:20Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Teimour Hosseinalizadeh, Nils Schlüter, Moritz Schulze Darup, Nima Monshizadeh</p>
    <p><b>Summary:</b> Search for the optimizer in computationally demanding model predictive
control (MPC) setups can be facilitated by Cloud as a service provider in
cyber-physical systems. This advantage introduces the risk that Cloud can
obtain unauthorized access to the privacy-sensitive parameters of the system
and cost function. To solve this issue, i.e., preventing Cloud from accessing
the parameters while benefiting from Cloud computation, random affine
transformations provide an exact yet light weight in computation solution. This
research deals with analyzing privacy preserving properties of these
transformations when they are adopted for MPC problems. We consider two common
strategies for outsourcing the optimization required in MPC problems, namely
separate and dense forms, and establish that random affine transformations
utilized in these forms are vulnerable to side-knowledge from Cloud.
Specifically, we prove that the privacy guarantees of these methods and their
extensions for separate form are undermined when a mild side-knowledge about
the problem in terms of structure of MPC cost function is available. In
addition, while we prove that outsourcing the MPC problem in the dense form
inherently leads to some degree of privacy for the system and cost function
parameters, we also establish that affine transformations applied to this form
are nevertheless prone to be undermined by a Cloud with mild side-knowledge.
Numerical simulations confirm our results.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.05562v1">Brave: Byzantine-Resilient and Privacy-Preserving Peer-to-Peer Federated
  Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2024-01-10T22:07:40Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhangchen Xu, Fengqing Jiang, Luyao Niu, Jinyuan Jia, Radha Poovendran</p>
    <p><b>Summary:</b> Federated learning (FL) enables multiple participants to train a global
machine learning model without sharing their private training data.
Peer-to-peer (P2P) FL advances existing centralized FL paradigms by eliminating
the server that aggregates local models from participants and then updates the
global model. However, P2P FL is vulnerable to (i) honest-but-curious
participants whose objective is to infer private training data of other
participants, and (ii) Byzantine participants who can transmit arbitrarily
manipulated local models to corrupt the learning process. P2P FL schemes that
simultaneously guarantee Byzantine resilience and preserve privacy have been
less studied. In this paper, we develop Brave, a protocol that ensures
Byzantine Resilience And privacy-preserving property for P2P FL in the presence
of both types of adversaries. We show that Brave preserves privacy by
establishing that any honest-but-curious adversary cannot infer other
participants' private data by observing their models. We further prove that
Brave is Byzantine-resilient, which guarantees that all benign participants
converge to an identical model that deviates from a global model trained
without Byzantine adversaries by a bounded distance. We evaluate Brave against
three state-of-the-art adversaries on a P2P FL for image classification tasks
on benchmark datasets CIFAR10 and MNIST. Our results show that the global model
learned with Brave in the presence of adversaries achieves comparable
classification accuracy to a global model trained in the absence of any
adversary.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.05126v1">Efficient Fine-Tuning with Domain Adaptation for Privacy-Preserving
  Vision Transformer</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-01-10T12:46:31Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Teru Nagamori, Sayaka Shiota, Hitoshi Kiya</p>
    <p><b>Summary:</b> We propose a novel method for privacy-preserving deep neural networks (DNNs)
with the Vision Transformer (ViT). The method allows us not only to train
models and test with visually protected images but to also avoid the
performance degradation caused from the use of encrypted images, whereas
conventional methods cannot avoid the influence of image encryption. A domain
adaptation method is used to efficiently fine-tune ViT with encrypted images.
In experiments, the method is demonstrated to outperform conventional methods
in an image classification task on the CIFAR-10 and ImageNet datasets in terms
of classification accuracy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.04423v1">Privacy-Preserving Sequential Recommendation with Collaborative
  Confusion</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Retrieval-5BC0EB">
  <p><b>Published on:</b> 2024-01-09T08:30:50Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Wei Wang, Yujie Lin, Pengjie Ren, Zhumin Chen, Tsunenori Mine, Jianli Zhao, Qiang Zhao, Moyan Zhang, Xianye Ben, Yujun Li</p>
    <p><b>Summary:</b> Sequential recommendation has attracted a lot of attention from both academia
and industry, however the privacy risks associated to gathering and
transferring users' personal interaction data are often underestimated or
ignored. Existing privacy-preserving studies are mainly applied to traditional
collaborative filtering or matrix factorization rather than sequential
recommendation. Moreover, these studies are mostly based on differential
privacy or federated learning, which often leads to significant performance
degradation, or has high requirements for communication. In this work, we
address privacy-preserving from a different perspective. Unlike existing
research, we capture collaborative signals of neighbor interaction sequences
and directly inject indistinguishable items into the target sequence before the
recommendation process begins, thereby increasing the perplexity of the target
sequence. Even if the target interaction sequence is obtained by attackers, it
is difficult to discern which ones are the actual user interaction records. To
achieve this goal, we propose a CoLlaborative-cOnfusion seqUential recommenDer,
namely CLOUD, which incorporates a collaborative confusion mechanism to edit
the raw interaction sequences before conducting recommendation. Specifically,
CLOUD first calculates the similarity between the target interaction sequence
and other neighbor sequences to find similar sequences. Then, CLOUD considers
the shared representation of the target sequence and similar sequences to
determine the operation to be performed: keep, delete, or insert. We design a
copy mechanism to make items from similar sequences have a higher probability
to be inserted into the target sequence. Finally, the modified sequence is used
to train the recommender and predict the next item.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.04306v1">Renyi Differential Privacy in the Shuffle Model: Enhanced Amplification
  Bounds</a></h3>
  
  <p><b>Published on:</b> 2024-01-09T01:47:46Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> E Chen, Yang Cao, Yifei Ge</p>
    <p><b>Summary:</b> The shuffle model of Differential Privacy (DP) has gained significant
attention in privacy-preserving data analysis due to its remarkable tradeoff
between privacy and utility. It is characterized by adding a shuffling
procedure after each user's locally differentially private perturbation, which
leads to a privacy amplification effect, meaning that the privacy guarantee of
a small level of noise, say $\epsilon_0$, can be enhanced to
$O(\epsilon_0/\sqrt{n})$ (the smaller, the more private) after shuffling all
$n$ users' perturbed data. Most studies in the shuffle DP focus on proving a
tighter privacy guarantee of privacy amplification. However, the current
results assume that the local privacy budget $\epsilon_0$ is within a limited
range. In addition, there remains a gap between the tightest lower bound and
the known upper bound of the privacy amplification. In this work, we push
forward the state-of-the-art by making the following contributions. Firstly, we
present the first asymptotically optimal analysis of Renyi Differential Privacy
(RDP) in the shuffle model without constraints on $\epsilon_0$. Secondly, we
introduce hypothesis testing for privacy amplification through shuffling,
offering a distinct analysis technique and a tighter upper bound. Furthermore,
we propose a DP-SGD algorithm based on RDP. Experiments demonstrate that our
approach outperforms existing methods significantly at the same privacy level.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.13692v1">Local Privacy-preserving Mechanisms and Applications in Machine Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-01-08T22:29:00Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Likun Qin, Tianshuo Qiu</p>
    <p><b>Summary:</b> The emergence and evolution of Local Differential Privacy (LDP) and its
various adaptations play a pivotal role in tackling privacy issues related to
the vast amounts of data generated by intelligent devices, which are crucial
for data-informed decision-making in the realm of crowdsensing. Utilizing these
extensive datasets can provide critical insights but also introduces
substantial privacy concerns for the individuals involved. LDP, noted for its
decentralized framework, excels in providing strong privacy protection for
individual users during the stages of data collection and processing. The core
principle of LDP lies in its technique of altering each user's data locally at
the client end before it is sent to the server, thus preventing privacy
violations at both stages. There are many LDP variances in the privacy research
community aimed to improve the utility-privacy tradeoff. On the other hand, one
of the major applications of the privacy-preserving mechanisms is machine
learning. In this paper, we firstly delves into a comprehensive analysis of LDP
and its variances, focusing on their various models, the diverse range of its
adaptations, and the underlying structure of privacy mechanisms; then we
discuss the state-of-art privacy mechanisms applications in machine learning.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.04076v2">Security and Privacy Issues in Cloud Storage</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Networking and Internet Architecture-04E762">
  <p><b>Published on:</b> 2024-01-08T18:27:57Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Norah Asiri</p>
    <p><b>Summary:</b> Even with the vast potential that cloud computing has, so far, it has not
been adopted by the consumers with the enthusiasm and pace that it be worthy;
this is a very reason statement why consumers still hesitated of using cloud
computing for their sensitive data and the threats that prevent the consumers
from shifting to use cloud computing in general and cloud storage in
particular. The cloud computing inherits the traditional potential security and
privacy threats besides its own issues due to its unique structures. Some
threats related to cloud computing are the insider malicious attacks from the
employees that even sometime the provider unconscious about, the lack of
transparency of agreement between consumer and provider, data loss, traffic
hijacking, shared technology and insecure application interface. Such threats
need remedies to make the consumer use its features in secure way. In this
review, we spot the light on the most security and privacy issues which can be
attributed as gaps that sometimes the consumers or even the enterprises are not
aware of. We also define the parties that involve in scenario of cloud
computing that also may attack the entire cloud systems. We also show the
consequences of these threats.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.03883v2">The Impact of Differential Privacy on Recommendation Accuracy and
  Popularity Bias</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Retrieval-5BC0EB">
  <p><b>Published on:</b> 2024-01-08T13:31:02Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Peter Müllner, Elisabeth Lex, Markus Schedl, Dominik Kowald</p>
    <p><b>Summary:</b> Collaborative filtering-based recommender systems leverage vast amounts of
behavioral user data, which poses severe privacy risks. Thus, often, random
noise is added to the data to ensure Differential Privacy (DP). However, to
date, it is not well understood, in which ways this impacts personalized
recommendations. In this work, we study how DP impacts recommendation accuracy
and popularity bias, when applied to the training data of state-of-the-art
recommendation models. Our findings are three-fold: First, we find that nearly
all users' recommendations change when DP is applied. Second, recommendation
accuracy drops substantially while recommended item popularity experiences a
sharp increase, suggesting that popularity bias worsens. Third, we find that DP
exacerbates popularity bias more severely for users who prefer unpopular items
than for users that prefer popular items.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.03552v1">Privacy-Preserving in Blockchain-based Federated Learning Systems</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-01-07T17:23:55Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sameera K. M., Serena Nicolazzo, Marco Arazzi, Antonino Nocera, Rafidha Rehiman K. A., Vinod P, Mauro Conti</p>
    <p><b>Summary:</b> Federated Learning (FL) has recently arisen as a revolutionary approach to
collaborative training Machine Learning models. According to this novel
framework, multiple participants train a global model collaboratively,
coordinating with a central aggregator without sharing their local data. As FL
gains popularity in diverse domains, security, and privacy concerns arise due
to the distributed nature of this solution. Therefore, integrating this
strategy with Blockchain technology has been consolidated as a preferred choice
to ensure the privacy and security of participants.
  This paper explores the research efforts carried out by the scientific
community to define privacy solutions in scenarios adopting Blockchain-Enabled
FL. It comprehensively summarizes the background related to FL and Blockchain,
evaluates existing architectures for their integration, and the primary attacks
and possible countermeasures to guarantee privacy in this setting. Finally, it
reviews the main application scenarios where Blockchain-Enabled FL approaches
have been proficiently applied. This survey can help academia and industry
practitioners understand which theories and techniques exist to improve the
performance of FL through Blockchain to preserve privacy and which are the main
challenges and future directions in this novel and still under-explored
context. We believe this work provides a novel contribution respect to the
previous surveys and is a valuable tool to explore the current landscape,
understand perspectives, and pave the way for advancements or improvements in
this amalgamation of Blockchain and Federated Learning.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.03218v2">MiniScope: Automated UI Exploration and Privacy Inconsistency Detection
  of MiniApps via Two-phase Iterative Hybrid Analysis</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Software Engineering-D91E36">
  <p><b>Published on:</b> 2024-01-06T13:54:36Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shenao Wang, Yuekang Li, Kailong Wang, Yi Liu, Hui Li, Yang Liu, Haoyu Wang</p>
    <p><b>Summary:</b> The advent of MiniApps, operating within larger SuperApps, has revolutionized
user experiences by offering a wide range of services without the need for
individual app downloads. However, this convenience has raised significant
privacy concerns, as these MiniApps often require access to sensitive data,
potentially leading to privacy violations. Our research addresses the critical
gaps in the analysis of MiniApps' privacy practices, especially focusing on
WeChat MiniApps in the Android ecosystem. Despite existing privacy regulations
and platform guidelines, there is a lack of effective mechanisms to safeguard
user privacy fully. We introduce MiniScope, a novel two-phase hybrid analysis
approach, specifically designed for the MiniApp environment. This approach
overcomes the limitations of existing static analysis techniques by
incorporating dynamic UI exploration for complete code coverage and accurate
privacy practice identification. Our methodology includes modeling UI
transition states, resolving cross-package callback control flows, and
automated iterative UI exploration. This allows for a comprehensive
understanding of MiniApps' privacy practices, addressing the unique challenges
of sub-package loading and event-driven callbacks. Our empirical evaluation of
over 120K MiniApps using MiniScope demonstrates its effectiveness in
identifying privacy inconsistencies. The results reveal significant issues,
with 5.7% of MiniApps over-collecting private data and 33.4% overclaiming data
collection. These findings emphasize the urgent need for more precise privacy
monitoring systems and highlight the responsibility of SuperApp operators to
enforce stricter privacy measures.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.02453v1">Adaptive Differential Privacy in Federated Learning: A Priority-Based
  Approach</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-01-04T03:01:15Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mahtab Talaei, Iman Izadi</p>
    <p><b>Summary:</b> Federated learning (FL) as one of the novel branches of distributed machine
learning (ML), develops global models through a private procedure without
direct access to local datasets. However, access to model updates (e.g.
gradient updates in deep neural networks) transferred between clients and
servers can reveal sensitive information to adversaries. Differential privacy
(DP) offers a framework that gives a privacy guarantee by adding certain
amounts of noise to parameters. This approach, although being effective in
terms of privacy, adversely affects model performance due to noise involvement.
Hence, it is always needed to find a balance between noise injection and the
sacrificed accuracy. To address this challenge, we propose adaptive noise
addition in FL which decides the value of injected noise based on features'
relative importance. Here, we first propose two effective methods for
prioritizing features in deep neural network models and then perturb models'
weights based on this information. Specifically, we try to figure out whether
the idea of adding more noise to less important parameters and less noise to
more important parameters can effectively save the model accuracy while
preserving privacy. Our experiments confirm this statement under some
conditions. The amount of noise injected, the proportion of parameters
involved, and the number of global iterations can significantly change the
output. While a careful choice of parameters by considering the properties of
datasets can improve privacy without intense loss of accuracy, a bad choice can
make the model performance worse.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.01589v1">The Security and Privacy of Mobile Edge Computing: An Artificial
  Intelligence Perspective</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-01-03T07:47:22Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Cheng Wang, Zenghui Yuan, Pan Zhou, Zichuan Xu, Ruixuan Li, Dapeng Oliver Wu</p>
    <p><b>Summary:</b> Mobile Edge Computing (MEC) is a new computing paradigm that enables cloud
computing and information technology (IT) services to be delivered at the
network's edge. By shifting the load of cloud computing to individual local
servers, MEC helps meet the requirements of ultralow latency, localized data
processing, and extends the potential of Internet of Things (IoT) for
end-users. However, the crosscutting nature of MEC and the multidisciplinary
components necessary for its deployment have presented additional security and
privacy concerns. Fortunately, Artificial Intelligence (AI) algorithms can cope
with excessively unpredictable and complex data, which offers a distinct
advantage in dealing with sophisticated and developing adversaries in the
security industry. Hence, in this paper we comprehensively provide a survey of
security and privacy in MEC from the perspective of AI. On the one hand, we use
European Telecommunications Standards Institute (ETSI) MEC reference
architecture as our based framework while merging the Software Defined Network
(SDN) and Network Function Virtualization (NFV) to better illustrate a
serviceable platform of MEC. On the other hand, we focus on new security and
privacy issues, as well as potential solutions from the viewpoints of AI.
Finally, we comprehensively discuss the opportunities and challenges associated
with applying AI to MEC security and privacy as possible future research
directions.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.01575v1">Enhancing Generalization of Invisible Facial Privacy Cloak via Gradient
  Accumulation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-01-03T07:00:32Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xuannan Liu, Yaoyao Zhong, Weihong Deng, Hongzhi Shi, Xingchen Cui, Yunfeng Yin, Dongchao Wen</p>
    <p><b>Summary:</b> The blooming of social media and face recognition (FR) systems has increased
people's concern about privacy and security. A new type of adversarial privacy
cloak (class-universal) can be applied to all the images of regular users, to
prevent malicious FR systems from acquiring their identity information. In this
work, we discover the optimization dilemma in the existing methods -- the local
optima problem in large-batch optimization and the gradient information
elimination problem in small-batch optimization. To solve these problems, we
propose Gradient Accumulation (GA) to aggregate multiple small-batch gradients
into a one-step iterative gradient to enhance the gradient stability and reduce
the usage of quantization operations. Experiments show that our proposed method
achieves high performance on the Privacy-Commons dataset against black-box face
recognition models.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.01294v1">Efficient Sparse Least Absolute Deviation Regression with Differential
  Privacy</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">  
  <p><b>Published on:</b> 2024-01-02T17:13:34Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Weidong Liu, Xiaojun Mao, Xiaofei Zhang, Xin Zhang</p>
    <p><b>Summary:</b> In recent years, privacy-preserving machine learning algorithms have
attracted increasing attention because of their important applications in many
scientific fields. However, in the literature, most privacy-preserving
algorithms demand learning objectives to be strongly convex and Lipschitz
smooth, which thus cannot cover a wide class of robust loss functions (e.g.,
quantile/least absolute loss). In this work, we aim to develop a fast
privacy-preserving learning solution for a sparse robust regression problem.
Our learning loss consists of a robust least absolute loss and an $\ell_1$
sparse penalty term. To fast solve the non-smooth loss under a given privacy
budget, we develop a Fast Robust And Privacy-Preserving Estimation (FRAPPE)
algorithm for least absolute deviation regression. Our algorithm achieves a
fast estimation by reformulating the sparse LAD problem as a penalized least
square estimation problem and adopts a three-stage noise injection to guarantee
the $(\epsilon,\delta)$-differential privacy. We show that our algorithm can
achieve better privacy and statistical accuracy trade-off compared with the
state-of-the-art privacy-preserving regression algorithms. In the end, we
conduct experiments to verify the efficiency of our proposed FRAPPE algorithm.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.01204v2">PPBFL: A Privacy Protected Blockchain-based Federated Learning Model</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-01-02T13:13:28Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yang Li, Chunhe Xia, Wanshuang Lin, Tianbo Wang</p>
    <p><b>Summary:</b> With the rapid development of machine learning and a growing concern for data
privacy, federated learning has become a focal point of attention. However,
attacks on model parameters and a lack of incentive mechanisms hinder the
effectiveness of federated learning. Therefore, we propose A Privacy Protected
Blockchain-based Federated Learning Model (PPBFL) to enhance the security of
federated learning and encourage active participation of nodes in model
training. Blockchain technology ensures the integrity of model parameters
stored in the InterPlanetary File System (IPFS), providing protection against
tampering. Within the blockchain, we introduce a Proof of Training Work (PoTW)
consensus algorithm tailored for federated learning, aiming to incentive
training nodes. This algorithm rewards nodes with greater computational power,
promoting increased participation and effort in the federated learning process.
A novel adaptive differential privacy algorithm is simultaneously applied to
local and global models. This safeguards the privacy of local data at training
clients, preventing malicious nodes from launching inference attacks.
Additionally, it enhances the security of the global model, preventing
potential security degradation resulting from the combination of numerous local
models. The possibility of security degradation is derived from the composition
theorem. By introducing reverse noise in the global model, a zero-bias estimate
of differential privacy noise between local and global models is achieved.
Furthermore, we propose a new mix transactions mechanism utilizing ring
signature technology to better protect the identity privacy of local training
clients. Security analysis and experimental results demonstrate that PPBFL,
compared to baseline methods, not only exhibits superior model performance but
also achieves higher security.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.01146v1">Privacy Preserving Personal Assistant with On-Device Diarization and
  Spoken Dialogue System for Home and Beyond</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2024-01-02T10:56:24Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Gérard Chollet, Hugues Sansen, Yannis Tevissen, Jérôme Boudy, Mossaab Hariz, Christophe Lohr, Fathy Yassa</p>
    <p><b>Summary:</b> In the age of personal voice assistants, the question of privacy arises.
These digital companions often lack memory of past interactions, while relying
heavily on the internet for speech processing, raising privacy concerns. Modern
smartphones now enable on-device speech processing, making cloud-based
solutions unnecessary. Personal assistants for the elderly should excel at
memory recall, especially in medical examinations. The e-ViTA project developed
a versatile conversational application with local processing and speaker
recognition. This paper highlights the importance of speaker diarization
enriched with sensor data fusion for contextualized conversation preservation.
The use cases applied to the e-VITA project have shown that truly personalized
dialogue is pivotal for individual voice assistants. Secure local processing
and sensor data fusion ensure virtual companions meet individual user needs
without compromising privacy or data security.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.00973v1">Facebook Report on Privacy of fNIRS data</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2024-01-01T23:30:31Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Md Imran Hossen, Sai Venkatesh Chilukoti, Liqun Shan, Vijay Srinivas Tida, Xiali Hei</p>
    <p><b>Summary:</b> The primary goal of this project is to develop privacy-preserving machine
learning model training techniques for fNIRS data. This project will build a
local model in a centralized setting with both differential privacy (DP) and
certified robustness. It will also explore collaborative federated learning to
train a shared model between multiple clients without sharing local fNIRS
datasets. To prevent unintentional private information leakage of such clients'
private datasets, we will also implement DP in the federated learning setting.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.00794v1">Privacy-Preserving Data in IoT-based Cloud Systems: A Comprehensive
  Survey with AI Integration</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-01-01T15:48:39Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> D. Dhinakaran, S. M. Udhaya Sankar, D. Selvaraj, S. Edwin Raja</p>
    <p><b>Summary:</b> As the integration of Internet of Things devices with cloud computing
proliferates, the paramount importance of privacy preservation comes to the
forefront. This survey paper meticulously explores the landscape of privacy
issues in the dynamic intersection of IoT and cloud systems. The comprehensive
literature review synthesizes existing research, illuminating key challenges
and discerning emerging trends in privacy preserving techniques. The
categorization of diverse approaches unveils a nuanced understanding of
encryption techniques, anonymization strategies, access control mechanisms, and
the burgeoning integration of artificial intelligence. Notable trends include
the infusion of machine learning for dynamic anonymization, homomorphic
encryption for secure computation, and AI-driven access control systems. The
culmination of this survey contributes a holistic view, laying the groundwork
for understanding the multifaceted strategies employed in securing sensitive
data within IoT-based cloud environments. The insights garnered from this
survey provide a valuable resource for researchers, practitioners, and
policymakers navigating the complex terrain of privacy preservation in the
evolving landscape of IoT and cloud computing</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2401.00793v2">SecFormer: Towards Fast and Accurate Privacy-Preserving Inference for
  Large Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-01-01T15:40:35Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jinglong Luo, Yehong Zhang, Jiaqi Zhang, Xin Mu, Hui Wang, Yue Yu, Zenglin Xu</p>
    <p><b>Summary:</b> With the growing use of large language models hosted on cloud platforms to
offer inference services, privacy concerns are escalating, especially
concerning sensitive data like investment plans and bank account details.
Secure Multi-Party Computing (SMPC) emerges as a promising solution to protect
the privacy of inference data and model parameters. However, the application of
SMPC in Privacy-Preserving Inference (PPI) for large language models,
particularly those based on the Transformer architecture, often leads to
considerable slowdowns or declines in performance. This is largely due to the
multitude of nonlinear operations in the Transformer architecture, which are
not well-suited to SMPC and difficult to circumvent or optimize effectively. To
address this concern, we introduce an advanced optimization framework called
SecFormer, to achieve fast and accurate PPI for Transformer models. By
implementing model design optimization, we successfully eliminate the high-cost
exponential and maximum operations in PPI without sacrificing model
performance. Additionally, we have developed a suite of efficient SMPC
protocols that utilize segmented polynomials, Fourier series and Goldschmidt's
method to handle other complex nonlinear functions within PPI, such as GeLU,
LayerNorm, and Softmax. Our extensive experiments reveal that SecFormer
outperforms MPCFormer in performance, showing improvements of $5.6\%$ and
$24.2\%$ for BERT$_{\text{BASE}}$ and BERT$_{\text{LARGE}}$, respectively. In
terms of efficiency, SecFormer is 3.56 and 3.58 times faster than Puma for
BERT$_{\text{BASE}}$ and BERT$_{\text{LARGE}}$, demonstrating its effectiveness
and speed.</p>
  </details>
</div>



<h2>2024-02</h2>

<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.04033v1">On provable privacy vulnerabilities of graph representations</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-02-06T14:26:22Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ruofan Wu, Guanhua Fang, Qiying Pan, Mingyang Zhang, Tengfei Liu, Weiqiang Wang, Wenbiao Zhao</p>
    <p><b>Summary:</b> Graph representation learning (GRL) is critical for extracting insights from
complex network structures, but it also raises security concerns due to
potential privacy vulnerabilities in these representations. This paper
investigates the structural vulnerabilities in graph neural models where
sensitive topological information can be inferred through edge reconstruction
attacks. Our research primarily addresses the theoretical underpinnings of
cosine-similarity-based edge reconstruction attacks (COSERA), providing
theoretical and empirical evidence that such attacks can perfectly reconstruct
sparse Erdos Renyi graphs with independent random features as graph size
increases. Conversely, we establish that sparsity is a critical factor for
COSERA's effectiveness, as demonstrated through analysis and experiments on
stochastic block models. Finally, we explore the resilience of (provably)
private graph representations produced via noisy aggregation (NAG) mechanism
against COSERA. We empirically delineate instances wherein COSERA demonstrates
both efficacy and deficiency in its capacity to function as an instrument for
elucidating the trade-off between privacy and utility.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.04013v1">Privacy Leakage on DNNs: A Survey of Model Inversion Attacks and
  Defenses</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-02-06T14:06:23Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Hao Fang, Yixiang Qiu, Hongyao Yu, Wenbo Yu, Jiawei Kong, Baoli Chong, Bin Chen, Xuan Wang, Shu-Tao Xia</p>
    <p><b>Summary:</b> Model Inversion (MI) attacks aim to disclose private information about the
training data by abusing access to the pre-trained models. These attacks enable
adversaries to reconstruct high-fidelity data that closely aligns with the
private training data, which has raised significant privacy concerns. Despite
the rapid advances in the field, we lack a comprehensive overview of existing
MI attacks and defenses. To fill this gap, this paper thoroughly investigates
this field and presents a holistic survey. Firstly, our work briefly reviews
the traditional MI on machine learning scenarios. We then elaborately analyze
and compare numerous recent attacks and defenses on \textbf{D}eep
\textbf{N}eural \textbf{N}etworks (DNNs) across multiple modalities and
learning tasks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.03907v1">Embedding Large Language Models into Extended Reality: Opportunities and
  Challenges for Inclusion, Engagement, and Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-02-06T11:19:40Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Efe Bozkir, Süleyman Özdel, Ka Hei Carrie Lau, Mengdi Wang, Hong Gao, Enkelejda Kasneci</p>
    <p><b>Summary:</b> Recent developments in computer graphics, hardware, artificial intelligence
(AI), and human-computer interaction likely lead to extended reality (XR)
devices and setups being more pervasive. While these devices and setups provide
users with interactive, engaging, and immersive experiences with different
sensing modalities, such as eye and hand trackers, many non-player characters
are utilized in a pre-scripted way or by conventional AI techniques. In this
paper, we argue for using large language models (LLMs) in XR by embedding them
in virtual avatars or as narratives to facilitate more inclusive experiences
through prompt engineering according to user profiles and fine-tuning the LLMs
for particular purposes. We argue that such inclusion will facilitate diversity
for XR use. In addition, we believe that with the versatile conversational
capabilities of LLMs, users will engage more with XR environments, which might
help XR be more used in everyday life. Lastly, we speculate that combining the
information provided to LLM-powered environments by the users and the biometric
data obtained through the sensors might lead to novel privacy invasions. While
studying such possible privacy invasions, user privacy concerns and preferences
should also be investigated. In summary, despite some challenges, embedding
LLMs into XR is a promising and novel research area with several opportunities.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.03702v1">On Learning Spatial Provenance in Privacy-Constrained Wireless Networks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Networking and Internet Architecture-04E762"> 
  <p><b>Published on:</b> 2024-02-06T04:44:36Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Manish Bansal, Pramsu Srivastava, J. Harshan</p>
    <p><b>Summary:</b> In Vehicle-to-Everything networks that involve multi-hop communication, the
Road Side Units (RSUs) typically aim to collect location information from the
participating vehicles to provide security and network diagnostics features.
While the vehicles commonly use the Global Positioning System (GPS) for
navigation, they may refrain from sharing their precise GPS coordinates with
the RSUs due to privacy concerns. Therefore, to jointly address the high
localization requirements by the RSUs as well as the vehicles' privacy, we
present a novel spatial-provenance framework wherein each vehicle uses Bloom
filters to embed their partial location information when forwarding the
packets. In this framework, the RSUs and the vehicles agree upon fragmenting
the coverage area into several smaller regions so that the vehicles can embed
the identity of their regions through Bloom filters. Given the probabilistic
nature of Bloom filters, we derive an analytical expression on the error-rates
in provenance recovery and then pose an optimization problem to choose the
underlying parameters. With the help of extensive simulation results, we show
that our method offers near-optimal Bloom filter parameters in learning spatial
provenance. Some interesting trade-offs between the communication-overhead,
spatial privacy of the vehicles and the error rates in provenance recovery are
also discussed.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.03688v1">A Survey of Privacy Threats and Defense in Vertical Federated Learning:
  From Model Life Cycle Perspective</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-02-06T04:22:44Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Lei Yu, Meng Han, Yiming Li, Changting Lin, Yao Zhang, Mingyang Zhang, Yan Liu, Haiqin Weng, Yuseok Jeon, Ka-Ho Chow, Stacy Patterson</p>
    <p><b>Summary:</b> Vertical Federated Learning (VFL) is a federated learning paradigm where
multiple participants, who share the same set of samples but hold different
features, jointly train machine learning models. Although VFL enables
collaborative machine learning without sharing raw data, it is still
susceptible to various privacy threats. In this paper, we conduct the first
comprehensive survey of the state-of-the-art in privacy attacks and defenses in
VFL. We provide taxonomies for both attacks and defenses, based on their
characterizations, and discuss open challenges and future research directions.
Specifically, our discussion is structured around the model's life cycle, by
delving into the privacy threats encountered during different stages of machine
learning and their corresponding countermeasures. This survey not only serves
as a resource for the research community but also offers clear guidance and
actionable insights for practitioners to safeguard data privacy throughout the
model's life cycle.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.03612v1">Privacy risk in GeoData: A survey</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-02-06T00:55:06Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mahrokh Abdollahi Lorestani, Thilina Ranbaduge, Thierry Rakotoarivelo</p>
    <p><b>Summary:</b> With the ubiquitous use of location-based services, large-scale
individual-level location data has been widely collected through
location-awareness devices. The exposure of location data constitutes a
significant privacy risk to users as it can lead to de-anonymisation, the
inference of sensitive information, and even physical threats. Geoprivacy
concerns arise on the issues of user identity de-anonymisation and location
exposure. In this survey, we analyse different geomasking techniques that have
been proposed to protect the privacy of individuals in geodata. We present a
taxonomy to characterise these techniques along different dimensions, and
conduct a survey of geomasking techniques. We then highlight shortcomings of
current techniques and discuss avenues for future research.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.03582v1">Matcha: An IDE Plugin for Creating Accurate Privacy Nutrition Labels</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-02-05T23:17:08Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tianshi Li, Lorrie Faith Cranor, Yuvraj Agarwal, Jason I. Hong</p>
    <p><b>Summary:</b> Apple and Google introduced their versions of privacy nutrition labels to the
mobile app stores to better inform users of the apps' data practices. However,
these labels are self-reported by developers and have been found to contain
many inaccuracies due to misunderstandings of the label taxonomy. In this work,
we present Matcha, an IDE plugin that uses automated code analysis to help
developers create accurate Google Play data safety labels. Developers can
benefit from Matcha's ability to detect user data accesses and transmissions
while staying in control of the generated label by adding custom Java
annotations and modifying an auto-generated XML specification. Our evaluation
with 12 developers showed that Matcha helped our participants improved the
accuracy of a label they created with Google's official tool for a real-world
app they developed. We found that participants preferred Matcha for its
accuracy benefits. Drawing on Matcha, we discuss general design recommendations
for developer tools used to create accurate standardized privacy notices.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.03531v1">Fairness and Privacy Guarantees in Federated Contextual Bandits</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-02-05T21:38:23Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sambhav Solanki, Shweta Jain, Sujit Gujar</p>
    <p><b>Summary:</b> This paper considers the contextual multi-armed bandit (CMAB) problem with
fairness and privacy guarantees in a federated environment. We consider
merit-based exposure as the desired fair outcome, which provides exposure to
each action in proportion to the reward associated. We model the algorithm's
effectiveness using fairness regret, which captures the difference between fair
optimal policy and the policy output by the algorithm. Applying fair CMAB
algorithm to each agent individually leads to fairness regret linear in the
number of agents. We propose that collaborative -- federated learning can be
more effective and provide the algorithm Fed-FairX-LinUCB that also ensures
differential privacy. The primary challenge in extending the existing privacy
framework is designing the communication protocol for communicating required
information across agents. A naive protocol can either lead to weaker privacy
guarantees or higher regret. We design a novel communication protocol that
allows for (i) Sub-linear theoretical bounds on fairness regret for
Fed-FairX-LinUCB and comparable bounds for the private counterpart,
Priv-FairX-LinUCB (relative to single-agent learning), (ii) Effective use of
privacy budget in Priv-FairX-LinUCB. We demonstrate the efficacy of our
proposed algorithm with extensive simulations-based experiments. We show that
both Fed-FairX-LinUCB and Priv-FairX-LinUCB achieve near-optimal fairness
regret.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.03435v1">Psychological Assessments with Large Language Models: A Privacy-Focused
  and Cost-Effective Approach</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2024-02-05T19:00:02Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sergi Blanco-Cuaresma</p>
    <p><b>Summary:</b> This study explores the use of Large Language Models (LLMs) to analyze text
comments from Reddit users, aiming to achieve two primary objectives: firstly,
to pinpoint critical excerpts that support a predefined psychological
assessment of suicidal risk; and secondly, to summarize the material to
substantiate the preassigned suicidal risk level. The work is circumscribed to
the use of "open-source" LLMs that can be run locally, thereby enhancing data
privacy. Furthermore, it prioritizes models with low computational
requirements, making it accessible to both individuals and institutions
operating on limited computing budgets. The implemented strategy only relies on
a carefully crafted prompt and a grammar to guide the LLM's text completion.
Despite its simplicity, the evaluation metrics show outstanding results, making
it a valuable privacy-focused and cost-effective approach. This work is part of
the Computational Linguistics and Clinical Psychology (CLPsych) 2024 shared
task.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.03114v1">Augmenting Security and Privacy in the Virtual Realm: An Analysis of
  Extended Reality Devices</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-02-05T15:45:11Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Derin Cayir, Abbas Acar, Riccardo Lazzeretti, Marco Angelini, Mauro Conti, Selcuk Uluagac</p>
    <p><b>Summary:</b> In this work, we present a device-centric analysis of security and privacy
attacks and defenses on Extended Reality (XR) devices, highlighting the need
for robust and privacy-aware security mechanisms. Based on our analysis, we
present future research directions and propose design considerations to help
ensure the security and privacy of XR devices.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.02672v1">Estimation of conditional average treatment effects on distributed data:
  A privacy-preserving approach</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-02-05T02:17:21Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yuji Kawamata, Ryoki Motai, Yukihiko Okada, Akira Imakura, Tetsuya Sakurai</p>
    <p><b>Summary:</b> Estimation of conditional average treatment effects (CATEs) is an important
topic in various fields such as medical and social sciences. CATEs can be
estimated with high accuracy if distributed data across multiple parties can be
centralized. However, it is difficult to aggregate such data if they contain
privacy information. To address this issue, we proposed data collaboration
double machine learning (DC-DML), a method that can estimate CATE models with
privacy preservation of distributed data, and evaluated the method through
numerical experiments. Our contributions are summarized in the following three
points. First, our method enables estimation and testing of semi-parametric
CATE models without iterative communication on distributed data.
Semi-parametric or non-parametric CATE models enable estimation and testing
that is more robust to model mis-specification than parametric models. However,
to our knowledge, no communication-efficient method has been proposed for
estimating and testing semi-parametric or non-parametric CATE models on
distributed data. Second, our method enables collaborative estimation between
different parties as well as multiple time points because the
dimensionality-reduced intermediate representations can be accumulated. Third,
our method performed as well or better than other methods in evaluation
experiments using synthetic, semi-synthetic and real-world datasets.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.02230v1">Federated Learning with Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB"> 
  <p><b>Published on:</b> 2024-02-03T18:21:38Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Adrien Banse, Jan Kreischer, Xavier Oliva i Jürgens</p>
    <p><b>Summary:</b> Federated learning (FL), as a type of distributed machine learning, is
capable of significantly preserving client's private data from being shared
among different parties. Nevertheless, private information can still be
divulged by analyzing uploaded parameter weights from clients. In this report,
we showcase our empirical benchmark of the effect of the number of clients and
the addition of differential privacy (DP) mechanisms on the performance of the
model on different types of data. Our results show that non-i.i.d and small
datasets have the highest decrease in performance in a distributed and
differentially private setting.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.01994v1">Human-Centered Privacy Research in the Age of Large Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-02-03T02:32:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tianshi Li, Sauvik Das, Hao-Ping Lee, Dakuo Wang, Bingsheng Yao, Zhiping Zhang</p>
    <p><b>Summary:</b> The emergence of large language models (LLMs), and their increased use in
user-facing systems, has led to substantial privacy concerns. To date, research
on these privacy concerns has been model-centered: exploring how LLMs lead to
privacy risks like memorization, or can be used to infer personal
characteristics about people from their content. We argue that there is a need
for more research focusing on the human aspect of these privacy issues: e.g.,
research on how design paradigms for LLMs affect users' disclosure behaviors,
users' mental models and preferences for privacy controls, and the design of
tools, systems, and artifacts that empower end-users to reclaim ownership over
their personal data. To build usable, efficient, and privacy-friendly systems
powered by these models with imperfect privacy properties, our goal is to
initiate discussions to outline an agenda for conducting human-centered
research on privacy issues in LLM-powered systems. This Special Interest Group
(SIG) aims to bring together researchers with backgrounds in usable security
and privacy, human-AI collaboration, NLP, or any other related domains to share
their perspectives and experiences on this problem, to help our community
establish a collective understanding of the challenges, research opportunities,
research methods, and strategies to collaborate with researchers outside of
HCI.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.01857v1">Position Paper: Assessing Robustness, Privacy, and Fairness in Federated
  Learning Integrated with Foundation Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2024-02-02T19:26:00Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xi Li, Jiaqi Wang</p>
    <p><b>Summary:</b> Federated Learning (FL), while a breakthrough in decentralized machine
learning, contends with significant challenges such as limited data
availability and the variability of computational resources, which can stifle
the performance and scalability of the models. The integration of Foundation
Models (FMs) into FL presents a compelling solution to these issues, with the
potential to enhance data richness and reduce computational demands through
pre-training and data augmentation. However, this incorporation introduces
novel issues in terms of robustness, privacy, and fairness, which have not been
sufficiently addressed in the existing research. We make a preliminary
investigation into this field by systematically evaluating the implications of
FM-FL integration across these dimensions. We analyze the trade-offs involved,
uncover the threats and issues introduced by this integration, and propose a
set of criteria and strategies for navigating these challenges. Furthermore, we
identify potential research directions for advancing this field, laying a
foundation for future development in creating reliable, secure, and equitable
FL systems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.01546v1">Privacy-Preserving Distributed Learning for Residential Short-Term Load
  Forecasting</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Multiagent Systems-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36"> 
  <p><b>Published on:</b> 2024-02-02T16:39:08Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yi Dong, Yingjie Wang, Mariana Gama, Mustafa A. Mustafa, Geert Deconinck, Xiaowei Huang</p>
    <p><b>Summary:</b> In the realm of power systems, the increasing involvement of residential
users in load forecasting applications has heightened concerns about data
privacy. Specifically, the load data can inadvertently reveal the daily
routines of residential users, thereby posing a risk to their property
security. While federated learning (FL) has been employed to safeguard user
privacy by enabling model training without the exchange of raw data, these FL
models have shown vulnerabilities to emerging attack techniques, such as Deep
Leakage from Gradients and poisoning attacks. To counteract these, we initially
employ a Secure-Aggregation (SecAgg) algorithm that leverages multiparty
computation cryptographic techniques to mitigate the risk of gradient leakage.
However, the introduction of SecAgg necessitates the deployment of additional
sub-center servers for executing the multiparty computation protocol, thereby
escalating computational complexity and reducing system robustness, especially
in scenarios where one or more sub-centers are unavailable. To address these
challenges, we introduce a Markovian Switching-based distributed training
framework, the convergence of which is substantiated through rigorous
theoretical analysis. The Distributed Markovian Switching (DMS) topology shows
strong robustness towards the poisoning attacks as well. Case studies employing
real-world power system load data validate the efficacy of our proposed
algorithm. It not only significantly minimizes communication complexity but
also maintains accuracy levels comparable to traditional FL methods, thereby
enhancing the scalability of our load forecasting algorithm.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.01296v1">Bi-CryptoNets: Leveraging Different-Level Privacy for Encrypted
  Inference</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-02-02T10:35:05Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Man-Jie Yuan, Zheng Zou, Wei Gao</p>
    <p><b>Summary:</b> Privacy-preserving neural networks have attracted increasing attention in
recent years, and various algorithms have been developed to keep the balance
between accuracy, computational complexity and information security from the
cryptographic view. This work takes a different view from the input data and
structure of neural networks. We decompose the input data (e.g., some images)
into sensitive and insensitive segments according to importance and privacy.
The sensitive segment includes some important and private information such as
human faces and we take strong homomorphic encryption to keep security, whereas
the insensitive one contains some background and we add perturbations. We
propose the bi-CryptoNets, i.e., plaintext and ciphertext branches, to deal
with two segments, respectively, and ciphertext branch could utilize the
information from plaintext branch by unidirectional connections. We adopt
knowledge distillation for our bi-CryptoNets by transferring representations
from a well-trained teacher neural network. Empirical studies show the
effectiveness and decrease of inference latency for our bi-CryptoNets.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.01226v1">HW-SW Optimization of DNNs for Privacy-preserving People Counting on
  Low-resolution Infrared Arrays</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Hardware Architecture-04E762">
  <p><b>Published on:</b> 2024-02-02T08:45:38Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Matteo Risso, Chen Xie, Francesco Daghero, Alessio Burrello, Seyedmorteza Mollaei, Marco Castellano, Enrico Macii, Massimo Poncino, Daniele Jahier Pagliari</p>
    <p><b>Summary:</b> Low-resolution infrared (IR) array sensors enable people counting
applications such as monitoring the occupancy of spaces and people flows while
preserving privacy and minimizing energy consumption. Deep Neural Networks
(DNNs) have been shown to be well-suited to process these sensor data in an
accurate and efficient manner. Nevertheless, the space of DNNs' architectures
is huge and its manual exploration is burdensome and often leads to sub-optimal
solutions. To overcome this problem, in this work, we propose a highly
automated full-stack optimization flow for DNNs that goes from neural
architecture search, mixed-precision quantization, and post-processing, down to
the realization of a new smart sensor prototype, including a Microcontroller
with a customized instruction set. Integrating these cross-layer optimizations,
we obtain a large set of Pareto-optimal solutions in the 3D-space of energy,
memory, and accuracy. Deploying such solutions on our hardware platform, we
improve the state-of-the-art achieving up to 4.2x model size reduction, 23.8x
code size reduction, and 15.38x energy reduction at iso-accuracy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.01198v1">Physical Layer Location Privacy in SIMO Communication Using Fake Paths
  Injection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36">  
  <p><b>Published on:</b> 2024-02-02T07:52:32Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Trong Duy Tran, Maxime Ferreira Da Costa, Linh Trung Nguyen</p>
    <p><b>Summary:</b> Fake path injection is an emerging paradigm for inducing privacy over
wireless networks. In this paper, fake paths are injected by the transmitter
into a SIMO multipath communication channel to preserve her physical location
from an eavesdropper. A novel statistical privacy metric is defined as the
ratio between the largest (resp. smallest) eigenvalues of Bob's (resp. Eve's)
Cram\'er-Rao lower bound on the SIMO multipath channel parameters to assess the
privacy enhancements. Leveraging the spectral properties of generalized
Vandermonde matrices, bounds on the privacy margin of the proposed scheme are
derived. Specifically, it is shown that the privacy margin increases
quadratically in the inverse of the separation between the true and the fake
paths under Eve's perspective. Numerical simulations further showcase the
approach's benefit.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.01096v1">Trustworthy Distributed AI Systems: Robustness, Privacy, and Governance</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2024-02-02T01:58:58Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Wenqi Wei, Ling Liu</p>
    <p><b>Summary:</b> Emerging Distributed AI systems are revolutionizing big data computing and
data processing capabilities with growing economic and societal impact.
However, recent studies have identified new attack surfaces and risks caused by
security, privacy, and fairness issues in AI systems. In this paper, we review
representative techniques, algorithms, and theoretical foundations for
trustworthy distributed AI through robustness guarantee, privacy protection,
and fairness awareness in distributed learning. We first provide a brief
overview of alternative architectures for distributed learning, discuss
inherent vulnerabilities for security, privacy, and fairness of AI algorithms
in distributed learning, and analyze why these problems are present in
distributed learning regardless of specific architectures. Then we provide a
unique taxonomy of countermeasures for trustworthy distributed AI, covering (1)
robustness to evasion attacks and irregular queries at inference, and
robustness to poisoning attacks, Byzantine attacks, and irregular data
distribution during training; (2) privacy protection during distributed
learning and model inference at deployment; and (3) AI fairness and governance
with respect to both data and models. We conclude with a discussion on open
challenges and future research directions toward trustworthy distributed AI,
such as the need for trustworthy AI policy guidelines, the AI
responsibility-utility co-design, and incentives and compliance.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.01001v1">Ensuring Data Privacy in AC Optimal Power Flow with a Distributed
  Co-Simulation Framework</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2024-02-01T20:31:08Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xinliang Dai, Alexander Kocher, Jovana Kovačević, Burak Dindar, Yuning Jiang, Colin N. Jones, Hüseyin Çakmak, Veit Hagenmeyer</p>
    <p><b>Summary:</b> During the energy transition, the significance of collaborative management
among institutions is rising, confronting challenges posed by data privacy
concerns. Prevailing research on distributed approaches, as an alternative to
centralized management, often lacks numerical convergence guarantees or is
limited to single-machine numerical simulation. To address this, we present a
distributed approach for solving AC Optimal Power Flow (OPF) problems within a
geographically distributed environment. This involves integrating the energy
system Co-Simulation (eCoSim) module in the eASiMOV framework with the
convergence-guaranteed distributed optimization algorithm, i.e., the Augmented
Lagrangian based Alternating Direction Inexact Newton method (ALADIN).
Comprehensive evaluations across multiple system scenarios reveal a marginal
performance slowdown compared to the centralized approach and the distributed
approach executed on single machines -- a justified trade-off for enhanced data
privacy. This investigation serves as empirical validation of the successful
execution of distributed AC OPF within a geographically distributed
environment, highlighting potential directions for future research.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.00342v1">Survey of Privacy Threats and Countermeasures in Federated Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-02-01T05:13:14Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Masahiro Hayashitani, Junki Mori, Isamu Teranishi</p>
    <p><b>Summary:</b> Federated learning is widely considered to be as a privacy-aware learning
method because no training data is exchanged directly between clients.
Nevertheless, there are threats to privacy in federated learning, and privacy
countermeasures have been studied. However, we note that common and unique
privacy threats among typical types of federated learning have not been
categorized and described in a comprehensive and specific way. In this paper,
we describe privacy threats and countermeasures for the typical types of
federated learning; horizontal federated learning, vertical federated learning,
and transfer federated learning.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2402.00906v1">BrainLeaks: On the Privacy-Preserving Properties of Neuromorphic
  Architectures against Model Inversion Attacks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Neural and Evolutionary Computing-5BC0EB">
  <p><b>Published on:</b> 2024-02-01T03:16:40Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Hamed Poursiami, Ihsen Alouani, Maryam Parsa</p>
    <p><b>Summary:</b> With the mainstream integration of machine learning into security-sensitive
domains such as healthcare and finance, concerns about data privacy have
intensified. Conventional artificial neural networks (ANNs) have been found
vulnerable to several attacks that can leak sensitive data. Particularly, model
inversion (MI) attacks enable the reconstruction of data samples that have been
used to train the model. Neuromorphic architectures have emerged as a paradigm
shift in neural computing, enabling asynchronous and energy-efficient
computation. However, little to no existing work has investigated the privacy
of neuromorphic architectures against model inversion. Our study is motivated
by the intuition that the non-differentiable aspect of spiking neural networks
(SNNs) might result in inherent privacy-preserving properties, especially
against gradient-based attacks. To investigate this hypothesis, we propose a
thorough exploration of SNNs' privacy-preserving capabilities. Specifically, we
develop novel inversion attack strategies that are comprehensively designed to
target SNNs, offering a comparative analysis with their conventional ANN
counterparts. Our experiments, conducted on diverse event-based and static
datasets, demonstrate the effectiveness of the proposed attack strategies and
therefore questions the assumption of inherent privacy-preserving in
neuromorphic architectures.</p>
  </details>
</div>

