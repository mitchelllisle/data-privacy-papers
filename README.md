
<h2>2024-08</h2>

<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.11649v1">Video-to-Text Pedestrian Monitoring (VTPM): Leveraging Computer Vision
  and Large Language Models for Privacy-Preserve Pedestrian Activity Monitoring
  at Intersections</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-08-21T14:21:53Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ahmed S. Abdelrahman, Mohamed Abdel-Aty, Dongdong Wang</p>
    <p><b>Summary:</b> Computer vision has advanced research methodologies, enhancing system
services across various fields. It is a core component in traffic monitoring
systems for improving road safety; however, these monitoring systems don't
preserve the privacy of pedestrians who appear in the videos, potentially
revealing their identities. Addressing this issue, our paper introduces
Video-to-Text Pedestrian Monitoring (VTPM), which monitors pedestrian movements
at intersections and generates real-time textual reports, including traffic
signal and weather information. VTPM uses computer vision models for pedestrian
detection and tracking, achieving a latency of 0.05 seconds per video frame.
Additionally, it detects crossing violations with 90.2% accuracy by
incorporating traffic signal data. The proposed framework is equipped with
Phi-3 mini-4k to generate real-time textual reports of pedestrian activity
while stating safety concerns like crossing violations, conflicts, and the
impact of weather on their behavior with latency of 0.33 seconds. To enhance
comprehensive analysis of the generated textual reports, Phi-3 medium is
fine-tuned for historical analysis of these generated textual reports. This
fine-tuning enables more reliable analysis about the pedestrian safety at
intersections, effectively detecting patterns and safety critical events. The
proposed VTPM offers a more efficient alternative to video footage by using
textual reports reducing memory usage, saving up to 253 million percent,
eliminating privacy issues, and enabling comprehensive interactive historical
analysis.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.11290v1">Privacy Preservation in Delay-Based Localization Systems: Artificial
  Noise or Artificial Multipath?</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2024-08-21T02:38:44Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yuchen Zhang, Hui Chen, Henk Wymeersch</p>
    <p><b>Summary:</b> Localization plays an increasingly pivotal role in 5G/6G systems, enabling
various applications. This paper focuses on the privacy concerns associated
with delay-based localization, where unauthorized base stations attempt to
infer the location of the end user. We propose a method to disrupt localization
at unauthorized nodes by injecting artificial components into the pilot signal,
exploiting model mismatches inherent in these nodes. Specifically, we
investigate the effectiveness of two techniques, namely artificial multipath
(AM) and artificial noise (AN), in mitigating location leakage. By leveraging
the misspecified Cram\'er-Rao bound framework, we evaluate the impact of these
techniques on unauthorized localization performance. Our results demonstrate
that pilot manipulation significantly degrades the accuracy of unauthorized
localization while minimally affecting legitimate localization. Moreover, we
find that the superiority of AM over AN varies depending on the specific
scenario.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.11263v1">Privacy-Preserving Data Management using Blockchains</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB">
  <p><b>Published on:</b> 2024-08-21T01:10:39Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Michael Mireku Kwakye</p>
    <p><b>Summary:</b> Privacy-preservation policies are guidelines formulated to protect data
providers private data. Previous privacy-preservation methodologies have
addressed privacy in which data are permanently stored in repositories and
disconnected from changing data provider privacy preferences. This occurrence
becomes evident as data moves to another data repository. Hence, the need for
data providers to control and flexibly update their existing privacy
preferences due to changing data usage continues to remain a problem. This
paper proposes a blockchain-based methodology for preserving data providers
private and sensitive data. The research proposes to tightly couple data
providers private attribute data element to privacy preferences and data
accessor data element into a privacy tuple. The implementation presents a
framework of tightly-coupled relational database and blockchains. This delivers
secure, tamper-resistant, and query-efficient platform for data management and
query processing. The evaluation analysis from the implementation validates
efficient query processing of privacy-aware queries on the privacy
infrastructure.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.10715v1">Fine-Tuning a Local LLaMA-3 Large Language Model for Automated
  Privacy-Preserving Physician Letter Generation in Radiation Oncology</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-08-20T10:31:36Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yihao Hou, Christoph Bert, Ahmed Gomaa, Godehard Lahmer, Daniel Hoefler, Thomas Weissmann, Raphaela Voigt, Philipp Schubert, Charlotte Schmitter, Alina Depardon, Sabine Semrau, Andreas Maier, Rainer Fietkau, Yixing Huang, Florian Putz</p>
    <p><b>Summary:</b> Generating physician letters is a time-consuming task in daily clinical
practice. This study investigates local fine-tuning of large language models
(LLMs), specifically LLaMA models, for physician letter generation in a
privacy-preserving manner within the field of radiation oncology. Our findings
demonstrate that base LLaMA models, without fine-tuning, are inadequate for
effectively generating physician letters. The QLoRA algorithm provides an
efficient method for local intra-institutional fine-tuning of LLMs with limited
computational resources (i.e., a single 48 GB GPU workstation within the
hospital). The fine-tuned LLM successfully learns radiation oncology-specific
information and generates physician letters in an institution-specific style.
ROUGE scores of the generated summary reports highlight the superiority of the
8B LLaMA-3 model over the 13B LLaMA-2 model. Further multidimensional physician
evaluations of 10 cases reveal that, although the fine-tuned LLaMA-3 model has
limited capacity to generate content beyond the provided input data, it
successfully generates salutations, diagnoses and treatment histories,
recommendations for further treatment, and planned schedules. Overall, clinical
benefit was rated highly by the clinical experts (average score of 3.44 on a
4-point scale). With careful physician review and correction, automated
LLM-based physician letter generation has significant practical value.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.10648v1">Smart Contract Coordinated Privacy Preserving Crowd-Sensing Campaigns</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Networking and Internet Architecture-04E762">
  <p><b>Published on:</b> 2024-08-20T08:41:57Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Luca Bedogni, Stefano Ferretti</p>
    <p><b>Summary:</b> Crowd-sensing has emerged as a powerful data retrieval model, enabling
diverse applications by leveraging active user participation. However, data
availability and privacy concerns pose significant challenges. Traditional
methods like data encryption and anonymization, while essential, may not fully
address these issues. For instance, in sparsely populated areas, anonymized
data can still be traced back to individual users. Additionally, the volume of
data generated by users can reveal their identities. To develop credible
crowd-sensing systems, data must be anonymized, aggregated and separated into
uniformly sized chunks. Furthermore, decentralizing the data management
process, rather than relying on a single server, can enhance security and
trust. This paper proposes a system utilizing smart contracts and blockchain
technologies to manage crowd-sensing campaigns. The smart contract handles user
subscriptions, data encryption, and decentralized storage, creating a secure
data marketplace. Incentive policies within the smart contract encourage user
participation and data diversity. Simulation results confirm the system's
viability, highlighting the importance of user participation for data
credibility and the impact of geographical data scarcity on rewards. This
approach aims to balance data origin and reduce cheating risks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.10647v1">Privacy-preserving Universal Adversarial Defense for Black-box Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2024-08-20T08:40:39Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Qiao Li, Cong Wu, Jing Chen, Zijun Zhang, Kun He, Ruiying Du, Xinxin Wang, Qingchuang Zhao, Yang Liu</p>
    <p><b>Summary:</b> Deep neural networks (DNNs) are increasingly used in critical applications
such as identity authentication and autonomous driving, where robustness
against adversarial attacks is crucial. These attacks can exploit minor
perturbations to cause significant prediction errors, making it essential to
enhance the resilience of DNNs. Traditional defense methods often rely on
access to detailed model information, which raises privacy concerns, as model
owners may be reluctant to share such data. In contrast, existing black-box
defense methods fail to offer a universal defense against various types of
adversarial attacks. To address these challenges, we introduce DUCD, a
universal black-box defense method that does not require access to the target
model's parameters or architecture. Our approach involves distilling the target
model by querying it with data, creating a white-box surrogate while preserving
data privacy. We further enhance this surrogate model using a certified defense
based on randomized smoothing and optimized noise selection, enabling robust
defense against a broad range of adversarial attacks. Comparative evaluations
between the certified defenses of the surrogate and target models demonstrate
the effectiveness of our approach. Experiments on multiple image classification
datasets show that DUCD not only outperforms existing black-box defenses but
also matches the accuracy of white-box defenses, all while enhancing data
privacy and reducing the success rate of membership inference attacks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.10468v2">Tracing Privacy Leakage of Language Models to Training Data via Adjusted
  Influence Functions</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-20T00:40:49Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jinxin Liu, Zao Yang</p>
    <p><b>Summary:</b> The responses generated by Large Language Models (LLMs) can include sensitive
information from individuals and organizations, leading to potential privacy
leakage. This work implements Influence Functions (IFs) to trace privacy
leakage back to the training data, thereby mitigating privacy concerns of
Language Models (LMs). However, we notice that current IFs struggle to
accurately estimate the influence of tokens with large gradient norms,
potentially overestimating their influence. When tracing the most influential
samples, this leads to frequently tracing back to samples with large gradient
norm tokens, overshadowing the actual most influential samples even if their
influences are well estimated. To address this issue, we propose Heuristically
Adjusted IF (HAIF), which reduces the weight of tokens with large gradient
norms, thereby significantly improving the accuracy of tracing the most
influential samples. To establish easily obtained groundtruth for tracing
privacy leakage, we construct two datasets, PII-E and PII-CR, representing two
distinct scenarios: one with identical text in the model outputs and
pre-training data, and the other where models leverage their reasoning
abilities to generate text divergent from pre-training data. HAIF significantly
improves tracing accuracy, enhancing it by 20.96\% to 73.71\% on the PII-E
dataset and 3.21\% to 45.93\% on the PII-CR dataset, compared to the best SOTA
IFs against various GPT-2 and QWen-1.5 models. HAIF also outperforms SOTA IFs
on real-world pretraining data CLUECorpus2020, demonstrating strong robustness
regardless prompt and response lengths.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.10442v1">Feasibility of assessing cognitive impairment via distributed camera
  network and privacy-preserving edge computing</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-08-19T22:34:43Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chaitra Hegde, Yashar Kiarashi, Allan I Levey, Amy D Rodriguez, Hyeokhyen Kwon, Gari D Clifford</p>
    <p><b>Summary:</b> INTRODUCTION: Mild cognitive impairment (MCI) is characterized by a decline
in cognitive functions beyond typical age and education-related expectations.
Since, MCI has been linked to reduced social interactions and increased aimless
movements, we aimed to automate the capture of these behaviors to enhance
longitudinal monitoring.
  METHODS: Using a privacy-preserving distributed camera network, we collected
movement and social interaction data from groups of individuals with MCI
undergoing therapy within a 1700$m^2$ space. We developed movement and social
interaction features, which were then used to train a series of machine
learning algorithms to distinguish between higher and lower cognitive
functioning MCI groups.
  RESULTS: A Wilcoxon rank-sum test revealed statistically significant
differences between high and low-functioning cohorts in features such as linear
path length, walking speed, change in direction while walking, entropy of
velocity and direction change, and number of group formations in the indoor
space. Despite lacking individual identifiers to associate with specific levels
of MCI, a machine learning approach using the most significant features
provided a 71% accuracy.
  DISCUSSION: We provide evidence to show that a privacy-preserving low-cost
camera network using edge computing framework has the potential to distinguish
between different levels of cognitive impairment from the movements and social
interactions captured during group activities.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.10053v1">Privacy Checklist: Privacy Violation Detection Grounding on Contextual
  Integrity Theory</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-19T14:48:04Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Haoran Li, Wei Fan, Yulin Chen, Jiayang Cheng, Tianshu Chu, Xuebing Zhou, Peizhao Hu, Yangqiu Song</p>
    <p><b>Summary:</b> Privacy research has attracted wide attention as individuals worry that their
private data can be easily leaked during interactions with smart devices,
social platforms, and AI applications. Computer science researchers, on the
other hand, commonly study privacy issues through privacy attacks and defenses
on segmented fields. Privacy research is conducted on various sub-fields,
including Computer Vision (CV), Natural Language Processing (NLP), and Computer
Networks. Within each field, privacy has its own formulation. Though pioneering
works on attacks and defenses reveal sensitive privacy issues, they are
narrowly trapped and cannot fully cover people's actual privacy concerns.
Consequently, the research on general and human-centric privacy research
remains rather unexplored. In this paper, we formulate the privacy issue as a
reasoning problem rather than simple pattern matching. We ground on the
Contextual Integrity (CI) theory which posits that people's perceptions of
privacy are highly correlated with the corresponding social context. Based on
such an assumption, we develop the first comprehensive checklist that covers
social identities, private attributes, and existing privacy regulations. Unlike
prior works on CI that either cover limited expert annotated norms or model
incomplete social context, our proposed privacy checklist uses the whole Health
Insurance Portability and Accountability Act of 1996 (HIPAA) as an example, to
show that we can resort to large language models (LLMs) to completely cover the
HIPAA's regulations. Additionally, our checklist also gathers expert
annotations across multiple ontologies to determine private information
including but not limited to personally identifiable information (PII). We use
our preliminary results on the HIPAA to shed light on future context-centric
privacy research to cover more privacy regulations, social norms and standards.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.09943v1">Calibrating Noise for Group Privacy in Subsampled Mechanisms</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-19T12:32:50Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yangfan Jiang, Xinjian Luo, Yin Yang, Xiaokui Xiao</p>
    <p><b>Summary:</b> Given a group size m and a sensitive dataset D, group privacy (GP) releases
information about D with the guarantee that the adversary cannot infer with
high confidence whether the underlying data is D or a neighboring dataset D'
that differs from D by m records. GP generalizes the well-established notion of
differential privacy (DP) for protecting individuals' privacy; in particular,
when m=1, GP reduces to DP. Compared to DP, GP is capable of protecting the
sensitive aggregate information of a group of up to m individuals, e.g., the
average annual income among members of a yacht club. Despite its longstanding
presence in the research literature and its promising applications, GP is often
treated as an afterthought, with most approaches first developing a DP
mechanism and then using a generic conversion to adapt it for GP, treating the
DP solution as a black box. As we point out in the paper, this methodology is
suboptimal when the underlying DP solution involves subsampling, e.g., in the
classic DP-SGD method for training deep learning models. In this case, the
DP-to-GP conversion is overly pessimistic in its analysis, leading to low
utility in the published results under GP.
  Motivated by this, we propose a novel analysis framework that provides tight
privacy accounting for subsampled GP mechanisms. Instead of converting a
black-box DP mechanism to GP, our solution carefully analyzes and utilizes the
inherent randomness in subsampled mechanisms, leading to a substantially
improved bound on the privacy loss with respect to GP. The proposed solution
applies to a wide variety of foundational mechanisms with subsampling.
Extensive experiments with real datasets demonstrate that compared to the
baseline convert-from-blackbox-DP approach, our GP mechanisms achieve noise
reductions of over an order of magnitude in several practical settings,
including deep neural network training.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.09935v1">Privacy Technologies for Financial Intelligence</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-19T12:13:53Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yang Li, Thilina Ranbaduge, Kee Siong Ng</p>
    <p><b>Summary:</b> Financial crimes like terrorism financing and money laundering can have real
impacts on society, including the abuse and mismanagement of public funds,
increase in societal problems such as drug trafficking and illicit gambling
with attendant economic costs, and loss of innocent lives in the case of
terrorism activities. Complex financial crimes can be hard to detect primarily
because data related to different pieces of the overall puzzle is usually
distributed across a network of financial institutions, regulators, and
law-enforcement agencies and they cannot be easily shared due to privacy
constraints. Recent advances in Privacy-Preserving Data Matching and Machine
Learning provide an opportunity for regulators and the financial industry to
come together to solve the risk-discovery problem with technology. This paper
provides a survey of the financial intelligence landscape and where
opportunities lie for privacy technologies to improve the state-of-the-art in
financial-crime detection.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.09659v1">An Algorithm for Enhancing Privacy-Utility Tradeoff in the Privacy
  Funnel and Other Lift-based Measures</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2024-08-19T02:43:18Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mohammad Amin Zarrabian, Parastoo Sadeghi</p>
    <p><b>Summary:</b> This paper investigates the privacy funnel, a privacy-utility tradeoff
problem in which mutual information quantifies both privacy and utility. The
objective is to maximize utility while adhering to a specified privacy budget.
However, the privacy funnel represents a non-convex optimization problem,
making it challenging to achieve an optimal solution. An existing proposed
approach to this problem involves substituting the mutual information with the
lift (the exponent of information density) and then solving the optimization.
Since mutual information is the expectation of the information density, this
substitution overestimates the privacy loss and results in a final smaller
bound on the privacy of mutual information than what is allowed in the budget.
This significantly compromises the utility. To overcome this limitation, we
propose using a privacy measure that is more relaxed than the lift but stricter
than mutual information while still allowing the optimization to be efficiently
solved. Instead of directly using information density, our proposed measure is
the average of information density over the sensitive data distribution for
each observed data realization. We then introduce a heuristic algorithm capable
of achieving solutions that produce extreme privacy values, which enhances
utility. The numerical results confirm improved utility at the same privacy
budget compared to existing solutions in the literature. Additionally, we
explore two other privacy measures, $\ell_{1}$-norm and strong
$\chi^2$-divergence, demonstrating the applicability of our algorithm to these
lift-based measures. We evaluate the performance of our method by comparing its
output with previous works. Finally, we validate our heuristic approach with a
theoretical framework that estimates the optimal utility for strong
$\chi^2$-divergence, numerically showing a perfect match.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.08722v1">A Novel Buffered Federated Learning Framework for Privacy-Driven Anomaly
  Detection in IIoT</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-16T13:01:59Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Samira Kamali Poorazad, Chafika Benzaid, Tarik Taleb</p>
    <p><b>Summary:</b> Industrial Internet of Things (IIoT) is highly sensitive to data privacy and
cybersecurity threats. Federated Learning (FL) has emerged as a solution for
preserving privacy, enabling private data to remain on local IIoT clients while
cooperatively training models to detect network anomalies. However, both
synchronous and asynchronous FL architectures exhibit limitations, particularly
when dealing with clients with varying speeds due to data heterogeneity and
resource constraints. Synchronous architecture suffers from straggler effects,
while asynchronous methods encounter communication bottlenecks. Additionally,
FL models are prone to adversarial inference attacks aimed at disclosing
private training data. To address these challenges, we propose a Buffered FL
(BFL) framework empowered by homomorphic encryption for anomaly detection in
heterogeneous IIoT environments. BFL utilizes a novel weighted average time
approach to mitigate both straggler effects and communication bottlenecks,
ensuring fairness between clients with varying processing speeds through
collaboration with a buffer-based server. The performance results, derived from
two datasets, show the superiority of BFL compared to state-of-the-art FL
methods, demonstrating improved accuracy and convergence speed while enhancing
privacy preservation.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.08666v1">A Multivocal Literature Review on Privacy and Fairness in Federated
  Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-08-16T11:15:52Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Beatrice Balbierer, Lukas Heinlein, Domenique Zipperling, Niklas Kühl</p>
    <p><b>Summary:</b> Federated Learning presents a way to revolutionize AI applications by
eliminating the necessity for data sharing. Yet, research has shown that
information can still be extracted during training, making additional
privacy-preserving measures such as differential privacy imperative. To
implement real-world federated learning applications, fairness, ranging from a
fair distribution of performance to non-discriminative behaviour, must be
considered. Particularly in high-risk applications (e.g. healthcare), avoiding
the repetition of past discriminatory errors is paramount. As recent research
has demonstrated an inherent tension between privacy and fairness, we conduct a
multivocal literature review to examine the current methods to integrate
privacy and fairness in federated learning. Our analyses illustrate that the
relationship between privacy and fairness has been neglected, posing a critical
risk for real-world applications. We highlight the need to explore the
relationship between privacy, fairness, and performance, advocating for the
creation of integrated federated learning frameworks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.08642v1">The Power of Bias: Optimizing Client Selection in Federated Learning
  with Heterogeneous Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-08-16T10:19:27Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jiating Ma, Yipeng Zhou, Qi Li, Quan Z. Sheng, Laizhong Cui, Jiangchuan Liu</p>
    <p><b>Summary:</b> To preserve the data privacy, the federated learning (FL) paradigm emerges in
which clients only expose model gradients rather than original data for
conducting model training. To enhance the protection of model gradients in FL,
differentially private federated learning (DPFL) is proposed which incorporates
differentially private (DP) noises to obfuscate gradients before they are
exposed. Yet, an essential but largely overlooked problem in DPFL is the
heterogeneity of clients' privacy requirement, which can vary significantly
between clients and extremely complicates the client selection problem in DPFL.
In other words, both the data quality and the influence of DP noises should be
taken into account when selecting clients. To address this problem, we conduct
convergence analysis of DPFL under heterogeneous privacy, a generic client
selection strategy, popular DP mechanisms and convex loss. Based on convergence
analysis, we formulate the client selection problem to minimize the value of
loss function in DPFL with heterogeneous privacy, which is a convex
optimization problem and can be solved efficiently. Accordingly, we propose the
DPFL-BCS (biased client selection) algorithm. The extensive experiment results
with real datasets under both convex and non-convex loss functions indicate
that DPFL-BCS can remarkably improve model utility compared with the SOTA
baselines.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.08529v1">Privacy-Preserving Vision Transformer Using Images Encrypted with
  Restricted Random Permutation Matrices</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-08-16T04:57:21Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kouki Horio, Kiyoshi Nishikawa, Hitoshi Kiya</p>
    <p><b>Summary:</b> We propose a novel method for privacy-preserving fine-tuning vision
transformers (ViTs) with encrypted images. Conventional methods using encrypted
images degrade model performance compared with that of using plain images due
to the influence of image encryption. In contrast, the proposed encryption
method using restricted random permutation matrices can provide a higher
performance than the conventional ones.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.08475v2">Models Matter: Setting Accurate Privacy Expectations for Local and
  Central Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2024-08-16T01:21:57Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mary Anne Smart, Priyanka Nanayakkara, Rachel Cummings, Gabriel Kaptchuk, Elissa Redmiles</p>
    <p><b>Summary:</b> Differential privacy is a popular privacy-enhancing technology that has been
deployed both in industry and government agencies. Unfortunately, existing
explanations of differential privacy fail to set accurate privacy expectations
for data subjects, which depend on the choice of deployment model. We design
and evaluate new explanations of differential privacy for the local and central
models, drawing inspiration from prior work explaining other privacy-enhancing
technologies. We find that consequences-focused explanations in the style of
privacy nutrition labels that lay out the implications of differential privacy
are a promising approach for setting accurate privacy expectations. Further, we
find that while process-focused explanations are not enough to set accurate
privacy expectations, combining consequences-focused explanations with a brief
description of how differential privacy works leads to greater trust.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.08107v1">Communication-robust and Privacy-safe Distributed Estimation for
  Heterogeneous Community-level Behind-the-meter Solar Power Generation</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36">
  <p><b>Published on:</b> 2024-08-15T12:11:03Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jinglei Feng, Zhengshuo Li</p>
    <p><b>Summary:</b> The rapid growth of behind-the-meter (BTM) solar power generation systems
presents challenges for distribution system planning and scheduling due to
invisible solar power generation. To address the data leakage problem of
centralized machine-learning methods in BTM solar power generation estimation,
the federated learning (FL) method has been investigated for its distributed
learning capability. However, the conventional FL method has encountered
various challenges, including heterogeneity, communication failures, and
malicious privacy attacks. To overcome these challenges, this study proposes a
communication-robust and privacy-safe distributed estimation method for
heterogeneous community-level BTM solar power generation. Specifically, this
study adopts multi-task FL as the main structure and learns the common and
unique features of all communities. Simultaneously, it embeds an updated
parameters estimation method into the multi-task FL, automatically identifies
similarities between any two clients, and estimates the updated parameters for
unavailable clients to mitigate the negative effects of communication failures.
Finally, this study adopts a differential privacy mechanism under the dynamic
privacy budget allocation strategy to combat malicious privacy attacks and
improve model training efficiency. Case studies show that in the presence of
heterogeneity and communication failures, the proposed method exhibits better
estimation accuracy and convergence performance as compared with traditional FL
and localized learning methods, while providing stronger privacy protection.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.08002v1">Practical Privacy-Preserving Identity Verification using Third-Party
  Cloud Services and FHE (Role of Data Encoding in Circuit Depth Management)</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-15T08:12:07Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Deep Inder Mohan, Srinivas Vivek</p>
    <p><b>Summary:</b> National digital identity verification systems have played a critical role in
the effective distribution of goods and services, particularly, in developing
countries. Due to the cost involved in deploying and maintaining such systems,
combined with a lack of in-house technical expertise, governments seek to
outsource this service to third-party cloud service providers to the extent
possible. This leads to increased concerns regarding the privacy of users'
personal data. In this work, we propose a practical privacy-preserving digital
identity (ID) verification protocol where the third-party cloud services
process the identity data encrypted using a (single-key) Fully Homomorphic
Encryption (FHE) scheme such as BFV. Though the role of a trusted entity such
as government is not completely eliminated, our protocol does significantly
reduces the computation load on such parties.
  A challenge in implementing a privacy-preserving ID verification protocol
using FHE is to support various types of queries such as exact and/or fuzzy
demographic and biometric matches including secure age comparisons. From a
cryptographic engineering perspective, our main technical contribution is a
user data encoding scheme that encodes demographic and biometric user data in
only two BFV ciphertexts and yet facilitates us to outsource various types of
ID verification queries to a third-party cloud. Our encoding scheme also
ensures that the only computation done by the trusted entity is a
query-agnostic "extended" decryption. This is in stark contrast with recent
works that outsource all the non-arithmetic operations to a trusted server. We
implement our protocol using the Microsoft SEAL FHE library and demonstrate its
practicality.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.07892v1">Personhood credentials: Artificial intelligence and the value of
  privacy-preserving tools to distinguish who is real online</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2024-08-15T02:41:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Steven Adler, Zoë Hitzig, Shrey Jain, Catherine Brewer, Wayne Chang, Renée DiResta, Eddy Lazzarin, Sean McGregor, Wendy Seltzer, Divya Siddarth, Nouran Soliman, Tobin South, Connor Spelliscy, Manu Sporny, Varya Srivastava, John Bailey, Brian Christian, Andrew Critch, Ronnie Falcon, Heather Flanagan, Kim Hamilton Duffy, Eric Ho, Claire R. Leibowicz, Srikanth Nadhamuni, Alan Z. Rozenshtein, David Schnurr, Evan Shapiro, Lacey Strahm, Andrew Trask, Zoe Weinberg, Cedric Whitney, Tom Zick</p>
    <p><b>Summary:</b> Anonymity is an important principle online. However, malicious actors have
long used misleading identities to conduct fraud, spread disinformation, and
carry out other deceptive schemes. With the advent of increasingly capable AI,
bad actors can amplify the potential scale and effectiveness of their
operations, intensifying the challenge of balancing anonymity and
trustworthiness online. In this paper, we analyze the value of a new tool to
address this challenge: "personhood credentials" (PHCs), digital credentials
that empower users to demonstrate that they are real people -- not AIs -- to
online services, without disclosing any personal information. Such credentials
can be issued by a range of trusted institutions -- governments or otherwise. A
PHC system, according to our definition, could be local or global, and does not
need to be biometrics-based. Two trends in AI contribute to the urgency of the
challenge: AI's increasing indistinguishability (i.e., lifelike content and
avatars, agentic activity) from people online, and AI's increasing scalability
(i.e., cost-effectiveness, accessibility). Drawing on a long history of
research into anonymous credentials and "proof-of-personhood" systems,
personhood credentials give people a way to signal their trustworthiness on
online platforms, and offer service providers new tools for reducing misuse by
bad actors. In contrast, existing countermeasures to automated deception --
such as CAPTCHAs -- are inadequate against sophisticated AI, while stringent
identity verification solutions are insufficiently private for many use-cases.
After surveying the benefits of personhood credentials, we also examine
deployment risks and design challenges. We conclude with actionable next steps
for policymakers, technologists, and standards bodies to consider in
consultation with the public.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.07614v1">Practical Considerations for Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-14T15:28:28Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kareem Amin, Alex Kulesza, Sergei Vassilvitskii</p>
    <p><b>Summary:</b> Differential privacy is the gold standard for statistical data release. Used
by governments, companies, and academics, its mathematically rigorous
guarantees and worst-case assumptions on the strength and knowledge of
attackers make it a robust and compelling framework for reasoning about
privacy. However, even with landmark successes, differential privacy has not
achieved widespread adoption in everyday data use and data protection. In this
work we examine some of the practical obstacles that stand in the way.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.07021v1">Improved Counting under Continual Observation with Pure Differential
  Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B">
  <p><b>Published on:</b> 2024-08-13T16:36:33Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Joel Daniel Andersson, Rasmus Pagh, Sahel Torkamani</p>
    <p><b>Summary:</b> Counting under continual observation is a well-studied problem in the area of
differential privacy. Given a stream of updates $x_1,x_2,\dots,x_T \in \{0,1\}$
the problem is to continuously release estimates of the prefix sums
$\sum_{i=1}^t x_i$ for $t=1,\dots,T$ while protecting each input $x_i$ in the
stream with differential privacy. Recently, significant leaps have been made in
our understanding of this problem under $\textit{approximate}$ differential
privacy, aka. $(\varepsilon,\delta)$$\textit{-differential privacy}$. However,
for the classical case of $\varepsilon$-differential privacy, we are not aware
of any improvement in mean squared error since the work of Honaker (TPDP 2015).
In this paper we present such an improvement, reducing the mean squared error
by a factor of about 4, asymptotically. The key technique is a new
generalization of the binary tree mechanism that uses a $k$-ary number system
with $\textit{negative digits}$ to improve the privacy-accuracy trade-off. Our
mechanism improves the mean squared error over all 'optimal'
$(\varepsilon,\delta)$-differentially private factorization mechanisms based on
Gaussian noise whenever $\delta$ is sufficiently small. Specifically, using
$k=19$ we get an asymptotic improvement over the bound given in the work by
Henzinger, Upadhyay and Upadhyay (SODA 2023) when $\delta = O(T^{-0.92})$.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.07006v1">The Complexities of Differential Privacy for Survey Data</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-13T16:15:42Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jörg Drechsler, James Bailie</p>
    <p><b>Summary:</b> The concept of differential privacy (DP) has gained substantial attention in
recent years, most notably since the U.S. Census Bureau announced the adoption
of the concept for its 2020 Decennial Census. However, despite its attractive
theoretical properties, implementing DP in practice remains challenging,
especially when it comes to survey data. In this paper we present some results
from an ongoing project funded by the U.S. Census Bureau that is exploring the
possibilities and limitations of DP for survey data. Specifically, we identify
five aspects that need to be considered when adopting DP in the survey context:
the multi-staged nature of data production; the limited privacy amplification
from complex sampling designs; the implications of survey-weighted estimates;
the weighting adjustments for nonresponse and other data deficiencies, and the
imputation of missing values. We summarize the project's key findings with
respect to each of these aspects and also discuss some of the challenges that
still need to be addressed before DP could become the new data protection
standard at statistical agencies.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.07004v1">Casper: Prompt Sanitization for Protecting User Privacy in Web-Based
  Large Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-08-13T16:08:37Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chun Jie Chong, Chenxi Hou, Zhihao Yao, Seyed Mohammadjavad Seyed Talebi</p>
    <p><b>Summary:</b> Web-based Large Language Model (LLM) services have been widely adopted and
have become an integral part of our Internet experience. Third-party plugins
enhance the functionalities of LLM by enabling access to real-world data and
services. However, the privacy consequences associated with these services and
their third-party plugins are not well understood. Sensitive prompt data are
stored, processed, and shared by cloud-based LLM providers and third-party
plugins. In this paper, we propose Casper, a prompt sanitization technique that
aims to protect user privacy by detecting and removing sensitive information
from user inputs before sending them to LLM services. Casper runs entirely on
the user's device as a browser extension and does not require any changes to
the online LLM services. At the core of Casper is a three-layered sanitization
mechanism consisting of a rule-based filter, a Machine Learning (ML)-based
named entity recognizer, and a browser-based local LLM topic identifier. We
evaluate Casper on a dataset of 4000 synthesized prompts and show that it can
effectively filter out Personal Identifiable Information (PII) and
privacy-sensitive topics with high accuracy, at 98.5% and 89.9%, respectively.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.08909v1">An Adaptive Differential Privacy Method Based on Federated Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2024-08-13T13:08:11Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhiqiang Wang, Xinyue Yu, Qianli Huang, Yongguang Gong</p>
    <p><b>Summary:</b> Differential privacy is one of the methods to solve the problem of privacy
protection in federated learning. Setting the same privacy budget for each
round will result in reduced accuracy in training. The existing methods of the
adjustment of privacy budget consider fewer influencing factors and tend to
ignore the boundaries, resulting in unreasonable privacy budgets. Therefore, we
proposed an adaptive differential privacy method based on federated learning.
The method sets the adjustment coefficient and scoring function according to
accuracy, loss, training rounds, and the number of datasets and clients. And
the privacy budget is adjusted based on them. Then the local model update is
processed according to the scaling factor and the noise. Fi-nally, the server
aggregates the noised local model update and distributes the noised global
model. The range of parameters and the privacy of the method are analyzed.
Through the experimental evaluation, it can reduce the privacy budget by about
16%, while the accuracy remains roughly the same.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.06460v1">Evaluating Privacy Measures for Load Hiding</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36">   
  <p><b>Published on:</b> 2024-08-12T19:21:34Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Vadim Arzamasov, Klemens Böhm</p>
    <p><b>Summary:</b> In smart grids, the use of smart meters to measure electricity consumption at
a household level raises privacy concerns. To address them, researchers have
designed various load hiding algorithms that manipulate the electricity
consumption measured. To compare how well these algorithms preserve privacy,
various privacy measures have been proposed. However, there currently is no
consensus on which privacy measure is most appropriate to use. In this study,
we aim to identify the most effective privacy measure(s) for load hiding
algorithms. We have crafted a series of experiments to assess the effectiveness
of these measures. found 20 of the 25 measures studied to be ineffective. Next,
focused on the well-known "appliance usage" secret, we have designed synthetic
data to find the measure that best deals with this secret. We observe that such
a measure, a variant of mutual information, actually exists.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.08904v1">Privacy in Federated Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-12T18:41:58Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jaydip Sen, Hetvi Waghela, Sneha Rakshit</p>
    <p><b>Summary:</b> Federated Learning (FL) represents a significant advancement in distributed
machine learning, enabling multiple participants to collaboratively train
models without sharing raw data. This decentralized approach enhances privacy
by keeping data on local devices. However, FL introduces new privacy
challenges, as model updates shared during training can inadvertently leak
sensitive information. This chapter delves into the core privacy concerns
within FL, including the risks of data reconstruction, model inversion attacks,
and membership inference. It explores various privacy-preserving techniques,
such as Differential Privacy (DP) and Secure Multi-Party Computation (SMPC),
which are designed to mitigate these risks. The chapter also examines the
trade-offs between model accuracy and privacy, emphasizing the importance of
balancing these factors in practical implementations. Furthermore, it discusses
the role of regulatory frameworks, such as GDPR, in shaping the privacy
standards for FL. By providing a comprehensive overview of the current state of
privacy in FL, this chapter aims to equip researchers and practitioners with
the knowledge necessary to navigate the complexities of secure federated
learning environments. The discussion highlights both the potential and
limitations of existing privacy-enhancing techniques, offering insights into
future research directions and the development of more robust solutions.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.06197v1">Lancelot: Towards Efficient and Privacy-Preserving Byzantine-Robust
  Federated Learning within Fully Homomorphic Encryption</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2024-08-12T14:48:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Siyang Jiang, Hao Yang, Qipeng Xie, Chuan Ma, Sen Wang, Guoliang Xing</p>
    <p><b>Summary:</b> In sectors such as finance and healthcare, where data governance is subject
to rigorous regulatory requirements, the exchange and utilization of data are
particularly challenging. Federated Learning (FL) has risen as a pioneering
distributed machine learning paradigm that enables collaborative model training
across multiple institutions while maintaining data decentralization. Despite
its advantages, FL is vulnerable to adversarial threats, particularly poisoning
attacks during model aggregation, a process typically managed by a central
server. However, in these systems, neural network models still possess the
capacity to inadvertently memorize and potentially expose individual training
instances. This presents a significant privacy risk, as attackers could
reconstruct private data by leveraging the information contained in the model
itself. Existing solutions fall short of providing a viable, privacy-preserving
BRFL system that is both completely secure against information leakage and
computationally efficient. To address these concerns, we propose Lancelot, an
innovative and computationally efficient BRFL framework that employs fully
homomorphic encryption (FHE) to safeguard against malicious client activities
while preserving data privacy. Our extensive testing, which includes medical
imaging diagnostics and widely-used public image datasets, demonstrates that
Lancelot significantly outperforms existing methods, offering more than a
twenty-fold increase in processing speed, all while maintaining data privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.06167v1">Blind-Match: Efficient Homomorphic Encryption-Based 1:N Matching for
  Privacy-Preserving Biometric Identification</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-12T14:13:08Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Hyunmin Choi, Jiwon Kim, Chiyoung Song, Simon S. Woo, Hyoungshick Kim</p>
    <p><b>Summary:</b> We present Blind-Match, a novel biometric identification system that
leverages homomorphic encryption (HE) for efficient and privacy-preserving 1:N
matching. Blind-Match introduces a HE-optimized cosine similarity computation
method, where the key idea is to divide the feature vector into smaller parts
for processing rather than computing the entire vector at once. By optimizing
the number of these parts, Blind-Match minimizes execution time while ensuring
data privacy through HE. Blind-Match achieves superior performance compared to
state-of-the-art methods across various biometric datasets. On the LFW face
dataset, Blind-Match attains a 99.63% Rank-1 accuracy with a 128-dimensional
feature vector, demonstrating its robustness in face recognition tasks. For
fingerprint identification, Blind-Match achieves a remarkable 99.55% Rank-1
accuracy on the PolyU dataset, even with a compact 16-dimensional feature
vector, significantly outperforming the state-of-the-art method, Blind-Touch,
which achieves only 59.17%. Furthermore, Blind-Match showcases practical
efficiency in large-scale biometric identification scenarios, such as Naver
Cloud's FaceSign, by processing 6,144 biometric samples in 0.74 seconds using a
128-dimensional feature vector.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.06395v1">Fast John Ellipsoid Computation with Differential Privacy Optimization</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-08-12T03:47:55Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jiuxiang Gu, Xiaoyu Li, Yingyu Liang, Zhenmei Shi, Zhao Song, Junwei Yu</p>
    <p><b>Summary:</b> Determining the John ellipsoid - the largest volume ellipsoid contained
within a convex polytope - is a fundamental problem with applications in
machine learning, optimization, and data analytics. Recent work has developed
fast algorithms for approximating the John ellipsoid using sketching and
leverage score sampling techniques. However, these algorithms do not provide
privacy guarantees for sensitive input data. In this paper, we present the
first differentially private algorithm for fast John ellipsoid computation. Our
method integrates noise perturbation with sketching and leverage score sampling
to achieve both efficiency and privacy. We prove that (1) our algorithm
provides $(\epsilon,\delta)$-differential privacy, and the privacy guarantee
holds for neighboring datasets that are $\epsilon_0$-close, allowing
flexibility in the privacy definition; (2) our algorithm still converges to a
$(1+\xi)$-approximation of the optimal John ellipsoid in
$O(\xi^{-2}(\log(n/\delta_0) + (L\epsilon_0)^{-2}))$ iterations where $n$ is
the number of data point, $L$ is the Lipschitz constant, $\delta_0$ is the
failure probability, and $\epsilon_0$ is the closeness of neighboring input
datasets. Our theoretical analysis demonstrates the algorithm's convergence and
privacy properties, providing a robust approach for balancing utility and
privacy in John ellipsoid computation. This is the first differentially private
algorithm for fast John ellipsoid computation, opening avenues for future
research in privacy-preserving optimization techniques.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.05723v1">Deep Learning with Data Privacy via Residual Perturbation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-08-11T08:26:43Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Wenqi Tao, Huaming Ling, Zuoqiang Shi, Bao Wang</p>
    <p><b>Summary:</b> Protecting data privacy in deep learning (DL) is of crucial importance.
Several celebrated privacy notions have been established and used for
privacy-preserving DL. However, many existing mechanisms achieve privacy at the
cost of significant utility degradation and computational overhead. In this
paper, we propose a stochastic differential equation-based residual
perturbation for privacy-preserving DL, which injects Gaussian noise into each
residual mapping of ResNets. Theoretically, we prove that residual perturbation
guarantees differential privacy (DP) and reduces the generalization gap of DL.
Empirically, we show that residual perturbation is computationally efficient
and outperforms the state-of-the-art differentially private stochastic gradient
descent (DPSGD) in utility maintenance without sacrificing membership privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.05543v1">PixelFade: Privacy-preserving Person Re-identification with Noise-guided
  Progressive Replacement</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-08-10T12:52:54Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Delong Zhang, Yi-Xing Peng, Xiao-Ming Wu, Ancong Wu, Wei-Shi Zheng</p>
    <p><b>Summary:</b> Online person re-identification services face privacy breaches from potential
data leakage and recovery attacks, exposing cloud-stored images to malicious
attackers and triggering public concern. The privacy protection of pedestrian
images is crucial. Previous privacy-preserving person re-identification methods
are unable to resist recovery attacks and compromise accuracy. In this paper,
we propose an iterative method (PixelFade) to optimize pedestrian images into
noise-like images to resist recovery attacks. We first give an in-depth study
of protected images from previous privacy methods, which reveal that the chaos
of protected images can disrupt the learning of recovery models. Accordingly,
Specifically, we propose Noise-guided Objective Function with the feature
constraints of a specific authorization model, optimizing pedestrian images to
normal-distributed noise images while preserving their original identity
information as per the authorization model. To solve the above non-convex
optimization problem, we propose a heuristic optimization algorithm that
alternately performs the Constraint Operation and the Partial Replacement
Operation. This strategy not only safeguards that original pixels are replaced
with noises to protect privacy, but also guides the images towards an improved
optimization direction to effectively preserve discriminative features.
Extensive experiments demonstrate that our PixelFade outperforms previous
methods in resisting recovery attacks and Re-ID performance. The code is
available at https://github.com/iSEE-Laboratory/PixelFade.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.05212v1">Preserving Privacy in Large Language Models: A Survey on Current Threats
  and Solutions</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-08-10T05:41:19Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Michele Miranda, Elena Sofia Ruzzetti, Andrea Santilli, Fabio Massimo Zanzotto, Sébastien Bratières, Emanuele Rodolà</p>
    <p><b>Summary:</b> Large Language Models (LLMs) represent a significant advancement in
artificial intelligence, finding applications across various domains. However,
their reliance on massive internet-sourced datasets for training brings notable
privacy issues, which are exacerbated in critical domains (e.g., healthcare).
Moreover, certain application-specific scenarios may require fine-tuning these
models on private data. This survey critically examines the privacy threats
associated with LLMs, emphasizing the potential for these models to memorize
and inadvertently reveal sensitive information. We explore current threats by
reviewing privacy attacks on LLMs and propose comprehensive solutions for
integrating privacy mechanisms throughout the entire learning pipeline. These
solutions range from anonymizing training datasets to implementing differential
privacy during training or inference and machine unlearning after training. Our
comprehensive review of existing literature highlights ongoing challenges,
available tools, and future directions for preserving privacy in LLMs. This
work aims to guide the development of more secure and trustworthy AI systems by
providing a thorough understanding of privacy preservation methods and their
effectiveness in mitigating risks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.05092v1">PriPHiT: Privacy-Preserving Hierarchical Training of Deep Neural
  Networks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">  
  <p><b>Published on:</b> 2024-08-09T14:33:34Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yamin Sepehri, Pedram Pad, Pascal Frossard, L. Andrea Dunbar</p>
    <p><b>Summary:</b> The training phase of deep neural networks requires substantial resources and
as such is often performed on cloud servers. However, this raises privacy
concerns when the training dataset contains sensitive content, e.g., face
images. In this work, we propose a method to perform the training phase of a
deep learning model on both an edge device and a cloud server that prevents
sensitive content being transmitted to the cloud while retaining the desired
information. The proposed privacy-preserving method uses adversarial early
exits to suppress the sensitive content at the edge and transmits the
task-relevant information to the cloud. This approach incorporates noise
addition during the training phase to provide a differential privacy guarantee.
We extensively test our method on different facial datasets with diverse face
attributes using various deep learning architectures, showcasing its
outstanding performance. We also demonstrate the effectiveness of privacy
preservation through successful defenses against different white-box and deep
reconstruction attacks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.04931v1">Privacy-Preserved Taxi Demand Prediction System Utilizing Distributed
  Data</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2024-08-09T08:24:47Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ren Ozeki, Haruki Yonekura, Hamada Rizk, Hirozumi Yamaguchi</p>
    <p><b>Summary:</b> Accurate taxi-demand prediction is essential for optimizing taxi operations
and enhancing urban transportation services. However, using customers' data in
these systems raises significant privacy and security concerns. Traditional
federated learning addresses some privacy issues by enabling model training
without direct data exchange but often struggles with accuracy due to varying
data distributions across different regions or service providers. In this
paper, we propose CC-Net: a novel approach using collaborative learning
enhanced with contrastive learning for taxi-demand prediction. Our method
ensures high performance by enabling multiple parties to collaboratively train
a demand-prediction model through hierarchical federated learning. In this
approach, similar parties are clustered together, and federated learning is
applied within each cluster. The similarity is defined without data exchange,
ensuring privacy and security. We evaluated our approach using real-world data
from five taxi service providers in Japan over fourteen months. The results
demonstrate that CC-Net maintains the privacy of customers' data while
improving prediction accuracy by at least 2.2% compared to existing techniques.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.04888v1">Locally Private Histograms in All Privacy Regimes</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Discrete Mathematics-04E762">
  <p><b>Published on:</b> 2024-08-09T06:22:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Clément L. Canonne, Abigail Gentle</p>
    <p><b>Summary:</b> Frequency estimation, a.k.a. histograms, is a workhorse of data analysis, and
as such has been thoroughly studied under differentially privacy. In
particular, computing histograms in the local model of privacy has been the
focus of a fruitful recent line of work, and various algorithms have been
proposed, achieving the order-optimal $\ell_\infty$ error in the high-privacy
(small $\varepsilon$) regime while balancing other considerations such as time-
and communication-efficiency. However, to the best of our knowledge, the
picture is much less clear when it comes to the medium- or low-privacy regime
(large $\varepsilon$), despite its increased relevance in practice. In this
paper, we investigate locally private histograms, and the very related
distribution learning task, in this medium-to-low privacy regime, and establish
near-tight (and somewhat unexpected) bounds on the $\ell_\infty$ error
achievable. Our theoretical findings emerge from a novel analysis, which
appears to improve bounds across the board for the locally private histogram
problem. We back our theoretical findings by an empirical comparison of
existing algorithms in all privacy regimes, to assess their typical performance
and behaviour beyond the worst-case setting.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.04684v1">Moving beyond privacy and airspace safety: Guidelines for just drones in
  policing</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2024-08-08T09:04:01Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mateusz Dolata, Gerhard Schwabe</p>
    <p><b>Summary:</b> The use of drones offers police forces potential gains in efficiency and
safety. However, their use may also harm public perception of the police if
drones are refused. Therefore, police forces should consider the perception of
bystanders and broader society to maximize drones' potential. This article
examines the concerns expressed by members of the public during a field trial
involving 52 test participants. Analysis of the group interviews suggests that
their worries go beyond airspace safety and privacy, broadly discussed in
existing literature and regulations. The interpretation of the results
indicates that the perceived justice of drone use is a significant factor in
acceptance. Leveraging the concept of organizational justice and data
collected, we propose a catalogue of guidelines for just operation of drones to
supplement the existing policy. We present the organizational justice
perspective as a framework to integrate the concerns of the public and
bystanders into legal work. Finally, we discuss the relevance of justice for
the legitimacy of the police's actions and provide implications for research
and practice.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.04315v1">Federated Cubic Regularized Newton Learning with
  Sparsification-amplified Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36"> 
  <p><b>Published on:</b> 2024-08-08T08:48:54Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Wei Huo, Changxin Liu, Kemi Ding, Karl Henrik Johansson, Ling Shi</p>
    <p><b>Summary:</b> This paper investigates the use of the cubic-regularized Newton method within
a federated learning framework while addressing two major concerns that
commonly arise in federated learning: privacy leakage and communication
bottleneck. We introduce a federated learning algorithm called Differentially
Private Federated Cubic Regularized Newton (DP-FCRN). By leveraging
second-order techniques, our algorithm achieves lower iteration complexity
compared to first-order methods. We also incorporate noise perturbation during
local computations to ensure privacy. Furthermore, we employ sparsification in
uplink transmission, which not only reduces the communication costs but also
amplifies the privacy guarantee. Specifically, this approach reduces the
necessary noise intensity without compromising privacy protection. We analyze
the convergence properties of our algorithm and establish the privacy
guarantee. Finally, we validate the effectiveness of the proposed algorithm
through experiments on a benchmark dataset.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.04188v1">Trustworthy Semantic-Enabled 6G Communication: A Task-oriented and
  Privacy-preserving Perspective</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36"> 
  <p><b>Published on:</b> 2024-08-08T03:16:42Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shuaishuai Guo, Anbang Zhang, Yanhu Wang, Chenyuan Feng, Tony Q. S. Quek</p>
    <p><b>Summary:</b> Trustworthy task-oriented semantic communication (ToSC) emerges as an
innovative approach in the 6G landscape, characterized by the transmission of
only vital information that is directly pertinent to a specific task. While
ToSC offers an efficient mode of communication, it concurrently raises concerns
regarding privacy, as sophisticated adversaries might possess the capability to
reconstruct the original data from the transmitted features. This article
provides an in-depth analysis of privacy-preserving strategies specifically
designed for ToSC relying on deep neural network-based joint source and channel
coding (DeepJSCC). The study encompasses a detailed comparative assessment of
trustworthy feature perturbation methods such as differential privacy and
encryption, alongside intrinsic security incorporation approaches like
adversarial learning to train the JSCC and learning-based vector quantization
(LBVQ). This comparative analysis underscores the integration of advanced
explainable learning algorithms into communication systems, positing a new
benchmark for privacy standards in the forthcoming 6G era.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.03897v1">Speech privacy-preserving methods using secret key for convolutional
  neural network models and their robustness evaluation</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Sound-D91E36">
  <p><b>Published on:</b> 2024-08-07T16:51:39Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shoko Niwa, Sayaka Shiota, Hitoshi Kiya</p>
    <p><b>Summary:</b> In this paper, we propose privacy-preserving methods with a secret key for
convolutional neural network (CNN)-based models in speech processing tasks. In
environments where untrusted third parties, like cloud servers, provide
CNN-based systems, ensuring the privacy of speech queries becomes essential.
This paper proposes encryption methods for speech queries using secret keys and
a model structure that allows for encrypted queries to be accepted without
decryption. Our approach introduces three types of secret keys: Shuffling,
Flipping, and random orthogonal matrix (ROM). In experiments, we demonstrate
that when the proposed methods are used with the correct key, identification
performance did not degrade. Conversely, when an incorrect key is used, the
performance significantly decreased. Particularly, with the use of ROM, we show
that even with a relatively small key space, high privacy-preserving
performance can be maintained many speech processing tasks. Furthermore, we
also demonstrate the difficulty of recovering original speech from encrypted
queries in various robustness evaluations.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.03578v1">Unraveling Privacy Threat Modeling Complexity: Conceptual Privacy
  Analysis Layers</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-07T06:30:20Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kim Wuyts, Avi Douglen</p>
    <p><b>Summary:</b> Analyzing privacy threats in software products is an essential part of
software development to ensure systems are privacy-respecting; yet it is still
a far from trivial activity. While there have been many advancements in the
past decade, they tend to focus on describing 'what' the threats are. What
isn't entirely clear yet is 'how' to actually find these threats. Privacy is a
complex domain. We propose to use four conceptual layers (feature, ecosystem,
business context, and environment) to capture this privacy complexity. These
layers can be used as a frame to structure and specify the privacy analysis
support in a more tangible and actionable way, thereby improving applicability
of the analysis process.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.03185v1">MaskAnyone Toolkit: Offering Strategies for Minimizing Privacy Risks and
  Maximizing Utility in Audio-Visual Data Archiving</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Multimedia-5BC0EB">
  <p><b>Published on:</b> 2024-08-06T13:35:27Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Babajide Alamu Owoyele, Martin Schilling, Rohan Sawahn, Niklas Kaemer, Pavel Zherebenkov, Bhuvanesh Verma, Wim Pouw, Gerard de Melo</p>
    <p><b>Summary:</b> This paper introduces MaskAnyone, a novel toolkit designed to navigate some
privacy and ethical concerns of sharing audio-visual data in research.
MaskAnyone offers a scalable, user-friendly solution for de-identifying
individuals in video and audio content through face-swapping and voice
alteration, supporting multi-person masking and real-time bulk processing. By
integrating this tool within research practices, we aim to enhance data
reproducibility and utility in social science research. Our approach draws on
Design Science Research, proposing that MaskAnyone can facilitate safer data
sharing and potentially reduce the storage of fully identifiable data. We
discuss the development and capabilities of MaskAnyone, explore its integration
into ethical research practices, and consider the broader implications of
audio-visual data masking, including issues of consent and the risk of misuse.
The paper concludes with a preliminary evaluation framework for assessing the
effectiveness and ethical integration of masking tools in such research
settings.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.02927v1">HARMONIC: Harnessing LLMs for Tabular Data Synthesis and Privacy
  Protection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-08-06T03:21:13Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yuxin Wang, Duanyu Feng, Yongfu Dai, Zhengyu Chen, Jimin Huang, Sophia Ananiadou, Qianqian Xie, Hao Wang</p>
    <p><b>Summary:</b> Data serves as the fundamental foundation for advancing deep learning,
particularly tabular data presented in a structured format, which is highly
conducive to modeling. However, even in the era of LLM, obtaining tabular data
from sensitive domains remains a challenge due to privacy or copyright
concerns. Hence, exploring how to effectively use models like LLMs to generate
realistic and privacy-preserving synthetic tabular data is urgent. In this
paper, we take a step forward to explore LLMs for tabular data synthesis and
privacy protection, by introducing a new framework HARMONIC for tabular data
generation and evaluation. In the tabular data generation of our framework,
unlike previous small-scale LLM-based methods that rely on continued
pre-training, we explore the larger-scale LLMs with fine-tuning to generate
tabular data and enhance privacy. Based on idea of the k-nearest neighbors
algorithm, an instruction fine-tuning dataset is constructed to inspire LLMs to
discover inter-row relationships. Then, with fine-tuning, LLMs are trained to
remember the format and connections of the data rather than the data itself,
which reduces the risk of privacy leakage. In the evaluation part of our
framework, we develop specific privacy risk metrics DLT for LLM synthetic data
generation, as well as performance evaluation metrics LLE for downstream LLM
tasks. Our experiments find that this tabular data generation framework
achieves equivalent performance to existing methods with better privacy, which
also demonstrates our evaluation framework for the effectiveness of synthetic
data and privacy risks in LLM scenarios.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.02750v1">Privacy-Safe Iris Presentation Attack Detection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> 
  <p><b>Published on:</b> 2024-08-05T18:09:02Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mahsa Mitcheff, Patrick Tinsley, Adam Czajka</p>
    <p><b>Summary:</b> This paper proposes a framework for a privacy-safe iris presentation attack
detection (PAD) method, designed solely with synthetically-generated,
identity-leakage-free iris images. Once trained, the method is evaluated in a
classical way using state-of-the-art iris PAD benchmarks. We designed two
generative models for the synthesis of ISO/IEC 19794-6-compliant iris images.
The first model synthesizes bona fide-looking samples. To avoid ``identity
leakage,'' the generated samples that accidentally matched those used in the
model's training were excluded. The second model synthesizes images of irises
with textured contact lenses and is conditioned by a given contact lens brand
to have better control over textured contact lens appearance when forming the
training set. Our experiments demonstrate that models trained solely on
synthetic data achieve a lower but still reasonable performance when compared
to solutions trained with iris images collected from human subjects. This is
the first-of-its-kind attempt to use solely synthetic data to train a
fully-functional iris PAD solution, and despite the performance gap between
regular and the proposed methods, this study demonstrates that with the
increasing fidelity of generative models, creating such privacy-safe iris PAD
methods may be possible. The source codes and generative models trained for
this work are offered along with the paper.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.02373v1">Operationalizing Contextual Integrity in Privacy-Conscious Assistants</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-08-05T10:53:51Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sahra Ghalebikesabi, Eugene Bagdasaryan, Ren Yi, Itay Yona, Ilia Shumailov, Aneesh Pappu, Chongyang Shi, Laura Weidinger, Robert Stanforth, Leonard Berrada, Pushmeet Kohli, Po-Sen Huang, Borja Balle</p>
    <p><b>Summary:</b> Advanced AI assistants combine frontier LLMs and tool access to autonomously
perform complex tasks on behalf of users. While the helpfulness of such
assistants can increase dramatically with access to user information including
emails and documents, this raises privacy concerns about assistants sharing
inappropriate information with third parties without user supervision. To steer
information-sharing assistants to behave in accordance with privacy
expectations, we propose to operationalize $\textit{contextual integrity}$
(CI), a framework that equates privacy with the appropriate flow of information
in a given context. In particular, we design and evaluate a number of
strategies to steer assistants' information-sharing actions to be CI compliant.
Our evaluation is based on a novel form filling benchmark composed of synthetic
data and human annotations, and it reveals that prompting frontier LLMs to
perform CI-based reasoning yields strong results.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.01711v1">Privacy in networks of quantum sensors</a></h3>
  
  <p><b>Published on:</b> 2024-08-03T08:39:44Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Majid Hassani, Santiago Scheiner, Matteo G. A. Paris, Damian Markham</p>
    <p><b>Summary:</b> We treat privacy in a network of quantum sensors where accessible information
is limited to specific functions of the network parameters, and all other
information remains private. We develop an analysis of privacy in terms of a
manipulation of the quantum Fisher information matrix, and find the optimal
state achieving maximum privacy in the estimation of linear combination of the
unknown parameters in a network of quantum sensors. We also discuss the effect
of uncorrelated noise on the privacy of the network. Moreover, we illustrate
our results with an example where the goal is to estimate the average value of
the unknown parameters in the network. In this example, we also introduce the
notion of quasi-privacy ($\epsilon$-privacy), quantifying how close the state
is to being private.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.01609v1">Fed-RD: Privacy-Preserving Federated Learning for Financial Crime
  Detection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computational Engineering, Finance, and Science-5BC0EB">
  <p><b>Published on:</b> 2024-08-03T00:07:10Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Md. Saikat Islam Khan, Aparna Gupta, Oshani Seneviratne, Stacy Patterson</p>
    <p><b>Summary:</b> We introduce Federated Learning for Relational Data (Fed-RD), a novel
privacy-preserving federated learning algorithm specifically developed for
financial transaction datasets partitioned vertically and horizontally across
parties. Fed-RD strategically employs differential privacy and secure
multiparty computation to guarantee the privacy of training data. We provide
theoretical analysis of the end-to-end privacy of the training algorithm and
present experimental results on realistic synthetic datasets. Our results
demonstrate that Fed-RD achieves high model accuracy with minimal degradation
as privacy increases, while consistently surpassing benchmark results.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.01228v2">The Phantom Menace: Unmasking Privacy Leakages in Vision-Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-08-02T12:36:13Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Simone Caldarella, Massimiliano Mancini, Elisa Ricci, Rahaf Aljundi</p>
    <p><b>Summary:</b> Vision-Language Models (VLMs) combine visual and textual understanding,
rendering them well-suited for diverse tasks like generating image captions and
answering visual questions across various domains. However, these capabilities
are built upon training on large amount of uncurated data crawled from the web.
The latter may include sensitive information that VLMs could memorize and leak,
raising significant privacy concerns. In this paper, we assess whether these
vulnerabilities exist, focusing on identity leakage. Our study leads to three
key findings: (i) VLMs leak identity information, even when the vision-language
alignment and the fine-tuning use anonymized data; (ii) context has little
influence on identity leakage; (iii) simple, widely used anonymization
techniques, like blurring, are not sufficient to address the problem. These
findings underscore the urgent need for robust privacy protection strategies
when deploying VLMs. Ethical awareness and responsible development practices
are essential to mitigate these risks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.01040v1">Privacy-Preserving Split Learning with Vision Transformers using
  Patch-Wise Random and Noisy CutMix</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-08-02T06:24:39Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Seungeun Oh, Sihun Baek, Jihong Park, Hyelin Nam, Praneeth Vepakomma, Ramesh Raskar, Mehdi Bennis, Seong-Lyun Kim</p>
    <p><b>Summary:</b> In computer vision, the vision transformer (ViT) has increasingly superseded
the convolutional neural network (CNN) for improved accuracy and robustness.
However, ViT's large model sizes and high sample complexity make it difficult
to train on resource-constrained edge devices. Split learning (SL) emerges as a
viable solution, leveraging server-side resources to train ViTs while utilizing
private data from distributed devices. However, SL requires additional
information exchange for weight updates between the device and the server,
which can be exposed to various attacks on private training data. To mitigate
the risk of data breaches in classification tasks, inspired from the CutMix
regularization, we propose a novel privacy-preserving SL framework that injects
Gaussian noise into smashed data and mixes randomly chosen patches of smashed
data across clients, coined DP-CutMixSL. Our analysis demonstrates that
DP-CutMixSL is a differentially private (DP) mechanism that strengthens privacy
protection against membership inference attacks during forward propagation.
Through simulations, we show that DP-CutMixSL improves privacy protection
against membership inference attacks, reconstruction attacks, and label
inference attacks, while also improving accuracy compared to DP-SL and
DP-MixSL.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.00950v1">PrivateGaze: Preserving User Privacy in Black-box Mobile Gaze Tracking
  Services</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-08-01T23:11:03Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Lingyu Du, Jinyuan Jia, Xucong Zhang, Guohao Lan</p>
    <p><b>Summary:</b> Eye gaze contains rich information about human attention and cognitive
processes. This capability makes the underlying technology, known as gaze
tracking, a critical enabler for many ubiquitous applications and has triggered
the development of easy-to-use gaze estimation services. Indeed, by utilizing
the ubiquitous cameras on tablets and smartphones, users can readily access
many gaze estimation services. In using these services, users must provide
their full-face images to the gaze estimator, which is often a black box. This
poses significant privacy threats to the users, especially when a malicious
service provider gathers a large collection of face images to classify
sensitive user attributes. In this work, we present PrivateGaze, the first
approach that can effectively preserve users' privacy in black-box gaze
tracking services without compromising gaze estimation performance.
Specifically, we proposed a novel framework to train a privacy preserver that
converts full-face images into obfuscated counterparts, which are effective for
gaze estimation while containing no privacy information. Evaluation on four
datasets shows that the obfuscated image can protect users' private
information, such as identity and gender, against unauthorized attribute
classification. Meanwhile, when used directly by the black-box gaze estimator
as inputs, the obfuscated images lead to comparable tracking performance to the
conventional, unprotected full-face images.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.00639v1">Privacy-preserving datasets by capturing feature distributions with
  Conditional VAEs</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> 
  <p><b>Published on:</b> 2024-08-01T15:26:24Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Francesco Di Salvo, David Tafler, Sebastian Doerrich, Christian Ledig</p>
    <p><b>Summary:</b> Large and well-annotated datasets are essential for advancing deep learning
applications, however often costly or impossible to obtain by a single entity.
In many areas, including the medical domain, approaches relying on data sharing
have become critical to address those challenges. While effective in increasing
dataset size and diversity, data sharing raises significant privacy concerns.
Commonly employed anonymization methods based on the k-anonymity paradigm often
fail to preserve data diversity, affecting model robustness. This work
introduces a novel approach using Conditional Variational Autoencoders (CVAEs)
trained on feature vectors extracted from large pre-trained vision foundation
models. Foundation models effectively detect and represent complex patterns
across diverse domains, allowing the CVAE to faithfully capture the embedding
space of a given data distribution to generate (sample) a diverse,
privacy-respecting, and potentially unbounded set of synthetic feature vectors.
Our method notably outperforms traditional approaches in both medical and
natural image domains, exhibiting greater dataset diversity and higher
robustness against perturbations while preserving sample privacy. These results
underscore the potential of generative models to significantly impact deep
learning applications in data-scarce and privacy-sensitive environments. The
source code is available at
https://github.com/francescodisalvo05/cvae-anonymization .</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.00382v1">Long-Term Conversation Analysis: Privacy-Utility Trade-off under Noise
  and Reverberation</a></h3>
  
  <p><b>Published on:</b> 2024-08-01T08:43:46Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jule Pohlhausen, Francesco Nespoli, Joerg Bitzer</p>
    <p><b>Summary:</b> Recordings in everyday life require privacy preservation of the speech
content and speaker identity. This contribution explores the influence of noise
and reverberation on the trade-off between privacy and utility for low-cost
privacy-preserving methods feasible for edge computing. These methods
compromise spectral and temporal smoothing, speaker anonymization using the
McAdams coefficient, sampling with a very low sampling rate, and combinations.
Privacy is assessed by automatic speech and speaker recognition, while our
utility considers voice activity detection and speaker diarization. Overall,
our evaluation shows that additional noise degrades the performance of all
models more than reverberation. This degradation corresponds to enhanced speech
privacy, while utility is less deteriorated for some methods.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.00294v1">RDP: Ranked Differential Privacy for Facial Feature Protection in
  Multiscale Sparsified Subspace</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Retrieval-5BC0EB">
  <p><b>Published on:</b> 2024-08-01T05:41:59Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Lu Ou, Shaolin Liao, Shihui Gao, Guandong Huang, Zheng Qi</p>
    <p><b>Summary:</b> With the widespread sharing of personal face images in applications' public
databases, face recognition systems faces real threat of being breached by
potential adversaries who are able to access users' face images and use them to
intrude the face recognition systems. In this paper, we propose a novel privacy
protection method in the multiscale sparsified feature subspaces to protect
sensitive facial features, by taking care of the influence or weight ranked
feature coefficients on the privacy budget, named "Ranked Differential Privacy
(RDP)". After the multiscale feature decomposition, the lightweight Laplacian
noise is added to the dimension-reduced sparsified feature coefficients
according to the geometric superposition method. Then, we rigorously prove that
the RDP satisfies Differential Privacy. After that, the nonlinear Lagrange
Multiplier (LM) method is formulated for the constraint optimization problem of
maximizing the utility of the visualization quality protected face images with
sanitizing noise, under a given facial features privacy budget. Then, two
methods are proposed to solve the nonlinear LM problem and obtain the optimal
noise scale parameters: 1) the analytical Normalization Approximation (NA)
method with identical average noise scale parameter for real-time online
applications; and 2) the LM optimization Gradient Descent (LMGD) numerical
method to obtain the nonlinear solution through iterative updating for more
accurate offline applications. Experimental results on two real-world datasets
show that our proposed RDP outperforms other state-of-the-art methods: at a
privacy budget of 0.2, the PSNR (Peak Signal-to-Noise Ratio) of the RDP is
about ~10 dB higher than (10 times as high as) the highest PSNR of all compared
methods.</p>
  </details>
</div>



<h2>2024-07</h2>

<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.21691v1">Explainable Artificial Intelligence for Quantifying Interfering and
  High-Risk Behaviors in Autism Spectrum Disorder in a Real-World Classroom
  Environment Using Privacy-Preserving Video Analysis</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-07-31T15:37:52Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Barun Das, Conor Anderson, Tania Villavicencio, Johanna Lantz, Jenny Foster, Theresa Hamlin, Ali Bahrami Rad, Gari D. Clifford, Hyeokhyen Kwon</p>
    <p><b>Summary:</b> Rapid identification and accurate documentation of interfering and high-risk
behaviors in ASD, such as aggression, self-injury, disruption, and restricted
repetitive behaviors, are important in daily classroom environments for
tracking intervention effectiveness and allocating appropriate resources to
manage care needs. However, having a staff dedicated solely to observing is
costly and uncommon in most educational settings. Recently, multiple research
studies have explored developing automated, continuous, and objective tools
using machine learning models to quantify behaviors in ASD. However, the
majority of the work was conducted under a controlled environment and has not
been validated for real-world conditions. In this work, we demonstrate that the
latest advances in video-based group activity recognition techniques can
quantify behaviors in ASD in real-world activities in classroom environments
while preserving privacy. Our explainable model could detect the episode of
problem behaviors with a 77% F1-score and capture distinctive behavior features
in different types of behaviors in ASD. To the best of our knowledge, this is
the first work that shows the promise of objectively quantifying behaviors in
ASD in a real-world environment, which is an important step toward the
development of a practical tool that can ease the burden of data collection for
classroom staff.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.21624v1">Grid-Based Decompositions for Spatial Data under Local Differential
  Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-07-31T14:17:44Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Berkay Kemal Balioglu, Alireza Khodaie, Ameer Taweel, Mehmet Emre Gursoy</p>
    <p><b>Summary:</b> Local differential privacy (LDP) has recently emerged as a popular privacy
standard. With the growing popularity of LDP, several recent works have applied
LDP to spatial data, and grid-based decompositions have been a common building
block in the collection and analysis of spatial data under DP and LDP. In this
paper, we study three grid-based decomposition methods for spatial data under
LDP: Uniform Grid (UG), PrivAG, and AAG. UG is a static approach that consists
of equal-sized cells. To enable data-dependent decomposition, PrivAG was
proposed by Yang et al. as the most recent adaptive grid method. To advance the
state-of-the-art in adaptive grids, in this paper we propose the Advanced
Adaptive Grid (AAG) method. For each grid cell, following the intuition that
the cell's intra-cell density distribution will be affected by its neighbors,
AAG performs uneven cell divisions depending on the neighboring cells'
densities. We experimentally compare UG, PrivAG, and AAG using three real-world
location datasets, varying privacy budgets, and query sizes. Results show that
AAG provides higher utility than PrivAG, demonstrating the superiority of our
proposed approach. Furthermore, UG's performance is heavily dependent on the
choice of grid size. When the grid size is chosen optimally in UG, AAG still
beats UG for small queries, but UG beats AAG for large (coarse-grained)
queries.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.21141v1">FL-DECO-BC: A Privacy-Preserving, Provably Secure, and
  Provenance-Preserving Federated Learning Framework with Decentralized Oracles
  on Blockchain for VANETs</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-07-30T19:09:10Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sathwik Narkedimilli, Rayachoti Arun Kumar, N. V. Saran Kumar, Ramapathruni Praneeth Reddy, Pavan Kumar C</p>
    <p><b>Summary:</b> Vehicular Ad-Hoc Networks (VANETs) hold immense potential for improving
traffic safety and efficiency. However, traditional centralized approaches for
machine learning in VANETs raise concerns about data privacy and security.
Federated Learning (FL) offers a solution that enables collaborative model
training without sharing raw data. This paper proposes FL-DECO-BC as a novel
privacy-preserving, provably secure, and provenance-preserving federated
learning framework specifically designed for VANETs. FL-DECO-BC leverages
decentralized oracles on blockchain to securely access external data sources
while ensuring data privacy through advanced techniques. The framework
guarantees provable security through cryptographic primitives and formal
verification methods. Furthermore, FL-DECO-BC incorporates a
provenance-preserving design to track data origin and history, fostering trust
and accountability. This combination of features empowers VANETs with secure
and privacy-conscious machine-learning capabilities, paving the way for
advanced traffic management and safety applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.20830v1">Federated Knowledge Recycling: Privacy-Preserving Synthetic Data Sharing</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-07-30T13:56:26Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Eugenio Lomurno, Matteo Matteucci</p>
    <p><b>Summary:</b> Federated learning has emerged as a paradigm for collaborative learning,
enabling the development of robust models without the need to centralise
sensitive data. However, conventional federated learning techniques have
privacy and security vulnerabilities due to the exposure of models, parameters
or updates, which can be exploited as an attack surface. This paper presents
Federated Knowledge Recycling (FedKR), a cross-silo federated learning approach
that uses locally generated synthetic data to facilitate collaboration between
institutions. FedKR combines advanced data generation techniques with a dynamic
aggregation process to provide greater security against privacy attacks than
existing methods, significantly reducing the attack surface. Experimental
results on generic and medical datasets show that FedKR achieves competitive
performance, with an average improvement in accuracy of 4.24% compared to
training models from local data, demonstrating particular effectiveness in data
scarcity scenarios.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.20640v1">Improved Bounds for Pure Private Agnostic Learning: Item-Level and
  User-Level Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-07-30T08:35:26Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Bo Li, Wei Wang, Peng Ye</p>
    <p><b>Summary:</b> Machine Learning has made remarkable progress in a wide range of fields. In
many scenarios, learning is performed on datasets involving sensitive
information, in which privacy protection is essential for learning algorithms.
In this work, we study pure private learning in the agnostic model -- a
framework reflecting the learning process in practice. We examine the number of
users required under item-level (where each user contributes one example) and
user-level (where each user contributes multiple examples) privacy and derive
several improved upper bounds. For item-level privacy, our algorithm achieves a
near optimal bound for general concept classes. We extend this to the
user-level setting, rendering a tighter upper bound than the one proved by
Ghazi et al. (2023). Lastly, we consider the problem of learning thresholds
under user-level privacy and present an algorithm with a nearly tight user
complexity.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.19828v1">Federated Learning based Latent Factorization of Tensors for
  Privacy-Preserving QoS Prediction</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-07-29T09:30:00Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shuai Zhong, Zengtong Tang, Di Wu</p>
    <p><b>Summary:</b> In applications related to big data and service computing, dynamic
connections tend to be encountered, especially the dynamic data of
user-perspective quality of service (QoS) in Web services. They are transformed
into high-dimensional and incomplete (HDI) tensors which include abundant
temporal pattern information. Latent factorization of tensors (LFT) is an
extremely efficient and typical approach for extracting such patterns from an
HDI tensor. However, current LFT models require the QoS data to be maintained
in a central place (e.g., a central server), which is impossible for
increasingly privacy-sensitive users. To address this problem, this article
creatively designs a federated learning based on latent factorization of
tensors (FL-LFT). It builds a data-density -oriented federated learning model
to enable isolated users to collaboratively train a global LFT model while
protecting user's privacy. Extensive experiments on a QoS dataset collected
from the real world verify that FL-LFT shows a remarkable increase in
prediction accuracy when compared to state-of-the-art federated learning (FL)
approaches.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.19703v1">Efficient Byzantine-Robust and Provably Privacy-Preserving Federated
  Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-07-29T04:55:30Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chenfei Nie, Qiang Li, Yuxin Yang, Yuede Ji, Binghui Wang</p>
    <p><b>Summary:</b> Federated learning (FL) is an emerging distributed learning paradigm without
sharing participating clients' private data. However, existing works show that
FL is vulnerable to both Byzantine (security) attacks and data reconstruction
(privacy) attacks. Almost all the existing FL defenses only address one of the
two attacks. A few defenses address the two attacks, but they are not efficient
and effective enough. We propose BPFL, an efficient Byzantine-robust and
provably privacy-preserving FL method that addresses all the issues.
Specifically, we draw on state-of-the-art Byzantine-robust FL methods and use
similarity metrics to measure the robustness of each participating client in
FL. The validity of clients are formulated as circuit constraints on similarity
metrics and verified via a zero-knowledge proof. Moreover, the client models
are masked by a shared random vector, which is generated based on homomorphic
encryption. In doing so, the server receives the masked client models rather
than the true ones, which are proven to be private. BPFL is also efficient due
to the usage of non-interactive zero-knowledge proof. Experimental results on
various datasets show that our BPFL is efficient, Byzantine-robust, and
privacy-preserving.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.19677v1">Navigating the United States Legislative Landscape on Voice Privacy:
  Existing Laws, Proposed Bills, Protection for Children, and Synthetic Data
  for AI</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Sound-D91E36">  
  <p><b>Published on:</b> 2024-07-29T03:43:16Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Satwik Dutta, John H. L. Hansen</p>
    <p><b>Summary:</b> Privacy is a hot topic for policymakers across the globe, including the
United States. Evolving advances in AI and emerging concerns about the misuse
of personal data have pushed policymakers to draft legislation on trustworthy
AI and privacy protection for its citizens. This paper presents the state of
the privacy legislation at the U.S. Congress and outlines how voice data is
considered as part of the legislation definition. This paper also reviews
additional privacy protection for children. This paper presents a holistic
review of enacted and proposed privacy laws, and consideration for voice data,
including guidelines for processing children's data, in those laws across the
fifty U.S. states. As a groundbreaking alternative to actual human data,
ethically generated synthetic data allows much flexibility to keep AI
innovation in progress. Given the consideration of synthetic data in AI
legislation by policymakers to be relatively new, as compared to that of
privacy laws, this paper reviews regulatory considerations for synthetic data.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.19401v1">Complete Security and Privacy for AI Inference in Decentralized Systems</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-07-28T05:09:17Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Hongyang Zhang, Yue Zhao, Claudio Angione, Harry Yang, James Buban, Ahmad Farhan, Fielding Johnston, Patrick Colangelo</p>
    <p><b>Summary:</b> The need for data security and model integrity has been accentuated by the
rapid adoption of AI and ML in data-driven domains including healthcare,
finance, and security. Large models are crucial for tasks like diagnosing
diseases and forecasting finances but tend to be delicate and not very
scalable. Decentralized systems solve this issue by distributing the workload
and reducing central points of failure. Yet, data and processes spread across
different nodes can be at risk of unauthorized access, especially when they
involve sensitive information. Nesa solves these challenges with a
comprehensive framework using multiple techniques to protect data and model
outputs. This includes zero-knowledge proofs for secure model verification. The
framework also introduces consensus-based verification checks for consistent
outputs across nodes and confirms model integrity. Split Learning divides
models into segments processed by different nodes for data privacy by
preventing full data access at any single point. For hardware-based security,
trusted execution environments are used to protect data and computations within
secure zones. Nesa's state-of-the-art proofs and principles demonstrate the
framework's effectiveness, making it a promising approach for securely
democratizing artificial intelligence.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.19364v1">Defogger: A Visual Analysis Approach for Data Exploration of Sensitive
  Data Protected by Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-07-28T02:14:12Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xumeng Wang, Shuangcheng Jiao, Chris Bryan</p>
    <p><b>Summary:</b> Differential privacy ensures the security of individual privacy but poses
challenges to data exploration processes because the limited privacy budget
incapacitates the flexibility of exploration and the noisy feedback of data
requests leads to confusing uncertainty. In this study, we take the lead in
describing corresponding exploration scenarios, including underlying
requirements and available exploration strategies. To facilitate practical
applications, we propose a visual analysis approach to the formulation of
exploration strategies. Our approach applies a reinforcement learning model to
provide diverse suggestions for exploration strategies according to the
exploration intent of users. A novel visual design for representing uncertainty
in correlation patterns is integrated into our prototype system to support the
proposed approach. Finally, we implemented a user study and two case studies.
The results of these studies verified that our approach can help develop
strategies that satisfy the exploration intent of users.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.19354v1">The Emerged Security and Privacy of LLM Agent: A Survey with Case
  Studies</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-07-28T00:26:24Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Feng He, Tianqing Zhu, Dayong Ye, Bo Liu, Wanlei Zhou, Philip S. Yu</p>
    <p><b>Summary:</b> Inspired by the rapid development of Large Language Models (LLMs), LLM agents
have evolved to perform complex tasks. LLM agents are now extensively applied
across various domains, handling vast amounts of data to interact with humans
and execute tasks. The widespread applications of LLM agents demonstrate their
significant commercial value; however, they also expose security and privacy
vulnerabilities. At the current stage, comprehensive research on the security
and privacy of LLM agents is highly needed. This survey aims to provide a
comprehensive overview of the newly emerged privacy and security issues faced
by LLM agents. We begin by introducing the fundamental knowledge of LLM agents,
followed by a categorization and analysis of the threats. We then discuss the
impacts of these threats on humans, environment, and other agents.
Subsequently, we review existing defensive strategies, and finally explore
future trends. Additionally, the survey incorporates diverse case studies to
facilitate a more accessible understanding. By highlighting these critical
security and privacy issues, the survey seeks to stimulate future research
towards enhancing the security and privacy of LLM agents, thereby increasing
their reliability and trustworthiness in future applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.19147v1">Reexamination of the realtime protection for user privacy in practical
  quantum private query</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-07-27T02:19:35Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chun-Yan Wei, Xiao-Qiu Cai, Tian-Yin Wang</p>
    <p><b>Summary:</b> Quantum private query (QPQ) is the quantum version for symmetrically private
retrieval. However, the user privacy in QPQ is generally guarded in the
non-realtime and cheat sensitive way. That is, the dishonest database holder's
cheating to elicit user privacy can only be discovered after the protocol is
finished (when the user finds some errors in the retrieved database item). Such
delayed detection may cause very unpleasant results for the user in real-life
applications. Current efforts to protect user privacy in realtime in existing
QPQ protocols mainly use two techniques, i.e., adding an honesty checking on
the database or allowing the user to reorder the qubits. We reexamine these two
kinds of QPQ protocols and find neither of them can work well. We give concrete
cheating strategies for both participants and show that honesty checking of
inner participant should be dealt more carefully in for example the choosing of
checking qubits. We hope such discussion can supply new concerns when detection
of dishonest participant is considered in quantum multi-party secure
computations.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.19119v1">Accuracy-Privacy Trade-off in the Mitigation of Membership Inference
  Attack in Federated Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-07-26T22:44:41Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sayyed Farid Ahamed, Soumya Banerjee, Sandip Roy, Devin Quinn, Marc Vucovich, Kevin Choi, Abdul Rahman, Alison Hu, Edward Bowen, Sachin Shetty</p>
    <p><b>Summary:</b> Over the last few years, federated learning (FL) has emerged as a prominent
method in machine learning, emphasizing privacy preservation by allowing
multiple clients to collaboratively build a model while keeping their training
data private. Despite this focus on privacy, FL models are susceptible to
various attacks, including membership inference attacks (MIAs), posing a
serious threat to data confidentiality. In a recent study, Rezaei \textit{et
al.} revealed the existence of an accuracy-privacy trade-off in deep ensembles
and proposed a few fusion strategies to overcome it. In this paper, we aim to
explore the relationship between deep ensembles and FL. Specifically, we
investigate whether confidence-based metrics derived from deep ensembles apply
to FL and whether there is a trade-off between accuracy and privacy in FL with
respect to MIA. Empirical investigations illustrate a lack of a non-monotonic
correlation between the number of clients and the accuracy-privacy trade-off.
By experimenting with different numbers of federated clients, datasets, and
confidence-metric-based fusion strategies, we identify and analytically justify
the clear existence of the accuracy-privacy trade-off.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.18789v1">Granularity is crucial when applying differential privacy to text: An
  investigation for neural machine translation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2024-07-26T14:52:37Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Doan Nam Long Vu, Timour Igamberdiev, Ivan Habernal</p>
    <p><b>Summary:</b> Applying differential privacy (DP) by means of the DP-SGD algorithm to
protect individual data points during training is becoming increasingly popular
in NLP. However, the choice of granularity at which DP is applied is often
neglected. For example, neural machine translation (NMT) typically operates on
the sentence-level granularity. From the perspective of DP, this setup assumes
that each sentence belongs to a single person and any two sentences in the
training dataset are independent. This assumption is however violated in many
real-world NMT datasets, e.g. those including dialogues. For proper application
of DP we thus must shift from sentences to entire documents. In this paper, we
investigate NMT at both the sentence and document levels, analyzing the
privacy/utility trade-off for both scenarios, and evaluating the risks of not
using the appropriate privacy granularity in terms of leaking personally
identifiable information (PII). Our findings indicate that the document-level
NMT system is more resistant to membership inference attacks, emphasizing the
significance of using the appropriate granularity when working with DP.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.18564v1">Unveiling Privacy Vulnerabilities: Investigating the Role of Structure
  in Graph Data</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Social and Information Networks-662E9B">
  <p><b>Published on:</b> 2024-07-26T07:40:54Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Hanyang Yuan, Jiarong Xu, Cong Wang, Ziqi Yang, Chunping Wang, Keting Yin, Yang Yang</p>
    <p><b>Summary:</b> The public sharing of user information opens the door for adversaries to
infer private data, leading to privacy breaches and facilitating malicious
activities. While numerous studies have concentrated on privacy leakage via
public user attributes, the threats associated with the exposure of user
relationships, particularly through network structure, are often neglected.
This study aims to fill this critical gap by advancing the understanding and
protection against privacy risks emanating from network structure, moving
beyond direct connections with neighbors to include the broader implications of
indirect network structural patterns. To achieve this, we first investigate the
problem of Graph Privacy Leakage via Structure (GPS), and introduce a novel
measure, the Generalized Homophily Ratio, to quantify the various mechanisms
contributing to privacy breach risks in GPS. Based on this insight, we develop
a novel graph private attribute inference attack, which acts as a pivotal tool
for evaluating the potential for privacy leakage through network structures
under worst-case scenarios. To protect users' private data from such
vulnerabilities, we propose a graph data publishing method incorporating a
learnable graph sampling technique, effectively transforming the original graph
into a privacy-preserving version. Extensive experiments demonstrate that our
attack model poses a significant threat to user privacy, and our graph data
publishing method successfully achieves the optimal privacy-utility trade-off
compared to baselines.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.18503v1">Homomorphic Encryption-Enabled Federated Learning for Privacy-Preserving
  Intrusion Detection in Resource-Constrained IoV Networks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-07-26T04:19:37Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Bui Duc Manh, Chi-Hieu Nguyen, Dinh Thai Hoang, Diep N. Nguyen</p>
    <p><b>Summary:</b> This paper aims to propose a novel framework to address the data privacy
issue for Federated Learning (FL)-based Intrusion Detection Systems (IDSs) in
Internet-of-Vehicles(IoVs) with limited computational resources. In particular,
in conventional FL systems, it is usually assumed that the computing nodes have
sufficient computational resources to process the training tasks. However, in
practical IoV systems, vehicles usually have limited computational resources to
process intensive training tasks, compromising the effectiveness of deploying
FL in IDSs. While offloading data from vehicles to the cloud can mitigate this
issue, it introduces significant privacy concerns for vehicle users (VUs). To
resolve this issue, we first propose a highly-effective framework using
homomorphic encryption to secure data that requires offloading to a centralized
server for processing. Furthermore, we develop an effective training algorithm
tailored to handle the challenges of FL-based systems with encrypted data. This
algorithm allows the centralized server to directly compute on quantum-secure
encrypted ciphertexts without needing decryption. This approach not only
safeguards data privacy during the offloading process from VUs to the
centralized server but also enhances the efficiency of utilizing FL for IDSs in
IoV systems. Our simulation results show that our proposed approach can achieve
a performance that is as close to that of the solution without encryption, with
a gap of less than 0.8%.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.18433v1">Investigating the Privacy Risk of Using Robot Vacuum Cleaners in Smart
  Environments</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-07-26T00:00:53Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Benjamin Ulsmaag, Jia-Chun Lin, Ming-Chang Lee</p>
    <p><b>Summary:</b> Robot vacuum cleaners have become increasingly popular and are widely used in
various smart environments. To improve user convenience, manufacturers also
introduced smartphone applications that enable users to customize cleaning
settings or access information about their robot vacuum cleaners. While this
integration enhances the interaction between users and their robot vacuum
cleaners, it results in potential privacy concerns because users' personal
information may be exposed. To address these concerns, end-to-end encryption is
implemented between the application, cloud service, and robot vacuum cleaners
to secure the exchanged information. Nevertheless, network header metadata
remains unencrypted and it is still vulnerable to network eavesdropping. In
this paper, we investigate the potential risk of private information exposure
through such metadata. A popular robot vacuum cleaner was deployed in a real
smart environment where passive network eavesdropping was conducted during
several selected cleaning events. Our extensive analysis, based on Association
Rule Learning, demonstrates that it is feasible to identify certain events
using only the captured Internet traffic metadata, thereby potentially exposing
private user information and raising privacy concerns.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.18353v1">Privacy-Preserving Model-Distributed Inference at the Edge</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-07-25T19:39:03Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Fatemeh Jafarian Dehkordi, Yasaman Keshtkarjahromi, Hulya Seferoglu</p>
    <p><b>Summary:</b> This paper focuses on designing a privacy-preserving Machine Learning (ML)
inference protocol for a hierarchical setup, where clients own/generate data,
model owners (cloud servers) have a pre-trained ML model, and edge servers
perform ML inference on clients' data using the cloud server's ML model. Our
goal is to speed up ML inference while providing privacy to both data and the
ML model. Our approach (i) uses model-distributed inference (model
parallelization) at the edge servers and (ii) reduces the amount of
communication to/from the cloud server. Our privacy-preserving hierarchical
model-distributed inference, privateMDI design uses additive secret sharing and
linearly homomorphic encryption to handle linear calculations in the ML
inference, and garbled circuit and a novel three-party oblivious transfer are
used to handle non-linear functions. privateMDI consists of offline and online
phases. We designed these phases in a way that most of the data exchange is
done in the offline phase while the communication overhead of the online phase
is reduced. In particular, there is no communication to/from the cloud server
in the online phase, and the amount of communication between the client and
edge servers is minimized. The experimental results demonstrate that privateMDI
significantly reduces the ML inference time as compared to the baselines.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.18157v1">Enhanced Privacy Bound for Shuffle Model with Personalized Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB">
  <p><b>Published on:</b> 2024-07-25T16:11:56Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yixuan Liu, Yuhan Liu, Li Xiong, Yujie Gu, Hong Chen</p>
    <p><b>Summary:</b> The shuffle model of Differential Privacy (DP) is an enhanced privacy
protocol which introduces an intermediate trusted server between local users
and a central data curator. It significantly amplifies the central DP guarantee
by anonymizing and shuffling the local randomized data. Yet, deriving a tight
privacy bound is challenging due to its complicated randomization protocol.
While most existing work are focused on unified local privacy settings, this
work focuses on deriving the central privacy bound for a more practical setting
where personalized local privacy is required by each user. To bound the privacy
after shuffling, we first need to capture the probability of each user
generating clones of the neighboring data points. Second, we need to quantify
the indistinguishability between two distributions of the number of clones on
neighboring datasets. Existing works either inaccurately capture the
probability, or underestimate the indistinguishability between neighboring
datasets. Motivated by this, we develop a more precise analysis, which yields a
general and tighter bound for arbitrary DP mechanisms. Firstly, we derive the
clone-generating probability by hypothesis testing %from a randomizer-specific
perspective, which leads to a more accurate characterization of the
probability. Secondly, we analyze the indistinguishability in the context of
$f$-DP, where the convexity of the distributions is leveraged to achieve a
tighter privacy bound. Theoretical and numerical results demonstrate that our
bound remarkably outperforms the existing results in the literature.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.18096v1">Privacy Threats and Countermeasures in Federated Learning for Internet
  of Things: A Systematic Review</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-07-25T15:01:56Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Adel ElZemity, Budi Arief</p>
    <p><b>Summary:</b> Federated Learning (FL) in the Internet of Things (IoT) environments can
enhance machine learning by utilising decentralised data, but at the same time,
it might introduce significant privacy and security concerns due to the
constrained nature of IoT devices. This represents a research challenge that we
aim to address in this paper. We systematically analysed recent literature to
identify privacy threats in FL within IoT environments, and evaluate the
defensive measures that can be employed to mitigate these threats. Using a
Systematic Literature Review (SLR) approach, we searched five publication
databases (Scopus, IEEE Xplore, Wiley, ACM, and Science Direct), collating
relevant papers published between 2017 and April 2024, a period which spans
from the introduction of FL until now. Guided by the PRISMA protocol, we
selected 49 papers to focus our systematic review on. We analysed these papers,
paying special attention to the privacy threats and defensive measures --
specifically within the context of IoT -- using inclusion and exclusion
criteria tailored to highlight recent advances and critical insights. We
identified various privacy threats, including inference attacks, poisoning
attacks, and eavesdropping, along with defensive measures such as Differential
Privacy and Secure Multi-Party Computation. These defences were evaluated for
their effectiveness in protecting privacy without compromising the functional
integrity of FL in IoT settings. Our review underscores the necessity for
robust and efficient privacy-preserving strategies tailored for IoT
environments. Notably, there is a need for strategies against replay, evasion,
and model stealing attacks. Exploring lightweight defensive measures and
emerging technologies such as blockchain may help improve the privacy of FL in
IoT, leading to the creation of FL models that can operate under variable
network conditions.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.05218v1">Comment on "An Efficient Privacy-Preserving Ranked Multi-Keyword
  Retrieval for Multiple Data Owners in Outsourced Cloud"</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-07-25T05:01:07Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Uma Sankararao Varri</p>
    <p><b>Summary:</b> Protecting the privacy of keywords in the field of search over outsourced
cloud data is a challenging task. In IEEE Transactions on Services Computing
(Vol. 17 No. 2, March/April 2024), Li et al. proposed PRMKR: efficient
privacy-preserving ranked multi-keyword retrieval scheme, which was claimed to
resist keyword guessing attack. However, we show that the scheme fails to
resist keyword guessing attack, index privacy, and trapdoor privacy. Further,
we propose a solution to address the above said issues by correcting the errors
in the important equations of the scheme.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.17663v1">Explaining the Model, Protecting Your Data: Revealing and Mitigating the
  Data Privacy Risks of Post-Hoc Model Explanations via Membership Inference</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-07-24T22:16:37Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Catherine Huang, Martin Pawelczyk, Himabindu Lakkaraju</p>
    <p><b>Summary:</b> Predictive machine learning models are becoming increasingly deployed in
high-stakes contexts involving sensitive personal data; in these contexts,
there is a trade-off between model explainability and data privacy. In this
work, we push the boundaries of this trade-off: with a focus on foundation
models for image classification fine-tuning, we reveal unforeseen privacy risks
of post-hoc model explanations and subsequently offer mitigation strategies for
such risks. First, we construct VAR-LRT and L1/L2-LRT, two new membership
inference attacks based on feature attribution explanations that are
significantly more successful than existing explanation-leveraging attacks,
particularly in the low false-positive rate regime that allows an adversary to
identify specific training set members with confidence. Second, we find
empirically that optimized differentially private fine-tuning substantially
diminishes the success of the aforementioned attacks, while maintaining high
model accuracy. We carry out a systematic empirical investigation of our 2 new
attacks with 5 vision transformer architectures, 5 benchmark datasets, 4
state-of-the-art post-hoc explanation methods, and 4 privacy strength settings.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.18982v1">Low-Latency Privacy-Preserving Deep Learning Design via Secure MPC</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-07-24T07:01:21Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ke Lin, Yasir Glani, Ping Luo</p>
    <p><b>Summary:</b> Secure multi-party computation (MPC) facilitates privacy-preserving
computation between multiple parties without leaking private information. While
most secure deep learning techniques utilize MPC operations to achieve feasible
privacy-preserving machine learning on downstream tasks, the overhead of the
computation and communication still hampers their practical application. This
work proposes a low-latency secret-sharing-based MPC design that reduces
unnecessary communication rounds during the execution of MPC protocols. We also
present a method for improving the computation of commonly used nonlinear
functions in deep learning by integrating multivariate multiplication and
coalescing different packets into one to maximize network utilization. Our
experimental results indicate that our method is effective in a variety of
settings, with a speedup in communication latency of $10\sim20\%$.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.17021v1">The EU-US Data Privacy Framework: Is the Dragon Eating its Own Tail?</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2024-07-24T06:00:47Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Marcelo Corrales Compagnucci</p>
    <p><b>Summary:</b> The European Commission adequacy decision on the EU US Data Privacy
Framework, adopted on July 10th, 2023, marks a crucial moment in transatlantic
data protection. Following an Executive Order issued by President Biden in
October 2022, this decision confirms that the United States meets European
Union standards for personal data protection. The decision extends to all
transfers from the European Economic Area to US entities participating in the
framework, promoting privacy rights while facilitating data exchange. Key
aspects include oversight of US public authorities access to transferred data,
the introduction of a dual tier redress mechanism, and granting new rights to
EU individuals, encompassing data access and rectification. However, the
framework presents both promise and challenges in health data transfers. While
streamlining exchange and aligning legal standards, it grapples with the
complexities of divergent privacy laws. The recent bill for the introduction of
a US federal privacy law emphasizes the urgent need for ongoing reform.
Lingering concerns persist regarding the framework resilience, especially amid
potential legal battles before the Court of Justice of the EU. The history of
transatlantic data transfers between the EU and the US is riddled with
vulnerabilities, reminiscent of the Ouroboros, an ancient symbol of a serpent
or dragon eating its own tail, hinting at the looming possibility of the
framework facing invalidation once again. This article delves into the main
requirements of the framework and offers insights on how healthcare
organizations can navigate it effectively.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.16929v2">Synthetic Data, Similarity-based Privacy Metrics, and Regulatory
  (Non-)Compliance</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2024-07-24T01:45:41Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Georgi Ganev</p>
    <p><b>Summary:</b> In this paper, we argue that similarity-based privacy metrics cannot ensure
regulatory compliance of synthetic data. Our analysis and counter-examples show
that they do not protect against singling out and linkability and, among other
fundamental issues, completely ignore the motivated intruder test.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.16735v1">Theoretical Analysis of Privacy Leakage in Trustworthy Federated
  Learning: A Perspective from Linear Algebra and Optimization Theory</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> 
  <p><b>Published on:</b> 2024-07-23T16:23:38Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xiaojin Zhang, Wei Chen</p>
    <p><b>Summary:</b> Federated learning has emerged as a promising paradigm for collaborative
model training while preserving data privacy. However, recent studies have
shown that it is vulnerable to various privacy attacks, such as data
reconstruction attacks. In this paper, we provide a theoretical analysis of
privacy leakage in federated learning from two perspectives: linear algebra and
optimization theory. From the linear algebra perspective, we prove that when
the Jacobian matrix of the batch data is not full rank, there exist different
batches of data that produce the same model update, thereby ensuring a level of
privacy. We derive a sufficient condition on the batch size to prevent data
reconstruction attacks. From the optimization theory perspective, we establish
an upper bound on the privacy leakage in terms of the batch size, the
distortion extent, and several other factors. Our analysis provides insights
into the relationship between privacy leakage and various aspects of federated
learning, offering a theoretical foundation for designing privacy-preserving
federated learning algorithms.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.16729v1">PateGail: A Privacy-Preserving Mobility Trajectory Generator with
  Imitation Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-07-23T14:59:23Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Huandong Wang, Changzheng Gao, Yuchen Wu, Depeng Jin, Lina Yao, Yong Li</p>
    <p><b>Summary:</b> Generating human mobility trajectories is of great importance to solve the
lack of large-scale trajectory data in numerous applications, which is caused
by privacy concerns. However, existing mobility trajectory generation methods
still require real-world human trajectories centrally collected as the training
data, where there exists an inescapable risk of privacy leakage. To overcome
this limitation, in this paper, we propose PateGail, a privacy-preserving
imitation learning model to generate mobility trajectories, which utilizes the
powerful generative adversary imitation learning model to simulate the
decision-making process of humans. Further, in order to protect user privacy,
we train this model collectively based on decentralized mobility data stored in
user devices, where personal discriminators are trained locally to distinguish
and reward the real and generated human trajectories. In the training process,
only the generated trajectories and their rewards obtained based on personal
discriminators are shared between the server and devices, whose privacy is
further preserved by our proposed perturbation mechanisms with theoretical
proof to satisfy differential privacy. Further, to better model the human
decision-making process, we propose a novel aggregation mechanism of the
rewards obtained from personal discriminators. We theoretically prove that
under the reward obtained based on the aggregation mechanism, our proposed
model maximizes the lower bound of the discounted total rewards of users.
Extensive experiments show that the trajectories generated by our model are
able to resemble real-world trajectories in terms of five key statistical
metrics, outperforming state-of-the-art algorithms by over 48.03%. Furthermore,
we demonstrate that the synthetic trajectories are able to efficiently support
practical applications, including mobility prediction and location
recommendation.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.16166v1">Robust Privacy Amidst Innovation with Large Language Models Through a
  Critical Assessment of the Risks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2024-07-23T04:20:14Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yao-Shun Chuang, Atiquer Rahman Sarkar, Noman Mohammed, Xiaoqian Jiang</p>
    <p><b>Summary:</b> This study examines integrating EHRs and NLP with large language models
(LLMs) to improve healthcare data management and patient care. It focuses on
using advanced models to create secure, HIPAA-compliant synthetic patient notes
for biomedical research. The study used de-identified and re-identified MIMIC
III datasets with GPT-3.5, GPT-4, and Mistral 7B to generate synthetic notes.
Text generation employed templates and keyword extraction for contextually
relevant notes, with one-shot generation for comparison. Privacy assessment
checked PHI occurrence, while text utility was tested using an ICD-9 coding
task. Text quality was evaluated with ROUGE and cosine similarity metrics to
measure semantic similarity with source notes. Analysis of PHI occurrence and
text utility via the ICD-9 coding task showed that the keyword-based method had
low risk and good performance. One-shot generation showed the highest PHI
exposure and PHI co-occurrence, especially in geographic location and date
categories. The Normalized One-shot method achieved the highest classification
accuracy. Privacy analysis revealed a critical balance between data utility and
privacy protection, influencing future data use and sharing. Re-identified data
consistently outperformed de-identified data. This study demonstrates the
effectiveness of keyword-based methods in generating privacy-protecting
synthetic clinical notes that retain data usability, potentially transforming
clinical data-sharing practices. The superior performance of re-identified over
de-identified data suggests a shift towards methods that enhance utility and
privacy by using dummy PHIs to perplex privacy attacks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.16164v1">Representation Magnitude has a Liability to Privacy Vulnerability</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-07-23T04:13:52Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xingli Fang, Jung-Eun Kim</p>
    <p><b>Summary:</b> The privacy-preserving approaches to machine learning (ML) models have made
substantial progress in recent years. However, it is still opaque in which
circumstances and conditions the model becomes privacy-vulnerable, leading to a
challenge for ML models to maintain both performance and privacy. In this
paper, we first explore the disparity between member and non-member data in the
representation of models under common training frameworks. We identify how the
representation magnitude disparity correlates with privacy vulnerability and
address how this correlation impacts privacy vulnerability. Based on the
observations, we propose Saturn Ring Classifier Module (SRCM), a plug-in
model-level solution to mitigate membership privacy leakage. Through a confined
yet effective representation space, our approach ameliorates models' privacy
vulnerability while maintaining generalizability. The code of this work can be
found here: \url{https://github.com/JEKimLab/AIES2024_SRCM}</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.15957v1">Escalation of Commitment: A Case Study of the United States Census
  Bureau Efforts to Implement Differential Privacy for the 2020 Decennial
  Census</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB">
  <p><b>Published on:</b> 2024-07-22T18:13:52Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Krish Muralidhar, Steven Ruggles</p>
    <p><b>Summary:</b> In 2017, the United States Census Bureau announced that because of high
disclosure risk in the methodology (data swapping) used to produce tabular data
for the 2010 census, a different protection mechanism based on differential
privacy would be used for the 2020 census. While there have been many studies
evaluating the result of this change, there has been no rigorous examination of
disclosure risk claims resulting from the released 2010 tabular data. In this
study we perform such an evaluation. We show that the procedures used to
evaluate disclosure risk are unreliable and resulted in inflated disclosure
risk. Demonstration data products released using the new procedure were also
shown to have poor utility. However, since the Census Bureau had already
committed to a different procedure, they had no option except to escalate their
commitment. The result of such escalation is that the 2020 tabular data release
offers neither privacy nor accuracy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.15407v1">A Solution toward Transparent and Practical AI Regulation: Privacy
  Nutrition Labels for Open-source Generative AI-based Applications</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Software Engineering-D91E36">
  <p><b>Published on:</b> 2024-07-22T06:24:13Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Meixue Si, Shidong Pan, Dianshu Liao, Xiaoyu Sun, Zhen Tao, Wenchang Shi, Zhenchang Xing</p>
    <p><b>Summary:</b> The rapid development and widespread adoption of Generative Artificial
Intelligence-based (GAI) applications have greatly enriched our daily lives,
benefiting people by enhancing creativity, personalizing experiences, improving
accessibility, and fostering innovation and efficiency across various domains.
However, along with the development of GAI applications, concerns have been
raised about transparency in their privacy practices. Traditional privacy
policies often fail to effectively communicate essential privacy information
due to their complexity and length, and open-source community developers often
neglect privacy practices even more. Only 12.2% of examined open-source GAI
apps provide a privacy policy. To address this, we propose a regulation-driven
GAI Privacy Label and introduce Repo2Label, a novel framework for automatically
generating these labels based on code repositories. Our user study indicates a
common endorsement of the proposed GAI privacy label format. Additionally,
Repo2Label achieves a precision of 0.81, recall of 0.88, and F1-score of 0.84
based on the benchmark dataset, significantly outperforming the developer
self-declared privacy notices. We also discuss the common regulatory
(in)compliance of open-source GAI apps, comparison with other privacy notices,
and broader impacts to different stakeholders. Our findings suggest that
Repo2Label could serve as a significant tool for bolstering the privacy
transparency of GAI apps and make them more practical and responsible.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.15224v1">PUFFLE: Balancing Privacy, Utility, and Fairness in Federated Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2024-07-21T17:22:18Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Luca Corbucci, Mikko A Heikkila, David Solans Noguero, Anna Monreale, Nicolas Kourtellis</p>
    <p><b>Summary:</b> Training and deploying Machine Learning models that simultaneously adhere to
principles of fairness and privacy while ensuring good utility poses a
significant challenge. The interplay between these three factors of
trustworthiness is frequently underestimated and remains insufficiently
explored. Consequently, many efforts focus on ensuring only two of these
factors, neglecting one in the process. The decentralization of the datasets
and the variations in distributions among the clients exacerbate the complexity
of achieving this ethical trade-off in the context of Federated Learning (FL).
For the first time in FL literature, we address these three factors of
trustworthiness. We introduce PUFFLE, a high-level parameterised approach that
can help in the exploration of the balance between utility, privacy, and
fairness in FL scenarios. We prove that PUFFLE can be effective across diverse
datasets, models, and data distributions, reducing the model unfairness up to
75%, with a maximum reduction in the utility of 17% in the worst-case scenario,
while maintaining strict privacy guarantees during the FL training.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.15220v1">Privacy-Preserving Multi-Center Differential Protein Abundance Analysis
  with FedProt</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-07-21T17:09:20Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yuliya Burankova, Miriam Abele, Mohammad Bakhtiari, Christine von Törne, Teresa Barth, Lisa Schweizer, Pieter Giesbertz, Johannes R. Schmidt, Stefan Kalkhof, Janina Müller-Deile, Peter A van Veelen, Yassene Mohammed, Elke Hammer, Lis Arend, Klaudia Adamowicz, Tanja Laske, Anne Hartebrodt, Tobias Frisch, Chen Meng, Julian Matschinske, Julian Späth, Richard Röttger, Veit Schwämmle, Stefanie M. Hauck, Stefan Lichtenthaler, Axel Imhof, Matthias Mann, Christina Ludwig, Bernhard Kuster, Jan Baumbach, Olga Zolotareva</p>
    <p><b>Summary:</b> Quantitative mass spectrometry has revolutionized proteomics by enabling
simultaneous quantification of thousands of proteins. Pooling patient-derived
data from multiple institutions enhances statistical power but raises
significant privacy concerns. Here we introduce FedProt, the first
privacy-preserving tool for collaborative differential protein abundance
analysis of distributed data, which utilizes federated learning and additive
secret sharing. In the absence of a multicenter patient-derived dataset for
evaluation, we created two, one at five centers from LFQ E.coli experiments and
one at three centers from TMT human serum. Evaluations using these datasets
confirm that FedProt achieves accuracy equivalent to DEqMS applied to pooled
data, with completely negligible absolute differences no greater than $\text{$4
\times 10^{-12}$}$. In contrast, -log10(p-values) computed by the most accurate
meta-analysis methods diverged from the centralized analysis results by up to
25-27. FedProt is available as a web tool with detailed documentation as a
FeatureCloud App.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.14938v1">From Ad Identifiers to Global Privacy Control: The Status Quo and Future
  of Opting Out of Ad Tracking on Android</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2024-07-20T17:06:23Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sebastian Zimmeck, Nishant Aggarwal, Zachary Liu, Konrad Kollnig</p>
    <p><b>Summary:</b> Apps and their integrated third party libraries often collect a variety of
data from people to show them personalized ads. This practice is often
privacy-invasive. Since 2013, Google has therefore allowed users to limit ad
tracking on Android via system settings. Further, under the 2018 California
Consumer Privacy Act (CCPA), apps must honor opt-outs from ad tracking under
the Global Privacy Control (GPC). The efficacy of these two methods to limit ad
tracking has not been studied in prior work. Our legal and technical analysis
details how the GPC applies to mobile apps and how it could be integrated
directly into Android, thereby developing a reference design for GPC on
Android. Our empirical analysis of 1,896 top-ranked Android apps shows that
both the Android system-level opt-out and the GPC signal rarely restrict ad
tracking. In our view, deleting the AdID and opting out under the CCPA has the
same meaning. Thus, the current AdID setting and APIs should be evolved towards
GPC and integrated into Android's Privacy Sandbox.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.14719v1">Universal Medical Imaging Model for Domain Generalization with Data
  Privacy</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-07-20T01:24:15Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ahmed Radwan, Islam Osman, Mohamed S. Shehata</p>
    <p><b>Summary:</b> Achieving domain generalization in medical imaging poses a significant
challenge, primarily due to the limited availability of publicly labeled
datasets in this domain. This limitation arises from concerns related to data
privacy and the necessity for medical expertise to accurately label the data.
In this paper, we propose a federated learning approach to transfer knowledge
from multiple local models to a global model, eliminating the need for direct
access to the local datasets used to train each model. The primary objective is
to train a global model capable of performing a wide variety of medical imaging
tasks. This is done while ensuring the confidentiality of the private datasets
utilized during the training of these models. To validate the effectiveness of
our approach, extensive experiments were conducted on eight datasets, each
corresponding to a different medical imaging application. The client's data
distribution in our experiments varies significantly as they originate from
diverse domains. Despite this variation, we demonstrate a statistically
significant improvement over a state-of-the-art baseline utilizing masked image
modeling over a diverse pre-training dataset that spans different body parts
and scanning types. This improvement is achieved by curating information
learned from clients without accessing any labeled dataset on the server.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.14717v1">Differential Privacy of Cross-Attention with Provable Guarantee</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-07-20T01:02:27Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jiuxiang Gu, Yingyu Liang, Zhenmei Shi, Zhao Song, Yufa Zhou</p>
    <p><b>Summary:</b> Cross-attention has become a fundamental module nowadays in many important
artificial intelligence applications, e.g., retrieval-augmented generation
(RAG), system prompt, guided stable diffusion, and many so on. Ensuring
cross-attention privacy is crucial and urgently needed because its key and
value matrices may contain sensitive information about companies and their
users, many of which profit solely from their system prompts or RAG data. In
this work, we design a novel differential privacy (DP) data structure to
address the privacy security of cross-attention with a theoretical guarantee.
In detail, let $n$ be the input token length of system prompt/RAG data, $d$ be
the feature dimension, $0 < \alpha \le 1$ be the relative error parameter, $R$
be the maximum value of the query and key matrices, $R_w$ be the maximum value
of the value matrix, and $r,s,\epsilon_s$ be parameters of polynomial kernel
methods. Then, our data structure requires $\widetilde{O}(ndr^2)$ memory
consumption with $\widetilde{O}(nr^2)$ initialization time complexity and
$\widetilde{O}(\alpha^{-1} r^2)$ query time complexity for a single token
query. In addition, our data structure can guarantee that the user query is
$(\epsilon, \delta)$-DP with $\widetilde{O}(n^{-1} \epsilon^{-1} \alpha^{-1/2}
R^{2s} R_w r^2)$ additive error and $n^{-1} (\alpha + \epsilon_s)$ relative
error between our output and the true answer. Furthermore, our result is robust
to adaptive queries in which users can intentionally attack the cross-attention
system. To our knowledge, this is the first work to provide DP for
cross-attention. We believe it can inspire more privacy algorithm design in
large generative models (LGMs).</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.14710v2">Universally Harmonizing Differential Privacy Mechanisms for Federated
  Learning: Boosting Accuracy and Convergence</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-07-20T00:11:59Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shuya Feng, Meisam Mohammady, Hanbin Hong, Shenao Yan, Ashish Kundu, Binghui Wang, Yuan Hong</p>
    <p><b>Summary:</b> Differentially private federated learning (DP-FL) is a promising technique
for collaborative model training while ensuring provable privacy for clients.
However, optimizing the tradeoff between privacy and accuracy remains a
critical challenge. To our best knowledge, we propose the first DP-FL framework
(namely UDP-FL), which universally harmonizes any randomization mechanism
(e.g., an optimal one) with the Gaussian Moments Accountant (viz. DP-SGD) to
significantly boost accuracy and convergence. Specifically, UDP-FL demonstrates
enhanced model performance by mitigating the reliance on Gaussian noise. The
key mediator variable in this transformation is the R\'enyi Differential
Privacy notion, which is carefully used to harmonize privacy budgets. We also
propose an innovative method to theoretically analyze the convergence for DP-FL
(including our UDP-FL ) based on mode connectivity analysis. Moreover, we
evaluate our UDP-FL through extensive experiments benchmarked against
state-of-the-art (SOTA) methods, demonstrating superior performance on both
privacy guarantees and model performance. Notably, UDP-FL exhibits substantial
resilience against different inference attacks, indicating a significant
advance in safeguarding sensitive data in federated learning environments.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.14641v1">Differential Privacy with Multiple Selections</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-07-19T19:34:51Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ashish Goel, Zhihao Jiang, Aleksandra Korolova, Kamesh Munagala, Sahasrajit Sarmasarkar</p>
    <p><b>Summary:</b> We consider the setting where a user with sensitive features wishes to obtain
a recommendation from a server in a differentially private fashion. We propose
a ``multi-selection'' architecture where the server can send back multiple
recommendations and the user chooses one from these that matches best with
their private features. When the user feature is one-dimensional -- on an
infinite line -- and the accuracy measure is defined w.r.t some increasing
function $\mathfrak{h}(.)$ of the distance on the line, we precisely
characterize the optimal mechanism that satisfies differential privacy. The
specification of the optimal mechanism includes both the distribution of the
noise that the user adds to its private value, and the algorithm used by the
server to determine the set of results to send back as a response and further
show that Laplace is an optimal noise distribution. We further show that this
optimal mechanism results in an error that is inversely proportional to the
number of results returned when the function $\mathfrak{h}(.)$ is the identity
function.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.13975v1">Personalized Privacy Protection Mask Against Unauthorized Facial
  Recognition</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-07-19T01:59:00Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ka-Ho Chow, Sihao Hu, Tiansheng Huang, Ling Liu</p>
    <p><b>Summary:</b> Face recognition (FR) can be abused for privacy intrusion. Governments,
private companies, or even individual attackers can collect facial images by
web scraping to build an FR system identifying human faces without their
consent. This paper introduces Chameleon, which learns to generate a
user-centric personalized privacy protection mask, coined as P3-Mask, to
protect facial images against unauthorized FR with three salient features.
First, we use a cross-image optimization to generate one P3-Mask for each user
instead of tailoring facial perturbation for each facial image of a user. It
enables efficient and instant protection even for users with limited computing
resources. Second, we incorporate a perceptibility optimization to preserve the
visual quality of the protected facial images. Third, we strengthen the
robustness of P3-Mask against unknown FR models by integrating focal
diversity-optimized ensemble learning into the mask generation process.
Extensive experiments on two benchmark datasets show that Chameleon outperforms
three state-of-the-art methods with instant protection and minimal degradation
of image quality. Furthermore, Chameleon enables cost-effective FR
authorization using the P3-Mask as a personalized de-obfuscation key, and it
demonstrates high resilience against adaptive adversaries.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.13881v1">Privacy-preserving gradient-based fair federated learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36"> 
  <p><b>Published on:</b> 2024-07-18T19:56:39Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Janis Adamek, Moritz Schulze Darup</p>
    <p><b>Summary:</b> Federated learning (FL) schemes allow multiple participants to
collaboratively train neural networks without the need to directly share the
underlying data.However, in early schemes, all participants eventually obtain
the same model. Moreover, the aggregation is typically carried out by a third
party, who obtains combined gradients or weights, which may reveal the model.
These downsides underscore the demand for fair and privacy-preserving FL
schemes. Here, collaborative fairness asks for individual model quality
depending on the individual data contribution. Privacy is demanded with respect
to any kind of data outsourced to the third party. Now, there already exist
some approaches aiming for either fair or privacy-preserving FL and a few works
even address both features. In our paper, we build upon these seminal works and
present a novel, fair and privacy-preserving FL scheme. Our approach, which
mainly relies on homomorphic encryption, stands out for exclusively using local
gradients. This increases the usability in comparison to state-of-the-art
approaches and thereby opens the door to applications in control.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.13725v1">Scalable Optimization for Locally Relevant Geo-Location Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-07-18T17:25:08Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chenxi Qiu, Ruiyao Liu, Primal Pappachan, Anna Squicciarini, Xinpeng Xie</p>
    <p><b>Summary:</b> Geo-obfuscation functions as a location privacy protection mechanism (LPPM),
enabling mobile users to share obfuscated locations with servers instead of
their exact locations. This technique protects users' location privacy during
server-side data breaches since the obfuscation process is irreversible. To
minimize the utility loss caused by data obfuscation, linear programming (LP)
is widely used. However, LP can face a polynomial explosion in decision
variables, making it impractical for large-scale geo-obfuscation applications.
In this paper, we propose a new LPPM called Locally Relevant Geo-obfuscation
(LR-Geo) to optimize geo-obfuscation using LP more efficiently. This is
accomplished by restricting the geo-obfuscation calculations for each user to
locally relevant (LR) locations near the user's actual location. To prevent LR
locations from inadvertently revealing a user's true whereabouts, users compute
the LP coefficients locally and upload only these coefficients to the server,
rather than the LR locations themselves. The server then solves the LP problem
using the provided coefficients. Additionally, we enhance the LP framework with
an exponential obfuscation mechanism to ensure that the obfuscation
distribution is indistinguishable across multiple users. By leveraging the
constraint structure of the LP formulation, we apply Benders' decomposition to
further boost computational efficiency. Our theoretical analysis confirms that,
even though geo-obfuscation is calculated independently for each user, it still
adheres to geo-indistinguishability constraints across multiple users with high
probability. Finally, experimental results using a real-world dataset
demonstrate that LR-Geo outperforms existing geo-obfuscation methods in terms
of computational time, data utility, and privacy protection.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.13621v1">Differential Privacy Mechanisms in Neural Tangent Kernel Regression</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-07-18T15:57:55Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jiuxiang Gu, Yingyu Liang, Zhizhou Sha, Zhenmei Shi, Zhao Song</p>
    <p><b>Summary:</b> Training data privacy is a fundamental problem in modern Artificial
Intelligence (AI) applications, such as face recognition, recommendation
systems, language generation, and many others, as it may contain sensitive user
information related to legal issues. To fundamentally understand how privacy
mechanisms work in AI applications, we study differential privacy (DP) in the
Neural Tangent Kernel (NTK) regression setting, where DP is one of the most
powerful tools for measuring privacy under statistical learning, and NTK is one
of the most popular analysis frameworks for studying the learning mechanisms of
deep neural networks. In our work, we can show provable guarantees for both
differential privacy and test accuracy of our NTK regression. Furthermore, we
conduct experiments on the basic image classification dataset CIFAR10 to
demonstrate that NTK regression can preserve good accuracy under a modest
privacy budget, supporting the validity of our analysis. To our knowledge, this
is the first work to provide a DP guarantee for NTK regression.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.13532v1">PriPL-Tree: Accurate Range Query for Arbitrary Distribution under Local
  Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB">
  <p><b>Published on:</b> 2024-07-18T14:05:35Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Leixia Wang, Qingqing Ye, Haibo Hu, Xiaofeng Meng</p>
    <p><b>Summary:</b> Answering range queries in the context of Local Differential Privacy (LDP) is
a widely studied problem in Online Analytical Processing (OLAP). Existing LDP
solutions all assume a uniform data distribution within each domain partition,
which may not align with real-world scenarios where data distribution is
varied, resulting in inaccurate estimates. To address this problem, we
introduce PriPL-Tree, a novel data structure that combines hierarchical tree
structures with piecewise linear (PL) functions to answer range queries for
arbitrary distributions. PriPL-Tree precisely models the underlying data
distribution with a few line segments, leading to more accurate results for
range queries. Furthermore, we extend it to multi-dimensional cases with novel
data-aware adaptive grids. These grids leverage the insights from marginal
distributions obtained through PriPL-Trees to partition the grids adaptively,
adapting the density of underlying distributions. Our extensive experiments on
both real and synthetic datasets demonstrate the effectiveness and superiority
of PriPL-Tree over state-of-the-art solutions in answering range queries across
arbitrary data distributions.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.13516v1">Optimal Mechanisms for Quantum Local Differential Privacy</a></h3>
  
  <p><b>Published on:</b> 2024-07-18T13:46:16Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ji Guan</p>
    <p><b>Summary:</b> In recent years, centralized differential privacy has been successfully
extended to quantum computing and information processing to safeguard privacy
and prevent leaks in neighboring relationships of quantum states. This paper
introduces a framework known as quantum local differential privacy (QLDP) and
initializes the algorithmic study of QLDP. QLDP utilizes a parameter $\epsilon$
to manage privacy leaks and ensure the privacy of individual quantum states.
The optimization of the QLDP value $\epsilon$, denoted as $\epsilon^*$, for any
quantum mechanism is addressed as an optimization problem. The introduction of
quantum noise is shown to provide privacy protections similar to classical
scenarios, with quantum depolarizing noise identified as the optimal unital
privatization mechanism within the QLDP framework. Unital mechanisms represent
a diverse set of quantum mechanisms that encompass frequently employed quantum
noise types. Quantum depolarizing noise optimizes both fidelity and trace
distance utilities, which are crucial metrics in the field of quantum
computation and information, and can be viewed as a quantum counterpart to
classical randomized response methods. Additionally, a composition theorem is
presented for the application of QLDP framework in distributed (spatially
separated) quantum systems, ensuring the validity (additivity of QLDP value)
irrespective of the states' independence, classical correlation, or
entanglement (quantum correlation). The study further explores the trade-off
between utility and privacy across different quantum noise mechanisms,
including unital and non-unital quantum noise mechanisms, through both
analytical and numerically experimental approaches. Meanwhile, this highlights
the optimization of quantum depolarizing noise in QLDP framework.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.13153v1">Preset-Voice Matching for Privacy Regulated Speech-to-Speech Translation
  Systems</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Sound-D91E36"> 
  <p><b>Published on:</b> 2024-07-18T04:42:01Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Daniel Platnick, Bishoy Abdelnour, Eamon Earl, Rahul Kumar, Zahra Rezaei, Thomas Tsangaris, Faraj Lagum</p>
    <p><b>Summary:</b> In recent years, there has been increased demand for speech-to-speech
translation (S2ST) systems in industry settings. Although successfully
commercialized, cloning-based S2ST systems expose their distributors to
liabilities when misused by individuals and can infringe on personality rights
when exploited by media organizations. This work proposes a regulated S2ST
framework called Preset-Voice Matching (PVM). PVM removes cross-lingual voice
cloning in S2ST by first matching the input voice to a similar prior consenting
speaker voice in the target-language. With this separation, PVM avoids cloning
the input speaker, ensuring PVM systems comply with regulations and reduce risk
of misuse. Our results demonstrate PVM can significantly improve S2ST system
run-time in multi-speaker settings and the naturalness of S2ST synthesized
speech. To our knowledge, PVM is the first explicitly regulated S2ST framework
leveraging similarly-matched preset-voices for dynamic S2ST tasks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2407.15868v1">A Survey on Differential Privacy for SpatioTemporal Data in
  Transportation Research</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">  
  <p><b>Published on:</b> 2024-07-18T03:19:29Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Rahul Bhadani</p>
    <p><b>Summary:</b> With low-cost computing devices, improved sensor technology, and the
proliferation of data-driven algorithms, we have more data than we know what to
do with. In transportation, we are seeing a surge in spatiotemporal data
collection. At the same time, concerns over user privacy have led to research
on differential privacy in applied settings. In this paper, we look at some
recent developments in differential privacy in the context of spatiotemporal
data. Spatiotemporal data contain not only features about users but also the
geographical locations of their frequent visits. Hence, the public release of
such data carries extreme risks. To address the need for such data in research
and inference without exposing private information, significant work has been
proposed. This survey paper aims to summarize these efforts and provide a
review of differential privacy mechanisms and related software. We also discuss
related work in transportation where such mechanisms have been applied.
Furthermore, we address the challenges in the deployment and mass adoption of
differential privacy in transportation spatiotemporal data for downstream
analyses.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2408.01428v1">Transferable Adversarial Facial Images for Privacy Protection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-07-18T02:16:11Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Minghui Li, Jiangxiong Wang, Hao Zhang, Ziqi Zhou, Shengshan Hu, Xiaobing Pei</p>
    <p><b>Summary:</b> The success of deep face recognition (FR) systems has raised serious privacy
concerns due to their ability to enable unauthorized tracking of users in the
digital world. Previous studies proposed introducing imperceptible adversarial
noises into face images to deceive those face recognition models, thus
achieving the goal of enhancing facial privacy protection. Nevertheless, they
heavily rely on user-chosen references to guide the generation of adversarial
noises, and cannot simultaneously construct natural and highly transferable
adversarial face images in black-box scenarios. In light of this, we present a
novel face privacy protection scheme with improved transferability while
maintain high visual quality. We propose shaping the entire face space directly
instead of exploiting one kind of facial characteristic like makeup information
to integrate adversarial noises. To achieve this goal, we first exploit global
adversarial latent search to traverse the latent space of the generative model,
thereby creating natural adversarial face images with high transferability. We
then introduce a key landmark regularization module to preserve the visual
identity information. Finally, we investigate the impacts of various kinds of
latent spaces and find that $\mathcal{F}$ latent space benefits the trade-off
between visual naturalness and adversarial transferability. Extensive
experiments over two datasets demonstrate that our approach significantly
enhances attack transferability while maintaining high visual quality,
outperforming state-of-the-art methods by an average 25% improvement in deep FR
models and 10% improvement on commercial FR APIs, including Face++, Aliyun, and
Tencent.</p>
  </details>
</div>

