
<h2>2025-01</h2>

<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.03222v1">Characterizing the Accuracy-Communication-Privacy Trade-off in
  Distributed Stochastic Convex Optimization</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36">  
  <p><b>Published on:</b> 2025-01-06T18:57:05Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sudeep Salgia, Nikola Pavlovic, Yuejie Chi, Qing Zhao</p>
    <p><b>Summary:</b> We consider the problem of differentially private stochastic convex
optimization (DP-SCO) in a distributed setting with $M$ clients, where each of
them has a local dataset of $N$ i.i.d. data samples from an underlying data
distribution. The objective is to design an algorithm to minimize a convex
population loss using a collaborative effort across $M$ clients, while ensuring
the privacy of the local datasets. In this work, we investigate the
accuracy-communication-privacy trade-off for this problem. We establish
matching converse and achievability results using a novel lower bound and a new
algorithm for distributed DP-SCO based on Vaidya's plane cutting method. Thus,
our results provide a complete characterization of the
accuracy-communication-privacy trade-off for DP-SCO in the distributed setting.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.02893v1">A Volumetric Approach to Privacy of Dynamical Systems</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36">
  <p><b>Published on:</b> 2025-01-06T10:15:21Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chuanghong Weng, Ehsan Nekouei</p>
    <p><b>Summary:</b> Information-theoretic metrics, such as mutual information, have been widely
used to evaluate privacy leakage in dynamic systems. However, these approaches
are typically limited to stochastic systems and face computational challenges.
In this paper, we introduce a novel volumetric framework for analyzing privacy
in systems affected by unknown but bounded noise. Our model considers a dynamic
system comprising public and private states, where an observation set of the
public state is released. An adversary utilizes the observed public state to
infer an uncertainty set of the private state, referred to as the inference
attack. We define the evolution dynamics of these inference attacks and
quantify the privacy level of the private state using the volume of its
uncertainty sets. For linear scalar systems, we derive an explicit formulation
of the uncertainty set. For multi-dimensional linear systems, we develop an
approximate computation method leveraging interval analysis. We investigate the
properties of the proposed volumetric privacy measure and demonstrate that it
is bounded by the information gain derived from the observation set.
Furthermore, we propose an optimization approach to designing privacy filter
using randomization and linear programming based on the proposed privacy
measure. The effectiveness of the optimal privacy filter design is evaluated
through a production-inventory case study, illustrating its robustness against
the inference attack.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.02804v1">Latency and Privacy-Aware Resource Allocation in Vehicular Edge
  Computing</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Performance-F9C80E">
  <p><b>Published on:</b> 2025-01-06T06:44:49Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> HHossein Ahmadvand, Fouzhan Foroutan</p>
    <p><b>Summary:</b> The rapid increase in the number of connected vehicles has led to the
generation of vast amounts of data. As a significant portion of this data
pertains to vehicle-to-vehicle and vehicle-to-infrastructure communications, it
is predominantly generated at the edge. Considering the enormous volume of
data, real-time applications, and privacy concerns, it is crucial to process
the data at the edge. Neglecting the management of processing resources in
vehicular edge computing (VEC) could lead to numerous challenges as a
substantial number of vehicles with diverse safety, economic, and entertainment
applications, along with their data processing, emerge in the near future [1].
Previous research in VEC resource allocation has primarily focused on issues
such as response time and privacy preservation techniques. However, an approach
that takes into account privacy-aware resource allocation based on vehicular
network architecture and application requirements has not yet been proposed. In
this paper, we present a privacy and latency-aware approach for allocating
processing resources at the edge of the vehicular network, considering the
specific requirements of different applications. Our approach involves
categorizing vehicular network applications based on their processing accuracy,
real-time processing needs, and privacy preservation requirements. We further
divide the vehicular network edge into two parts: the user layer (OBUs) is
considered for processing applications with privacy requirements, while the
allocation of resources in the RSUs and cloud layer is based on the specific
needs of different applications. In this study, we evaluate the quality of
service based on parameters such as privacy preservation, processing cost,
meeting deadlines, and result quality. Comparative analyses demonstrate that
our approach enhances service quality by 55% compared to existing
state-of-the-art methods.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.02354v1">PrivDPR: Synthetic Graph Publishing with Deep PageRank under
  Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-01-04T18:19:21Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sen Zhang, Haibo Hu, Qingqing Ye, Jianliang Xu</p>
    <p><b>Summary:</b> The objective of privacy-preserving synthetic graph publishing is to
safeguard individuals' privacy while retaining the utility of original data.
Most existing methods focus on graph neural networks under differential privacy
(DP), and yet two fundamental problems in generating synthetic graphs remain
open. First, the current research often encounters high sensitivity due to the
intricate relationships between nodes in a graph. Second, DP is usually
achieved through advanced composition mechanisms that tend to converge
prematurely when working with a small privacy budget. In this paper, inspired
by the simplicity, effectiveness, and ease of analysis of PageRank, we design
PrivDPR, a novel privacy-preserving deep PageRank for graph synthesis. In
particular, we achieve DP by adding noise to the gradient for a specific weight
during learning. Utilizing weight normalization as a bridge, we theoretically
reveal that increasing the number of layers in PrivDPR can effectively mitigate
the high sensitivity and privacy budget splitting. Through formal privacy
analysis, we prove that the synthetic graph generated by PrivDPR satisfies
node-level DP. Experiments on real-world graph datasets show that PrivDPR
preserves high data utility across multiple graph structural properties.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.02091v1">PriveShield: Enhancing User Privacy Using Automatic Isolated Profiles in
  Browsers</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-01-03T20:29:33Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Seyed Ali Akhavani, Engin Kirda, Amin Kharraz</p>
    <p><b>Summary:</b> Online tracking is a widespread practice on the web with questionable ethics,
security, and privacy concerns. While web tracking can offer personalized and
curated content to Internet users, it operates as a sophisticated surveillance
mechanism to gather extensive user information. This paper introduces
PriveShield, a light-weight privacy mechanism that disrupts the information
gathering cycle while offering more control to Internet users to maintain their
privacy. PriveShield is implemented as a browser extension that offers an
adjustable privacy feature to surf the web with multiple identities or accounts
simultaneously without any changes to underlying browser code or services. When
necessary, multiple factors are automatically analyzed on the client side to
isolate cookies and other information that are the basis of online tracking.
PriveShield creates isolated profiles for clients based on their browsing
history, interactions with websites, and the amount of time they spend on
specific websites. This allows the users to easily prevent unwanted browsing
information from being shared with third parties and ad exchanges without the
need for manual configuration. Our evaluation results from 54 real-world
scenarios show that our extension is effective in preventing retargeted ads in
91% of those scenarios.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.01786v1">Advancing privacy in learning analytics using differential privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-01-03T12:36:11Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Qinyi Liu, Ronas Shakya, Mohammad Khalil, Jelena Jovanovic</p>
    <p><b>Summary:</b> This paper addresses the challenge of balancing learner data privacy with the
use of data in learning analytics (LA) by proposing a novel framework by
applying Differential Privacy (DP). The need for more robust privacy protection
keeps increasing, driven by evolving legal regulations and heightened privacy
concerns, as well as traditional anonymization methods being insufficient for
the complexities of educational data. To address this, we introduce the first
DP framework specifically designed for LA and provide practical guidance for
its implementation. We demonstrate the use of this framework through a LA usage
scenario and validate DP in safeguarding data privacy against potential attacks
through an experiment on a well-known LA dataset. Additionally, we explore the
trade-offs between data privacy and utility across various DP settings. Our
work contributes to the field of LA by offering a practical DP framework that
can support researchers and practitioners in adopting DP in their works.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.01639v2">Implications of Artificial Intelligence on Health Data Privacy and
  Confidentiality</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-01-03T05:17:23Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ahmad Momani</p>
    <p><b>Summary:</b> The rapid integration of artificial intelligence (AI) in healthcare is
revolutionizing medical diagnostics, personalized medicine, and operational
efficiency. However, alongside these advancements, significant challenges arise
concerning patient data privacy, ethical considerations, and regulatory
compliance. This paper examines the dual impact of AI on healthcare,
highlighting its transformative potential and the critical need for
safeguarding sensitive health information. It explores the role of the Health
Insurance Portability and Accountability Act (HIPAA) as a regulatory framework
for ensuring data privacy and security, emphasizing the importance of robust
safeguards and ethical standards in AI-driven healthcare. Through case studies,
including AI applications in diabetic retinopathy, oncology, and the
controversies surrounding data sharing, this study underscores the ethical and
legal complexities of AI implementation. A balanced approach that fosters
innovation while maintaining patient trust and privacy is imperative. The
findings emphasize the importance of continuous education, transparency, and
adherence to regulatory frameworks to harness AI's full potential responsibly
and ethically in healthcare.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.01353v1">Privacy Preservation in MIMO-OFDM Localization Systems: A Beamforming
  Approach</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2025-01-02T17:08:15Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yuchen Zhang, Hui Chen, Musa Furkan Keskin, Alireza Pourafzal, Pinjun Zheng, Henk Wymeersch, Tareq Y. Al-Naffouri</p>
    <p><b>Summary:</b> We investigate an uplink MIMO-OFDM localization scenario where a legitimate
base station (BS) aims to localize a user equipment (UE) using pilot signals
transmitted by the UE, while an unauthorized BS attempts to localize the UE by
eavesdropping on these pilots, posing a risk to the UE's location privacy. To
enhance legitimate localization performance while protecting the UE's privacy,
we formulate an optimization problem regarding the beamformers at the UE,
aiming to minimize the Cram\'er-Rao bound (CRB) for legitimate localization
while constraining the CRB for unauthorized localization above a threshold. A
penalty dual decomposition optimization framework is employed to solve the
problem, leading to a novel beamforming approach for location privacy
preservation. Numerical results confirm the effectiveness of the proposed
approach and demonstrate its superiority over existing benchmarks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.01131v1">Privacy Bills of Materials: A Transparent Privacy Information Inventory
  for Collaborative Privacy Notice Generation in Mobile App Development</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Software Engineering-D91E36">
  <p><b>Published on:</b> 2025-01-02T08:14:52Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhen Tao, Shidong Pan, Zhenchang Xing, Xiaoyu Sun, Omar Haggag, John Grundy, Ze Shi Li, Jingjie Li, Liming Zhu</p>
    <p><b>Summary:</b> Privacy regulations mandate that developers must provide authentic and
comprehensive privacy notices, e.g., privacy policies or labels, to inform
users of their apps' privacy practices. However, due to a lack of knowledge of
privacy requirements, developers often struggle to create accurate privacy
notices, especially for sophisticated mobile apps with complex features and in
crowded development teams. To address these challenges, we introduce Privacy
Bills of Materials (PriBOM), a systematic software engineering approach that
leverages different development team roles to better capture and coordinate
mobile app privacy information. PriBOM facilitates transparency-centric privacy
documentation and specific privacy notice creation, enabling traceability and
trackability of privacy practices. We present a pre-fill of PriBOM based on
static analysis and privacy notice analysis techniques. We demonstrate the
perceived usefulness of PriBOM through a human evaluation with 150 diverse
participants. Our findings suggest that PriBOM could serve as a significant
solution for providing privacy support in DevOps for mobile apps.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.01063v1">FAPL-DM-BC: A Secure and Scalable FL Framework with Adaptive Privacy and
  Dynamic Masking, Blockchain, and XAI for the IoVs</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-01-02T05:21:52Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sathwik Narkedimilli, Amballa Venkata Sriram, Sujith Makam, MSVPJ Sathvik, Sai Prashanth Mallellu</p>
    <p><b>Summary:</b> The FAPL-DM-BC solution is a new FL-based privacy, security, and scalability
solution for the Internet of Vehicles (IoV). It leverages Federated Adaptive
Privacy-Aware Learning (FAPL) and Dynamic Masking (DM) to learn and adaptively
change privacy policies in response to changing data sensitivity and state in
real-time, for the optimal privacy-utility tradeoff. Secure Logging and
Verification, Blockchain-based provenance and decentralized validation, and
Cloud Microservices Secure Aggregation using FedAvg (Federated Averaging) and
Secure Multi-Party Computation (SMPC). Two-model feedback, driven by
Model-Agnostic Explainable AI (XAI), certifies local predictions and
explanations to drive it to the next level of efficiency. Combining local
feedback with world knowledge through a weighted mean computation, FAPL-DM-BC
assures federated learning that is secure, scalable, and interpretable.
Self-driving cars, traffic management, and forecasting, vehicular network
cybersecurity in real-time, and smart cities are a few possible applications of
this integrated, privacy-safe, and high-performance IoV platform.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.00824v1">Information Sifting Funnel: Privacy-preserving Collaborative Inference
  Against Model Inversion Attacks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2025-01-01T13:00:01Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Rongke Liu</p>
    <p><b>Summary:</b> The complexity of neural networks and inference tasks, coupled with demands
for computational efficiency and real-time feedback, poses significant
challenges for resource-constrained edge devices. Collaborative inference
mitigates this by assigning shallow feature extraction to edge devices and
offloading features to the cloud for further inference, reducing computational
load. However, transmitted features remain susceptible to model inversion
attacks (MIAs), which can reconstruct original input data. Current defenses,
such as perturbation and information bottleneck techniques, offer explainable
protection but face limitations, including the lack of standardized criteria
for assessing MIA difficulty, challenges in mutual information estimation, and
trade-offs among usability, privacy, and deployability.
  To address these challenges, we introduce the first criterion to evaluate MIA
difficulty in collaborative inference, supported by theoretical analysis of
existing attacks and defenses, validated using experiments with the Mutual
Information Neural Estimator (MINE). Based on these findings, we propose
SiftFunnel, a privacy-preserving framework for collaborative inference. The
edge model is trained with linear and non-linear correlation constraints to
reduce redundant information in transmitted features, enhancing privacy
protection. Label smoothing and a cloud-based upsampling module are added to
balance usability and privacy. To improve deployability, the edge model
incorporates a funnel-shaped structure and attention mechanisms, preserving
both privacy and usability. Extensive experiments demonstrate that SiftFunnel
outperforms state-of-the-art defenses against MIAs, achieving superior privacy
protection with less than 3% accuracy loss and striking an optimal balance
among usability, privacy, and practicality.</p>
  </details>
</div>



<h2>2024-12</h2>

<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.00588v1">Privacy-Preserving Distributed Defense Framework for DC Microgrids
  Against Exponentially Unbounded False Data Injection Attacks</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36">
  <p><b>Published on:</b> 2024-12-31T18:25:05Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yi Zhang, Mohamadamin Rajabinezhad, Yichao Wang, Junbo Zhao, Shan Zuo</p>
    <p><b>Summary:</b> This paper introduces a novel, fully distributed control framework for DC
microgrids, enhancing resilience against exponentially unbounded false data
injection (EU-FDI) attacks. Our framework features a consensus-based secondary
control for each converter, effectively addressing these advanced threats. To
further safeguard sensitive operational data, a privacy-preserving mechanism is
incorporated into the control design, ensuring that critical information
remains secure even under adversarial conditions. Rigorous Lyapunov stability
analysis confirms the framework's ability to maintain critical DC microgrid
operations like voltage regulation and load sharing under EU-FDI threats. The
framework's practicality is validated through hardware-in-the-loop experiments,
demonstrating its enhanced resilience and robust privacy protection against the
complex challenges posed by quick variant FDI attacks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2501.00363v1">SPDZCoder: Teaching LLMs to Synthesize Privacy Computing Code without
  Massive Training Data</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Software Engineering-D91E36">
  <p><b>Published on:</b> 2024-12-31T09:29:38Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xiaoning Dong, Peilin Xin, Wei Xu</p>
    <p><b>Summary:</b> Privacy computing receives increasing attention but writing privacy computing
code remains challenging for developers due to limited library functions that
necessitate extensive function implementation from scratch as well as the
data-oblivious requirement which contradicts intuitive thinking and usual
practices of programmers. Large language models (LLMs) have demonstrated
surprising capabilities in coding tasks and achieved state-of-the-art
performance across many benchmarks. However, even with extensive prompting,
existing LLMs struggle with code translation task for privacy computing, such
as translating Python to MP-SPDZ, due to the scarcity of MP-SPDZ data required
for effective pre-training or fine-tuning. To address the limitation, this
paper proposes SPDZCoder, a rule-based framework to teach LLMs to synthesize
privacy computing code without asking experts to write tons of code and by
leveraging the instruction-following and in-context learning ability of LLMs.
Specifically, SPDZCoder decouples the translation task into the refactoring
stage and the generation stage, which can mitigate the semantic-expressing
differences at different levels. In addition, SPDZCoder can further improve its
performance by a feedback stage. SPDZCoder does not require fine-tuning since
it adopts an in-context learning paradigm of LLMs. To evaluate SPDZCoder, we
manually created a benchmark dataset, named SPDZEval, containing six classes of
difficult tasks to implement in MP-SPDZ. We conduct experiments on SPDZEval and
the experimental results shows that SPDZCoder achieves the state-of-the-art
performance in pass@1 and pass@2 across six data splits. Specifically,
SPDZCoder achieves an overall correctness of 85.94% and 92.01% in pass@1 and
pass@2, respectively, significantly surpassing baselines (at most 30.35% and
49.84% in pass@1 and pass@2, respectively) by a large margin.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.21069v1">Privacy-Aware Multi-Device Cooperative Edge Inference with Distributed
  Resource Bidding</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Networking and Internet Architecture-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36">
  <p><b>Published on:</b> 2024-12-30T16:37:17Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Wenhao Zhuang, Yuyi Mao</p>
    <p><b>Summary:</b> Mobile edge computing (MEC) has empowered mobile devices (MDs) in supporting
artificial intelligence (AI) applications through collaborative efforts with
proximal MEC servers. Unfortunately, despite the great promise of device-edge
cooperative AI inference, data privacy becomes an increasing concern. In this
paper, we develop a privacy-aware multi-device cooperative edge inference
system for classification tasks, which integrates a distributed bidding
mechanism for the MEC server's computational resources. Intermediate feature
compression is adopted as a principled approach to minimize data privacy
leakage. To determine the bidding values and feature compression ratios in a
distributed fashion, we formulate a decentralized partially observable Markov
decision process (DEC-POMDP) model, for which, a multi-agent deep deterministic
policy gradient (MADDPG)-based algorithm is developed. Simulation results
demonstrate the effectiveness of the proposed algorithm in privacy-preserving
cooperative edge inference. Specifically, given a sufficient level of data
privacy protection, the proposed algorithm achieves 0.31-0.95% improvements in
classification accuracy compared to the approach being agnostic to the wireless
channel conditions. The performance is further enhanced by 1.54-1.67% by
considering the difficulties of inference data.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.20798v2">A Tale of Two Imperatives: Privacy and Explainability</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-12-30T08:43:28Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Supriya Manna, Niladri Sett</p>
    <p><b>Summary:</b> Deep learning's preponderance across scientific domains has reshaped
high-stakes decision-making, making it essential to follow rigorous operational
frameworks that include both Right-to-Privacy (RTP) and Right-to-Explanation
(RTE). This paper examines the complexities of combining these two
requirements. For RTP, we focus on `Differential privacy' (DP), which is
considered the current \textit{gold standard} for privacy-preserving machine
learning due to its strong quantitative guarantee of privacy. For RTE, we focus
on post-hoc explainers: they are the \textit{go-to} option for model auditing
as they operate independently of model training. We formally investigate DP
models and various commonly-used post-hoc explainers: how to evaluate these
explainers subject to RTP, and analyze the intrinsic interactions between DP
models and these explainers. Furthermore, our work throws light on how RTP and
RTE can be effectively combined in high-stakes applications. Our study
concludes by outlining an industrial software pipeline, with the example of a
wildly used use-case, that respects both RTP and RTE requirements.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.20762v1">Enhancing Privacy in Federated Learning through Quantum Teleportation
  Integration</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">  
  <p><b>Published on:</b> 2024-12-30T07:15:21Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Koffka Khan</p>
    <p><b>Summary:</b> Federated learning enables collaborative model training across multiple
clients without sharing raw data, thereby enhancing privacy. However, the
exchange of model updates can still expose sensitive information. Quantum
teleportation, a process that transfers quantum states between distant
locations without physical transmission of the particles themselves, has
recently been implemented in real-world networks. This position paper explores
the potential of integrating quantum teleportation into federated learning
frameworks to bolster privacy. By leveraging quantum entanglement and the
no-cloning theorem, quantum teleportation ensures that data remains secure
during transmission, as any eavesdropping attempt would be detectable. We
propose a novel architecture where quantum teleportation facilitates the secure
exchange of model parameters and gradients among clients and servers. This
integration aims to mitigate risks associated with data leakage and adversarial
attacks inherent in classical federated learning setups. We also discuss the
practical challenges of implementing such a system, including the current
limitations of quantum network infrastructure and the need for hybrid
quantum-classical protocols. Our analysis suggests that, despite these
challenges, the convergence of quantum communication technologies and federated
learning presents a promising avenue for achieving unprecedented levels of
privacy in distributed machine learning.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.20733v1">Towards nation-wide analytical healthcare infrastructures: A
  privacy-preserving augmented knee rehabilitation case study</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Multimedia-5BC0EB">
  <p><b>Published on:</b> 2024-12-30T06:14:48Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Boris Bačić, Claudiu Vasile, Chengwei Feng, Marian G. Ciucă</p>
    <p><b>Summary:</b> The purpose of this paper is to contribute towards the near-future
privacy-preserving big data analytical healthcare platforms, capable of
processing streamed or uploaded timeseries data or videos from patients. The
experimental work includes a real-life knee rehabilitation video dataset
capturing a set of exercises from simple and personalised to more general and
challenging movements aimed for returning to sport. To convert video from
mobile into privacy-preserving diagnostic timeseries data, we employed Google
MediaPipe pose estimation. The developed proof-of-concept algorithms can
augment knee exercise videos by overlaying the patient with stick figure
elements while updating generated timeseries plot with knee angle estimation
streamed as CSV file format. For patients and physiotherapists, video with
side-to-side timeseries visually indicating potential issues such as excessive
knee flexion or unstable knee movements or stick figure overlay errors is
possible by setting a-priori knee-angle parameters. To address adherence to
rehabilitation programme and quantify exercise sets and repetitions, our
adaptive algorithm can correctly identify (91.67%-100%) of all exercises from
side- and front-view videos. Transparent algorithm design for adaptive visual
analysis of various knee exercise patterns contributes towards the
interpretable AI and will inform near-future privacy-preserving, non-vendor
locking, open-source developments for both end-user computing devices and as
on-premises non-proprietary cloud platforms that can be deployed within the
national healthcare system.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.20641v1">SafeSynthDP: Leveraging Large Language Models for Privacy-Preserving
  Synthetic Data Generation Using Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-12-30T01:10:10Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Md Mahadi Hasan Nahid, Sadid Bin Hasan</p>
    <p><b>Summary:</b> Machine learning (ML) models frequently rely on training data that may
include sensitive or personal information, raising substantial privacy
concerns. Legislative frameworks such as the General Data Protection Regulation
(GDPR) and the California Consumer Privacy Act (CCPA) have necessitated the
development of strategies that preserve privacy while maintaining the utility
of data. In this paper, we investigate the capability of Large Language Models
(LLMs) to generate synthetic datasets integrated with Differential Privacy (DP)
mechanisms, thereby enabling data-driven research and model training without
direct exposure of sensitive information. Our approach incorporates DP-based
noise injection methods, including Laplace and Gaussian distributions, into the
data generation process. We then evaluate the utility of these DP-enhanced
synthetic datasets by comparing the performance of ML models trained on them
against models trained on the original data. To substantiate privacy
guarantees, we assess the resilience of the generated synthetic data to
membership inference attacks and related threats. The experimental results
demonstrate that integrating DP within LLM-driven synthetic data generation
offers a viable balance between privacy protection and data utility. This study
provides a foundational methodology and insight into the privacy-preserving
capabilities of LLMs, paving the way for compliant and effective ML research
and applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.20603v1">Privacy-Preserving Identity and Access Management in Multiple Cloud
  Environments: Models, Issues, and Solutions</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-12-29T22:15:19Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Alfredo Cuzzocrea, Islam Belmerabet</p>
    <p><b>Summary:</b> This paper focuses the attention on privacy-preserving identity and access
management in multiple Cloud environments, which is an annoying problem in the
modern big data era. Within this conceptual context, the paper describes
contemporaneous models and issues, and put the basis for future solid
solutions. Finally, we provide a summary table where we embed an innovative
taxonomy of state-of-the-art research proposals in the reference scientific
field.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.19496v1">Multi-P$^2$A: A Multi-perspective Benchmark on Privacy Assessment for
  Large Vision-Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-12-27T07:33:39Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jie Zhang, Xiangkui Cao, Zhouyu Han, Shiguang Shan, Xilin Chen</p>
    <p><b>Summary:</b> Large Vision-Language Models (LVLMs) exhibit impressive potential across
various tasks but also face significant privacy risks, limiting their practical
applications. Current researches on privacy assessment for LVLMs is limited in
scope, with gaps in both assessment dimensions and privacy categories. To
bridge this gap, we propose Multi-P$^2$A, a comprehensive benchmark for
evaluating the privacy preservation capabilities of LVLMs in terms of privacy
awareness and leakage. Privacy awareness measures the model's ability to
recognize the privacy sensitivity of input data, while privacy leakage assesses
the risk of the model unintentionally disclosing privacy information in its
output. We design a range of sub-tasks to thoroughly evaluate the model's
privacy protection offered by LVLMs. Multi-P$^2$A covers 26 categories of
personal privacy, 15 categories of trade secrets, and 18 categories of state
secrets, totaling 31,962 samples. Based on Multi-P$^2$A, we evaluate the
privacy preservation capabilities of 21 open-source and 2 closed-source LVLMs.
Our results reveal that current LVLMs generally pose a high risk of
facilitating privacy breaches, with vulnerabilities varying across personal
privacy, trade secret, and state secret.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.19291v1">RAG with Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-12-26T17:34:26Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Nicolas Grislain</p>
    <p><b>Summary:</b> Retrieval-Augmented Generation (RAG) has emerged as the dominant technique to
provide *Large Language Models* (LLM) with fresh and relevant context,
mitigating the risk of hallucinations and improving the overall quality of
responses in environments with large and fast moving knowledge bases. However,
the integration of external documents into the generation process raises
significant privacy concerns. Indeed, when added to a prompt, it is not
possible to guarantee a response will not inadvertently expose confidential
data, leading to potential breaches of privacy and ethical dilemmas. This paper
explores a practical solution to this problem suitable to general knowledge
extraction from personal data. It shows *differentially private token
generation* is a viable approach to private RAG.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.18992v1">Optimal Federated Learning for Functional Mean Estimation under
  Heterogeneous Privacy Constraints</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Statistics Theory-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">  
  <p><b>Published on:</b> 2024-12-25T22:06:12Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tony Cai, Abhinav Chakraborty, Lasse Vuursteen</p>
    <p><b>Summary:</b> Federated learning (FL) is a distributed machine learning technique designed
to preserve data privacy and security, and it has gained significant importance
due to its broad range of applications. This paper addresses the problem of
optimal functional mean estimation from discretely sampled data in a federated
setting.
  We consider a heterogeneous framework where the number of individuals,
measurements per individual, and privacy parameters vary across one or more
servers, under both common and independent design settings. In the common
design setting, the same design points are measured for each individual,
whereas in the independent design, each individual has their own random
collection of design points. Within this framework, we establish minimax upper
and lower bounds for the estimation error of the underlying mean function,
highlighting the nuanced differences between common and independent designs
under distributed privacy constraints.
  We propose algorithms that achieve the optimal trade-off between privacy and
accuracy and provide optimality results that quantify the fundamental limits of
private functional mean estimation across diverse distributed settings. These
results characterize the cost of privacy and offer practical insights into the
potential for privacy-preserving statistical analysis in federated
environments.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.18716v1">Design and Evaluation of Privacy-Preserving Protocols for
  Agent-Facilitated Mobile Money Services in Kenya</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2024-12-25T00:27:13Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Karen Sowon, Collins W. Munyendo, Lily Klucinec, Eunice Maingi, Gerald Suleh, Lorrie Faith Cranor, Giulia Fanti, Conrad Tucker, Assane Gueye</p>
    <p><b>Summary:</b> Mobile Money (MoMo), a technology that allows users to complete digital
financial transactions using a mobile phone without requiring a bank account,
has become a common method for processing financial transactions in Africa and
other developing regions. Operationally, users can deposit (exchange cash for
mobile money tokens) and withdraw with the help of human agents who facilitate
a near end-to-end process from customer onboarding to authentication and
recourse. During deposit and withdraw operations, know-your-customer (KYC)
processes require agents to access and verify customer information such as name
and ID number, which can introduce privacy and security risks. In this work, we
design alternative protocols for mobile money deposits and withdrawals that
protect users' privacy while enabling KYC checks. These workflows redirect the
flow of sensitive information from the agent to the MoMo provider, thus
allowing the agent to facilitate transactions without accessing a customer's
personal information. We evaluate the usability and efficiency of our proposed
protocols in a role play and semi-structured interview study with 32 users and
15 agents in Kenya. We find that users and agents both generally appear to
prefer the new protocols, due in part to convenient and efficient verification
using biometrics, better data privacy and access control, as well as better
security mechanisms for delegated transactions. Our results also highlight some
challenges and limitations that suggest the need for more work to build
deployable solutions.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.19837v1">Data Poisoning Attacks to Local Differential Privacy Protocols for
  Graphs</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB">
  <p><b>Published on:</b> 2024-12-23T11:16:23Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xi He, Kai Huang, Qingqing Ye, Haibo Hu</p>
    <p><b>Summary:</b> Graph analysis has become increasingly popular with the prevalence of big
data and machine learning. Traditional graph data analysis methods often assume
the existence of a trusted third party to collect and store the graph data,
which does not align with real-world situations. To address this, some research
has proposed utilizing Local Differential Privacy (LDP) to collect graph data
or graph metrics (e.g., clustering coefficient). This line of research focuses
on collecting two atomic graph metrics (the adjacency bit vectors and node
degrees) from each node locally under LDP to synthesize an entire graph or
generate graph metrics. However, they have not considered the security issues
of LDP for graphs.
  In this paper, we bridge the gap by demonstrating that an attacker can inject
fake users into LDP protocols for graphs and design data poisoning attacks to
degrade the quality of graph metrics. In particular, we present three data
poisoning attacks to LDP protocols for graphs. As a proof of concept, we focus
on data poisoning attacks on two classical graph metrics: degree centrality and
clustering coefficient. We further design two countermeasures for these data
poisoning attacks. Experimental study on real-world datasets demonstrates that
our attacks can largely degrade the quality of collected graph metrics, and the
proposed countermeasures cannot effectively offset the effect, which calls for
the development of new defenses.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.17317v1">Better Knowledge Enhancement for Privacy-Preserving Cross-Project Defect
  Prediction</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Software Engineering-D91E36">
  <p><b>Published on:</b> 2024-12-23T06:21:15Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yuying Wang, Yichen Li, Haozhao Wang, Lei Zhao, Xiaofang Zhang</p>
    <p><b>Summary:</b> Cross-Project Defect Prediction (CPDP) poses a non-trivial challenge to
construct a reliable defect predictor by leveraging data from other projects,
particularly when data owners are concerned about data privacy. In recent
years, Federated Learning (FL) has become an emerging paradigm to guarantee
privacy information by collaborative training a global model among multiple
parties without sharing raw data. While the direct application of FL to the
CPDP task offers a promising solution to address privacy concerns, the data
heterogeneity arising from proprietary projects across different companies or
organizations will bring troubles for model training. In this paper, we study
the privacy-preserving cross-project defect prediction with data heterogeneity
under the federated learning framework. To address this problem, we propose a
novel knowledge enhancement approach named FedDP with two simple but effective
solutions: 1. Local Heterogeneity Awareness and 2. Global Knowledge
Distillation. Specifically, we employ open-source project data as the
distillation dataset and optimize the global model with the heterogeneity-aware
local model ensemble via knowledge distillation. Experimental results on 19
projects from two datasets demonstrate that our method significantly
outperforms baselines.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.17038v3">ErasableMask: A Robust and Erasable Privacy Protection Scheme against
  Black-box Face Recognition Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-12-22T14:30:26Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sipeng Shen, Yunming Zhang, Dengpan Ye, Xiuwen Shi, Long Tang, Haoran Duan, Jiacheng Deng, Ziyi Liu</p>
    <p><b>Summary:</b> While face recognition (FR) models have brought remarkable convenience in
face verification and identification, they also pose substantial privacy risks
to the public. Existing facial privacy protection schemes usually adopt
adversarial examples to disrupt face verification of FR models. However, these
schemes often suffer from weak transferability against black-box FR models and
permanently damage the identifiable information that cannot fulfill the
requirements of authorized operations such as forensics and authentication. To
address these limitations, we propose ErasableMask, a robust and erasable
privacy protection scheme against black-box FR models. Specifically, via
rethinking the inherent relationship between surrogate FR models, ErasableMask
introduces a novel meta-auxiliary attack, which boosts black-box
transferability by learning more general features in a stable and balancing
optimization strategy. It also offers a perturbation erasion mechanism that
supports the erasion of semantic perturbations in protected face without
degrading image quality. To further improve performance, ErasableMask employs a
curriculum learning strategy to mitigate optimization conflicts between
adversarial attack and perturbation erasion. Extensive experiments on the
CelebA-HQ and FFHQ datasets demonstrate that ErasableMask achieves the
state-of-the-art performance in transferability, achieving over 72% confidence
on average in commercial FR systems. Moreover, ErasableMask also exhibits
outstanding perturbation erasion performance, achieving over 90% erasion
success rate.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.16916v1">On the Differential Privacy and Interactivity of Privacy Sandbox Reports</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-12-22T08:22:57Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Badih Ghazi, Charlie Harrison, Arpana Hosabettu, Pritish Kamath, Alexander Knop, Ravi Kumar, Ethan Leeman, Pasin Manurangsi, Vikas Sahu</p>
    <p><b>Summary:</b> The Privacy Sandbox initiative from Google includes APIs for enabling
privacy-preserving advertising functionalities as part of the effort around
limiting third-party cookies. In particular, the Private Aggregation API (PAA)
and the Attribution Reporting API (ARA) can be used for ad measurement while
providing different guardrails for safeguarding user privacy, including a
framework for satisfying differential privacy (DP). In this work, we provide a
formal model for analyzing the privacy of these APIs and show that they satisfy
a formal DP guarantee under certain assumptions. Our analysis handles the case
where both the queries and database can change interactively based on previous
responses from the API.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.16893v1">Preventing Non-intrusive Load Monitoring Privacy Invasion: A Precise
  Adversarial Attack Scheme for Networked Smart Meters</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-12-22T07:06:46Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jialing He, Jiacheng Wang, Ning Wang, Shangwei Guo, Liehuang Zhu, Dusit Niyato, Tao Xiang</p>
    <p><b>Summary:</b> Smart grid, through networked smart meters employing the non-intrusive load
monitoring (NILM) technique, can considerably discern the usage patterns of
residential appliances. However, this technique also incurs privacy leakage. To
address this issue, we propose an innovative scheme based on adversarial attack
in this paper. The scheme effectively prevents NILM models from violating
appliance-level privacy, while also ensuring accurate billing calculation for
users. To achieve this objective, we overcome two primary challenges. First, as
NILM models fall under the category of time-series regression models, direct
application of traditional adversarial attacks designed for classification
tasks is not feasible. To tackle this issue, we formulate a novel adversarial
attack problem tailored specifically for NILM and providing a theoretical
foundation for utilizing the Jacobian of the NILM model to generate
imperceptible perturbations. Leveraging the Jacobian, our scheme can produce
perturbations, which effectively misleads the signal prediction of NILM models
to safeguard users' appliance-level privacy. The second challenge pertains to
fundamental utility requirements, where existing adversarial attack schemes
struggle to achieve accurate billing calculation for users. To handle this
problem, we introduce an additional constraint, mandating that the sum of added
perturbations within a billing period must be precisely zero. Experimental
validation on real-world power datasets REDD and UK-DALE demonstrates the
efficacy of our proposed solutions, which can significantly amplify the
discrepancy between the output of the targeted NILM model and the actual power
signal of appliances, and enable accurate billing at the same time.
Additionally, our solutions exhibit transferability, making the generated
perturbation signal from one target model applicable to other diverse NILM
models.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.16825v1">SoK: Usability Studies in Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-12-22T02:21:57Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Onyinye Dibia, Brad Stenger, Steven Baldasty, Mako Bates, Ivoline C. Ngong, Yuanyuan Feng, Joseph P. Near</p>
    <p><b>Summary:</b> Differential Privacy (DP) has emerged as a pivotal approach for safeguarding
individual privacy in data analysis, yet its practical adoption is often
hindered by challenges in usability in implementation and communication of the
privacy protection levels. This paper presents a comprehensive systematization
of existing research on the usability of and communication about DP,
synthesizing insights from studies on both the practical use of DP tools and
strategies for conveying DP parameters that determine the privacy protection
levels such as epsilon. By reviewing and analyzing these studies, we identify
core usability challenges, best practices, and critical gaps in current DP
tools that affect adoption across diverse user groups, including developers,
data analysts, and non-technical stakeholders. Our analysis highlights
actionable insights and pathways for future research that emphasizes
user-centered design and clear communication, fostering the development of more
accessible DP tools that meet practical needs and support broader adoption.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.16669v1">Label Privacy in Split Learning for Large Models with
  Parameter-Efficient Training</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-12-21T15:32:03Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Philip Zmushko, Marat Mansurov, Ruslan Svirschevski, Denis Kuznedelev, Max Ryabinin, Aleksandr Beznosikov</p>
    <p><b>Summary:</b> As deep learning models become larger and more expensive, many practitioners
turn to fine-tuning APIs. These web services allow fine-tuning a model between
two parties: the client that provides the data, and the server that hosts the
model. While convenient, these APIs raise a new concern: the data of the client
is at risk of privacy breach during the training procedure. This challenge
presents an important practical case of vertical federated learning, where the
two parties perform parameter-efficient fine-tuning (PEFT) of a large model. In
this study, we systematically search for a way to fine-tune models over an API
while keeping the labels private. We analyze the privacy of LoRA, a popular
approach for parameter-efficient fine-tuning when training over an API. Using
this analysis, we propose P$^3$EFT, a multi-party split learning algorithm that
takes advantage of existing PEFT properties to maintain privacy at a lower
performance overhead. To validate our algorithm, we fine-tune
DeBERTa-v2-XXLarge, Flan-T5 Large and LLaMA-2 7B using LoRA adapters on a range
of NLP tasks. We find that P$^3$EFT is competitive with existing
privacy-preserving methods in multi-party and two-party setups while having
higher accuracy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.16667v1">The Good, the Bad, and the (Un)Usable: A Rapid Literature Review on
  Privacy as Code</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Software Engineering-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2024-12-21T15:30:17Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Nicolás E. Díaz Ferreyra, Sirine Khelifi, Nalin Arachchilage, Riccardo Scandariato</p>
    <p><b>Summary:</b> Privacy and security are central to the design of information systems endowed
with sound data protection and cyber resilience capabilities. Still, developers
often struggle to incorporate these properties into software projects as they
either lack proper cybersecurity training or do not consider them a priority.
Prior work has tried to support privacy and security engineering activities
through threat modeling methods for scrutinizing flaws in system architectures.
Moreover, several techniques for the automatic identification of
vulnerabilities and the generation of secure code implementations have also
been proposed in the current literature. Conversely, such as-code approaches
seem under-investigated in the privacy domain, with little work elaborating on
(i) the automatic detection of privacy properties in source code or (ii) the
generation of privacy-friendly code. In this work, we seek to characterize the
current research landscape of Privacy as Code (PaC) methods and tools by
conducting a rapid literature review. Our results suggest that PaC research is
in its infancy, especially regarding the performance evaluation and usability
assessment of the existing approaches. Based on these findings, we outline and
discuss prospective research directions concerning empirical studies with
software practitioners, the curation of benchmark datasets, and the role of
generative AI technologies.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.16504v1">Privacy in Fine-tuning Large Language Models: Attacks, Defenses, and
  Future Directions</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-12-21T06:41:29Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Hao Du, Shang Liu, Lele Zheng, Yang Cao, Atsuyoshi Nakamura, Lei Chen</p>
    <p><b>Summary:</b> Fine-tuning has emerged as a critical process in leveraging Large Language
Models (LLMs) for specific downstream tasks, enabling these models to achieve
state-of-the-art performance across various domains. However, the fine-tuning
process often involves sensitive datasets, introducing privacy risks that
exploit the unique characteristics of this stage. In this paper, we provide a
comprehensive survey of privacy challenges associated with fine-tuning LLMs,
highlighting vulnerabilities to various privacy attacks, including membership
inference, data extraction, and backdoor attacks. We further review defense
mechanisms designed to mitigate privacy risks in the fine-tuning phase, such as
differential privacy, federated learning, and knowledge unlearning, discussing
their effectiveness and limitations in addressing privacy risks and maintaining
model utility. By identifying key gaps in existing research, we highlight
challenges and propose directions to advance the development of
privacy-preserving methods for fine-tuning LLMs, promoting their responsible
use in diverse applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.16369v1">Navigating AI to Unpack Youth Privacy Concerns: An In-Depth Exploration
  and Systematic Review</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-12-20T22:00:06Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ajay Kumar Shrestha, Ankur Barthwal, Molly Campbell, Austin Shouli, Saad Syed, Sandhya Joshi, Julita Vassileva</p>
    <p><b>Summary:</b> This systematic literature review investigates perceptions, concerns, and
expectations of young digital citizens regarding privacy in artificial
intelligence (AI) systems, focusing on social media platforms, educational
technology, gaming systems, and recommendation algorithms. Using a rigorous
methodology, the review started with 2,000 papers, narrowed down to 552 after
initial screening, and finally refined to 108 for detailed analysis. Data
extraction focused on privacy concerns, data-sharing practices, the balance
between privacy and utility, trust factors in AI, transparency expectations,
and strategies to enhance user control over personal data. Findings reveal
significant privacy concerns among young users, including a perceived lack of
control over personal information, potential misuse of data by AI, and fears of
data breaches and unauthorized access. These issues are worsened by unclear
data collection practices and insufficient transparency in AI applications. The
intention to share data is closely associated with perceived benefits and data
protection assurances. The study also highlights the role of parental mediation
and the need for comprehensive education on data privacy. Balancing privacy and
utility in AI applications is crucial, as young digital citizens value
personalized services but remain wary of privacy risks. Trust in AI is
significantly influenced by transparency, reliability, predictable behavior,
and clear communication about data usage. Strategies to improve user control
over personal data include access to and correction of data, clear consent
mechanisms, and robust data protection assurances. The review identifies
research gaps and suggests future directions, such as longitudinal studies,
multicultural comparisons, and the development of ethical AI frameworks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.16144v1">FedGAT: A Privacy-Preserving Federated Approximation Algorithm for Graph
  Attention Networks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2024-12-20T18:48:46Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Siddharth Ambekar, Yuhang Yao, Ryan Li, Carlee Joe-Wong</p>
    <p><b>Summary:</b> Federated training methods have gained popularity for graph learning with
applications including friendship graphs of social media sites and
customer-merchant interaction graphs of huge online marketplaces. However,
privacy regulations often require locally generated data to be stored on local
clients. The graph is then naturally partitioned across clients, with no client
permitted access to information stored on another. Cross-client edges arise
naturally in such cases and present an interesting challenge to federated
training methods, as training a graph model at one client requires feature
information of nodes on the other end of cross-client edges. Attempting to
retain such edges often incurs significant communication overhead, and dropping
them altogether reduces model performance. In simpler models such as Graph
Convolutional Networks, this can be fixed by communicating a limited amount of
feature information across clients before training, but GATs (Graph Attention
Networks) require additional information that cannot be pre-communicated, as it
changes from training round to round. We introduce the Federated Graph
Attention Network (FedGAT) algorithm for semi-supervised node classification,
which approximates the behavior of GATs with provable bounds on the
approximation error. FedGAT requires only one pre-training communication round,
significantly reducing the communication overhead for federated GAT training.
We then analyze the error in the approximation and examine the communication
overhead and computational complexity of the algorithm. Experiments show that
FedGAT achieves nearly the same accuracy as a GAT model in a centralised
setting, and its performance is robust to the number of clients as well as data
distribution.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.15590v1">SemDP: Semantic-level Differential Privacy Protection for Face Datasets</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-12-20T06:00:59Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xiaoting Zhang, Tao Wang, Junhao Ji</p>
    <p><b>Summary:</b> While large-scale face datasets have advanced deep learning-based face
analysis, they also raise privacy concerns due to the sensitive personal
information they contain. Recent schemes have implemented differential privacy
to protect face datasets. However, these schemes generally treat each image as
a separate database, which does not fully meet the core requirements of
differential privacy. In this paper, we propose a semantic-level differential
privacy protection scheme that applies to the entire face dataset. Unlike
pixel-level differential privacy approaches, our scheme guarantees that
semantic privacy in faces is not compromised. The key idea is to convert
unstructured data into structured data to enable the application of
differential privacy. Specifically, we first extract semantic information from
the face dataset to build an attribute database, then apply differential
perturbations to obscure this attribute data, and finally use an image
synthesis model to generate a protected face dataset. Extensive experimental
results show that our scheme can maintain visual naturalness and balance the
privacy-utility trade-off compared to the mainstream schemes.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.15538v1">FedRLHF: A Convergence-Guaranteed Federated Framework for
  Privacy-Preserving and Personalized RLHF</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2024-12-20T03:56:31Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Flint Xiaofeng Fan, Cheston Tan, Yew-Soon Ong, Roger Wattenhofer, Wei-Tsang Ooi</p>
    <p><b>Summary:</b> In the era of increasing privacy concerns and demand for personalized
experiences, traditional Reinforcement Learning with Human Feedback (RLHF)
frameworks face significant challenges due to their reliance on centralized
data. We introduce Federated Reinforcement Learning with Human Feedback
(FedRLHF), a novel framework that decentralizes the RLHF process. FedRLHF
enables collaborative policy learning across multiple clients without
necessitating the sharing of raw data or human feedback, thereby ensuring
robust privacy preservation. Leveraging federated reinforcement learning, each
client integrates human feedback locally into their reward functions and
updates their policies through personalized RLHF processes. We establish
rigorous theoretical foundations for FedRLHF, providing convergence guarantees,
and deriving sample complexity bounds that scale efficiently with the number of
clients. Empirical evaluations on the MovieLens and IMDb datasets demonstrate
that FedRLHF not only preserves user privacy but also achieves performance on
par with centralized RLHF, while enhancing personalization across diverse
client environments.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.16246v1">Web Privacy based on Contextual Integrity: Measuring the Collapse of
  Online Contexts</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2024-12-19T23:30:29Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ido Sivan-Sevilla, Parthav Poudel</p>
    <p><b>Summary:</b> The collapse of social contexts has been amplified by digital infrastructures
but surprisingly received insufficient attention from Web privacy scholars.
Users are persistently identified within and across distinct web contexts, in
varying degrees, through and by different websites and trackers, losing the
ability to maintain a fragmented identity. To systematically evaluate this
structural privacy harm we operationalize the theory of Privacy as Contextual
Integrity and measure persistent user identification within and between
distinct Web contexts. We crawl the top-700 popular websites across the
contexts of health, finance, news & media, LGBTQ, eCommerce, adult, and
education websites, for 27 days, to learn how persistent browser identification
via third-party cookies and JavaScript fingerprinting is diffused within and
between web contexts. Past work measured Web tracking in bulk, highlighting the
volume of trackers and tracking techniques. These measurements miss a crucial
privacy implication of Web tracking - the collapse of online contexts. Our
findings reveal how persistent browser identification varies between and within
contexts, diffusing user IDs to different distances, contrasting known tracking
distributions across websites, and conducted as a joint or separate effort via
cookie IDs and JS fingerprinting. Our network analysis can inform the
construction of browser storage containers to protect users against real-time
context collapse. This is a first modest step in measuring Web privacy as
contextual integrity, opening new avenues for contextual Web privacy research.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.15047v1">Measuring, Modeling, and Helping People Account for Privacy Risks in
  Online Self-Disclosures with AI</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-12-19T16:53:40Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Isadora Krsek, Anubha Kabra, Yao Dou, Tarek Naous, Laura A. Dabbish, Alan Ritter, Wei Xu, Sauvik Das</p>
    <p><b>Summary:</b> In pseudonymous online fora like Reddit, the benefits of self-disclosure are
often apparent to users (e.g., I can vent about my in-laws to understanding
strangers), but the privacy risks are more abstract (e.g., will my partner be
able to tell that this is me?). Prior work has sought to develop natural
language processing (NLP) tools that help users identify potentially risky
self-disclosures in their text, but none have been designed for or evaluated
with the users they hope to protect. Absent this assessment, these tools will
be limited by the social-technical gap: users need assistive tools that help
them make informed decisions, not paternalistic tools that tell them to avoid
self-disclosure altogether. To bridge this gap, we conducted a study with N =
21 Reddit users; we had them use a state-of-the-art NLP disclosure detection
model on two of their authored posts and asked them questions to understand if
and how the model helped, where it fell short, and how it could be improved to
help them make more informed decisions. Despite its imperfections, users
responded positively to the model and highlighted its use as a tool that can
help them catch mistakes, inform them of risks they were unaware of, and
encourage self-reflection. However, our work also shows how, to be useful and
usable, AI for supporting privacy decision-making must account for posting
context, disclosure norms, and users' lived threat models, and provide
explanations that help contextualize detected risks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.14832v2">Federated Heavy Hitter Analytics with Local Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB">
  <p><b>Published on:</b> 2024-12-19T13:20:32Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yuemin Zhang, Qingqing Ye, Haibo Hu</p>
    <p><b>Summary:</b> Federated heavy hitter analytics enables service providers to better
understand the preferences of cross-party users by analyzing the most frequent
items. As with federated learning, it faces challenges of privacy concerns,
statistical heterogeneity, and expensive communication. Local differential
privacy (LDP), as the de facto standard for privacy-preserving data collection,
solves the privacy challenge by letting each user perturb her data locally and
report the sanitized version. However, in federated settings, applying LDP
complicates the other two challenges, due to the deteriorated utility by the
injected LDP noise or increasing communication/computation costs by
perturbation mechanism. To tackle these problems, we propose a novel
target-aligning prefix tree mechanism satisfying $\epsilon$-LDP, for federated
heavy hitter analytics. In particular, we propose an adaptive extension
strategy to address the inconsistencies between covering necessary prefixes and
estimating heavy hitters within a party to enhance the utility. We also present
a consensus-based pruning strategy that utilizes noisy prior knowledge from
other parties to further align the inconsistency between finding heavy hitters
in each party and providing reasonable frequency information to identify the
global ones. To the best of our knowledge, our study is the first solution to
the federated heavy hitter analytics in a cross-party setting while satisfying
the stringent $\epsilon$-LDP. Comprehensive experiments on both real-world and
synthetic datasets confirm the effectiveness of our proposed mechanism.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.13953v1">Towards privacy-preserving cooperative control via encrypted distributed
  optimization</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36">
  <p><b>Published on:</b> 2024-12-18T15:32:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Philipp Binfet, Janis Adamek, Nils Schlüter, Moritz Schulze Darup</p>
    <p><b>Summary:</b> Cooperative control is crucial for the effective operation of dynamical
multi-agent systems. Especially for distributed control schemes, it is
essential to exchange data between the agents. This becomes a privacy threat if
the data is sensitive. Encrypted control has shown the potential to address
this risk and ensure confidentiality. However, existing approaches mainly focus
on cloud-based control and distributed schemes are restrictive.
  In this paper, we present a novel privacy-preserving cooperative control
scheme based on encrypted distributed optimization. More precisely, we focus on
a secure distributed solution of a general consensus problem, which has
manifold applications in cooperative control, by means of the alternating
direction method of multipliers (ADMM). As a unique feature of our approach, we
explicitly take into account the common situation that local decision variables
contain copies of quantities associated with neighboring agents and ensure the
neighbor's privacy. We show the effectiveness of our method based on a
numerical case study dealing with the formation of mobile robots.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.13939v1">Security and Privacy of Digital Twins for Advanced Manufacturing: A
  Survey</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36">
  <p><b>Published on:</b> 2024-12-18T15:21:53Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Alexander D. Zemskov, Yao Fu, Runchao Li, Xufei Wang, Vispi Karkaria, Ying-Kuan Tsai, Wei Chen, Jianjing Zhang, Robert Gao, Jian Cao, Kenneth A. Loparo, Pan Li</p>
    <p><b>Summary:</b> In Industry 4.0, the digital twin is one of the emerging technologies,
offering simulation abilities to predict, refine, and interpret conditions and
operations, where it is crucial to emphasize a heightened concentration on the
associated security and privacy risks. To be more specific, the adoption of
digital twins in the manufacturing industry relies on integrating technologies
like cyber-physical systems, the Industrial Internet of Things, virtualization,
and advanced manufacturing. The interactions of these technologies give rise to
numerous security and privacy vulnerabilities that remain inadequately
explored. Towards that end, this paper analyzes the cybersecurity threats of
digital twins for advanced manufacturing in the context of data collection,
data sharing, machine learning and deep learning, and system-level security and
privacy. We also provide several solutions to the threats in those four
categories that can help establish more trust in digital twins.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.13818v1">Fed-AugMix: Balancing Privacy and Utility via Data Augmentation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-12-18T13:05:55Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Haoyang Li, Wei Chen, Xiaojin Zhang</p>
    <p><b>Summary:</b> Gradient leakage attacks pose a significant threat to the privacy guarantees
of federated learning. While distortion-based protection mechanisms are
commonly employed to mitigate this issue, they often lead to notable
performance degradation. Existing methods struggle to preserve model
performance while ensuring privacy. To address this challenge, we propose a
novel data augmentation-based framework designed to achieve a favorable
privacy-utility trade-off, with the potential to enhance model performance in
certain cases. Our framework incorporates the AugMix algorithm at the client
level, enabling data augmentation with controllable severity. By integrating
the Jensen-Shannon divergence into the loss function, we embed the distortion
introduced by AugMix into the model gradients, effectively safeguarding privacy
against deep leakage attacks. Moreover, the JS divergence promotes model
consistency across different augmentations of the same image, enhancing both
robustness and performance. Extensive experiments on benchmark datasets
demonstrate the effectiveness and stability of our method in protecting
privacy. Furthermore, our approach maintains, and in some cases improves, model
performance, showcasing its ability to achieve a robust privacy-utility
trade-off.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.13678v1">Clio: Privacy-Preserving Insights into Real-World AI Use</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-12-18T10:05:43Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Alex Tamkin, Miles McCain, Kunal Handa, Esin Durmus, Liane Lovitt, Ankur Rathi, Saffron Huang, Alfred Mountfield, Jerry Hong, Stuart Ritchie, Michael Stern, Brian Clarke, Landon Goldberg, Theodore R. Sumers, Jared Mueller, William McEachen, Wes Mitchell, Shan Carter, Jack Clark, Jared Kaplan, Deep Ganguli</p>
    <p><b>Summary:</b> How are AI assistants being used in the real world? While model providers in
theory have a window into this impact via their users' data, both privacy
concerns and practical challenges have made analyzing this data difficult. To
address these issues, we present Clio (Claude insights and observations), a
privacy-preserving platform that uses AI assistants themselves to analyze and
surface aggregated usage patterns across millions of conversations, without the
need for human reviewers to read raw conversations. We validate this can be
done with a high degree of accuracy and privacy by conducting extensive
evaluations. We demonstrate Clio's usefulness in two broad ways. First, we
share insights about how models are being used in the real world from one
million Claude.ai Free and Pro conversations, ranging from providing advice on
hairstyles to providing guidance on Git operations and concepts. We also
identify the most common high-level use cases on Claude.ai (coding, writing,
and research tasks) as well as patterns that differ across languages (e.g.,
conversations in Japanese discuss elder care and aging populations at
higher-than-typical rates). Second, we use Clio to make our systems safer by
identifying coordinated attempts to abuse our systems, monitoring for unknown
unknowns during critical periods like launches of new capabilities or major
world events, and improving our existing monitoring systems. We also discuss
the limitations of our approach, as well as risks and ethical concerns. By
enabling analysis of real-world AI usage, Clio provides a scalable platform for
empirically grounded AI safety and governance.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.13522v1">Privacy-Preserving Cyberattack Detection in Blockchain-Based IoT Systems
  Using AI and Homomorphic Encryption</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-12-18T05:46:53Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Bui Duc Manh, Chi-Hieu Nguyen, Dinh Thai Hoang, Diep N. Nguyen, Ming Zeng, Quoc-Viet Pham</p>
    <p><b>Summary:</b> This work proposes a novel privacy-preserving cyberattack detection framework
for blockchain-based Internet-of-Things (IoT) systems. In our approach,
artificial intelligence (AI)-driven detection modules are strategically
deployed at blockchain nodes to identify real-time attacks, ensuring high
accuracy and minimal delay. To achieve this efficiency, the model training is
conducted by a cloud service provider (CSP). Accordingly, blockchain nodes send
their data to the CSP for training, but to safeguard privacy, the data is
encrypted using homomorphic encryption (HE) before transmission. This
encryption method allows the CSP to perform computations directly on encrypted
data without the need for decryption, preserving data privacy throughout the
learning process. To handle the substantial volume of encrypted data, we
introduce an innovative packing algorithm in a Single-Instruction-Multiple-Data
(SIMD) manner, enabling efficient training on HE-encrypted data. Building on
this, we develop a novel deep neural network training algorithm optimized for
encrypted data. We further propose a privacy-preserving distributed learning
approach based on the FedAvg algorithm, which parallelizes the training across
multiple workers, significantly improving computation time. Upon completion,
the CSP distributes the trained model to the blockchain nodes, enabling them to
perform real-time, privacy-preserved detection. Our simulation results
demonstrate that our proposed method can not only mitigate the training time
but also achieve detection accuracy that is approximately identical to the
approach without encryption, with a gap of around 0.01%. Additionally, our real
implementations on various blockchain consensus algorithms and hardware
configurations show that our proposed framework can also be effectively adapted
to real-world systems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.13440v1">Safeguarding Virtual Healthcare: A Novel Attacker-Centric Model for Data
  Security and Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2024-12-18T02:21:53Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Suvineetha Herath, Haywood Gelman, John Hastings, Yong Wang</p>
    <p><b>Summary:</b> The rapid growth of remote healthcare delivery has introduced significant
security and privacy risks to protected health information (PHI). Analysis of a
comprehensive healthcare security breach dataset covering 2009-2023 reveals
their significant prevalence and impact. This study investigates the root
causes of such security incidents and introduces the Attacker-Centric Approach
(ACA), a novel threat model tailored to protect PHI. ACA addresses limitations
in existing threat models and regulatory frameworks by adopting a holistic
attacker-focused perspective, examining threats from the viewpoint of cyber
adversaries, their motivations, tactics, and potential attack vectors.
Leveraging established risk management frameworks, ACA provides a multi-layered
approach to threat identification, risk assessment, and proactive mitigation
strategies. A comprehensive threat library classifies physical, third-party,
external, and internal threats. ACA's iterative nature and feedback mechanisms
enable continuous adaptation to emerging threats, ensuring sustained
effectiveness. ACA allows healthcare providers to proactively identify and
mitigate vulnerabilities, fostering trust and supporting the secure adoption of
virtual care technologies.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.12775v1">RemoteRAG: A Privacy-Preserving LLM Cloud RAG Service</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Retrieval-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-12-17T10:36:52Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yihang Cheng, Lan Zhang, Junyang Wang, Mu Yuan, Yunhao Yao</p>
    <p><b>Summary:</b> Retrieval-augmented generation (RAG) improves the service quality of large
language models by retrieving relevant documents from credible literature and
integrating them into the context of the user query. Recently, the rise of the
cloud RAG service has made it possible for users to query relevant documents
conveniently. However, directly sending queries to the cloud brings potential
privacy leakage. In this paper, we are the first to formally define the
privacy-preserving cloud RAG service to protect the user query and propose
RemoteRAG as a solution regarding privacy, efficiency, and accuracy. For
privacy, we introduce $(n,\epsilon)$-DistanceDP to characterize privacy leakage
of the user query and the leakage inferred from relevant documents. For
efficiency, we limit the search range from the total documents to a small
number of selected documents related to a perturbed embedding generated from
$(n,\epsilon)$-DistanceDP, so that computation and communication costs required
for privacy protection significantly decrease. For accuracy, we ensure that the
small range includes target documents related to the user query with detailed
theoretical analysis. Experimental results also demonstrate that RemoteRAG can
resist existing embedding inversion attack methods while achieving no loss in
retrieval under various settings. Moreover, RemoteRAG is efficient, incurring
only $0.67$ seconds and $46.66$KB of data transmission ($2.72$ hours and $1.43$
GB with the non-optimized privacy-preserving scheme) when retrieving from a
total of $10^6$ documents.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.12387v2">Differential Privacy Preserving Distributed Quantum Computing</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2024-12-16T22:46:46Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Hui Zhong, Keyi Ju, Jiachen Shen, Xinyue Zhang, Xiaoqi Qin, Tomoaki Ohtsuki, Miao Pan, Zhu Han</p>
    <p><b>Summary:</b> Existing quantum computers can only operate with hundreds of qubits in the
Noisy Intermediate-Scale Quantum (NISQ) state, while quantum distributed
computing (QDC) is regarded as a reliable way to address this limitation,
allowing quantum computers to achieve their full computational potential.
However, similar to classical distributed computing, QDC also faces the problem
of privacy leakage. Existing research has introduced quantum differential
privacy (QDP) for privacy protection in central quantum computing, but there is
no dedicated privacy protection mechanisms for QDC. To fill this research gap,
our paper introduces a novel concept called quantum R\'enyi differential
privacy (QRDP), which incorporates the advantages of classical R\'enyi DP and
is applicable in the QDC domain. Based on the new quantum R\'enyi divergence,
QRDP provides delicate and flexible privacy protection by introducing parameter
$\alpha$. In particular, the QRDP composition is well suited for QDC, since it
allows for more precise control of the total privacy budget in scenarios
requiring multiple quantum operations. We analyze a variety of noise mechanisms
that can implement QRDP, and derive the lowest privacy budget provided by these
mechanisms. Finally, we investigate the impact of different quantum parameters
on QRDP. Through our simulations, we also find that adding noise will make the
data less usable, but increase the level of privacy protection.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.12374v1">Privacy in Metalearning and Multitask Learning: Modeling and Separations</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-12-16T22:07:33Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Maryam Aliakbarpour, Konstantina Bairaktari, Adam Smith, Marika Swanberg, Jonathan Ullman</p>
    <p><b>Summary:</b> Model personalization allows a set of individuals, each facing a different
learning task, to train models that are more accurate for each person than
those they could develop individually. The goals of personalization are
captured in a variety of formal frameworks, such as multitask learning and
metalearning. Combining data for model personalization poses risks for privacy
because the output of an individual's model can depend on the data of other
individuals. In this work we undertake a systematic study of differentially
private personalized learning. Our first main contribution is to construct a
taxonomy of formal frameworks for private personalized learning. This taxonomy
captures different formal frameworks for learning as well as different threat
models for the attacker. Our second main contribution is to prove separations
between the personalized learning problems corresponding to different choices.
In particular, we prove a novel separation between private multitask learning
and private metalearning.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.11951v1">The Impact of Generalization Techniques on the Interplay Among Privacy,
  Utility, and Fairness in Image Classification</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-12-16T16:35:31Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ahmad Hassanpour, Amir Zarei, Khawla Mallat, Anderson Santana de Oliveira, Bian Yang</p>
    <p><b>Summary:</b> This study investigates the trade-offs between fairness, privacy, and utility
in image classification using machine learning (ML). Recent research suggests
that generalization techniques can improve the balance between privacy and
utility. One focus of this work is sharpness-aware training (SAT) and its
integration with differential privacy (DP-SAT) to further improve this balance.
Additionally, we examine fairness in both private and non-private learning
models trained on datasets with synthetic and real-world biases. We also
measure the privacy risks involved in these scenarios by performing membership
inference attacks (MIAs) and explore the consequences of eliminating
high-privacy risk samples, termed outliers. Moreover, we introduce a new
metric, named \emph{harmonic score}, which combines accuracy, privacy, and
fairness into a single measure.
  Through empirical analysis using generalization techniques, we achieve an
accuracy of 81.11\% under $(8, 10^{-5})$-DP on CIFAR-10, surpassing the 79.5\%
reported by De et al. (2022). Moreover, our experiments show that memorization
of training samples can begin before the overfitting point, and generalization
techniques do not guarantee the prevention of this memorization. Our analysis
of synthetic biases shows that generalization techniques can amplify model bias
in both private and non-private models. Additionally, our results indicate that
increased bias in training data leads to reduced accuracy, greater
vulnerability to privacy attacks, and higher model bias. We validate these
findings with the CelebA dataset, demonstrating that similar trends persist
with real-world attribute imbalances. Finally, our experiments show that
removing outlier data decreases accuracy and further amplifies model bias.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.11737v1">Efficiently Achieving Secure Model Training and Secure Aggregation to
  Ensure Bidirectional Privacy-Preservation in Federated Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-12-16T12:58:21Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xue Yang, Depan Peng, Yan Feng, Xiaohu Tang, Weijun Fang, Jun Shao</p>
    <p><b>Summary:</b> Bidirectional privacy-preservation federated learning is crucial as both
local gradients and the global model may leak privacy. However, only a few
works attempt to achieve it, and they often face challenges such as excessive
communication and computational overheads, or significant degradation of model
accuracy, which hinders their practical applications. In this paper, we design
an efficient and high-accuracy bidirectional privacy-preserving scheme for
federated learning to complete secure model training and secure aggregation. To
efficiently achieve bidirectional privacy, we design an efficient and
accuracy-lossless model perturbation method on the server side (called
$\mathbf{MP\_Server}$) that can be combined with local differential privacy
(LDP) to prevent clients from accessing the model, while ensuring that the
local gradients obtained on the server side satisfy LDP. Furthermore, to ensure
model accuracy, we customize a distributed differential privacy mechanism on
the client side (called $\mathbf{DDP\_Client}$). When combined with
$\mathbf{MP\_Server}$, it ensures LDP of the local gradients, while ensuring
that the aggregated result matches the accuracy of central differential privacy
(CDP). Extensive experiments demonstrate that our scheme significantly
outperforms state-of-the-art bidirectional privacy-preservation baselines
(SOTAs) in terms of computational cost, model accuracy, and defense ability
against privacy attacks. Particularly, given target accuracy, the training time
of SOTAs is approximately $200$ times, or even over $1000$ times, longer than
that of our scheme. When the privacy budget is set relatively small, our scheme
incurs less than $6\%$ accuracy loss compared to the privacy-ignoring method,
while SOTAs suffer up to $20\%$ accuracy loss. Experimental results also show
that the defense capability of our scheme outperforms than SOTAs.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.11572v1">DB-PAISA: Discovery-Based Privacy-Agile IoT Sensing+Actuation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-12-16T08:57:24Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Isita Bagayatkar, Youngil Kim, Gene Tsudik</p>
    <p><b>Summary:</b> Internet of Things (IoT) devices are becoming increasingly commonplace in
numerous public and semi-private settings. Currently, most such devices lack
mechanisms to facilitate their discovery by casual (nearby) users who are not
owners or operators. However, these users are potentially being sensed, and/or
actuated upon, by these devices, without their knowledge or consent. This
naturally triggers privacy, security, and safety issues.
  To address this problem, some recent work explored device transparency in the
IoT ecosystem. The intuitive approach is for each device to periodically and
securely broadcast (announce) its presence and capabilities to all nearby
users. While effective, when no new users are present, this push-based approach
generates a substantial amount of unnecessary network traffic and needlessly
interferes with normal device operation.
  In this work, we construct DB-PAISA which addresses these issues via a
pull-based method, whereby devices reveal their presence and capabilities only
upon explicit user request. Each device guarantees a secure timely response
(even if fully compromised by malware) based on a small active Root-of-Trust
(RoT). DB-PAISA requires no hardware modifications and is suitable for a range
of current IoT devices. To demonstrate its feasibility and practicality, we
built a fully functional and publicly available prototype. It is implemented
atop a commodity MCU (NXP LCP55S69) and operates in tandem with a
smartphone-based app. Using this prototype, we evaluate energy consumption and
other performance factors.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.11394v1">Privacy-Preserving Brain-Computer Interfaces: A Systematic Review</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2024-12-16T02:45:37Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> K. Xia, W. Duch, Y. Sun, K. Xu, W. Fang, H. Luo, Y. Zhang, D. Sang, X. Xu, F-Y Wang, D. Wu</p>
    <p><b>Summary:</b> A brain-computer interface (BCI) establishes a direct communication pathway
between the human brain and a computer. It has been widely used in medical
diagnosis, rehabilitation, education, entertainment, etc. Most research so far
focuses on making BCIs more accurate and reliable, but much less attention has
been paid to their privacy. Developing a commercial BCI system usually requires
close collaborations among multiple organizations, e.g., hospitals,
universities, and/or companies. Input data in BCIs, e.g., electroencephalogram
(EEG), contain rich privacy information, and the developed machine learning
model is usually proprietary. Data and model transmission among different
parties may incur significant privacy threats, and hence privacy protection in
BCIs must be considered. Unfortunately, there does not exist any contemporary
and comprehensive review on privacy-preserving BCIs. This paper fills this gap,
by describing potential privacy threats and protection strategies in BCIs. It
also points out several challenges and future research directions in developing
privacy-preserving BCIs.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.11390v1">Accurate, Robust and Privacy-Preserving Brain-Computer Interface
  Decoding</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> 
  <p><b>Published on:</b> 2024-12-16T02:37:38Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xiaoqing Chen, Tianwang Jia, Dongrui Wu</p>
    <p><b>Summary:</b> An electroencephalogram (EEG) based brain-computer interface (BCI) enables
direct communication between the brain and external devices. However, EEG-based
BCIs face at least three major challenges in real-world applications: data
scarcity and individual differences, adversarial vulnerability, and data
privacy. While previous studies have addressed one or two of these issues,
simultaneous accommodation of all three challenges remains challenging and
unexplored. This paper fills this gap, by proposing an Augmented Robustness
Ensemble (ARE) algorithm and integrating it into three privacy protection
scenarios (centralized source-free transfer, federated source-free transfer,
and source data perturbation), achieving simultaneously accurate decoding,
adversarial robustness, and privacy protection of EEG-based BCIs. Experiments
on three public EEG datasets demonstrated that our proposed approach
outperformed over 10 classic and state-of-the-art approaches in both accuracy
and robustness in all three privacy-preserving scenarios, even outperforming
state-of-the-art transfer learning approaches that do not consider privacy
protection at all. This is the first time that three major challenges in
EEG-based BCIs can be addressed simultaneously, significantly improving the
practicalness of EEG decoding in real-world BCIs.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.11066v1">Learning Robust and Privacy-Preserving Representations via Information
  Theory</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-12-15T05:51:48Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Binghui Zhang, Sayedeh Leila Noorbakhsh, Yun Dong, Yuan Hong, Binghui Wang</p>
    <p><b>Summary:</b> Machine learning models are vulnerable to both security attacks (e.g.,
adversarial examples) and privacy attacks (e.g., private attribute inference).
We take the first step to mitigate both the security and privacy attacks, and
maintain task utility as well. Particularly, we propose an
information-theoretic framework to achieve the goals through the lens of
representation learning, i.e., learning representations that are robust to both
adversarial examples and attribute inference adversaries. We also derive novel
theoretical results under our framework, e.g., the inherent trade-off between
adversarial robustness/utility and attribute privacy, and guaranteed attribute
privacy leakage against attribute inference adversaries.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.10652v1">Centaur: Bridging the Impossible Trinity of Privacy, Efficiency, and
  Performance in Privacy-Preserving Transformer Inference</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-12-14T02:50:30Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jinglong Luo, Guanzhong Chen, Yehong Zhang, Shiyu Liu, Hui Wang, Yue Yu, Xun Zhou, Yuan Qi, Zenglin Xu</p>
    <p><b>Summary:</b> As pre-trained models, like Transformers, are increasingly deployed on cloud
platforms for inference services, the privacy concerns surrounding model
parameters and inference data are becoming more acute. Current
Privacy-Preserving Transformer Inference (PPTI) frameworks struggle with the
"impossible trinity" of privacy, efficiency, and performance. For instance,
Secure Multi-Party Computation (SMPC)-based solutions offer strong privacy
guarantees but come with significant inference overhead and performance
trade-offs. On the other hand, PPTI frameworks that use random permutations
achieve inference efficiency close to that of plaintext and maintain accurate
results but require exposing some model parameters and intermediate results,
thereby risking substantial privacy breaches. Addressing this "impossible
trinity" with a single technique proves challenging. To overcome this
challenge, we propose Centaur, a novel hybrid PPTI framework. Unlike existing
methods, Centaur protects model parameters with random permutations and
inference data with SMPC, leveraging the structure of Transformer models. By
designing a series of efficient privacy-preserving algorithms, Centaur
leverages the strengths of both techniques to achieve a better balance between
privacy, efficiency, and performance in PPTI. We comprehensively evaluate the
effectiveness of Centaur on various types of Transformer models and datasets.
Experimental results demonstrate that the privacy protection capabilities
offered by Centaur can withstand various existing model inversion attack
methods. In terms of performance and efficiency, Centaur not only maintains the
same performance as plaintext inference but also improves inference speed by
$5.0-30.4$ times.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.10612v1">Meeting Utility Constraints in Differential Privacy: A Privacy-Boosting
  Approach</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2024-12-13T23:34:30Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Bo Jiang, Wanrong Zhang, Donghang Lu, Jian Du, Sagar Sharma, Qiang Yan</p>
    <p><b>Summary:</b> Data engineering often requires accuracy (utility) constraints on results,
posing significant challenges in designing differentially private (DP)
mechanisms, particularly under stringent privacy parameter $\epsilon$. In this
paper, we propose a privacy-boosting framework that is compatible with most
noise-adding DP mechanisms. Our framework enhances the likelihood of outputs
falling within a preferred subset of the support to meet utility requirements
while enlarging the overall variance to reduce privacy leakage. We characterize
the privacy loss distribution of our framework and present the privacy profile
formulation for $(\epsilon,\delta)$-DP and R\'enyi DP (RDP) guarantees. We
study special cases involving data-dependent and data-independent utility
formulations. Through extensive experiments, we demonstrate that our framework
achieves lower privacy loss than standard DP mechanisms under utility
constraints. Notably, our approach is particularly effective in reducing
privacy loss with large query sensitivity relative to the true answer, offering
a more practical and flexible approach to designing differentially private
mechanisms that meet specific utility constraints.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.09812v1">ScaleOT: Privacy-utility-scalable Offsite-tuning with Dynamic
  LayerReplace and Selective Rank Compression</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-12-13T03:00:48Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kai Yao, Zhaorui Tan, Tiandi Ye, Lichun Li, Yuan Zhao, Wenyan Liu, Wei Wang, Jianke Zhu</p>
    <p><b>Summary:</b> Offsite-tuning is a privacy-preserving method for tuning large language
models (LLMs) by sharing a lossy compressed emulator from the LLM owners with
data owners for downstream task tuning. This approach protects the privacy of
both the model and data owners. However, current offsite tuning methods often
suffer from adaptation degradation, high computational costs, and limited
protection strength due to uniformly dropping LLM layers or relying on
expensive knowledge distillation. To address these issues, we propose ScaleOT,
a novel privacy-utility-scalable offsite-tuning framework that effectively
balances privacy and utility. ScaleOT introduces a novel layerwise lossy
compression algorithm that uses reinforcement learning to obtain the importance
of each layer. It employs lightweight networks, termed harmonizers, to replace
the raw LLM layers. By combining important original LLM layers and harmonizers
in different ratios, ScaleOT generates emulators tailored for optimal
performance with various model scales for enhanced privacy protection.
Additionally, we present a rank reduction method to further compress the
original LLM layers, significantly enhancing privacy with negligible impact on
utility. Comprehensive experiments show that ScaleOT can achieve nearly
lossless offsite tuning performance compared with full fine-tuning while
obtaining better model privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.09256v1">Differential Privacy Releasing of Hierarchical Origin/Destination Data
  with a TopDown Approach</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B">
  <p><b>Published on:</b> 2024-12-12T13:14:15Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Fabrizio Boninsegna, Francesco Silvestri</p>
    <p><b>Summary:</b> This paper presents a novel method to generate differentially private tabular
datasets for hierarchical data, with a specific focus on origin-destination
(O/D) trips. The approach builds upon the TopDown algorithm, a constraint-based
mechanism designed to incorporate invariant queries into tabular data,
developed by the US Census. O/D hierarchical data refers to datasets
representing trips between geographical areas organized in a hierarchical
structure (e.g., region $\rightarrow$ province $\rightarrow$ city). The
developed method is crafted to improve accuracy on queries spanning wider
geographical areas that can be obtained by aggregation. Maintaining high
accuracy for aggregated geographical queries is a crucial attribute of the
differentially private dataset, particularly for practitioners. Furthermore,
the approach is designed to minimize false positives detection and to replicate
the sparsity of the sensitive data. The key technical contributions of this
paper include a novel TopDown algorithm that employs constrained optimization
with Chebyshev distance minimization, with theoretical guarantees based on the
maximum absolute error. Additionally, we propose a new integer optimization
algorithm that significantly reduces the incidence of false positives. The
effectiveness of the proposed approach is validated using both real-world and
synthetic O/D datasets, demonstrating its ability to generate private data with
high utility and a reduced number of false positives. We emphasize that the
proposed algorithm is applicable to any tabular data with a hierarchical
structure.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.09222v1">Building a Privacy Web with SPIDEr -- Secure Pipeline for Information
  De-Identification with End-to-End Encryption</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2024-12-12T12:24:12Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Novoneel Chakraborty, Anshoo Tandon, Kailash Reddy, Kaushal Kirpekar, Bryan Paul Robert, Hari Dilip Kumar, Abhilash Venkatesh, Abhay Sharma</p>
    <p><b>Summary:</b> Data de-identification makes it possible to glean insights from data while
preserving user privacy. The use of Trusted Execution Environments (TEEs) allow
for the execution of de-identification applications on the cloud without the
need for a user to trust the third-party application provider. In this paper,
we present \textit{SPIDEr - Secure Pipeline for Information De-Identification
with End-to-End Encryption}, our implementation of an end-to-end encrypted data
de-identification pipeline. SPIDEr supports classical anonymisation techniques
such as suppression, pseudonymisation, generalisation, and aggregation, as well
as techniques that offer a formal privacy guarantee such as k-anonymisation and
differential privacy. To enable scalability and improve performance on
constrained TEE hardware, we enable batch processing of data for differential
privacy computations. We present our design of the control flows for end-to-end
secure execution of de-identification operations within a TEE. As part of the
control flow for running SPIDEr within the TEE, we perform attestation, a
process that verifies that the software binaries were properly instantiated on
a known, trusted platform.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.09195v1">On the Generation and Removal of Speaker Adversarial Perturbation for
  Voice-Privacy Protection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Sound-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> 
  <p><b>Published on:</b> 2024-12-12T11:46:07Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chenyang Guo, Liping Chen, Zhuhai Li, Kong Aik Lee, Zhen-Hua Ling, Wu Guo</p>
    <p><b>Summary:</b> Neural networks are commonly known to be vulnerable to adversarial attacks
mounted through subtle perturbation on the input data. Recent development in
voice-privacy protection has shown the positive use cases of the same technique
to conceal speaker's voice attribute with additive perturbation signal
generated by an adversarial network. This paper examines the reversibility
property where an entity generating the adversarial perturbations is authorized
to remove them and restore original speech (e.g., the speaker him/herself). A
similar technique could also be used by an investigator to deanonymize a
voice-protected speech to restore criminals' identities in security and
forensic analysis. In this setting, the perturbation generative module is
assumed to be known in the removal process. To this end, a joint training of
perturbation generation and removal modules is proposed. Experimental results
on the LibriSpeech dataset demonstrated that the subtle perturbations added to
the original speech can be predicted from the anonymized speech while achieving
the goal of privacy protection. By removing these perturbations from the
anonymized sample, the original speech can be restored. Audio samples can be
found in \url{https://voiceprivacy.github.io/Perturbation-Generation-Removal/}.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.08559v1">Underestimated Privacy Risks for Minority Populations in Large Language
  Model Unlearning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-12-11T17:22:07Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Rongzhe Wei, Mufei Li, Mohsen Ghassemi, Eleonora Kreačić, Yifan Li, Xiang Yue, Bo Li, Vamsi K. Potluru, Pan Li, Eli Chien</p>
    <p><b>Summary:</b> Large Language Models are trained on extensive datasets that often contain
sensitive, human-generated information, raising significant concerns about
privacy breaches. While certified unlearning approaches offer strong privacy
guarantees, they rely on restrictive model assumptions that are not applicable
to LLMs. As a result, various unlearning heuristics have been proposed, with
the associated privacy risks assessed only empirically. The standard evaluation
pipelines typically randomly select data for removal from the training set,
apply unlearning techniques, and use membership inference attacks to compare
the unlearned models against models retrained without the to-be-unlearned data.
However, since every data point is subject to the right to be forgotten,
unlearning should be considered in the worst-case scenario from the privacy
perspective. Prior work shows that data outliers may exhibit higher
memorization effects. Intuitively, they are harder to be unlearn and thus the
privacy risk of unlearning them is underestimated in the current evaluation. In
this paper, we leverage minority data to identify such a critical flaw in
previously widely adopted evaluations. We substantiate this claim through
carefully designed experiments, including unlearning canaries related to
minority groups, inspired by privacy auditing literature. Using personally
identifiable information as a representative minority identifier, we
demonstrate that minority groups experience at least 20% more privacy leakage
in most cases across six unlearning approaches, three MIAs, three benchmark
datasets, and two LLMs of different scales. Given that the right to be
forgotten should be upheld for every individual, we advocate for a more
rigorous evaluation of LLM unlearning methods. Our minority-aware evaluation
framework represents an initial step toward ensuring more equitable assessments
of LLM unlearning efficacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.08544v1">Training Data Reconstruction: Privacy due to Uncertainty?</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-12-11T17:00:29Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Christina Runkel, Kanchana Vaishnavi Gandikota, Jonas Geiping, Carola-Bibiane Schönlieb, Michael Moeller</p>
    <p><b>Summary:</b> Being able to reconstruct training data from the parameters of a neural
network is a major privacy concern. Previous works have shown that
reconstructing training data, under certain circumstances, is possible. In this
work, we analyse such reconstructions empirically and propose a new formulation
of the reconstruction as a solution to a bilevel optimisation problem. We
demonstrate that our formulation as well as previous approaches highly depend
on the initialisation of the training images $x$ to reconstruct. In particular,
we show that a random initialisation of $x$ can lead to reconstructions that
resemble valid training samples while not being part of the actual training
dataset. Thus, our experiments on affine and one-hidden layer networks suggest
that when reconstructing natural images, yet an adversary cannot identify
whether reconstructed images have indeed been part of the set of training
samples.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.08534v1">Protecting Confidentiality, Privacy and Integrity in Collaborative
  Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-12-11T16:48:18Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Dong Chen, Alice Dethise, Istemi Ekin Akkus, Ivica Rimac, Klaus Satzke, Antti Koskela, Marco Canini, Wei Wang, Ruichuan Chen</p>
    <p><b>Summary:</b> A collaboration between dataset owners and model owners is needed to
facilitate effective machine learning (ML) training. During this collaboration,
however, dataset owners and model owners want to protect the confidentiality of
their respective assets (i.e., datasets, models and training code), with the
dataset owners also caring about the privacy of individual users whose data is
in their datasets. Existing solutions either provide limited confidentiality
for models and training code, or suffer from privacy issues due to collusion.
  We present Citadel++, a scalable collaborative ML training system designed to
simultaneously protect the confidentiality of datasets, models and training
code, as well as the privacy of individual users. Citadel++ enhances
differential privacy techniques to safeguard the privacy of individual user
data while maintaining model utility. By employing Virtual Machine-level
Trusted Execution Environments (TEEs) and improved integrity protection
techniques through various OS-level mechanisms, Citadel++ effectively preserves
the confidentiality of datasets, models and training code, and enforces our
privacy mechanisms even when the models and training code have been maliciously
designed. Our experiments show that Citadel++ provides privacy, model utility
and performance while adhering to confidentiality and privacy requirements of
dataset owners and model owners, outperforming the state-of-the-art
privacy-preserving training systems by up to 543x on CPU and 113x on GPU TEEs.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.08276v1">Local Features Meet Stochastic Anonymization: Revolutionizing
  Privacy-Preserving Face Recognition for Black-Box Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-12-11T10:49:15Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yuanwei Liu, Chengyu Jia, Ruqi Xiao, Xuemai Jia, Hui Wei, Kui Jiang, Zheng Wang</p>
    <p><b>Summary:</b> The task of privacy-preserving face recognition (PPFR) currently faces two
major unsolved challenges: (1) existing methods are typically effective only on
specific face recognition models and struggle to generalize to black-box face
recognition models; (2) current methods employ data-driven reversible
representation encoding for privacy protection, making them susceptible to
adversarial learning and reconstruction of the original image. We observe that
face recognition models primarily rely on local features ({e.g., face contour,
skin texture, and so on) for identification. Thus, by disrupting global
features while enhancing local features, we achieve effective recognition even
in black-box environments. Additionally, to prevent adversarial models from
learning and reversing the anonymization process, we adopt an adversarial
learning-based approach with irreversible stochastic injection to ensure the
stochastic nature of the anonymization. Experimental results demonstrate that
our method achieves an average recognition accuracy of 94.21\% on black-box
models, outperforming existing methods in both privacy protection and
anti-reconstruction capabilities.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.07687v2">Privacy-Preserving Customer Support: A Framework for Secure and Scalable
  Interactions</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">   
  <p><b>Published on:</b> 2024-12-10T17:20:47Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Anant Prakash Awasthi, Girdhar Gopal Agarwal, Chandraketu Singh, Rakshit Varma, Sanchit Sharma</p>
    <p><b>Summary:</b> The growing reliance on artificial intelligence (AI) in customer support has
significantly improved operational efficiency and user experience. However,
traditional machine learning (ML) approaches, which require extensive local
training on sensitive datasets, pose substantial privacy risks and compliance
challenges with regulations like the General Data Protection Regulation (GDPR)
and California Consumer Privacy Act (CCPA). Existing privacy-preserving
techniques, such as anonymization, differential privacy, and federated
learning, address some concerns but face limitations in utility, scalability,
and complexity. This paper introduces the Privacy-Preserving Zero-Shot Learning
(PP-ZSL) framework, a novel approach leveraging large language models (LLMs) in
a zero-shot learning mode. Unlike conventional ML methods, PP-ZSL eliminates
the need for local training on sensitive data by utilizing pre-trained LLMs to
generate responses directly. The framework incorporates real-time data
anonymization to redact or mask sensitive information, retrieval-augmented
generation (RAG) for domain-specific query resolution, and robust
post-processing to ensure compliance with regulatory standards. This
combination reduces privacy risks, simplifies compliance, and enhances
scalability and operational efficiency. Empirical analysis demonstrates that
the PP-ZSL framework provides accurate, privacy-compliant responses while
significantly lowering the costs and complexities of deploying AI-driven
customer support systems. The study highlights potential applications across
industries, including financial services, healthcare, e-commerce, legal
support, telecommunications, and government services. By addressing the dual
challenges of privacy and performance, this framework establishes a foundation
for secure, efficient, and regulatory-compliant AI applications in customer
interactions.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.06960v1">Simplications: Why and how we should rethink data of/by/for the people
  in smart homes and its privacy implications</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2024-12-09T20:08:26Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Albrecht Kurze, Alexa Becker</p>
    <p><b>Summary:</b> More and more smart devices enter our homes. Often these devices come with a
variety of sensors, mostly simple sensors, e.g., for light, temperature,
humidity or motion. And they all collect data. While it is data of the home
environment it is also data of domestic life in the home. Thus it is data of
the people and by the people in the home capturing their presence, arrival and
departure, typical domestic activities, bad habits, health status etc. Based on
previous as well as ongoing research we know that people are actually able to
make sense of simple sensor data and that they will make use of it for their
own purposes. Simple sensors, when critically reflected, are often only
"simple" in a technical sense. The unreflected design and use of these sensors
can easily lead to unintended implications, i.e. for privacy. However, it may
not even need a Big Brother or data experts or AI to make the data of these
sensors sensitive, e.g., if used for lateral surveillance within families.
Often unintended but wicked implications emerge despite good intentions, such
as improving efficiency or energy saving through collecting sensor data. Thus
sensor data from the home is actually data of/by/for the people in the home.
First, we explain how this might have relevance across scales of community of
people - not only for the domain of the home but also in broader meaning.
Second, we relate our previous as well as ongoing research in the domain of
smart homes to this topic.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.06728v1">Byzantine-Eavesdropper Alliance: How to Achieve Symmetric Privacy in
  Quantum $X$-Secure $B$-Byzantine $E$-Eavesdropped $U$-Unresponsive
  $T$-Colluding PIR?</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Networking and Internet Architecture-04E762">   
  <p><b>Published on:</b> 2024-12-09T18:17:24Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mohamed Nomeir, Alptug Aytekin, Sennur Ulukus</p>
    <p><b>Summary:</b> We consider the quantum \emph{symmetric} private information retrieval
(QSPIR) problem in a system with $N$ databases and $K$ messages, with $U$
unresponsive servers, $T$-colluding servers, and $X$-security parameter, under
several fundamental threat models. In the first model, there are
$\mathcal{E}_1$ eavesdropped links in the uplink direction (the direction from
the user to the $N$ servers), $\mathcal{E}_2$ eavesdropped links in the
downlink direction (the direction from the servers to the user), where
$|\mathcal{E}_1|, |\mathcal{E}_2| \leq E$; we coin this eavesdropper setting as
\emph{dynamic} eavesdroppers. We show that super-dense coding gain can be
achieved for some regimes. In the second model, we consider the case with
Byzantine servers, i.e., servers that can coordinate to devise a plan to harm
the privacy and security of the system together with static eavesdroppers, by
listening to the same links in both uplink and downlink directions. It is
important to note the considerable difference between the two threat models,
since the eavesdroppers can take huge advantage of the presence of the
Byzantine servers. Unlike the previous works in SPIR with Byzantine servers,
that assume that the Byzantine servers can send only random symbols independent
of the stored messages, we follow the definition of Byzantine servers in
\cite{byzantine_tpir}, where the Byzantine servers can send symbols that can be
functions of the storage, queries, as well as the random symbols in a way that
can produce worse harm to the system. In the third and the most novel threat
model, we consider the presence of Byzantine servers and dynamic eavesdroppers
together. We show that having dynamic eavesdroppers along with Byzantine
servers in the same system model creates more threats to the system than having
static eavesdroppers with Byzantine servers.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.06689v1">Impact of Privacy Parameters on Deep Learning Models for Image
  Classification</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-12-09T17:31:55Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Basanta Chaulagain</p>
    <p><b>Summary:</b> The project aims to develop differentially private deep learning models for
image classification on CIFAR-10 datasets \cite{cifar10} and analyze the impact
of various privacy parameters on model accuracy. We have implemented five
different deep learning models, namely ConvNet, ResNet18, EfficientNet, ViT,
and DenseNet121 and three supervised classifiers namely K-Nearest Neighbors,
Naive Bayes Classifier and Support Vector Machine. We evaluated the performance
of these models under varying settings. Our best performing model to date is
EfficientNet with test accuracy of $59.63\%$ with the following parameters
(Adam optimizer, batch size 256, epoch size 100, epsilon value 5.0, learning
rate $1e-3$, clipping threshold 1.0, and noise multiplier 0.912).</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.06541v2">Numerical Estimation of Spatial Distributions under Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB">
  <p><b>Published on:</b> 2024-12-09T14:53:57Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Leilei Du, Peng Cheng, Libin Zheng, Xiang Lian, Lei Chen, Wei Xi, Wangze Ni</p>
    <p><b>Summary:</b> Estimating spatial distributions is important in data analysis, such as
traffic flow forecasting and epidemic prevention. To achieve accurate spatial
distribution estimation, the analysis needs to collect sufficient user data.
However, collecting data directly from individuals could compromise their
privacy. Most previous works focused on private distribution estimation for
one-dimensional data, which does not consider spatial data relation and leads
to poor accuracy for spatial distribution estimation. In this paper, we address
the problem of private spatial distribution estimation, where we collect
spatial data from individuals and aim to minimize the distance between the
actual distribution and estimated one under Local Differential Privacy (LDP).
To leverage the numerical nature of the domain, we project spatial data and its
relationships onto a one-dimensional distribution. We then use this projection
to estimate the overall spatial distribution. Specifically, we propose a
reporting mechanism called Disk Area Mechanism (DAM), which projects the
spatial domain onto a line and optimizes the estimation using the sliced
Wasserstein distance. Through extensive experiments, we show the effectiveness
of our DAM approach on both real and synthetic data sets, compared with the
state-of-the-art methods, such as Multi-dimensional Square Wave Mechanism
(MDSW) and Subset Exponential Mechanism with Geo-I (SEM-Geo-I). Our results
show that our DAM always performs better than MDSW and is better than SEM-Geo-I
when the data granularity is fine enough.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.06248v1">Rendering-Refined Stable Diffusion for Privacy Compliant Synthetic Data</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-12-09T06:47:29Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kartik Patwari, David Schneider, Xiaoxiao Sun, Chen-Nee Chuah, Lingjuan Lyu, Vivek Sharma</p>
    <p><b>Summary:</b> Growing privacy concerns and regulations like GDPR and CCPA necessitate
pseudonymization techniques that protect identity in image datasets. However,
retaining utility is also essential. Traditional methods like masking and
blurring degrade quality and obscure critical context, especially in
human-centric images. We introduce Rendering-Refined Stable Diffusion (RefSD),
a pipeline that combines 3D-rendering with Stable Diffusion, enabling
prompt-based control over human attributes while preserving posture. Unlike
standard diffusion models that fail to retain posture or GANs that lack realism
and flexible attribute control, RefSD balances posture preservation, realism,
and customization. We also propose HumanGenAI, a framework for human perception
and utility evaluation. Human perception assessments reveal attribute-specific
strengths and weaknesses of RefSD. Our utility experiments show that models
trained on RefSD pseudonymized data outperform those trained on real data in
detection tasks, with further performance gains when combining RefSD with real
data. For classification tasks, we consistently observe performance
improvements when using RefSD data with real data, confirming the utility of
our pseudonymized data.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.06196v1">BECS: A Privacy-Preserving Computing Sharing Mechanism in 6G Computing
  Power Network</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Networking and Internet Architecture-04E762"> 
  <p><b>Published on:</b> 2024-12-09T04:26:14Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kun Yan, Wenping Ma, Shaohui Sun</p>
    <p><b>Summary:</b> 5G networks provide secure and reliable information transmission services for
the Internet of Everything, thus paving the way for 6G networks, which is
anticipated to be an AI-based network, supporting unprecedented intelligence
across applications. Abundant computing resources will establish the 6G
Computing Power Network (CPN) to facilitate ubiquitous intelligent services. In
this article, we propose BECS, a computing sharing mechanism based on
evolutionary algorithm and blockchain, designed to balance task offloading
among user devices, edge devices, and cloud resources within 6G CPN, thereby
enhancing the computing resource utilization. We model computing sharing as a
multi-objective optimization problem, aiming to improve resource utilization
while balancing other issues. To tackle this NP-hard problem, we devise a
kernel distance-based dominance relation and incorporated it into the
Non-dominated Sorting Genetic Algorithm III, significantly enhancing the
diversity of the evolutionary population. In addition, we propose a pseudonym
scheme based on zero-knowledge proof to protect the privacy of users
participating in computing sharing. Finally, the security analysis and
simulation results demonstrate that BECS can fully and effectively utilize all
computing resources in 6G CPN, significantly improving the computing resource
utilization while protecting user privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.06120v1">Lightweight Federated Learning with Differential Privacy and Straggler
  Resilience</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2024-12-09T00:54:00Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shu Hong, Xiaojun Lin, Lingjie Duan</p>
    <p><b>Summary:</b> Federated learning (FL) enables collaborative model training through model
parameter exchanges instead of raw data. To avoid potential inference attacks
from exchanged parameters, differential privacy (DP) offers rigorous guarantee
against various attacks. However, conventional methods of ensuring DP by adding
local noise alone often result in low training accuracy. Combining secure
multi-party computation (SMPC) with DP, while improving the accuracy, incurs
high communication and computation overheads and straggler vulnerability, in
either client-to-server or client-to-client links. In this paper, we propose
LightDP-FL, a novel lightweight scheme that ensures provable DP against
untrusted peers and server, while maintaining straggler-resilience, low
overheads and high training accuracy. Our approach incorporates both individual
and pairwise noise into each client's parameter, which can be implemented with
minimal overheads. Given the uncertain straggler and colluder sets, we utilize
the upper bound on the numbers of stragglers and colluders to prove sufficient
noise variance conditions to ensure DP in the worst case. Moreover, we optimize
the expected convergence bound to ensure accuracy performance by flexibly
controlling the noise variances. Using the CIFAR-10 dataset, our experimental
results demonstrate that LightDP-FL achieves faster convergence and stronger
straggler resilience of our scheme compared to baseline methods of the same DP
level.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.06113v1">Privacy-Preserving Large Language Models: Mechanisms, Applications, and
  Future Directions</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-12-09T00:24:09Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Guoshenghui Zhao, Eric Song</p>
    <p><b>Summary:</b> The rapid advancement of large language models (LLMs) has revolutionized
natural language processing, enabling applications in diverse domains such as
healthcare, finance and education. However, the growing reliance on extensive
data for training and inference has raised significant privacy concerns,
ranging from data leakage to adversarial attacks. This survey comprehensively
explores the landscape of privacy-preserving mechanisms tailored for LLMs,
including differential privacy, federated learning, cryptographic protocols,
and trusted execution environments. We examine their efficacy in addressing key
privacy challenges, such as membership inference and model inversion attacks,
while balancing trade-offs between privacy and model utility. Furthermore, we
analyze privacy-preserving applications of LLMs in privacy-sensitive domains,
highlighting successful implementations and inherent limitations. Finally, this
survey identifies emerging research directions, emphasizing the need for novel
frameworks that integrate privacy by design into the lifecycle of LLMs. By
synthesizing state-of-the-art approaches and future trends, this paper provides
a foundation for developing robust, privacy-preserving large language models
that safeguard sensitive information without compromising performance.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.05894v1">FedRBE -- a decentralized privacy-preserving federated batch effect
  correction tool for omics data based on limma</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-12-08T11:23:31Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yuliya Burankova, Julian Klemm, Jens J. G. Lohmann, Ahmad Taheri, Niklas Probul, Jan Baumbach, Olga Zolotareva</p>
    <p><b>Summary:</b> Batch effects in omics data obscure true biological signals and constitute a
major challenge for privacy-preserving analyses of distributed patient data.
Existing batch effect correction methods either require data centralization,
which may easily conflict with privacy requirements, or lack support for
missing values and automated workflows. To bridge this gap, we developed
fedRBE, a federated implementation of limma's removeBatchEffect method. We
implemented it as an app for the FeatureCloud platform. Unlike its existing
analogs, fedRBE effectively handles data with missing values and offers an
automated, user-friendly online user interface
(https://featurecloud.ai/app/fedrbe). Leveraging secure multi-party computation
provides enhanced security guarantees over classical federated learning
approaches. We evaluated our fedRBE algorithm on simulated and real omics data,
achieving performance comparable to the centralized method with negligible
differences (no greater than 3.6E-13). By enabling collaborative correction
without data sharing, fedRBE facilitates large-scale omics studies where batch
effect correction is crucial.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.05767v2">DeMem: Privacy-Enhanced Robust Adversarial Learning via De-Memorization</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-12-08T00:22:58Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xiaoyu Luo, Qiongxiu Li</p>
    <p><b>Summary:</b> Adversarial robustness, the ability of a model to withstand manipulated
inputs that cause errors, is essential for ensuring the trustworthiness of
machine learning models in real-world applications. However, previous studies
have shown that enhancing adversarial robustness through adversarial training
increases vulnerability to privacy attacks. While differential privacy can
mitigate these attacks, it often compromises robustness against both natural
and adversarial samples. Our analysis reveals that differential privacy
disproportionately impacts low-risk samples, causing an unintended performance
drop. To address this, we propose DeMem, which selectively targets high-risk
samples, achieving a better balance between privacy protection and model
robustness. DeMem is versatile and can be seamlessly integrated into various
adversarial training techniques. Extensive evaluations across multiple training
methods and datasets demonstrate that DeMem significantly reduces privacy
leakage while maintaining robustness against both natural and adversarial
samples. These results confirm DeMem's effectiveness and broad applicability in
enhancing privacy without compromising robustness.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.05734v1">PrivAgent: Agentic-based Red-teaming for LLM Privacy Leakage</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-12-07T20:09:01Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yuzhou Nie, Zhun Wang, Ye Yu, Xian Wu, Xuandong Zhao, Wenbo Guo, Dawn Song</p>
    <p><b>Summary:</b> Recent studies have discovered that LLMs have serious privacy leakage
concerns, where an LLM may be fooled into outputting private information under
carefully crafted adversarial prompts. These risks include leaking system
prompts, personally identifiable information, training data, and model
parameters. Most existing red-teaming approaches for privacy leakage rely on
humans to craft the adversarial prompts. A few automated methods are proposed
for system prompt extraction, but they cannot be applied to more severe risks
(e.g., training data extraction) and have limited effectiveness even for system
prompt extraction.
  In this paper, we propose PrivAgent, a novel black-box red-teaming framework
for LLM privacy leakage. We formulate different risks as a search problem with
a unified attack goal. Our framework trains an open-source LLM through
reinforcement learning as the attack agent to generate adversarial prompts for
different target models under different risks. We propose a novel reward
function to provide effective and fine-grained rewards for the attack agent.
Finally, we introduce customizations to better fit our general framework to
system prompt extraction and training data extraction. Through extensive
evaluations, we first show that PrivAgent outperforms existing automated
methods in system prompt leakage against six popular LLMs. Notably, our
approach achieves a 100% success rate in extracting system prompts from
real-world applications in OpenAI's GPT Store. We also show PrivAgent's
effectiveness in extracting training data from an open-source LLM with a
success rate of 5.9%. We further demonstrate PrivAgent's effectiveness in
evading the existing guardrail defense and its helpfulness in enabling better
safety alignment. Finally, we validate our customized designs through a
detailed ablation study. We release our code here
https://github.com/rucnyz/RedAgent.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.05636v1">A Game-Theoretic Framework for Privacy-Aware Client Sampling in
  Federated Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Science and Game Theory-5BC0EB">
  <p><b>Published on:</b> 2024-12-07T12:42:57Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Wenhao Yuan, Xuehe Wang</p>
    <p><b>Summary:</b> This paper aims to design a Privacy-aware Client Sampling framework in
Federated learning, named FedPCS, to tackle the heterogeneous client sampling
issues and improve model performance. First, we obtain a pioneering upper bound
for the accuracy loss of the FL model with privacy-aware client sampling
probabilities. Based on this, we model the interactions between the central
server and participating clients as a two-stage Stackelberg game. In Stage I,
the central server designs the optimal time-dependent reward for cost
minimization by considering the trade-off between the accuracy loss of the FL
model and the rewards allocated. In Stage II, each client determines the
correction factor that dynamically adjusts its privacy budget based on the
reward allocated to maximize its utility. To surmount the obstacle of
approximating other clients' private information, we introduce the mean-field
estimator to estimate the average privacy budget. We analytically demonstrate
the existence and convergence of the fixed point for the mean-field estimator
and derive the Stackelberg Nash Equilibrium to obtain the optimal strategy
profile. By rigorously theoretical convergence analysis, we guarantee the
robustness of FedPCS. Moreover, considering the conventional sampling strategy
in privacy-preserving FL, we prove that the random sampling approach's PoA can
be arbitrarily large. To remedy such efficiency loss, we show that the proposed
privacy-aware client sampling strategy successfully reduces PoA, which is upper
bounded by a reachable constant. To address the challenge of varying privacy
requirements throughout different training phases in FL, we extend our model
and analysis and derive the adaptive optimal sampling ratio for the central
server. Experimental results on different datasets demonstrate the superiority
of FedPCS compared with the existing SOTA FL strategies under IID and Non-IID
datasets.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.05533v1">Can large language models be privacy preserving and fair medical coders?</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-12-07T04:27:05Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ali Dadsetan, Dorsa Soleymani, Xijie Zeng, Frank Rudzicz</p>
    <p><b>Summary:</b> Protecting patient data privacy is a critical concern when deploying machine
learning algorithms in healthcare. Differential privacy (DP) is a common method
for preserving privacy in such settings and, in this work, we examine two key
trade-offs in applying DP to the NLP task of medical coding (ICD
classification). Regarding the privacy-utility trade-off, we observe a
significant performance drop in the privacy preserving models, with more than a
40% reduction in micro F1 scores on the top 50 labels in the MIMIC-III dataset.
From the perspective of the privacy-fairness trade-off, we also observe an
increase of over 3% in the recall gap between male and female patients in the
DP models. Further understanding these trade-offs will help towards the
challenges of real-world deployment.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.05183v1">Privacy Drift: Evolving Privacy Concerns in Incremental Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-12-06T17:04:09Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sayyed Farid Ahamed, Soumya Banerjee, Sandip Roy, Aayush Kapoor, Marc Vucovich, Kevin Choi, Abdul Rahman, Edward Bowen, Sachin Shetty</p>
    <p><b>Summary:</b> In the evolving landscape of machine learning (ML), Federated Learning (FL)
presents a paradigm shift towards decentralized model training while preserving
user data privacy. This paper introduces the concept of ``privacy drift", an
innovative framework that parallels the well-known phenomenon of concept drift.
While concept drift addresses the variability in model accuracy over time due
to changes in the data, privacy drift encapsulates the variation in the leakage
of private information as models undergo incremental training. By defining and
examining privacy drift, this study aims to unveil the nuanced relationship
between the evolution of model performance and the integrity of data privacy.
Through rigorous experimentation, we investigate the dynamics of privacy drift
in FL systems, focusing on how model updates and data distribution shifts
influence the susceptibility of models to privacy attacks, such as membership
inference attacks (MIA). Our results highlight a complex interplay between
model accuracy and privacy safeguards, revealing that enhancements in model
performance can lead to increased privacy risks. We provide empirical evidence
from experiments on customized datasets derived from CIFAR-100 (Canadian
Institute for Advanced Research, 100 classes), showcasing the impact of data
and concept drift on privacy. This work lays the groundwork for future research
on privacy-aware machine learning, aiming to achieve a delicate balance between
model accuracy and data privacy in decentralized environments.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.05164v1">A Differentially Private Kaplan-Meier Estimator for Privacy-Preserving
  Survival Analysis</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-12-06T16:29:53Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Narasimha Raghavan Veeraragavan, Sai Praneeth Karimireddy, Jan Franz Nygård</p>
    <p><b>Summary:</b> This paper presents a differentially private approach to Kaplan-Meier
estimation that achieves accurate survival probability estimates while
safeguarding individual privacy. The Kaplan-Meier estimator is widely used in
survival analysis to estimate survival functions over time, yet applying it to
sensitive datasets, such as clinical records, risks revealing private
information. To address this, we introduce a novel algorithm that applies
time-indexed Laplace noise, dynamic clipping, and smoothing to produce a
privacy-preserving survival curve while maintaining the cumulative structure of
the Kaplan-Meier estimator. By scaling noise over time, the algorithm accounts
for decreasing sensitivity as fewer individuals remain at risk, while dynamic
clipping and smoothing prevent extreme values and reduce fluctuations,
preserving the natural shape of the survival curve.
  Our results, evaluated on the NCCTG lung cancer dataset, show that the
proposed method effectively lowers root mean squared error (RMSE) and enhances
accuracy across privacy budgets ($\epsilon$). At $\epsilon = 10$, the algorithm
achieves an RMSE as low as 0.04, closely approximating non-private estimates.
Additionally, membership inference attacks reveal that higher $\epsilon$ values
(e.g., $\epsilon \geq 6$) significantly reduce influential points, particularly
at higher thresholds, lowering susceptibility to inference attacks. These
findings confirm that our approach balances privacy and utility, advancing
privacy-preserving survival analysis.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.04697v1">Privacy-Preserving Retrieval Augmented Generation with Differential
  Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2024-12-06T01:20:16Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tatsuki Koga, Ruihan Wu, Kamalika Chaudhuri</p>
    <p><b>Summary:</b> With the recent remarkable advancement of large language models (LLMs), there
has been a growing interest in utilizing them in the domains with highly
sensitive data that lies outside their training data. For this purpose,
retrieval augmented generation (RAG) is particularly effective -- it assists
LLMs by directly providing relevant information from the external knowledge
sources. However, without extra privacy safeguards, RAG outputs risk leaking
sensitive information from the external data source. In this work, we explore
RAG under differential privacy (DP), a formal guarantee of data privacy. The
main challenge with differentially private RAG is how to generate long accurate
answers within a moderate privacy budget. We address this by proposing an
algorithm that smartly spends privacy budget only for the tokens that require
the sensitive information and uses the non-private LLM for other tokens. Our
extensive empirical evaluations reveal that our algorithm outperforms the
non-RAG baseline under a reasonable privacy budget of $\epsilon\approx 10$
across different models and datasets.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.04408v1">Providing Differential Privacy for Federated Learning Over Wireless: A
  Cross-layer Framework</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> 
  <p><b>Published on:</b> 2024-12-05T18:27:09Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jiayu Mao, Tongxin Yin, Aylin Yener, Mingyan Liu</p>
    <p><b>Summary:</b> Federated Learning (FL) is a distributed machine learning framework that
inherently allows edge devices to maintain their local training data, thus
providing some level of privacy. However, FL's model updates still pose a risk
of privacy leakage, which must be mitigated. Over-the-air FL (OTA-FL) is an
adapted FL design for wireless edge networks that leverages the natural
superposition property of the wireless medium. We propose a wireless physical
layer (PHY) design for OTA-FL which improves differential privacy (DP) through
a decentralized, dynamic power control that utilizes both inherent Gaussian
noise in the wireless channel and a cooperative jammer (CJ) for additional
artificial noise generation when higher privacy levels are required. Although
primarily implemented within the Upcycled-FL framework, where a
resource-efficient method with first-order approximations is used at every even
iteration to decrease the required information from clients, our power control
strategy is applicable to any FL framework, including FedAvg and FedProx as
shown in the paper. This adaptation showcases the flexibility and effectiveness
of our design across different learning algorithms while maintaining a strong
emphasis on privacy. Our design removes the need for client-side artificial
noise injection for DP, utilizing a cooperative jammer to enhance privacy
without affecting transmission efficiency for higher privacy demands. Privacy
analysis is provided using the Moments Accountant method. We perform a
convergence analysis for non-convex objectives to tackle heterogeneous data
distributions, highlighting the inherent trade-offs between privacy and
accuracy. Numerical results show that our approach with various FL algorithms
outperforms the state-of-the-art under the same DP conditions on the non-i.i.d.
FEMNIST dataset, and highlight the cooperative jammer's effectiveness in
ensuring strict privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.04178v1">Multi-Layer Privacy-Preserving Record Linkage with Clerical Review based
  on gradual information disclosure</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-12-05T14:18:50Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Florens Rohde, Victor Christen, Martin Franke, Erhard Rahm</p>
    <p><b>Summary:</b> Privacy-Preserving Record linkage (PPRL) is an essential component in data
integration tasks of sensitive information. The linkage quality determines the
usability of combined datasets and (machine learning) applications based on
them. We present a novel privacy-preserving protocol that integrates clerical
review in PPRL using a multi-layer active learning process. Uncertain match
candidates are reviewed on several layers by human and non-human oracles to
reduce the amount of disclosed information per record and in total. Predictions
are propagated back to update previous layers, resulting in an improved linkage
performance for non-reviewed candidates as well. The data owners remain in
control of the amount of information they share for each record. Therefore, our
approach follows need-to-know and data sovereignty principles. The experimental
evaluation on real-world datasets shows considerable linkage quality
improvements with limited labeling effort and privacy risks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.04031v1">Dimension Reduction via Random Projection for Privacy in Multi-Agent
  Systems</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-12-05T10:09:13Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Puspanjali Ghoshal, Ashok Singh Sairam</p>
    <p><b>Summary:</b> The agents in a Multi-Agent System (MAS) make observations about the system
and send that information to a fusion center. The fusion center aggregates the
information and concludes about the system parameters with as much accuracy as
possible. However for the purposes of better efficiency of the system at large,
the agents need to append some private parameters to the observed data. In this
scenario, the data sent to the fusion center is faced with privacy risks. The
data communicated to the fusion center must be secured against data privacy
breaches and inference attacks in a decentralized manner. However, this in turn
leads to a loss of utility of the data being sent to the fusion center. We
quantify the utility and privacy of the system using Cosine similarity. We
formulate our MAS problem in terms of deducing a concept for which
compression-based methods are there in literature. Next, we propose a novel
sanitization mechanism for our MAS using one such compression-based method
while addressing the utility-privacy tradeoff problem.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.04518v1">Privacy-Preserving Gesture Tracking System Utilizing Frequency-Hopping
  RFID Signals</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Networking and Internet Architecture-04E762">
  <p><b>Published on:</b> 2024-12-05T08:51:02Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Bojun Zhang</p>
    <p><b>Summary:</b> Gesture tracking technology provides users with a hands free interactive
experience without the need to hold or touch devices. However, current gesture
tracking research has primarily focused on tracking accuracy while neglecting
issues of user privacy protection and security. This study aims to develop a
gesture tracking system based on frequency hopping RFID signals that
effectively protects user privacy without compromising tracking efficiency and
accuracy. By introducing frequency hopping technology, we have designed a
mechanism that prevents potential eavesdroppers from obtaining raw RFID
signals, thereby enhancing the systems privacy protection capabilities. The
system architec ture includes the collection of RFID signals, data processing,
signal recovery, and gesture tracking. Experimental results show that our
method significantly improves privacy protection levels while maintaining real
time and accuracy. This research not only provides a new perspective for the
field of gesture tracking but also offers valuable insights for the use of RFID
technology in privacy-sensitive applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.15228v1">Image Privacy Protection: A Survey</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-12-05T08:09:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Wenying Wen, Ziye Yuan, Yushu Zhang, Tao Wang, Xiangli Xiao, Ruoyu Zhao, Yuming Fang</p>
    <p><b>Summary:</b> Images serve as a crucial medium for communication, presenting information in
a visually engaging format that facilitates rapid comprehension of key points.
Meanwhile, during transmission and storage, they contain significant sensitive
information. If not managed properly, this information may be vulnerable to
exploitation for personal gain, potentially infringing on privacy rights and
other legal entitlements. Consequently, researchers continue to propose some
approaches for preserving image privacy and publish reviews that provide
comprehensive and methodical summaries of these approaches. However, existing
reviews tend to categorize either by specific scenarios, or by specific privacy
objectives. This classification somewhat restricts the reader's ability to
grasp a holistic view of image privacy protection and poses challenges in
developing a total understanding of the subject that transcends different
scenarios and privacy objectives. Instead of examining image privacy protection
from a single aspect, it is more desirable to consider user needs for a
comprehensive understanding. To fill this gap, we conduct a systematic review
of image privacy protection approaches based on privacy protection goals.
Specifically, we define the attribute known as privacy sensitive domains and
use it as the core classification dimension to construct a comprehensive
framework for image privacy protection that encompasses various scenarios and
privacy objectives. This framework offers a deep understanding of the
multi-layered aspects of image privacy, categorizing its protection into three
primary levels: data-level, content-level, and feature-level. For each
category, we analyze the main approaches and features of image privacy
protection and systematically review representative solutions. Finally, we
discuss the challenges and future directions of image privacy protection.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.03924v1">Privacy-Preserving in Medical Image Analysis: A Review of Methods and
  Applications</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2024-12-05T06:56:06Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yanming Zhu, Xuefei Yin, Alan Wee-Chung Liew, Hui Tian</p>
    <p><b>Summary:</b> With the rapid advancement of artificial intelligence and deep learning,
medical image analysis has become a critical tool in modern healthcare,
significantly improving diagnostic accuracy and efficiency. However, AI-based
methods also raise serious privacy concerns, as medical images often contain
highly sensitive patient information. This review offers a comprehensive
overview of privacy-preserving techniques in medical image analysis, including
encryption, differential privacy, homomorphic encryption, federated learning,
and generative adversarial networks. We explore the application of these
techniques across various medical image analysis tasks, such as diagnosis,
pathology, and telemedicine. Notably, we organizes the review based on specific
challenges and their corresponding solutions in different medical image
analysis applications, so that technical applications are directly aligned with
practical issues, addressing gaps in the current research landscape.
Additionally, we discuss emerging trends, such as zero-knowledge proofs and
secure multi-party computation, offering insights for future research. This
review serves as a valuable resource for researchers and practitioners and can
help advance privacy-preserving in medical image analysis.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.02987v1">Advancing Conversational Psychotherapy: Integrating Privacy,
  Dual-Memory, and Domain Expertise with Large Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2024-12-04T03:02:46Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> XiuYu Zhang, Zening Luo</p>
    <p><b>Summary:</b> Mental health has increasingly become a global issue that reveals the
limitations of traditional conversational psychotherapy, constrained by
location, time, expense, and privacy concerns. In response to these challenges,
we introduce SoulSpeak, a Large Language Model (LLM)-enabled chatbot designed
to democratize access to psychotherapy. SoulSpeak improves upon the
capabilities of standard LLM-enabled chatbots by incorporating a novel
dual-memory component that combines short-term and long-term context via
Retrieval Augmented Generation (RAG) to offer personalized responses while
ensuring the preservation of user privacy and intimacy through a dedicated
privacy module. In addition, it leverages a counseling chat dataset of
therapist-client interactions and various prompting techniques to align the
generated responses with psychotherapeutic methods. We introduce two fine-tuned
BERT models to evaluate the system against existing LLMs and human therapists:
the Conversational Psychotherapy Preference Model (CPPM) to simulate human
preference among responses and another to assess response relevance to user
input. CPPM is useful for training and evaluating psychotherapy-focused
language models independent from SoulSpeak, helping with the constrained
resources available for psychotherapy. Furthermore, the effectiveness of the
dual-memory component and the robustness of the privacy module are also
examined. Our findings highlight the potential and challenge of enhancing
mental health care by offering an alternative that combines the expertise of
traditional therapy with the advantages of LLMs, providing a promising way to
address the accessibility and personalization gap in current mental health
services.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.02934v1">BGTplanner: Maximizing Training Accuracy for Differentially Private
  Federated Recommenders via Strategic Privacy Budget Allocation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2024-12-04T01:07:04Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xianzhi Zhang, Yipeng Zhou, Miao Hu, Di Wu, Pengshan Liao, Mohsen Guizani, Michael Sheng</p>
    <p><b>Summary:</b> To mitigate the rising concern about privacy leakage, the federated
recommender (FR) paradigm emerges, in which decentralized clients co-train the
recommendation model without exposing their raw user-item rating data. The
differentially private federated recommender (DPFR) further enhances FR by
injecting differentially private (DP) noises into clients. Yet, current DPFRs,
suffering from noise distortion, cannot achieve satisfactory accuracy. Various
efforts have been dedicated to improving DPFRs by adaptively allocating the
privacy budget over the learning process. However, due to the intricate
relation between privacy budget allocation and model accuracy, existing works
are still far from maximizing DPFR accuracy. To address this challenge, we
develop BGTplanner (Budget Planner) to strategically allocate the privacy
budget for each round of DPFR training, improving overall training performance.
Specifically, we leverage the Gaussian process regression and historical
information to predict the change in recommendation accuracy with a certain
allocated privacy budget. Additionally, Contextual Multi-Armed Bandit (CMAB) is
harnessed to make privacy budget allocation decisions by reconciling the
current improvement and long-term privacy constraints. Our extensive
experimental results on real datasets demonstrate that \emph{BGTplanner}
achieves an average improvement of 6.76\% in training performance compared to
state-of-the-art baselines.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.02868v1">A Novel Compact LLM Framework for Local, High-Privacy EHR Data
  Applications</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-12-03T22:06:55Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yixiang Qu, Yifan Dai, Shilin Yu, Pradham Tanikella, Travis Schrank, Trevor Hackman, Didong Li, Di Wu</p>
    <p><b>Summary:</b> Large Language Models (LLMs) have shown impressive capabilities in natural
language processing, yet their use in sensitive domains like healthcare,
particularly with Electronic Health Records (EHR), faces significant challenges
due to privacy concerns and limited computational resources. This paper
presents a compact LLM framework designed for local deployment in settings with
strict privacy requirements and limited access to high-performance GPUs. We
introduce a novel preprocessing technique that uses information extraction
methods, e.g., regular expressions, to filter and emphasize critical
information in clinical notes, enhancing the performance of smaller LLMs on EHR
data. Our framework is evaluated using zero-shot and few-shot learning
paradigms on both private and publicly available (MIMIC-IV) datasets, and we
also compare its performance with fine-tuned LLMs on the MIMIC-IV dataset. The
results demonstrate that our preprocessing approach significantly boosts the
prediction accuracy of smaller LLMs, making them suitable for high-privacy,
resource-constrained applications. This study offers valuable insights into
optimizing LLM performance for sensitive, data-intensive tasks while addressing
computational and privacy limitations.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.02578v1">Private Linear Regression with Differential Privacy and PAC Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-12-03T17:04:14Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Hillary Yang</p>
    <p><b>Summary:</b> Linear regression is a fundamental tool for statistical analysis, which has
motivated the development of linear regression methods that satisfy provable
privacy guarantees so that the learned model reveals little about any one data
point used to construct it. Most existing privacy-preserving linear regression
methods rely on the well-established framework of differential privacy, while
the newly proposed PAC Privacy has not yet been explored in this context. In
this paper, we systematically compare linear regression models trained with
differential privacy and PAC privacy across three real-world datasets,
observing several key findings that impact the performance of
privacy-preserving linear regression.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.02538v2">On Privacy, Security, and Trustworthiness in Distributed Wireless Large
  AI Models (WLAM)</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">  
  <p><b>Published on:</b> 2024-12-03T16:32:19Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhaohui Yang, Wei Xu, Le Liang, Yuanhao Cui, Zhijin Qin, Merouane Debbah</p>
    <p><b>Summary:</b> Combining wireless communication with large artificial intelligence (AI)
models can open up a myriad of novel application scenarios. In sixth generation
(6G) networks, ubiquitous communication and computing resources allow large AI
models to serve democratic large AI models-related services to enable real-time
applications like autonomous vehicles, smart cities, and Internet of Things
(IoT) ecosystems. However, the security considerations and sustainable
communication resources limit the deployment of large AI models over
distributed wireless networks. This paper provides a comprehensive overview of
privacy, security, and trustworthy for distributed wireless large AI model
(WLAM). In particular, a detailed privacy and security are analysis for
distributed WLAM is fist revealed. The classifications and theoretical findings
about privacy and security in distributed WLAM are discussed. Then the
trustworthy and ethics for implementing distributed WLAM are described.
Finally, the comprehensive applications of distributed WLAM are presented in
the context of electromagnetic signal processing.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.02340v1">Federated Analytics in Practice: Engineering for Privacy, Scalability
  and Practicality</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-12-03T10:03:12Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Harish Srinivas, Graham Cormode, Mehrdad Honarkhah, Samuel Lurye, Jonathan Hehir, Lunwen He, George Hong, Ahmed Magdy, Dzmitry Huba, Kaikai Wang, Shen Guo, Shoubhik Bhattacharya</p>
    <p><b>Summary:</b> Cross-device Federated Analytics (FA) is a distributed computation paradigm
designed to answer analytics queries about and derive insights from data held
locally on users' devices. On-device computations combined with other privacy
and security measures ensure that only minimal data is transmitted off-device,
achieving a high standard of data protection. Despite FA's broad relevance, the
applicability of existing FA systems is limited by compromised accuracy; lack
of flexibility for data analytics; and an inability to scale effectively. In
this paper, we describe our approach to combine privacy, scalability, and
practicality to build and deploy a system that overcomes these limitations. Our
FA system leverages trusted execution environments (TEEs) and optimizes the use
of on-device computing resources to facilitate federated data processing across
large fleets of devices, while ensuring robust, defensible, and verifiable
privacy safeguards. We focus on federated analytics (statistics and
monitoring), in contrast to systems for federated learning (ML workloads), and
we flag the key differences.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.07796v1">MRP-LLM: Multitask Reflective Large Language Models for
  Privacy-Preserving Next POI Recommendation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Retrieval-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-12-03T09:45:02Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ziqing Wu, Zhu Sun, Dongxia Wang, Lu Zhang, Jie Zhang, Yew Soon Ong</p>
    <p><b>Summary:</b> Large language models (LLMs) have shown promising potential for next
Point-of-Interest (POI) recommendation. However, existing methods only perform
direct zero-shot prompting, leading to ineffective extraction of user
preferences, insufficient injection of collaborative signals, and a lack of
user privacy protection. As such, we propose a novel Multitask Reflective Large
Language Model for Privacy-preserving Next POI Recommendation (MRP-LLM), aiming
to exploit LLMs for better next POI recommendation while preserving user
privacy. Specifically, the Multitask Reflective Preference Extraction Module
first utilizes LLMs to distill each user's fine-grained (i.e., categorical,
temporal, and spatial) preferences into a knowledge base (KB). The Neighbor
Preference Retrieval Module retrieves and summarizes the preferences of similar
users from the KB to obtain collaborative signals. Subsequently, aggregating
the user's preferences with those of similar users, the Multitask Next POI
Recommendation Module generates the next POI recommendations via multitask
prompting. Meanwhile, during data collection, a Privacy Transmission Module is
specifically devised to preserve sensitive POI data. Extensive experiments on
three real-world datasets demonstrate the efficacy of our proposed MRP-LLM in
providing more accurate next POI recommendations with user privacy preserved.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.02130v1">A privacy-preserving distributed credible evidence fusion algorithm for
  collective decision-making</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2024-12-03T03:36:42Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chaoxiong Ma, Yan Liang, Xinyu Yang, Han Wu, Huixia Zhang</p>
    <p><b>Summary:</b> The theory of evidence reasoning has been applied to collective
decision-making in recent years. However, existing distributed evidence fusion
methods lead to participants' preference leakage and fusion failures as they
directly exchange raw evidence and do not assess evidence credibility like
centralized credible evidence fusion (CCEF) does. To do so, a
privacy-preserving distributed credible evidence fusion method with three-level
consensus (PCEF) is proposed in this paper. In evidence difference measure
(EDM) neighbor consensus, an evidence-free equivalent expression of EDM among
neighbored agents is derived with the shared dot product protocol for pignistic
probability and the identical judgment of two events with maximal subjective
probabilities, so that evidence privacy is guaranteed due to such irreversible
evidence transformation. In EDM network consensus, the non-neighbored EDMs are
inferred and neighbored EDMs reach uniformity via interaction between linear
average consensus (LAC) and low-rank matrix completion with rank adaptation to
guarantee EDM consensus convergence and no solution of inferring raw evidence
in numerical iteration style. In fusion network consensus, a privacy-preserving
LAC with a self-cancelling differential privacy term is proposed, where each
agent adds its randomness to the sharing content and step-by-step cancels such
randomness in consensus iterations. Besides, the sufficient condition of the
convergence to the CCEF is explored, and it is proven that raw evidence is
impossibly inferred in such an iterative consensus. The simulations show that
PCEF is close to CCEF both in credibility and fusion results and obtains higher
decision accuracy with less time-comsuming than existing methods.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.01756v1">Adversarial Sample-Based Approach for Tighter Privacy Auditing in Final
  Model-Only Scenarios</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-12-02T17:52:16Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sangyeon Yoon, Wonje Jeung, Albert No</p>
    <p><b>Summary:</b> Auditing Differentially Private Stochastic Gradient Descent (DP-SGD) in the
final model setting is challenging and often results in empirical lower bounds
that are significantly looser than theoretical privacy guarantees. We introduce
a novel auditing method that achieves tighter empirical lower bounds without
additional assumptions by crafting worst-case adversarial samples through
loss-based input-space auditing. Our approach surpasses traditional
canary-based heuristics and is effective in both white-box and black-box
scenarios. Specifically, with a theoretical privacy budget of $\varepsilon =
10.0$, our method achieves empirical lower bounds of $6.68$ in white-box
settings and $4.51$ in black-box settings, compared to the baseline of $4.11$
for MNIST. Moreover, we demonstrate that significant privacy auditing results
can be achieved using in-distribution (ID) samples as canaries, obtaining an
empirical lower bound of $4.33$ where traditional methods produce near-zero
leakage detection. Our work offers a practical framework for reliable and
accurate privacy auditing in differentially private machine learning.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.01671v2">Verified Foundations for Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-12-02T16:19:47Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Markus de Medeiros, Muhammad Naveed, Tancrede Lepoint, Temesghen Kahsai, Tristan Ravitch, Stefan Zetzsche, Anjali Joshi, Joseph Tassarotti, Aws Albarghouthi, Jean-Baptiste Tristan</p>
    <p><b>Summary:</b> Differential privacy (DP) has become the gold standard for privacy-preserving
data analysis, but implementing it correctly has proven challenging. Prior work
has focused on verifying DP at a high level, assuming the foundations are
correct and a perfect source of randomness is available. However, the
underlying theory of differential privacy can be very complex and subtle. Flaws
in basic mechanisms and random number generation have been a critical source of
vulnerabilities in real-world DP systems.
  In this paper, we present SampCert, the first comprehensive, mechanized
foundation for differential privacy. SampCert is written in Lean with over
12,000 lines of proof. It offers a generic and extensible notion of DP, a
framework for constructing and composing DP mechanisms, and formally verified
implementations of Laplace and Gaussian sampling algorithms. SampCert provides
(1) a mechanized foundation for developing the next generation of
differentially private algorithms, and (2) mechanically verified primitives
that can be deployed in production systems. Indeed, SampCert's verified
algorithms power the DP offerings of Amazon Web Services (AWS), demonstrating
its real-world impact.
  SampCert's key innovations include: (1) A generic DP foundation that can be
instantiated for various DP definitions (e.g., pure, concentrated, R\'enyi DP);
(2) formally verified discrete Laplace and Gaussian sampling algorithms that
avoid the pitfalls of floating-point implementations; and (3) a simple
probability monad and novel proof techniques that streamline the formalization.
To enable proving complex correctness properties of DP and random number
generation, SampCert makes heavy use of Lean's extensive Mathlib library,
leveraging theorems in Fourier analysis, measure and probability theory, number
theory, and topology.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.01650v2">Privacy-Preserving Federated Learning via Homomorphic Adversarial
  Networks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2024-12-02T15:59:35Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Wenhan Dong, Chao Lin, Xinlei He, Xinyi Huang, Shengmin Xu</p>
    <p><b>Summary:</b> Privacy-preserving federated learning (PPFL) aims to train a global model for
multiple clients while maintaining their data privacy. However, current PPFL
protocols exhibit one or more of the following insufficiencies: considerable
degradation in accuracy, the requirement for sharing keys, and cooperation
during the key generation or decryption processes. As a mitigation, we develop
the first protocol that utilizes neural networks to implement PPFL, as well as
incorporating an Aggregatable Hybrid Encryption scheme tailored to the needs of
PPFL. We name these networks as Homomorphic Adversarial Networks (HANs) which
demonstrate that neural networks are capable of performing tasks similar to
multi-key homomorphic encryption (MK-HE) while solving the problems of key
distribution and collaborative decryption. Our experiments show that HANs are
robust against privacy attacks. Compared with non-private federated learning,
experiments conducted on multiple datasets demonstrate that HANs exhibit a
negligible accuracy loss (at most 1.35%). Compared to traditional MK-HE
schemes, HANs increase encryption aggregation speed by 6,075 times while
incurring a 29.2 times increase in communication overhead.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.01541v1">Effectiveness of L2 Regularization in Privacy-Preserving Machine
  Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2024-12-02T14:31:11Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Nikolaos Chandrinos, Iliana Loi, Panagiotis Zachos, Ioannis Symeonidis, Aristotelis Spiliotis, Maria Panou, Konstantinos Moustakas</p>
    <p><b>Summary:</b> Artificial intelligence, machine learning, and deep learning as a service
have become the status quo for many industries, leading to the widespread
deployment of models that handle sensitive data. Well-performing models, the
industry seeks, usually rely on a large volume of training data. However, the
use of such data raises serious privacy concerns due to the potential risks of
leaks of highly sensitive information. One prominent threat is the Membership
Inference Attack, where adversaries attempt to deduce whether a specific data
point was used in a model's training process. An adversary's ability to
determine an individual's presence represents a significant privacy threat,
especially when related to a group of users sharing sensitive information.
Hence, well-designed privacy-preserving machine learning solutions are
critically needed in the industry. In this work, we compare the effectiveness
of L2 regularization and differential privacy in mitigating Membership
Inference Attack risks. Even though regularization techniques like L2
regularization are commonly employed to reduce overfitting, a condition that
enhances the effectiveness of Membership Inference Attacks, their impact on
mitigating these attacks has not been systematically explored.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2412.01141v1">Lossless and Privacy-Preserving Graph Convolution Network for Federated
  Item Recommendation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Retrieval-5BC0EB">
  <p><b>Published on:</b> 2024-12-02T05:31:22Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Guowei Wu, Weike Pan, Qiang Yang, Zhong Ming</p>
    <p><b>Summary:</b> Graph neural network (GNN) has emerged as a state-of-the-art solution for
item recommendation. However, existing GNN-based recommendation methods rely on
a centralized storage of fragmented user-item interaction sub-graphs and
training on an aggregated global graph, which will lead to privacy concerns. As
a response, some recent works develop GNN-based federated recommendation
methods by exploiting decentralized and fragmented user-item sub-graphs in
order to preserve user privacy. However, due to privacy constraints, the graph
convolution process in existing federated recommendation methods is incomplete
compared with the centralized counterpart, causing a degradation of the
recommendation performance. In this paper, we propose a novel lossless and
privacy-preserving graph convolution network (LP-GCN), which fully completes
the graph convolution process with decentralized user-item interaction
sub-graphs while ensuring privacy. It is worth mentioning that its performance
is equivalent to that of the non-federated (i.e., centralized) counterpart.
Moreover, we validate its effectiveness through both theoretical analysis and
empirical studies. Extensive experiments on three real-world datasets show that
our LP-GCN outperforms the existing federated recommendation methods. The code
will be publicly available once the paper is accepted.</p>
  </details>
</div>

