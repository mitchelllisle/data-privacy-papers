
<h2>2025-04</h2>

<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.21752v1">VDDP: Verifiable Distributed Differential Privacy under the
  Client-Server-Verifier Setup</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB">
  <p><b>Published on:</b> 2025-04-30T15:46:55Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Haochen Sun, Xi He</p>
    <p><b>Summary:</b> Despite differential privacy (DP) often being considered the de facto
standard for data privacy, its realization is vulnerable to unfaithful
execution of its mechanisms by servers, especially in distributed settings.
Specifically, servers may sample noise from incorrect distributions or generate
correlated noise while appearing to follow established protocols. This work
analyzes these malicious behaviors in a general differential privacy framework
within a distributed client-server-verifier setup. To address these adversarial
problems, we propose a novel definition called Verifiable Distributed
Differential Privacy (VDDP) by incorporating additional verification
mechanisms. We also explore the relationship between zero-knowledge proofs
(ZKP) and DP, demonstrating that while ZKPs are sufficient for achieving DP
under verifiability requirements, they are not necessary. Furthermore, we
develop two novel and efficient mechanisms that satisfy VDDP: (1) the
Verifiable Distributed Discrete Laplacian Mechanism (VDDLM), which offers up to
a $4 \times 10^5$x improvement in proof generation efficiency with only
0.1-0.2x error compared to the previous state-of-the-art verifiable
differentially private mechanism; (2) an improved solution to Verifiable
Randomized Response (VRR) under local DP, a special case of VDDP, achieving up
a reduction of up to 5000x in communication costs and the verifier's overhead.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.21646v1">Diffusion-based Adversarial Identity Manipulation for Facial Privacy
  Protection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-04-30T13:49:59Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Liqin Wang, Qianyue Hu, Wei Lu, Xiangyang Luo</p>
    <p><b>Summary:</b> The success of face recognition (FR) systems has led to serious privacy
concerns due to potential unauthorized surveillance and user tracking on social
networks. Existing methods for enhancing privacy fail to generate natural face
images that can protect facial privacy. In this paper, we propose
diffusion-based adversarial identity manipulation (DiffAIM) to generate natural
and highly transferable adversarial faces against malicious FR systems. To be
specific, we manipulate facial identity within the low-dimensional latent space
of a diffusion model. This involves iteratively injecting gradient-based
adversarial identity guidance during the reverse diffusion process,
progressively steering the generation toward the desired adversarial faces. The
guidance is optimized for identity convergence towards a target while promoting
semantic divergence from the source, facilitating effective impersonation while
maintaining visual naturalness. We further incorporate structure-preserving
regularization to preserve facial structure consistency during manipulation.
Extensive experiments on both face verification and identification tasks
demonstrate that compared with the state-of-the-art, DiffAIM achieves stronger
black-box attack transferability while maintaining superior visual quality. We
also demonstrate the effectiveness of the proposed approach for commercial FR
APIs, including Face++ and Aliyun.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.21413v1">An Inversion Theorem for Buffered Linear Toeplitz (BLT) Matrices and
  Applications to Streaming Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> 
  <p><b>Published on:</b> 2025-04-30T08:14:09Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> H. Brendan McMahan, Krishna Pillutla</p>
    <p><b>Summary:</b> Buffered Linear Toeplitz (BLT) matrices are a family of parameterized
lower-triangular matrices that play an important role in streaming differential
privacy with correlated noise. Our main result is a BLT inversion theorem: the
inverse of a BLT matrix is itself a BLT matrix with different parameters. We
also present an efficient and differentiable $O(d^3)$ algorithm to compute the
parameters of the inverse BLT matrix, where $d$ is the degree of the original
BLT (typically $d < 10$). Our characterization enables direct optimization of
BLT parameters for privacy mechanisms through automatic differentiation.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.21297v1">Participatory AI, Public Sector AI, Differential Privacy, Conversational
  Interfaces, Explainable AI, Citizen Engagement in AI</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Emerging Technologies-F9C80E"> 
  <p><b>Published on:</b> 2025-04-30T04:10:50Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Wenjun Yang, Eyhab Al-Masri</p>
    <p><b>Summary:</b> This paper introduces a conversational interface system that enables
participatory design of differentially private AI systems in public sector
applications. Addressing the challenge of balancing mathematical privacy
guarantees with democratic accountability, we propose three key contributions:
(1) an adaptive $\epsilon$-selection protocol leveraging TOPSIS multi-criteria
decision analysis to align citizen preferences with differential privacy (DP)
parameters, (2) an explainable noise-injection framework featuring real-time
Mean Absolute Error (MAE) visualizations and GPT-4-powered impact analysis, and
(3) an integrated legal-compliance mechanism that dynamically modulates privacy
budgets based on evolving regulatory constraints. Our results advance
participatory AI practices by demonstrating how conversational interfaces can
enhance public engagement in algorithmic privacy mechanisms, ensuring that
privacy-preserving AI in public sector governance remains both mathematically
robust and democratically accountable.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.21182v1">Federated One-Shot Learning with Data Privacy and Objective-Hiding</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">  
  <p><b>Published on:</b> 2025-04-29T21:25:34Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Maximilian Egger, Rüdiger Urbanke, Rawad Bitar</p>
    <p><b>Summary:</b> Privacy in federated learning is crucial, encompassing two key aspects:
safeguarding the privacy of clients' data and maintaining the privacy of the
federator's objective from the clients. While the first aspect has been
extensively studied, the second has received much less attention.
  We present a novel approach that addresses both concerns simultaneously,
drawing inspiration from techniques in knowledge distillation and private
information retrieval to provide strong information-theoretic privacy
guarantees.
  Traditional private function computation methods could be used here; however,
they are typically limited to linear or polynomial functions. To overcome these
constraints, our approach unfolds in three stages. In stage 0, clients perform
the necessary computations locally. In stage 1, these results are shared among
the clients, and in stage 2, the federator retrieves its desired objective
without compromising the privacy of the clients' data. The crux of the method
is a carefully designed protocol that combines secret-sharing-based multi-party
computation and a graph-based private information retrieval scheme. We show
that our method outperforms existing tools from the literature when properly
adapted to this setting.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.20941v1">Conformal-DP: Differential Privacy on Riemannian Manifolds via Conformal
  Transformation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">  
  <p><b>Published on:</b> 2025-04-29T17:05:55Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Peilin He, Liou Tang, M. Amin Rahimian, James Joshi</p>
    <p><b>Summary:</b> Differential Privacy (DP) has been established as a safeguard for private
data sharing by adding perturbations to information release. Prior research on
DP has extended beyond data in the flat Euclidean space and addressed data on
curved manifolds, e.g., diffusion tensor MRI, social networks, or organ shape
analysis, by adding perturbations along geodesic distances. However, existing
manifold-aware DP methods rely on the assumption that samples are uniformly
distributed across the manifold. In reality, data densities vary, leading to a
biased noise imbalance across manifold regions, weakening the privacy-utility
trade-offs. To address this gap, we propose a novel mechanism: Conformal-DP,
utilizing conformal transformations on the Riemannian manifold to equalize
local sample density and to redefine geodesic distances accordingly while
preserving the intrinsic geometry of the manifold. Our theoretical analysis
yields two main results. First, we prove that the conformal factor computed
from local kernel-density estimates is explicitly data-density-aware; Second,
under the conformal metric, the mechanism satisfies $ \varepsilon
$-differential privacy on any complete Riemannian manifold and admits a
closed-form upper bound on the expected geodesic error that depends only on the
maximal density ratio, not on global curvatureof the manifold. Our experimental
results validate that the mechanism achieves high utility while providing the $
\varepsilon $-DP guarantee for both homogeneous and especially heterogeneous
manifold data.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.20926v1">Bipartite Randomized Response Mechanism for Local Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-04-29T16:39:50Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shun Zhang, Hai Zhu, Zhili Chen, Neal N. Xiong</p>
    <p><b>Summary:</b> With the increasing importance of data privacy, Local Differential Privacy
(LDP) has recently become a strong measure of privacy for protecting each
user's privacy from data analysts without relying on a trusted third party. In
many cases, both data providers and data analysts hope to maximize the utility
of released data. In this paper, we study the fundamental trade-off formulated
as a constrained optimization problem: maximizing data utility subject to the
constraint of LDP budgets. In particular, the Generalized Randomized Response
(GRR) treats all discrete data equally except for the true data. For this, we
introduce an adaptive LDP mechanism called Bipartite Randomized Response (BRR),
which solves the above privacy-utility maximization problem from the global
standpoint. We prove that for any utility function and any privacy level,
solving the maximization problem is equivalent to confirming how many
high-utility data to be treated equally as the true data on release
probability, the outcome of which gives the optimal randomized response.
Further, solving this linear program can be computationally cheap in theory.
Several examples of utility functions defined by distance metrics and
applications in decision trees and deep learning are presented. The results of
various experiments show that our BRR significantly outperforms the
state-of-the-art LDP mechanisms of both continuous and distributed types.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.20827v1">DP-SMOTE: Integrating Differential Privacy and Oversampling Technique to
  Preserve Privacy in Smart Homes</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-04-29T14:50:50Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Amr Tarek Elsayed, Almohammady Sobhi Alsharkawy, Mohamed Sayed Farag, Shaban Ebrahim Abu Yusuf</p>
    <p><b>Summary:</b> Smart homes represent intelligent environments where interconnected devices
gather information, enhancing users living experiences by ensuring comfort,
safety, and efficient energy management. To enhance the quality of life,
companies in the smart device industry collect user data, including activities,
preferences, and power consumption. However, sharing such data necessitates
privacy-preserving practices. This paper introduces a robust method for secure
sharing of data to service providers, grounded in differential privacy (DP).
This empowers smart home residents to contribute usage statistics while
safeguarding their privacy. The approach incorporates the Synthetic Minority
Oversampling technique (SMOTe) and seamlessly integrates Gaussian noise to
generate synthetic data, enabling data and statistics sharing while preserving
individual privacy. The proposed method employs the SMOTe algorithm and applies
Gaussian noise to generate data. Subsequently, it employs a k-anonymity
function to assess reidentification risk before sharing the data. The
simulation outcomes demonstrate that our method delivers strong performance in
safeguarding privacy and in accuracy, recall, and f-measure metrics. This
approach is particularly effective in smart homes, offering substantial utility
in privacy at a reidentification risk of 30%, with Gaussian noise set to 0.3,
SMOTe at 500%, and the application of a k-anonymity function with k = 2.
Additionally, it shows a high classification accuracy, ranging from 90% to 98%,
across various classification techniques.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.20700v1">Building Trust in Healthcare with Privacy Techniques: Blockchain in the
  Cloud</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-04-29T12:31:37Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ferhat Ozgur Catak, Chunming Rong, Øyvind Meinich-Bache, Sara Brunner, Kjersti Engan</p>
    <p><b>Summary:</b> This study introduces a cutting-edge architecture developed for the
NewbornTime project, which uses advanced AI to analyze video data at birth and
during newborn resuscitation, with the aim of improving newborn care. The
proposed architecture addresses the crucial issues of patient consent, data
security, and investing trust in healthcare by integrating Ethereum blockchain
with cloud computing. Our blockchain-based consent application simplifies
patient consent's secure and transparent management. We explain the smart
contract mechanisms and privacy measures employed, ensuring data protection
while permitting controlled data sharing among authorized parties. This work
demonstrates the potential of combining blockchain and cloud technologies in
healthcare, emphasizing their role in maintaining data integrity, with
implications for computer science and healthcare innovation.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.20639v1">Multi-Message Secure Aggregation with Demand Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2025-04-29T11:11:27Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chenyi Sun, Ziting Zhang, Kai Wan, Giuseppe Caire</p>
    <p><b>Summary:</b> This paper considers a multi-message secure aggregation with privacy problem,
in which a server aims to compute $\sf K_c\geq 1$ linear combinations of local
inputs from $\sf K$ distributed users. The problem addresses two tasks: (1)
security, ensuring that the server can only obtain the desired linear
combinations without any else information about the users' inputs, and (2)
privacy, preventing users from learning about the server's computation task. In
addition, the effect of user dropouts is considered, where at most $\sf{K-U}$
users can drop out and the identity of these users cannot be predicted in
advance. We propose two schemes for $\sf K_c$ is equal to (1) and $\sf 2\leq
K_c\leq U-1$, respectively. For $\sf K_c$ is equal to (1), we introduce
multiplicative encryption of the server's demand using a random variable, where
users share coded keys offline and transmit masked models in the first round,
followed by aggregated coded keys in the second round for task recovery. For
$\sf{2\leq K_c \leq U-1}$, we use robust symmetric private computation to
recover linear combinations of keys in the second round. The objective is to
minimize the number of symbols sent by each user during the two rounds. Our
proposed schemes have achieved the optimal rate region when $ \sf K_c $ is
equal to (1) and the order optimal rate (within 2) when $\sf{2\leq K_c \leq
U-1}$.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.20350v2">SoK: Enhancing Privacy-Preserving Software Development from a
  Developers' Perspective</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Software Engineering-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-04-29T01:38:01Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tharaka Wijesundara, Matthew Warren, Nalin Asanka Gamagedara Arachchilage</p>
    <p><b>Summary:</b> In software development, privacy preservation has become essential with the
rise of privacy concerns and regulations such as GDPR and CCPA. While several
tools, guidelines, methods, methodologies, and frameworks have been proposed to
support developers embedding privacy into software applications, most of them
are proofs-of-concept without empirical evaluations, making their practical
applicability uncertain. These solutions should be evaluated for different
types of scenarios (e.g., industry settings such as rapid software development
environments, teams with different privacy knowledge, etc.) to determine what
their limitations are in various industry settings and what changes are
required to refine current solutions before putting them into industry and
developing new developer-supporting approaches. For that, a thorough review of
empirically evaluated current solutions will be very effective. However, the
existing secondary studies that examine the available developer support provide
broad overviews but do not specifically analyze empirically evaluated solutions
and their limitations. Therefore, this Systematic Literature Review (SLR) aims
to identify and analyze empirically validated solutions that are designed to
help developers in privacy-preserving software development. The findings will
provide valuable insights for researchers to improve current privacy-preserving
solutions and for practitioners looking for effective and validated solutions
to embed privacy into software development.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.20282v1">FedCCL: Federated Clustered Continual Learning Framework for
  Privacy-focused Energy Forecasting</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-04-28T21:51:27Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Michael A. Helcig, Stefan Nastic</p>
    <p><b>Summary:</b> Privacy-preserving distributed model training is crucial for modern machine
learning applications, yet existing Federated Learning approaches struggle with
heterogeneous data distributions and varying computational capabilities.
Traditional solutions either treat all participants uniformly or require costly
dynamic clustering during training, leading to reduced efficiency and delayed
model specialization. We present FedCCL (Federated Clustered Continual
Learning), a framework specifically designed for environments with static
organizational characteristics but dynamic client availability. By combining
static pre-training clustering with an adapted asynchronous FedAvg algorithm,
FedCCL enables new clients to immediately profit from specialized models
without prior exposure to their data distribution, while maintaining reduced
coordination overhead and resilience to client disconnections. Our approach
implements an asynchronous Federated Learning protocol with a three-tier model
topology - global, cluster-specific, and local models - that efficiently
manages knowledge sharing across heterogeneous participants. Evaluation using
photovoltaic installations across central Europe demonstrates that FedCCL's
location-based clustering achieves an energy prediction error of 3.93%
(+-0.21%), while maintaining data privacy and showing that the framework
maintains stability for population-independent deployments, with 0.14
percentage point degradation in performance for new installations. The results
demonstrate that FedCCL offers an effective framework for privacy-preserving
distributed learning, maintaining high accuracy and adaptability even with
dynamic participant populations.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.21036v2">Can Differentially Private Fine-tuning LLMs Protect Against Privacy
  Attacks?</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-04-28T05:34:53Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Hao Du, Shang Liu, Yang Cao</p>
    <p><b>Summary:</b> Fine-tuning large language models (LLMs) has become an essential strategy for
adapting them to specialized tasks; however, this process introduces
significant privacy challenges, as sensitive training data may be inadvertently
memorized and exposed. Although differential privacy (DP) offers strong
theoretical guarantees against such leakage, its empirical privacy
effectiveness on LLMs remains unclear, especially under different fine-tuning
methods. In this paper, we systematically investigate the impact of DP across
fine-tuning methods and privacy budgets, using both data extraction and
membership inference attacks to assess empirical privacy risks. Our main
findings are as follows: (1) Differential privacy reduces model utility, but
its impact varies significantly across different fine-tuning methods. (2)
Without DP, the privacy risks of models fine-tuned with different approaches
differ considerably. (3) When DP is applied, even a relatively high privacy
budget can substantially lower privacy risk. (4) The privacy-utility trade-off
under DP training differs greatly among fine-tuning methods, with some methods
being unsuitable for DP due to severe utility degradation. Our results provide
practical guidance for privacy-conscious deployment of LLMs and pave the way
for future research on optimizing the privacy-utility trade-off in fine-tuning
methodologies.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.21035v2">A False Sense of Privacy: Evaluating Textual Data Sanitization Beyond
  Surface-level Privacy Leakage</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-04-28T01:16:27Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Rui Xin, Niloofar Mireshghallah, Shuyue Stella Li, Michael Duan, Hyunwoo Kim, Yejin Choi, Yulia Tsvetkov, Sewoong Oh, Pang Wei Koh</p>
    <p><b>Summary:</b> Sanitizing sensitive text data typically involves removing personally
identifiable information (PII) or generating synthetic data under the
assumption that these methods adequately protect privacy; however, their
effectiveness is often only assessed by measuring the leakage of explicit
identifiers but ignoring nuanced textual markers that can lead to
re-identification. We challenge the above illusion of privacy by proposing a
new framework that evaluates re-identification attacks to quantify individual
privacy risks upon data release. Our approach shows that seemingly innocuous
auxiliary information -- such as routine social activities -- can be used to
infer sensitive attributes like age or substance use history from sanitized
data. For instance, we demonstrate that Azure's commercial PII removal tool
fails to protect 74\% of information in the MedQA dataset. Although
differential privacy mitigates these risks to some extent, it significantly
reduces the utility of the sanitized text for downstream tasks. Our findings
indicate that current sanitization techniques offer a \textit{false sense of
privacy}, highlighting the need for more robust methods that protect against
semantic-level information leakage.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.19373v2">Doxing via the Lens: Revealing Privacy Leakage in Image Geolocation for
  Agentic Multi-Modal Large Reasoning Model</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-04-27T22:26:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Weidi Luo, Qiming Zhang, Tianyu Lu, Xiaogeng Liu, Yue Zhao, Zhen Xiang, Chaowei Xiao</p>
    <p><b>Summary:</b> The increasing capabilities of agentic multi-modal large reasoning models,
such as ChatGPT o3, have raised critical concerns regarding privacy leakage
through inadvertent image geolocation. In this paper, we conduct the first
systematic and controlled study on the potential privacy risks associated with
visual reasoning abilities of ChatGPT o3. We manually collect and construct a
dataset comprising 50 real-world images that feature individuals alongside
privacy-relevant environmental elements, capturing realistic and sensitive
scenarios for analysis. Our experimental evaluation reveals that ChatGPT o3 can
predict user locations with high precision, achieving street-level accuracy
(within one mile) in 60% of cases. Through analysis, we identify key visual
cues, including street layout and front yard design, that significantly
contribute to the model inference success. Additionally, targeted occlusion
experiments demonstrate that masking critical features effectively mitigates
geolocation accuracy, providing insights into potential defense mechanisms. Our
findings highlight an urgent need for privacy-aware development for agentic
multi-modal large reasoning models, particularly in applications involving
private imagery.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.19274v1">TeleSparse: Practical Privacy-Preserving Verification of Deep Neural
  Networks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-04-27T15:14:09Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mohammad M Maheri, Hamed Haddadi, Alex Davidson</p>
    <p><b>Summary:</b> Verification of the integrity of deep learning inference is crucial for
understanding whether a model is being applied correctly. However, such
verification typically requires access to model weights and (potentially
sensitive or private) training data. So-called Zero-knowledge Succinct
Non-Interactive Arguments of Knowledge (ZK-SNARKs) would appear to provide the
capability to verify model inference without access to such sensitive data.
However, applying ZK-SNARKs to modern neural networks, such as transformers and
large vision models, introduces significant computational overhead.
  We present TeleSparse, a ZK-friendly post-processing mechanisms to produce
practical solutions to this problem. TeleSparse tackles two fundamental
challenges inherent in applying ZK-SNARKs to modern neural networks: (1)
Reducing circuit constraints: Over-parameterized models result in numerous
constraints for ZK-SNARK verification, driving up memory and proof generation
costs. We address this by applying sparsification to neural network models,
enhancing proof efficiency without compromising accuracy or security. (2)
Minimizing the size of lookup tables required for non-linear functions, by
optimizing activation ranges through neural teleportation, a novel adaptation
for narrowing activation functions' range.
  TeleSparse reduces prover memory usage by 67% and proof generation time by
46% on the same model, with an accuracy trade-off of approximately 1%. We
implement our framework using the Halo2 proving system and demonstrate its
effectiveness across multiple architectures (Vision-transformer, ResNet,
MobileNet) and datasets (ImageNet,CIFAR-10,CIFAR-100). This work opens new
directions for ZK-friendly model design, moving toward scalable,
resource-efficient verifiable deep learning.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.19101v1">Privacy-Preserving Federated Embedding Learning for Localized
  Retrieval-Augmented Generation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-04-27T04:26:02Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Qianren Mao, Qili Zhang, Hanwen Hao, Zhentao Han, Runhua Xu, Weifeng Jiang, Qi Hu, Zhijun Chen, Tyler Zhou, Bo Li, Yangqiu Song, Jin Dong, Jianxin Li, Philip S. Yu</p>
    <p><b>Summary:</b> Retrieval-Augmented Generation (RAG) has recently emerged as a promising
solution for enhancing the accuracy and credibility of Large Language Models
(LLMs), particularly in Question & Answer tasks. This is achieved by
incorporating proprietary and private data from integrated databases. However,
private RAG systems face significant challenges due to the scarcity of private
domain data and critical data privacy issues. These obstacles impede the
deployment of private RAG systems, as developing privacy-preserving RAG systems
requires a delicate balance between data security and data availability. To
address these challenges, we regard federated learning (FL) as a highly
promising technology for privacy-preserving RAG services. We propose a novel
framework called Federated Retrieval-Augmented Generation (FedE4RAG). This
framework facilitates collaborative training of client-side RAG retrieval
models. The parameters of these models are aggregated and distributed on a
central-server, ensuring data privacy without direct sharing of raw data. In
FedE4RAG, knowledge distillation is employed for communication between the
server and client models. This technique improves the generalization of local
RAG retrievers during the federated learning process. Additionally, we apply
homomorphic encryption within federated learning to safeguard model parameters
and mitigate concerns related to data leakage. Extensive experiments conducted
on the real-world dataset have validated the effectiveness of FedE4RAG. The
results demonstrate that our proposed framework can markedly enhance the
performance of private RAG systems while maintaining robust data privacy
protection.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.18411v1">Heavy-Tailed Privacy: The Symmetric alpha-Stable Privacy Mechanism</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B">
  <p><b>Published on:</b> 2025-04-25T15:14:02Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Christopher C. Zawacki, Eyad H. Abed</p>
    <p><b>Summary:</b> With the rapid growth of digital platforms, there is increasing apprehension
about how personal data is collected, stored, and used by various entities.
These concerns arise from the increasing frequency of data breaches,
cyber-attacks, and misuse of personal information for targeted advertising and
surveillance. To address these matters, Differential Privacy (DP) has emerged
as a prominent tool for quantifying a digital system's level of protection. The
Gaussian mechanism is commonly used because the Gaussian density is closed
under convolution, and is a common method utilized when aggregating datasets.
However, the Gaussian mechanism only satisfies an approximate form of
Differential Privacy. In this work, we present and analyze of the Symmetric
alpha-Stable (SaS) mechanism. We prove that the mechanism achieves pure
differential privacy while remaining closed under convolution. Additionally, we
study the nuanced relationship between the level of privacy achieved and the
parameters of the density. Lastly, we compare the expected error introduced to
dataset queries by the Gaussian and SaS mechanisms. From our analysis, we
believe the SaS Mechanism is an appealing choice for privacy-focused
applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.18078v1">Privacy-Preserving Personalized Federated Learning for Distributed
  Photovoltaic Disaggregation under Statistical Heterogeneity</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-04-25T05:09:27Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xiaolu Chen, Chenghao Huang, Yanru Zhang, Hao Wang</p>
    <p><b>Summary:</b> The rapid expansion of distributed photovoltaic (PV) installations worldwide,
many being behind-the-meter systems, has significantly challenged energy
management and grid operations, as unobservable PV generation further
complicates the supply-demand balance. Therefore, estimating this generation
from net load, known as PV disaggregation, is critical. Given privacy concerns
and the need for large training datasets, federated learning becomes a
promising approach, but statistical heterogeneity, arising from geographical
and behavioral variations among prosumers, poses new challenges to PV
disaggregation. To overcome these challenges, a privacy-preserving distributed
PV disaggregation framework is proposed using Personalized Federated Learning
(PFL). The proposed method employs a two-level framework that combines local
and global modeling. At the local level, a transformer-based PV disaggregation
model is designed to generate solar irradiance embeddings for representing
local PV conditions. A novel adaptive local aggregation mechanism is adopted to
mitigate the impact of statistical heterogeneity on the local model, extracting
a portion of global information that benefits the local model. At the global
level, a central server aggregates information uploaded from multiple data
centers, preserving privacy while enabling cross-center knowledge sharing.
Experiments on real-world data demonstrate the effectiveness of this proposed
framework, showing improved accuracy and robustness compared to benchmark
methods.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.18032v1">Enhancing Privacy-Utility Trade-offs to Mitigate Memorization in
  Diffusion Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-04-25T02:51:23Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chen Chen, Daochang Liu, Mubarak Shah, Chang Xu</p>
    <p><b>Summary:</b> Text-to-image diffusion models have demonstrated remarkable capabilities in
creating images highly aligned with user prompts, yet their proclivity for
memorizing training set images has sparked concerns about the originality of
the generated images and privacy issues, potentially leading to legal
complications for both model owners and users, particularly when the memorized
images contain proprietary content. Although methods to mitigate these issues
have been suggested, enhancing privacy often results in a significant decrease
in the utility of the outputs, as indicated by text-alignment scores. To bridge
the research gap, we introduce a novel method, PRSS, which refines the
classifier-free guidance approach in diffusion models by integrating prompt
re-anchoring (PR) to improve privacy and incorporating semantic prompt search
(SS) to enhance utility. Extensive experiments across various privacy levels
demonstrate that our approach consistently improves the privacy-utility
trade-off, establishing a new state-of-the-art.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.18007v1">Differential Privacy-Driven Framework for Enhancing Heart Disease
  Prediction</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-04-25T01:27:40Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yazan Otoum, Amiya Nayak</p>
    <p><b>Summary:</b> With the rapid digitalization of healthcare systems, there has been a
substantial increase in the generation and sharing of private health data.
Safeguarding patient information is essential for maintaining consumer trust
and ensuring compliance with legal data protection regulations. Machine
learning is critical in healthcare, supporting personalized treatment, early
disease detection, predictive analytics, image interpretation, drug discovery,
efficient operations, and patient monitoring. It enhances decision-making,
accelerates research, reduces errors, and improves patient outcomes. In this
paper, we utilize machine learning methodologies, including differential
privacy and federated learning, to develop privacy-preserving models that
enable healthcare stakeholders to extract insights without compromising
individual privacy. Differential privacy introduces noise to data to guarantee
statistical privacy, while federated learning enables collaborative model
training across decentralized datasets. We explore applying these technologies
to Heart Disease Data, demonstrating how they preserve privacy while delivering
valuable insights and comprehensive analysis. Our results show that using a
federated learning model with differential privacy achieved a test accuracy of
85%, ensuring patient data remained secure and private throughout the process.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.17703v1">Federated Learning: A Survey on Privacy-Preserving Collaborative
  Intelligence</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-04-24T16:10:29Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Edward Collins, Michel Wang</p>
    <p><b>Summary:</b> Federated Learning (FL) has emerged as a transformative paradigm in the field
of distributed machine learning, enabling multiple clients such as mobile
devices, edge nodes, or organizations to collaboratively train a shared global
model without the need to centralize sensitive data. This decentralized
approach addresses growing concerns around data privacy, security, and
regulatory compliance, making it particularly attractive in domains such as
healthcare, finance, and smart IoT systems. This survey provides a concise yet
comprehensive overview of Federated Learning, beginning with its core
architecture and communication protocol. We discuss the standard FL lifecycle,
including local training, model aggregation, and global updates. A particular
emphasis is placed on key technical challenges such as handling non-IID
(non-independent and identically distributed) data, mitigating system and
hardware heterogeneity, reducing communication overhead, and ensuring privacy
through mechanisms like differential privacy and secure aggregation.
Furthermore, we examine emerging trends in FL research, including personalized
FL, cross-device versus cross-silo settings, and integration with other
paradigms such as reinforcement learning and quantum computing. We also
highlight real-world applications and summarize benchmark datasets and
evaluation metrics commonly used in FL research. Finally, we outline open
research problems and future directions to guide the development of scalable,
efficient, and trustworthy FL systems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.18596v1">Optimizing the Privacy-Utility Balance using Synthetic Data and
  Configurable Perturbation Pipelines</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Probability-5BC0EB">
  <p><b>Published on:</b> 2025-04-24T15:52:53Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Anantha Sharma, Swetha Devabhaktuni, Eklove Mohan</p>
    <p><b>Summary:</b> This paper explores the strategic use of modern synthetic data generation and
advanced data perturbation techniques to enhance security, maintain analytical
utility, and improve operational efficiency when managing large datasets, with
a particular focus on the Banking, Financial Services, and Insurance (BFSI)
sector. We contrast these advanced methods encompassing generative models like
GANs, sophisticated context-aware PII transformation, configurable statistical
perturbation, and differential privacy with traditional anonymization
approaches.
  The goal is to create realistic, privacy-preserving datasets that retain high
utility for complex machine learning tasks and analytics, a critical need in
the data-sensitive industries like BFSI, Healthcare, Retail, and
Telecommunications. We discuss how these modern techniques potentially offer
significant improvements in balancing privacy preservation while maintaining
data utility compared to older methods. Furthermore, we examine the potential
for operational gains, such as reduced overhead and accelerated analytics, by
using these privacy-enhanced datasets. We also explore key use cases where
these methods can mitigate regulatory risks and enable scalable, data-driven
innovation without compromising sensitive customer information.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.17523v1">From Randomized Response to Randomized Index: Answering Subset Counting
  Queries with Local Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-04-24T13:08:11Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Qingqing Ye, Liantong Yu, Kai Huang, Xiaokui Xiao, Weiran Liu, Haibo Hu</p>
    <p><b>Summary:</b> Local Differential Privacy (LDP) is the predominant privacy model for
safeguarding individual data privacy. Existing perturbation mechanisms
typically require perturbing the original values to ensure acceptable privacy,
which inevitably results in value distortion and utility deterioration. In this
work, we propose an alternative approach -- instead of perturbing values, we
apply randomization to indexes of values while ensuring rigorous LDP
guarantees. Inspired by the deniability of randomized indexes, we present CRIAD
for answering subset counting queries on set-value data. By integrating a
multi-dummy, multi-sample, and multi-group strategy, CRIAD serves as a fully
scalable solution that offers flexibility across various privacy requirements
and domain sizes, and achieves more accurate query results than any existing
methods. Through comprehensive theoretical analysis and extensive experimental
evaluations, we validate the effectiveness of CRIAD and demonstrate its
superiority over traditional value-perturbation mechanisms.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.17360v1">PatientDx: Merging Large Language Models for Protecting Data-Privacy in
  Healthcare</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-04-24T08:21:04Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jose G. Moreno, Jesus Lovon, M'Rick Robin-Charlet, Christine Damase-Michel, Lynda Tamine</p>
    <p><b>Summary:</b> Fine-tuning of Large Language Models (LLMs) has become the default practice
for improving model performance on a given task. However, performance
improvement comes at the cost of training on vast amounts of annotated data
which could be sensitive leading to significant data privacy concerns. In
particular, the healthcare domain is one of the most sensitive domains exposed
to data privacy issues. In this paper, we present PatientDx, a framework of
model merging that allows the design of effective LLMs for health-predictive
tasks without requiring fine-tuning nor adaptation on patient data. Our
proposal is based on recently proposed techniques known as merging of LLMs and
aims to optimize a building block merging strategy. PatientDx uses a pivotal
model adapted to numerical reasoning and tunes hyperparameters on examples
based on a performance metric but without training of the LLM on these data.
Experiments using the mortality tasks of the MIMIC-IV dataset show improvements
up to 7% in terms of AUROC when compared to initial models. Additionally, we
confirm that when compared to fine-tuned models, our proposal is less prone to
data leak problems without hurting performance. Finally, we qualitatively show
the capabilities of our proposal through a case study. Our best model is
publicly available at https://huggingface.co/ Jgmorenof/mistral\_merged\_0\_4.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.17274v1">Signal Recovery from Random Dot-Product Graphs Under Local Differential
  Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Statistics Theory-D91E36">   
  <p><b>Published on:</b> 2025-04-24T06:02:02Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Siddharth Vishwanath, Jonathan Hehir</p>
    <p><b>Summary:</b> We consider the problem of recovering latent information from graphs under
$\varepsilon$-edge local differential privacy where the presence of
relationships/edges between two users/vertices remains confidential, even from
the data curator. For the class of generalized random dot-product graphs, we
show that a standard local differential privacy mechanism induces a specific
geometric distortion in the latent positions. Leveraging this insight, we show
that consistent recovery of the latent positions is achievable by appropriately
adjusting the statistical inference procedure for the privatized graph.
Furthermore, we prove that our procedure is nearly minimax-optimal under local
edge differential privacy constraints. Lastly, we show that this framework
allows for consistent recovery of geometric and topological information
underlying the latent positions, as encoded in their persistence diagrams. Our
results extend previous work from the private community detection literature to
a substantially richer class of models and inferential tasks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.16683v1">MCMC for Bayesian estimation of Differential Privacy from Membership
  Inference Attacks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> 
  <p><b>Published on:</b> 2025-04-23T13:10:37Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ceren Yildirim, Kamer Kaya, Sinan Yildirim, Erkay Savas</p>
    <p><b>Summary:</b> We propose a new framework for Bayesian estimation of differential privacy,
incorporating evidence from multiple membership inference attacks (MIA).
Bayesian estimation is carried out via a Markov chain Monte Carlo (MCMC)
algorithm, named MCMC-DP-Est, which provides an estimate of the full posterior
distribution of the privacy parameter (e.g., instead of just credible
intervals). Critically, the proposed method does not assume that privacy
auditing is performed with the most powerful attack on the worst-case (dataset,
challenge point) pair, which is typically unrealistic. Instead, MCMC-DP-Est
jointly estimates the strengths of MIAs used and the privacy of the training
algorithm, yielding a more cautious privacy analysis. We also present an
economical way to generate measurements for the performance of an MIA that is
to be used by the MCMC method to estimate privacy. We present the use of the
methods with numerical examples with both artificial and real data.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.16557v1">Beyond Anonymization: Object Scrubbing for Privacy-Preserving 2D and 3D
  Vision Tasks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-04-23T09:33:10Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Murat Bilgehan Ertan, Ronak Sahu, Phuong Ha Nguyen, Kaleel Mahmood, Marten van Dijk</p>
    <p><b>Summary:</b> We introduce ROAR (Robust Object Removal and Re-annotation), a scalable
framework for privacy-preserving dataset obfuscation that eliminates sensitive
objects instead of modifying them. Our method integrates instance segmentation
with generative inpainting to remove identifiable entities while preserving
scene integrity. Extensive evaluations on 2D COCO-based object detection show
that ROAR achieves 87.5% of the baseline detection average precision (AP),
whereas image dropping achieves only 74.2% of the baseline AP, highlighting the
advantage of scrubbing in preserving dataset utility. The degradation is even
more severe for small objects due to occlusion and loss of fine-grained
details. Furthermore, in NeRF-based 3D reconstruction, our method incurs a PSNR
loss of at most 1.66 dB while maintaining SSIM and improving LPIPS,
demonstrating superior perceptual quality. Our findings establish object
removal as an effective privacy framework, achieving strong privacy guarantees
with minimal performance trade-offs. The results highlight key challenges in
generative inpainting, occlusion-robust segmentation, and task-specific
scrubbing, setting the foundation for future advancements in privacy-preserving
vision systems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.16535v1">Decentralized Quantile Regression for Feature-Distributed Massive
  Datasets with Privacy Guarantees</a></h3>
  
  <p><b>Published on:</b> 2025-04-23T09:04:21Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Peiwen Xiao, Xiaohui Liu, Guangming Pan, Wei Long</p>
    <p><b>Summary:</b> In this paper, we introduce a novel decentralized surrogate gradient-based
algorithm for quantile regression in a feature-distributed setting, where
global features are dispersed across multiple machines within a decentralized
network. The proposed algorithm, \texttt{DSG-cqr}, utilizes a convolution-type
smoothing approach to address the non-smooth nature of the quantile loss
function. \texttt{DSG-cqr} is fully decentralized, conjugate-free, easy to
implement, and achieves linear convergence up to statistical precision. To
ensure privacy, we adopt the Gaussian mechanism to provide
$(\epsilon,\delta)$-differential privacy. To overcome the exact residual
calculation problem, we estimate residuals using auxiliary variables and
develop a confidence interval construction method based on Wald statistics.
Theoretical properties are established, and the practical utility of the
methods is also demonstrated through extensive simulations and a real-world
data application.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.18581v2">Enhancing Privacy in Semantic Communication over Wiretap Channels
  leveraging Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2025-04-23T08:42:44Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Weixuan Chen, Shunpu Tang, Qianqian Yang, Zhiguo Shi, Dusit Niyato</p>
    <p><b>Summary:</b> Semantic communication (SemCom) improves transmission efficiency by focusing
on task-relevant information. However, transmitting semantic-rich data over
insecure channels introduces privacy risks. This paper proposes a novel SemCom
framework that integrates differential privacy (DP) mechanisms to protect
sensitive semantic features. This method employs the generative adversarial
network (GAN) inversion technique to extract disentangled semantic features and
uses neural networks (NNs) to approximate the DP application and removal
processes, effectively mitigating the non-invertibility issue of DP.
Additionally, an NN-based encryption scheme is introduced to strengthen the
security of channel inputs. Simulation results demonstrate that the proposed
approach effectively prevents eavesdroppers from reconstructing sensitive
information by generating chaotic or fake images, while ensuring high-quality
image reconstruction for legitimate users. The system exhibits robust
performance across various privacy budgets and channel conditions, achieving an
optimal balance between privacy protection and reconstruction fidelity.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.16371v1">The Safety-Privacy Tradeoff in Linear Bandits</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Optimization and Control-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-04-23T02:48:02Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Arghavan Zibaie, Spencer Hutchinson, Ramtin Pedarsani, Mahnoosh Alizadeh</p>
    <p><b>Summary:</b> We consider a collection of linear stochastic bandit problems, each modeling
the random response of different agents to proposed interventions, coupled
together by a global safety constraint. We assume a central coordinator must
choose actions to play on each bandit with the objective of regret
minimization, while also ensuring that the expected response of all agents
satisfies the global safety constraints at each round, in spite of uncertainty
about the bandits' parameters. The agents consider their observed responses to
be private and in order to protect their sensitive information, the data
sharing with the central coordinator is performed under local differential
privacy (LDP). However, providing higher level of privacy to different agents
would have consequences in terms of safety and regret. We formalize these
tradeoffs by building on the notion of the sharpness of the safety set - a
measure of how the geometric properties of the safe set affects the growth of
regret - and propose a unilaterally unimprovable vector of privacy levels for
different agents given a maximum regret budget.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.16000v1">How Private is Your Attention? Bridging Privacy with In-Context Learning</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-04-22T16:05:26Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Soham Bonnerjee, Zhen Wei,  Yeon, Anna Asch, Sagnik Nandy, Promit Ghosal</p>
    <p><b>Summary:</b> In-context learning (ICL)-the ability of transformer-based models to perform
new tasks from examples provided at inference time-has emerged as a hallmark of
modern language models. While recent works have investigated the mechanisms
underlying ICL, its feasibility under formal privacy constraints remains
largely unexplored. In this paper, we propose a differentially private
pretraining algorithm for linear attention heads and present the first
theoretical analysis of the privacy-accuracy trade-off for ICL in linear
regression. Our results characterize the fundamental tension between
optimization and privacy-induced noise, formally capturing behaviors observed
in private training via iterative methods. Additionally, we show that our
method is robust to adversarial perturbations of training prompts, unlike
standard ridge regression. All theoretical findings are supported by extensive
simulations across diverse settings.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.15995v1">OPUS-VFL: Incentivizing Optimal Privacy-Utility Tradeoffs in Vertical
  Federated Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-04-22T16:00:11Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sindhuja Madabushi, Ahmad Faraz Khan, Haider Ali, Jin-Hee Cho</p>
    <p><b>Summary:</b> Vertical Federated Learning (VFL) enables organizations with disjoint feature
spaces but shared user bases to collaboratively train models without sharing
raw data. However, existing VFL systems face critical limitations: they often
lack effective incentive mechanisms, struggle to balance privacy-utility
tradeoffs, and fail to accommodate clients with heterogeneous resource
capabilities. These challenges hinder meaningful participation, degrade model
performance, and limit practical deployment. To address these issues, we
propose OPUS-VFL, an Optimal Privacy-Utility tradeoff Strategy for VFL.
OPUS-VFL introduces a novel, privacy-aware incentive mechanism that rewards
clients based on a principled combination of model contribution, privacy
preservation, and resource investment. It employs a lightweight leave-one-out
(LOO) strategy to quantify feature importance per client, and integrates an
adaptive differential privacy mechanism that enables clients to dynamically
calibrate noise levels to optimize their individual utility. Our framework is
designed to be scalable, budget-balanced, and robust to inference and poisoning
attacks. Extensive experiments on benchmark datasets (MNIST, CIFAR-10, and
CIFAR-100) demonstrate that OPUS-VFL significantly outperforms state-of-the-art
VFL baselines in both efficiency and robustness. It reduces label inference
attack success rates by up to 20%, increases feature inference reconstruction
error (MSE) by over 30%, and achieves up to 25% higher incentives for clients
that contribute meaningfully while respecting privacy and cost constraints.
These results highlight the practicality and innovation of OPUS-VFL as a
secure, fair, and performance-driven solution for real-world VFL.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.15580v1">On the Price of Differential Privacy for Hierarchical Clustering</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-04-22T04:39:40Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chengyuan Deng, Jie Gao, Jalaj Upadhyay, Chen Wang, Samson Zhou</p>
    <p><b>Summary:</b> Hierarchical clustering is a fundamental unsupervised machine learning task
with the aim of organizing data into a hierarchy of clusters. Many applications
of hierarchical clustering involve sensitive user information, therefore
motivating recent studies on differentially private hierarchical clustering
under the rigorous framework of Dasgupta's objective. However, it has been
shown that any privacy-preserving algorithm under edge-level differential
privacy necessarily suffers a large error. To capture practical applications of
this problem, we focus on the weight privacy model, where each edge of the
input graph is at least unit weight. We present a novel algorithm in the weight
privacy model that shows significantly better approximation than known
impossibility results in the edge-level DP setting. In particular, our
algorithm achieves $O(\log^{1.5}n/\varepsilon)$ multiplicative error for
$\varepsilon$-DP and runs in polynomial time, where $n$ is the size of the
input graph, and the cost is never worse than the optimal additive error in
existing work. We complement our algorithm by showing if the unit-weight
constraint does not apply, the lower bound for weight-level DP hierarchical
clustering is essentially the same as the edge-level DP, i.e.
$\Omega(n^2/\varepsilon)$ additive error. As a result, we also obtain a new
lower bound of $\tilde{\Omega}(1/\varepsilon)$ additive error for balanced
sparsest cuts in the weight-level DP model, which may be of independent
interest. Finally, we evaluate our algorithm on synthetic and real-world
datasets. Our experimental results show that our algorithm performs well in
terms of extra cost and has good scalability to large graphs.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.18569v1">Large Language Model Empowered Privacy-Protected Framework for PHI
  Annotation in Clinical Notes</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-04-22T03:18:36Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Guanchen Wu, Linzhi Zheng, Han Xie, Zhen Xiang, Jiaying Lu, Darren Liu, Delgersuren Bold, Bo Li, Xiao Hu, Carl Yang</p>
    <p><b>Summary:</b> The de-identification of private information in medical data is a crucial
process to mitigate the risk of confidentiality breaches, particularly when
patient personal details are not adequately removed before the release of
medical records. Although rule-based and learning-based methods have been
proposed, they often struggle with limited generalizability and require
substantial amounts of annotated data for effective performance. Recent
advancements in large language models (LLMs) have shown significant promise in
addressing these issues due to their superior language comprehension
capabilities. However, LLMs present challenges, including potential privacy
risks when using commercial LLM APIs and high computational costs for deploying
open-source LLMs locally. In this work, we introduce LPPA, an LLM-empowered
Privacy-Protected PHI Annotation framework for clinical notes, targeting the
English language. By fine-tuning LLMs locally with synthetic notes, LPPA
ensures strong privacy protection and high PHI annotation accuracy. Extensive
experiments demonstrate LPPA's effectiveness in accurately de-identifying
private information, offering a scalable and efficient solution for enhancing
patient privacy protection.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.15525v1">Federated Latent Factor Learning for Recovering Wireless Sensor Networks
  Signal with Privacy-Preserving</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-04-22T02:01:19Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chengjun Yu, Yixin Ran, Yangyi Xia, Jia Wu, Xiaojing Liu</p>
    <p><b>Summary:</b> Wireless Sensor Networks (WSNs) are a cutting-edge domain in the field of
intelligent sensing. Due to sensor failures and energy-saving strategies, the
collected data often have massive missing data, hindering subsequent analysis
and decision-making. Although Latent Factor Learning (LFL) has been proven
effective in recovering missing data, it fails to sufficiently consider data
privacy protection. To address this issue, this paper innovatively proposes a
federated latent factor learning (FLFL) based spatial signal recovery (SSR)
model, named FLFL-SSR. Its main idea is two-fold: 1) it designs a sensor-level
federated learning framework, where each sensor uploads only gradient updates
instead of raw data to optimize the global model, and 2) it proposes a local
spatial sharing strategy, allowing sensors within the same spatial region to
share their latent feature vectors, capturing spatial correlations and
enhancing recovery accuracy. Experimental results on two real-world WSNs
datasets demonstrate that the proposed model outperforms existing federated
methods in terms of recovery performance.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.15233v1">A Review on Privacy in DAG-Based DLTs</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-04-21T17:08:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mayank Raikwar</p>
    <p><b>Summary:</b> Directed Acyclic Graph (DAG)-based Distributed Ledger Technologies (DLTs)
have emerged as a promising solution to the scalability issues inherent in
traditional blockchains. However, amidst the focus on scalability, the crucial
aspect of privacy within DAG-based DLTs has been largely overlooked. This paper
seeks to address this gap by providing a comprehensive examination of privacy
notions and challenges within DAG-based DLTs. We delve into potential
methodologies to enhance privacy within these systems, while also analyzing the
associated hurdles and real-world implementations within state-of-the-art
DAG-based DLTs. By exploring these methodologies, we not only illuminate the
current landscape of privacy in DAG-based DLTs but also outline future research
directions in this evolving field.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.15090v1">Federated Latent Factor Model for Bias-Aware Recommendation with
  Privacy-Preserving</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-04-21T13:24:30Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Junxiang Gao, Yixin Ran, Jia Chen</p>
    <p><b>Summary:</b> A recommender system (RS) aims to provide users with personalized item
recommendations, enhancing their overall experience. Traditional RSs collect
and process all user data on a central server. However, this centralized
approach raises significant privacy concerns, as it increases the risk of data
breaches and privacy leakages, which are becoming increasingly unacceptable to
privacy-sensitive users. To address these privacy challenges, federated
learning has been integrated into RSs, ensuring that user data remains secure.
In centralized RSs, the issue of rating bias is effectively addressed by
jointly analyzing all users' raw interaction data. However, this becomes a
significant challenge in federated RSs, as raw data is no longer accessible due
to privacy-preserving constraints. To overcome this problem, we propose a
Federated Bias-Aware Latent Factor (FBALF) model. In FBALF, training bias is
explicitly incorporated into every local model's loss function, allowing for
the effective elimination of rating bias without compromising data privacy.
Extensive experiments conducted on three real-world datasets demonstrate that
FBALF achieves significantly higher recommendation accuracy compared to other
state-of-the-art federated RSs.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.14993v1">Dual Utilization of Perturbation for Stream Data Publication under Local
  Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB">
  <p><b>Published on:</b> 2025-04-21T09:51:18Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Rong Du, Qingqing Ye, Yaxin Xiao, Liantong Yu, Yue Fu, Haibo Hu</p>
    <p><b>Summary:</b> Stream data from real-time distributed systems such as IoT, tele-health, and
crowdsourcing has become an important data source. However, the collection and
analysis of user-generated stream data raise privacy concerns due to the
potential exposure of sensitive information. To address these concerns, local
differential privacy (LDP) has emerged as a promising standard. Nevertheless,
applying LDP to stream data presents significant challenges, as stream data
often involves a large or even infinite number of values. Allocating a given
privacy budget across these data points would introduce overwhelming LDP noise
to the original stream data.
  Beyond existing approaches that merely use perturbed values for estimating
statistics, our design leverages them for both perturbation and estimation.
This dual utilization arises from a key observation: each user knows their own
ground truth and perturbed values, enabling a precise computation of the
deviation error caused by perturbation. By incorporating this deviation into
the perturbation process of subsequent values, the previous noise can be
calibrated. Following this insight, we introduce the Iterative Perturbation
Parameterization (IPP) method, which utilizes current perturbed results to
calibrate the subsequent perturbation process. To enhance the robustness of
calibration and reduce sensitivity, two algorithms, namely Accumulated
Perturbation Parameterization (APP) and Clipped Accumulated Perturbation
Parameterization (CAPP) are further developed. We prove that these three
algorithms satisfy $w$-event differential privacy while significantly improving
utility. Experimental results demonstrate that our techniques outperform
state-of-the-art LDP stream publishing solutions in terms of utility, while
retaining the same privacy guarantee.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.14780v1">Delay-Angle Information Spoofing for Channel State Information-Free
  Location-Privacy Enhancement</a></h3>
  
  <p><b>Published on:</b> 2025-04-21T00:40:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jianxiu Li, Urbashi Mitra</p>
    <p><b>Summary:</b> In this paper, a delay-angle information spoofing (DAIS) strategy is proposed
to enhance the location privacy at the physical layer. More precisely, the
location-relevant delays and angles are artificially shifted without the aid of
channel state information (CSI) at the transmitter, such that the location
perceived by the eavesdropper is incorrect and distinct from the true one. By
leveraging the intrinsic structure of the wireless channel, a precoder is
designed to achieve DAIS while the legitimate localizer can remove the
obfuscation via securely receiving a modest amount of information, i.e., the
delay-angle shifts. A lower bound on eavesdropper's localization error is
derived, revealing that location privacy is enhanced not only due to estimation
error, but also by the geometric mismatch introduced by DAIS. Furthermore, the
lower bound is explicitly expressed as a function of the delay-angle shifts,
characterizing performance trends and providing the appropriate design of these
shift parameters. The statistical hardness of maliciously inferring the
delay-angle shifts by a single-antenna eavesdropper as well as the challenges
for a multi-antenna eavesdropper are investigated to assess the robustness of
the proposed DAIS strategy. Numerical results show that the proposed DAIS
strategy results in more than 15 dB performance degradation for the
eavesdropper as compared with that for the legitimate localizer at high
signal-to-noise ratios, and provides more effective location-privacy
enhancement than the prior art.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.14730v1">Optimal Additive Noise Mechanisms for Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2025-04-20T20:04:41Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Atefeh Gilani, Juan Felipe Gomez, Shahab Asoodeh, Flavio P. Calmon, Oliver Kosut, Lalitha Sankar</p>
    <p><b>Summary:</b> We propose a unified optimization framework for designing continuous and
discrete noise distributions that ensure differential privacy (DP) by
minimizing R\'enyi DP, a variant of DP, under a cost constraint. R\'enyi DP has
the advantage that by considering different values of the R\'enyi parameter
$\alpha$, we can tailor our optimization for any number of compositions. To
solve the optimization problem, we reduce it to a finite-dimensional convex
formulation and perform preconditioned gradient descent. The resulting noise
distributions are then compared to their Gaussian and Laplace counterparts.
Numerical results demonstrate that our optimized distributions are consistently
better, with significant improvements in $(\varepsilon, \delta)$-DP guarantees
in the moderate composition regimes, compared to Gaussian and Laplace
distributions with the same variance.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.14368v1">Do You Really Need Public Data? Surrogate Public Data for Differential
  Privacy on Tabular Data</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-04-19T17:55:10Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shlomi Hod, Lucas Rosenblatt, Julia Stoyanovich</p>
    <p><b>Summary:</b> Differentially private (DP) machine learning often relies on the availability
of public data for tasks like privacy-utility trade-off estimation,
hyperparameter tuning, and pretraining. While public data assumptions may be
reasonable in text and image domains, they are less likely to hold for tabular
data due to tabular data heterogeneity across domains. We propose leveraging
powerful priors to address this limitation; specifically, we synthesize
realistic tabular data directly from schema-level specifications - such as
variable names, types, and permissible ranges - without ever accessing
sensitive records. To that end, this work introduces the notion of "surrogate"
public data - datasets generated independently of sensitive data, which consume
no privacy loss budget and are constructed solely from publicly available
schema or metadata. Surrogate public data are intended to encode plausible
statistical assumptions (informed by publicly available information) into a
dataset with many downstream uses in private mechanisms. We automate the
process of generating surrogate public data with large language models (LLMs);
in particular, we propose two methods: direct record generation as CSV files,
and automated structural causal model (SCM) construction for sampling records.
Through extensive experiments, we demonstrate that surrogate public tabular
data can effectively replace traditional public data when pretraining
differentially private tabular classifiers. To a lesser extent, surrogate
public data are also useful for hyperparameter tuning of DP synthetic data
generators, and for estimating the privacy-utility tradeoff.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.14301v1">Balancing Privacy and Action Performance: A Penalty-Driven Approach to
  Image Anonymization</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Multimedia-5BC0EB">
  <p><b>Published on:</b> 2025-04-19T13:52:33Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Nazia Aslam, Kamal Nasrollahi</p>
    <p><b>Summary:</b> The rapid development of video surveillance systems for object detection,
tracking, activity recognition, and anomaly detection has revolutionized our
day-to-day lives while setting alarms for privacy concerns. It isn't easy to
strike a balance between visual privacy and action recognition performance in
most computer vision models. Is it possible to safeguard privacy without
sacrificing performance? It poses a formidable challenge, as even minor privacy
enhancements can lead to substantial performance degradation. To address this
challenge, we propose a privacy-preserving image anonymization technique that
optimizes the anonymizer using penalties from the utility branch, ensuring
improved action recognition performance while minimally affecting privacy
leakage. This approach addresses the trade-off between minimizing privacy
leakage and maintaining high action performance. The proposed approach is
primarily designed to align with the regulatory standards of the EU AI Act and
GDPR, ensuring the protection of personally identifiable information while
maintaining action performance. To the best of our knowledge, we are the first
to introduce a feature-based penalty scheme that exclusively controls the
action features, allowing freedom to anonymize private attributes. Extensive
experiments were conducted to validate the effectiveness of the proposed
method. The results demonstrate that applying a penalty to anonymizer from
utility branch enhances action performance while maintaining nearly consistent
privacy leakage across different penalty settings.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.14208v1">FedCIA: Federated Collaborative Information Aggregation for
  Privacy-Preserving Recommendation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Retrieval-5BC0EB">
  <p><b>Published on:</b> 2025-04-19T06:59:34Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mingzhe Han, Dongsheng Li, Jiafeng Xia, Jiahao Liu, Hansu Gu, Peng Zhang, Ning Gu, Tun Lu</p>
    <p><b>Summary:</b> Recommendation algorithms rely on user historical interactions to deliver
personalized suggestions, which raises significant privacy concerns. Federated
recommendation algorithms tackle this issue by combining local model training
with server-side model aggregation, where most existing algorithms use a
uniform weighted summation to aggregate item embeddings from different client
models. This approach has three major limitations: 1) information loss during
aggregation, 2) failure to retain personalized local features, and 3)
incompatibility with parameter-free recommendation algorithms. To address these
limitations, we first review the development of recommendation algorithms and
recognize that their core function is to share collaborative information,
specifically the global relationship between users and items. With this
understanding, we propose a novel aggregation paradigm named collaborative
information aggregation, which focuses on sharing collaborative information
rather than item parameters. Based on this new paradigm, we introduce the
federated collaborative information aggregation (FedCIA) method for
privacy-preserving recommendation. This method requires each client to upload
item similarity matrices for aggregation, which allows clients to align their
local models without constraining embeddings to a unified vector space. As a
result, it mitigates information loss caused by direct summation, preserves the
personalized embedding distributions of individual clients, and supports the
aggregation of parameter-free models. Theoretical analysis and experimental
results on real-world datasets demonstrate the superior performance of FedCIA
compared with the state-of-the-art federated recommendation algorithms. Code is
available at https://github.com/Mingzhe-Han/FedCIA.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.13526v1">Multi-class Item Mining under Local Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-04-18T07:37:06Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yulian Mao, Qingqing Ye, Rong Du, Qi Wang, Kai Huang, Haibo Hu</p>
    <p><b>Summary:</b> Item mining, a fundamental task for collecting statistical data from users,
has raised increasing privacy concerns. To address these concerns, local
differential privacy (LDP) was proposed as a privacy-preserving technique.
Existing LDP item mining mechanisms primarily concentrate on global statistics,
i.e., those from the entire dataset. Nevertheless, they fall short of
user-tailored tasks such as personalized recommendations, whereas classwise
statistics can improve task accuracy with fine-grained information. Meanwhile,
the introduction of class labels brings new challenges. Label perturbation may
result in invalid items for aggregation. To this end, we propose frameworks for
multi-class item mining, along with two mechanisms: validity perturbation to
reduce the impact of invalid data, and correlated perturbation to preserve the
relationship between labels and items. We also apply these optimized methods to
two multi-class item mining queries: frequency estimation and top-$k$ item
mining. Through theoretical analysis and extensive experiments, we verify the
effectiveness and superiority of these methods.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.13267v1">Leveraging Functional Encryption and Deep Learning for
  Privacy-Preserving Traffic Forecasting</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36"> 
  <p><b>Published on:</b> 2025-04-17T18:21:55Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Isaac Adom, Mohammmad Iqbal Hossain, Hassan Mahmoud, Ahmad Alsharif, Mahmoud Nabil Mahmoud, Yang Xiao</p>
    <p><b>Summary:</b> Over the past few years, traffic congestion has continuously plagued the
nation's transportation system creating several negative impacts including
longer travel times, increased pollution rates, and higher collision risks. To
overcome these challenges, Intelligent Transportation Systems (ITS) aim to
improve mobility and vehicular systems, ensuring higher levels of safety by
utilizing cutting-edge technologies, sophisticated sensing capabilities, and
innovative algorithms. Drivers' participatory sensing, current/future location
reporting, and machine learning algorithms have considerably improved real-time
congestion monitoring and future traffic management. However, each driver's
sensitive spatiotemporal location information can create serious privacy
concerns. To address these challenges, we propose in this paper a secure,
privacy-preserving location reporting and traffic forecasting system that
guarantees privacy protection of driver data while maintaining high traffic
forecasting accuracy. Our novel k-anonymity scheme utilizes functional
encryption to aggregate encrypted location information submitted by drivers
while ensuring the privacy of driver location data. Additionally, using the
aggregated encrypted location information as input, this research proposes a
deep learning model that incorporates a Convolutional-Long Short-Term Memory
(Conv-LSTM) module to capture spatial and short-term temporal features and a
Bidirectional Long Short-Term Memory (Bi-LSTM) module to recover long-term
periodic patterns for traffic forecasting. With extensive evaluation on real
datasets, we demonstrate the effectiveness of the proposed scheme with less
than 10% mean absolute error for a 60-minute forecasting horizon, all while
protecting driver privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2504.12931v1">Explainable AI in Usable Privacy and Security: Challenges and
  Opportunities</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-04-17T13:28:01Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Vincent Freiberger, Arthur Fleig, Erik Buchmann</p>
    <p><b>Summary:</b> Large Language Models (LLMs) are increasingly being used for automated
evaluations and explaining them. However, concerns about explanation quality,
consistency, and hallucinations remain open research challenges, particularly
in high-stakes contexts like privacy and security, where user trust and
decision-making are at stake. In this paper, we investigate these issues in the
context of PRISMe, an interactive privacy policy assessment tool that leverages
LLMs to evaluate and explain website privacy policies. Based on a prior user
study with 22 participants, we identify key concerns regarding LLM judgment
transparency, consistency, and faithfulness, as well as variations in user
preferences for explanation detail and engagement. We discuss potential
strategies to mitigate these concerns, including structured evaluation
criteria, uncertainty estimation, and retrieval-augmented generation (RAG). We
identify a need for adaptive explanation strategies tailored to different user
profiles for LLM-as-a-judge. Our goal is to showcase the application area of
usable privacy and security to be promising for Human-Centered Explainable AI
(HCXAI) to make an impact.</p>
  </details>
</div>



<h2>2025-05</h2>

<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.10496v1">CheXGenBench: A Unified Benchmark For Fidelity, Privacy and Utility of
  Synthetic Chest Radiographs</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-05-15T16:59:17Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Raman Dutt, Pedro Sanchez, Yongchen Yao, Steven McDonagh, Sotirios A. Tsaftaris, Timothy Hospedales</p>
    <p><b>Summary:</b> We introduce CheXGenBench, a rigorous and multifaceted evaluation framework
for synthetic chest radiograph generation that simultaneously assesses
fidelity, privacy risks, and clinical utility across state-of-the-art
text-to-image generative models. Despite rapid advancements in generative AI
for real-world imagery, medical domain evaluations have been hindered by
methodological inconsistencies, outdated architectural comparisons, and
disconnected assessment criteria that rarely address the practical clinical
value of synthetic samples. CheXGenBench overcomes these limitations through
standardised data partitioning and a unified evaluation protocol comprising
over 20 quantitative metrics that systematically analyse generation quality,
potential privacy vulnerabilities, and downstream clinical applicability across
11 leading text-to-image architectures. Our results reveal critical
inefficiencies in the existing evaluation protocols, particularly in assessing
generative fidelity, leading to inconsistent and uninformative comparisons. Our
framework establishes a standardised benchmark for the medical AI community,
enabling objective and reproducible comparisons while facilitating seamless
integration of both existing and future generative models. Additionally, we
release a high-quality, synthetic dataset, SynthCheX-75K, comprising 75K
radiographs generated by the top-performing model (Sana 0.6B) in our benchmark
to support further research in this critical domain. Through CheXGenBench, we
establish a new state-of-the-art and release our framework, models, and
SynthCheX-75K dataset at https://raman1121.github.io/CheXGenBench/</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.10264v1">Cutting Through Privacy: A Hyperplane-Based Data Reconstruction Attack
  in Federated Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-15T13:16:32Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Francesco Diana, André Nusser, Chuan Xu, Giovanni Neglia</p>
    <p><b>Summary:</b> Federated Learning (FL) enables collaborative training of machine learning
models across distributed clients without sharing raw data, ostensibly
preserving data privacy. Nevertheless, recent studies have revealed critical
vulnerabilities in FL, showing that a malicious central server can manipulate
model updates to reconstruct clients' private training data. Existing data
reconstruction attacks have important limitations: they often rely on
assumptions about the clients' data distribution or their efficiency
significantly degrades when batch sizes exceed just a few tens of samples.
  In this work, we introduce a novel data reconstruction attack that overcomes
these limitations. Our method leverages a new geometric perspective on fully
connected layers to craft malicious model parameters, enabling the perfect
recovery of arbitrarily large data batches in classification tasks without any
prior knowledge of clients' data. Through extensive experiments on both image
and tabular datasets, we demonstrate that our attack outperforms existing
methods and achieves perfect reconstruction of data batches two orders of
magnitude larger than the state of the art.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.09929v1">Security and Privacy Measurement on Chinese Consumer IoT Traffic based
  on Device Lifecycle</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-15T03:27:16Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chenghua Jin, Yan Jia, Yuxin Song, Qingyin Tan, Rui Yang, Zheli Liu</p>
    <p><b>Summary:</b> In recent years, consumer Internet of Things (IoT) devices have become widely
used in daily life. With the popularity of devices, related security and
privacy risks arise at the same time as they collect user-related data and
transmit it to various service providers. Although China accounts for a larger
share of the consumer IoT industry, current analyses on consumer IoT device
traffic primarily focus on regions such as Europe, the United States, and
Australia. Research on China, however, is currently rather rare. This study
constructs the first large-scale dataset about consumer IoT device traffic in
China. Specifically, we propose a fine-grained traffic collection guidance
covering the entire lifecycle of consumer IoT devices, gathering traffic from
70 devices spanning 36 brands and 8 device categories. Based on this dataset,
we analyze traffic destinations and encryption practices across different
device types during the entire lifecycle and compare the findings with the
results of other regions. Compared to other regions, our results show that
consumer IoT devices in China rely more on domestic services and overally
perform better in terms of encryption practices. However, there are still 20/35
devices improperly conduct certificate validation, and 5/70 devices use
insecure encryption protocols. To facilitate future research, we open-source
our traffic collection guidance and make our dataset publicly available.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.09921v1">PIG: Privacy Jailbreak Attack on LLMs via Gradient-based Iterative
  In-Context Optimization</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-05-15T03:11:57Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yidan Wang, Yanan Cao, Yubing Ren, Fang Fang, Zheng Lin, Binxing Fang</p>
    <p><b>Summary:</b> Large Language Models (LLMs) excel in various domains but pose inherent
privacy risks. Existing methods to evaluate privacy leakage in LLMs often use
memorized prefixes or simple instructions to extract data, both of which
well-alignment models can easily block. Meanwhile, Jailbreak attacks bypass LLM
safety mechanisms to generate harmful content, but their role in privacy
scenarios remains underexplored. In this paper, we examine the effectiveness of
jailbreak attacks in extracting sensitive information, bridging privacy leakage
and jailbreak attacks in LLMs. Moreover, we propose PIG, a novel framework
targeting Personally Identifiable Information (PII) and addressing the
limitations of current jailbreak methods. Specifically, PIG identifies PII
entities and their types in privacy queries, uses in-context learning to build
a privacy context, and iteratively updates it with three gradient-based
strategies to elicit target PII. We evaluate PIG and existing jailbreak methods
using two privacy-related datasets. Experiments on four white-box and two
black-box LLMs show that PIG outperforms baseline methods and achieves
state-of-the-art (SoTA) results. The results underscore significant privacy
risks in LLMs, emphasizing the need for stronger safeguards. Our code is
availble at
\href{https://github.com/redwyd/PrivacyJailbreak}{https://github.com/redwyd/PrivacyJailbreak}.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.09276v1">Privacy-Preserving Runtime Verification</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Formal Languages and Automata Theory-D91E36">
  <p><b>Published on:</b> 2025-05-14T10:49:07Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Thomas A. Henzinger, Mahyar Karimi, K. S. Thejaswini</p>
    <p><b>Summary:</b> Runtime verification offers scalable solutions to improve the safety and
reliability of systems. However, systems that require verification or
monitoring by a third party to ensure compliance with a specification might
contain sensitive information, causing privacy concerns when usual runtime
verification approaches are used. Privacy is compromised if protected
information about the system, or sensitive data that is processed by the
system, is revealed. In addition, revealing the specification being monitored
may undermine the essence of third-party verification.
  In this work, we propose two novel protocols for the privacy-preserving
runtime verification of systems against formal sequential specifications. In
our first protocol, the monitor verifies whether the system satisfies the
specification without learning anything else, though both parties are aware of
the specification. Our second protocol ensures that the system remains
oblivious to the monitored specification, while the monitor learns only whether
the system satisfies the specification and nothing more. Our protocols adapt
and improve existing techniques used in cryptography, and more specifically,
multi-party computation.
  The sequential specification defines the observation step of the monitor,
whose granularity depends on the situation (e.g., banks may be monitored on a
daily basis). Our protocols exchange a single message per observation step,
after an initialisation phase. This design minimises communication overhead,
enabling relatively lightweight privacy-preserving monitoring. We implement our
approach for monitoring specifications described by register automata and
evaluate it experimentally.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.08719v1">PWC-MoE: Privacy-Aware Wireless Collaborative Mixture of Experts</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-05-13T16:27:07Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yang Su, Na Yan, Yansha Deng, Robert Schober</p>
    <p><b>Summary:</b> Large language models (LLMs) hosted on cloud servers alleviate the
computational and storage burdens on local devices but raise privacy concerns
due to sensitive data transmission and require substantial communication
bandwidth, which is challenging in constrained environments. In contrast, small
language models (SLMs) running locally enhance privacy but suffer from limited
performance on complex tasks. To balance computational cost, performance, and
privacy protection under bandwidth constraints, we propose a privacy-aware
wireless collaborative mixture of experts (PWC-MoE) framework. Specifically,
PWC-MoE employs a sparse privacy-aware gating network to dynamically route
sensitive tokens to privacy experts located on local clients, while
non-sensitive tokens are routed to non-privacy experts located at the remote
base station. To achieve computational efficiency, the gating network ensures
that each token is dynamically routed to and processed by only one expert. To
enhance scalability and prevent overloading of specific experts, we introduce a
group-wise load-balancing mechanism for the gating network that evenly
distributes sensitive tokens among privacy experts and non-sensitive tokens
among non-privacy experts. To adapt to bandwidth constraints while preserving
model performance, we propose a bandwidth-adaptive and importance-aware token
offloading scheme. This scheme incorporates an importance predictor to evaluate
the importance scores of non-sensitive tokens, prioritizing the most important
tokens for transmission to the base station based on their predicted importance
and the available bandwidth. Experiments demonstrate that the PWC-MoE framework
effectively preserves privacy and maintains high performance even in
bandwidth-constrained environments, offering a practical solution for deploying
LLMs in privacy-sensitive and bandwidth-limited scenarios.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.08847v1">On the interplay of Explainability, Privacy and Predictive Performance
  with Explanation-assisted Model Extraction</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-05-13T15:27:06Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Fatima Ezzeddine, Rinad Akel, Ihab Sbeity, Silvia Giordano, Marc Langheinrich, Omran Ayoub</p>
    <p><b>Summary:</b> Machine Learning as a Service (MLaaS) has gained important attraction as a
means for deploying powerful predictive models, offering ease of use that
enables organizations to leverage advanced analytics without substantial
investments in specialized infrastructure or expertise. However, MLaaS
platforms must be safeguarded against security and privacy attacks, such as
model extraction (MEA) attacks. The increasing integration of explainable AI
(XAI) within MLaaS has introduced an additional privacy challenge, as attackers
can exploit model explanations particularly counterfactual explanations (CFs)
to facilitate MEA. In this paper, we investigate the trade offs among model
performance, privacy, and explainability when employing Differential Privacy
(DP), a promising technique for mitigating CF facilitated MEA. We evaluate two
distinct DP strategies: implemented during the classification model training
and at the explainer during CF generation.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.08237v1">Privacy-Preserving Analytics for Smart Meter (AMI) Data: A Hybrid
  Approach to Comply with CPUC Privacy Regulations</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> 
  <p><b>Published on:</b> 2025-05-13T05:30:35Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Benjamin Westrich</p>
    <p><b>Summary:</b> Advanced Metering Infrastructure (AMI) data from smart electric and gas
meters enables valuable insights for utilities and consumers, but also raises
significant privacy concerns. In California, regulatory decisions (CPUC
D.11-07-056 and D.11-08-045) mandate strict privacy protections for customer
energy usage data, guided by the Fair Information Practice Principles (FIPPs).
We comprehensively explore solutions drawn from data anonymization,
privacy-preserving machine learning (differential privacy and federated
learning), synthetic data generation, and cryptographic techniques (secure
multiparty computation, homomorphic encryption). This allows advanced
analytics, including machine learning models, statistical and econometric
analysis on energy consumption data, to be performed without compromising
individual privacy.
  We evaluate each technique's theoretical foundations, effectiveness, and
trade-offs in the context of utility data analytics, and we propose an
integrated architecture that combines these methods to meet real-world needs.
The proposed hybrid architecture is designed to ensure compliance with
California's privacy rules and FIPPs while enabling useful analytics, from
forecasting and personalized insights to academic research and econometrics,
while strictly protecting individual privacy. Mathematical definitions and
derivations are provided where appropriate to demonstrate privacy guarantees
and utility implications rigorously. We include comparative evaluations of the
techniques, an architecture diagram, and flowcharts to illustrate how they work
together in practice. The result is a blueprint for utility data scientists and
engineers to implement privacy-by-design in AMI data handling, supporting both
data-driven innovation and strict regulatory compliance.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.07766v1">Privacy Risks of Robot Vision: A User Study on Image Modalities and
  Resolution</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Robotics-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-05-12T17:16:12Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xuying Huang, Sicong Pan, Maren Bennewitz</p>
    <p><b>Summary:</b> User privacy is a crucial concern in robotic applications, especially when
mobile service robots are deployed in personal or sensitive environments.
However, many robotic downstream tasks require the use of cameras, which may
raise privacy risks. To better understand user perceptions of privacy in
relation to visual data, we conducted a user study investigating how different
image modalities and image resolutions affect users' privacy concerns. The
results show that depth images are broadly viewed as privacy-safe, and a
similarly high proportion of respondents feel the same about semantic
segmentation images. Additionally, the majority of participants consider 32*32
resolution RGB images to be almost sufficiently privacy-preserving, while most
believe that 16*16 resolution can fully guarantee privacy protection.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.07672v2">OnPrem.LLM: A Privacy-Conscious Document Intelligence Toolkit</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-05-12T15:36:27Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Arun S. Maiya</p>
    <p><b>Summary:</b> We present OnPrem$.$LLM, a Python-based toolkit for applying large language
models (LLMs) to sensitive, non-public data in offline or restricted
environments. The system is designed for privacy-preserving use cases and
provides prebuilt pipelines for document processing and storage,
retrieval-augmented generation (RAG), information extraction, summarization,
classification, and prompt/output processing with minimal configuration.
OnPrem$.$LLM supports multiple LLM backends -- including llama$.$cpp, Ollama,
vLLM, and Hugging Face Transformers -- with quantized model support, GPU
acceleration, and seamless backend switching. Although designed for fully local
execution, OnPrem$.$LLM also supports integration with a wide range of cloud
LLM providers when permitted, enabling hybrid deployments that balance
performance with data control. A no-code web interface extends accessibility to
non-technical users.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.07583v1">Privacy-Preserving Real-Time Vietnamese-English Translation on iOS using
  Edge AI</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Software Engineering-D91E36">
  <p><b>Published on:</b> 2025-05-12T14:05:39Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Cong Le</p>
    <p><b>Summary:</b> This research addresses the growing need for privacy-preserving and
accessible language translation by developing a fully offline Neural Machine
Translation (NMT) system for Vietnamese-English translation on iOS devices.
Given increasing concerns about data privacy and unreliable network
connectivity, on-device translation offers critical advantages. This project
confronts challenges in deploying complex NMT models on resource-limited mobile
devices, prioritizing efficiency, accuracy, and a seamless user experience.
Leveraging advances such as MobileBERT and, specifically, the lightweight
\textbf{TinyLlama 1.1B Chat v1.0} in GGUF format, \textbf{a} quantized
Transformer-based model is implemented and optimized. The application is
realized as a real-time iOS prototype, tightly integrating modern iOS
frameworks and privacy-by-design principles. Comprehensive documentation covers
model selection, technical architecture, challenges, and final implementation,
including functional Swift code for deployment.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.07306v1">Enabling Privacy-Aware AI-Based Ergonomic Analysis</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-05-12T07:52:48Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sander De Coninck, Emilio Gamba, Bart Van Doninck, Abdellatif Bey-Temsamani, Sam Leroux, Pieter Simoens</p>
    <p><b>Summary:</b> Musculoskeletal disorders (MSDs) are a leading cause of injury and
productivity loss in the manufacturing industry, incurring substantial economic
costs. Ergonomic assessments can mitigate these risks by identifying workplace
adjustments that improve posture and reduce strain. Camera-based systems offer
a non-intrusive, cost-effective method for continuous ergonomic tracking, but
they also raise significant privacy concerns. To address this, we propose a
privacy-aware ergonomic assessment framework utilizing machine learning
techniques. Our approach employs adversarial training to develop a lightweight
neural network that obfuscates video data, preserving only the essential
information needed for human pose estimation. This obfuscation ensures
compatibility with standard pose estimation algorithms, maintaining high
accuracy while protecting privacy. The obfuscated video data is transmitted to
a central server, where state-of-the-art keypoint detection algorithms extract
body landmarks. Using multi-view integration, 3D keypoints are reconstructed
and evaluated with the Rapid Entire Body Assessment (REBA) method. Our system
provides a secure, effective solution for ergonomic monitoring in industrial
environments, addressing both privacy and workplace safety concerns.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.07085v1">Privacy of Groups in Dense Street Imagery</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Emerging Technologies-F9C80E">
  <p><b>Published on:</b> 2025-05-11T18:16:08Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Matt Franchi, Hauke Sandhaus, Madiha Zahrah Choksi, Severin Engelmann, Wendy Ju, Helen Nissenbaum</p>
    <p><b>Summary:</b> Spatially and temporally dense street imagery (DSI) datasets have grown
unbounded. In 2024, individual companies possessed around 3 trillion unique
images of public streets. DSI data streams are only set to grow as companies
like Lyft and Waymo use DSI to train autonomous vehicle algorithms and analyze
collisions. Academic researchers leverage DSI to explore novel approaches to
urban analysis. Despite good-faith efforts by DSI providers to protect
individual privacy through blurring faces and license plates, these measures
fail to address broader privacy concerns. In this work, we find that increased
data density and advancements in artificial intelligence enable harmful group
membership inferences from supposedly anonymized data. We perform a penetration
test to demonstrate how easily sensitive group affiliations can be inferred
from obfuscated pedestrians in 25,232,608 dashcam images taken in New York
City. We develop a typology of identifiable groups within DSI and analyze
privacy implications through the lens of contextual integrity. Finally, we
discuss actionable recommendations for researchers working with data from DSI
providers.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.07041v1">Empirical Analysis of Asynchronous Federated Learning on Heterogeneous
  Devices: Efficiency, Fairness, and Privacy Trade-offs</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-05-11T16:25:06Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Samaneh Mohammadi, Iraklis Symeonidis, Ali Balador, Francesco Flammini</p>
    <p><b>Summary:</b> Device heterogeneity poses major challenges in Federated Learning (FL), where
resource-constrained clients slow down synchronous schemes that wait for all
updates before aggregation. Asynchronous FL addresses this by incorporating
updates as they arrive, substantially improving efficiency. While its
efficiency gains are well recognized, its privacy costs remain largely
unexplored, particularly for high-end devices that contribute updates more
frequently, increasing their cumulative privacy exposure. This paper presents
the first comprehensive analysis of the efficiency-fairness-privacy trade-off
in synchronous vs. asynchronous FL under realistic device heterogeneity. We
empirically compare FedAvg and staleness-aware FedAsync using a physical
testbed of five edge devices spanning diverse hardware tiers, integrating Local
Differential Privacy (LDP) and the Moments Accountant to quantify per-client
privacy loss. Using Speech Emotion Recognition (SER) as a privacy-critical
benchmark, we show that FedAsync achieves up to 10x faster convergence but
exacerbates fairness and privacy disparities: high-end devices contribute 6-10x
more updates and incur up to 5x higher privacy loss, while low-end devices
suffer amplified accuracy degradation due to infrequent, stale, and
noise-perturbed updates. These findings motivate the need for adaptive FL
protocols that jointly optimize aggregation and privacy mechanisms based on
client capacity and participation dynamics, moving beyond static,
one-size-fits-all solutions.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.06860v1">DP-TRAE: A Dual-Phase Merging Transferable Reversible Adversarial
  Example for Image Privacy Protection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-05-11T06:11:10Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xia Du, Jiajie Zhu, Jizhe Zhou, Chi-man Pun, Zheng Lin, Cong Wu, Zhe Chen, Jun Luo</p>
    <p><b>Summary:</b> In the field of digital security, Reversible Adversarial Examples (RAE)
combine adversarial attacks with reversible data hiding techniques to
effectively protect sensitive data and prevent unauthorized analysis by
malicious Deep Neural Networks (DNNs). However, existing RAE techniques
primarily focus on white-box attacks, lacking a comprehensive evaluation of
their effectiveness in black-box scenarios. This limitation impedes their
broader deployment in complex, dynamic environments. Further more, traditional
black-box attacks are often characterized by poor transferability and high
query costs, significantly limiting their practical applicability. To address
these challenges, we propose the Dual-Phase Merging Transferable Reversible
Attack method, which generates highly transferable initial adversarial
perturbations in a white-box model and employs a memory augmented black-box
strategy to effectively mislead target mod els. Experimental results
demonstrate the superiority of our approach, achieving a 99.0% attack success
rate and 100% recovery rate in black-box scenarios, highlighting its robustness
in privacy protection. Moreover, we successfully implemented a black-box attack
on a commercial model, further substantiating the potential of this approach
for practical use.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.06759v1">Privacy-aware Berrut Approximated Coded Computing applied to general
  distributed learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2025-05-10T21:27:40Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xavier Martínez-Luaña, Manuel Fernández-Veiga, Rebeca P. Díaz-Redondo, Ana Fernández-Vilas</p>
    <p><b>Summary:</b> Coded computing is one of the techniques that can be used for privacy
protection in Federated Learning. However, most of the constructions used for
coded computing work only under the assumption that the computations involved
are exact, generally restricted to special classes of functions, and require
quantized inputs. This paper considers the use of Private Berrut Approximate
Coded Computing (PBACC) as a general solution to add strong but non-perfect
privacy to federated learning. We derive new adapted PBACC algorithms for
centralized aggregation, secure distributed training with centralized data, and
secure decentralized training with decentralized data, thus enlarging
significantly the applications of the method and the existing privacy
protection tools available for these paradigms. Particularly, PBACC can be used
robustly to attain privacy guarantees in decentralized federated learning for a
variety of models. Our numerical results show that the achievable quality of
different learning models (convolutional neural networks, variational
autoencoders, and Cox regression) is minimally altered by using these new
computing schemes, and that the privacy leakage can be bounded strictly to less
than a fraction of one bit per participant. Additionally, the computational
cost of the encoding and decoding processes depends only of the degree of
decentralization of the data.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.06747v1">DPolicy: Managing Privacy Risks Across Multiple Releases with
  Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-10T19:49:51Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Nicolas Küchler, Alexander Viand, Hidde Lycklama, Anwar Hithnawi</p>
    <p><b>Summary:</b> Differential Privacy (DP) has emerged as a robust framework for
privacy-preserving data releases and has been successfully applied in
high-profile cases, such as the 2020 US Census. However, in organizational
settings, the use of DP remains largely confined to isolated data releases.
This approach restricts the potential of DP to serve as a framework for
comprehensive privacy risk management at an organizational level. Although one
might expect that the cumulative privacy risk of isolated releases could be
assessed using DP's compositional property, in practice, individual DP
guarantees are frequently tailored to specific releases, making it difficult to
reason about their interaction or combined impact. At the same time, less
tailored DP guarantees, which compose more easily, also offer only limited
insight because they lead to excessively large privacy budgets that convey
limited meaning. To address these limitations, we present DPolicy, a system
designed to manage cumulative privacy risks across multiple data releases using
DP. Unlike traditional approaches that treat each release in isolation or rely
on a single (global) DP guarantee, our system employs a flexible framework that
considers multiple DP guarantees simultaneously, reflecting the diverse
contexts and scopes typical of real-world DP deployments. DPolicy introduces a
high-level policy language to formalize privacy guarantees, making
traditionally implicit assumptions on scopes and contexts explicit. By deriving
the DP guarantees required to enforce complex privacy semantics from these
high-level policies, DPolicy enables fine-grained privacy risk management on an
organizational scale. We implement and evaluate DPolicy, demonstrating how it
mitigates privacy risks that can emerge without comprehensive,
organization-wide privacy risk management.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.07872v1">Revenue Optimization in Video Caching Networks with Privacy-Preserving
  Demand Predictions</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Networking and Internet Architecture-04E762"> 
  <p><b>Published on:</b> 2025-05-09T21:05:20Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yijing Zhang, Ferdous Pervej, Andreas F. Molisch</p>
    <p><b>Summary:</b> Performance of video streaming, which accounts for most of the traffic in
wireless communication, can be significantly improved by caching popular videos
at the wireless edge. Determining the cache content that optimizes performance
(defined via a revenue function) is thus an important task, and prediction of
the future demands based on past history can make this process much more
efficient. However, since practical video caching networks involve various
parties (e.g., users, isp, and csp) that do not wish to reveal information such
as past history to each other, privacy-preserving solutions are required.
Motivated by this, we propose a proactive caching method based on users'
privacy-preserving multi-slot future demand predictions -- obtained from a
trained Transformer -- to optimize revenue. Specifically, we first use a
privacy-preserving fl algorithm to train a Transformer to predict multi-slot
future demands of the users. However, prediction accuracy is not perfect and
decreases the farther into the future the prediction is done. We model the
impact of prediction errors invoking the file popularities, based on which we
formulate a long-term system revenue optimization to make the cache placement
decisions. As the formulated problem is NP-hard, we use a greedy algorithm to
efficiently obtain an approximate solution. Simulation results validate that
(i) the fl solution achieves results close to the centralized
(non-privacy-preserving) solution and (ii) optimization of revenue may provide
different solutions than the classical chr criterion.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.06122v1">Interaction-Aware Parameter Privacy-Preserving Data Sharing in Coupled
  Systems via Particle Filter Reinforcement Learning</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36">
  <p><b>Published on:</b> 2025-05-09T15:25:48Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Haokun Yu, Jingyuan Zhou, Kaidi Yang</p>
    <p><b>Summary:</b> This paper addresses the problem of parameter privacy-preserving data sharing
in coupled systems, where a data provider shares data with a data user but
wants to protect its sensitive parameters. The shared data affects not only the
data user's decision-making but also the data provider's operations through
system interactions. To trade off control performance and privacy, we propose
an interaction-aware privacy-preserving data sharing approach. Our approach
generates distorted data by minimizing a combination of (i) mutual information,
quantifying privacy leakage of sensitive parameters, and (ii) the impact of
distorted data on the data provider's control performance, considering the
interactions between stakeholders. The optimization problem is formulated into
a Bellman equation and solved by a particle filter reinforcement learning
(RL)-based approach. Compared to existing RL-based methods, our formulation
significantly reduces history dependency and efficiently handles scenarios with
continuous state space. Validated in a mixed-autonomy platoon scenario, our
method effectively protects sensitive driving behavior parameters of
human-driven vehicles (HDVs) against inference attacks while maintaining
negligible impact on fuel efficiency.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.05922v2">Cape: Context-Aware Prompt Perturbation Mechanism with Differential
  Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-05-09T09:54:07Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Haoqi Wu, Wei Dai, Li Wang, Qiang Yan</p>
    <p><b>Summary:</b> Large Language Models (LLMs) have gained significant popularity due to their
remarkable capabilities in text understanding and generation. However, despite
their widespread deployment in inference services such as ChatGPT, concerns
about the potential leakage of sensitive user data have arisen. Existing
solutions primarily rely on privacy-enhancing technologies to mitigate such
risks, facing the trade-off among efficiency, privacy, and utility. To narrow
this gap, we propose Cape, a context-aware prompt perturbation mechanism based
on differential privacy, to enable efficient inference with an improved
privacy-utility trade-off. Concretely, we introduce a hybrid utility function
that better captures the token similarity. Additionally, we propose a
bucketized sampling mechanism to handle large sampling space, which might lead
to long-tail phenomenons. Extensive experiments across multiple datasets, along
with ablation studies, demonstrate that Cape achieves a better privacy-utility
trade-off compared to prior state-of-the-art works.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.05920v1">Privacy-Preserving Credit Card Approval Using Homomorphic SVM: Toward
  Secure Inference in FinTech Applications</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-09T09:46:56Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b>  Faneela, Baraq Ghaleb, Jawad Ahmad, William J. Buchanan, Sana Ullah Jan</p>
    <p><b>Summary:</b> The growing use of machine learning in cloud environments raises critical
concerns about data security and privacy, especially in finance. Fully
Homomorphic Encryption (FHE) offers a solution by enabling computations on
encrypted data, but its high computational cost limits practicality. In this
paper, we propose PP-FinTech, a privacy-preserving scheme for financial
applications that employs a CKKS-based encrypted soft-margin SVM, enhanced with
a hybrid kernel for modeling non-linear patterns and an adaptive thresholding
mechanism for robust encrypted classification. Experiments on the Credit Card
Approval dataset demonstrate comparable performance to the plaintext models,
highlighting PP-FinTech's ability to balance privacy, and efficiency in secure
financial ML systems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.05859v1">Integrating Building Thermal Flexibility Into Distribution System: A
  Privacy-Preserved Dispatch Approach</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36">
  <p><b>Published on:</b> 2025-05-09T07:53:08Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shuai Lu, Zeyin Hou, Wei Gu, Yijun Xu</p>
    <p><b>Summary:</b> The inherent thermal storage capacity of buildings brings considerable
thermal flexibility to the heating/cooling loads, which are promising demand
response resources for power systems. It is widely believed that integrating
the thermal flexibility of buildings into the distribution system can improve
the operating economy and reliability of the system. However, the private
information of the buildings needs to be transferred to the distribution system
operator (DSO) to achieve a coordinated optimization, bringing serious privacy
concerns to users. Given this issue, we propose a novel privacy-preserved
optimal dispatch approach for the distribution system incorporating buildings.
Using it, the DSO can exploit the thermal flexibility of buildings without
accessing their private information, such as model parameters and indoor
temperature profiles. Specifically, we first develop an optimal dispatch model
for the distribution system integrating buildings, which can be extended to
other storage-like flexibility resources. Second, we reveal that the
privacy-preserved integration of buildings is a joint privacy preservation
problem for both parameters and state variables and then design a
privacy-preserved algorithm based on transformation-based encryption,
constraint relaxation, and constraint extension techniques. Besides, we
implement a detailed privacy analysis for the proposed method, considering both
semi-honest adversaries and external eavesdroppers. Case studies demonstrate
the accuracy, privacy-preserved performance, and computational efficiency of
the proposed method.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.05843v1">Enhancing Noisy Functional Encryption for Privacy-Preserving Machine
  Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-09T07:33:09Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Linda Scheu-Hachtel, Jasmin Zalonis</p>
    <p><b>Summary:</b> Functional encryption (FE) has recently attracted interest in
privacy-preserving machine learning (PPML) for its unique ability to compute
specific functions on encrypted data. A related line of work focuses on noisy
FE, which ensures differential privacy in the output while keeping the data
encrypted. We extend the notion of noisy multi-input functional encryption
(NMIFE) to (dynamic) noisy multi-client functional encryption ((Dy)NMCFE),
which allows for more flexibility in the number of data holders and analyses,
while protecting the privacy of the data holder with fine-grained access
through the usage of labels. Following our new definition of DyNMCFE, we
present DyNo, a concrete inner-product DyNMCFE scheme. Our scheme captures all
the functionalities previously introduced in noisy FE schemes, while being
significantly more efficient in terms of space and runtime and fulfilling a
stronger security notion by allowing the corruption of clients. To further
prove the applicability of DyNMCFE, we present a protocol for PPML based on
DyNo. According to this protocol, we train a privacy-preserving logistic
regression.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.05816v1">On the Price of Differential Privacy for Spectral Clustering over
  Stochastic Block Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Social and Information Networks-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> 
  <p><b>Published on:</b> 2025-05-09T06:34:56Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Antti Koskela, Mohamed Seif, Andrea J. Goldsmith</p>
    <p><b>Summary:</b> We investigate privacy-preserving spectral clustering for community detection
within stochastic block models (SBMs). Specifically, we focus on edge
differential privacy (DP) and propose private algorithms for community
recovery. Our work explores the fundamental trade-offs between the privacy
budget and the accurate recovery of community labels. Furthermore, we establish
information-theoretic conditions that guarantee the accuracy of our methods,
providing theoretical assurances for successful community recovery under edge
DP.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.05707v1">Crowding Out The Noise: Algorithmic Collective Action Under Differential
  Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-09T00:55:12Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Rushabh Solanki, Meghana Bhange, Ulrich Aïvodji, Elliot Creager</p>
    <p><b>Summary:</b> The integration of AI into daily life has generated considerable attention
and excitement, while also raising concerns about automating algorithmic harms
and re-entrenching existing social inequities. While the responsible deployment
of trustworthy AI systems is a worthy goal, there are many possible ways to
realize it, from policy and regulation to improved algorithm design and
evaluation. In fact, since AI trains on social data, there is even a
possibility for everyday users, citizens, or workers to directly steer its
behavior through Algorithmic Collective Action, by deliberately modifying the
data they share with a platform to drive its learning process in their favor.
This paper considers how these grassroots efforts to influence AI interact with
methods already used by AI firms and governments to improve model
trustworthiness. In particular, we focus on the setting where the AI firm
deploys a differentially private model, motivated by the growing regulatory
focus on privacy and data protection. We investigate how the use of
Differentially Private Stochastic Gradient Descent (DPSGD) affects the
collective's ability to influence the learning process. Our findings show that
while differential privacy contributes to the protection of individual data, it
introduces challenges for effective algorithmic collective action. We
characterize lower bounds on the success of algorithmic collective action under
differential privacy as a function of the collective's size and the firm's
privacy parameters, and verify these trends experimentally by simulating
collective action during the training of deep neural network classifiers across
several datasets.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.05648v1">Privacy-Preserving Transformers: SwiftKey's Differential Privacy
  Implementation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-05-08T21:08:04Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Abdelrahman Abouelenin, Mohamed Abdelrehim, Raffy Fahim, Amr Hendy, Mohamed Afify</p>
    <p><b>Summary:</b> In this paper we train a transformer using differential privacy (DP) for
language modeling in SwiftKey. We run multiple experiments to balance the
trade-off between the model size, run-time speed and accuracy. We show that we
get small and consistent gains in the next-word-prediction and accuracy with
graceful increase in memory and speed compared to the production GRU. This is
obtained by scaling down a GPT2 architecture to fit the required size and a two
stage training process that builds a seed model on general data and DP
finetunes it on typing data. The transformer is integrated using ONNX offering
both flexibility and efficiency.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.05613v1">Optimal Regret of Bernoulli Bandits under Global Differential Privacy</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">  <img alt="Category Badge" src="https://img.shields.io/badge/Statistics Theory-D91E36"> 
  <p><b>Published on:</b> 2025-05-08T19:48:58Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Achraf Azize, Yulian Wu, Junya Honda, Francesco Orabona, Shinji Ito, Debabrota Basu</p>
    <p><b>Summary:</b> As sequential learning algorithms are increasingly applied to real life,
ensuring data privacy while maintaining their utilities emerges as a timely
question. In this context, regret minimisation in stochastic bandits under
$\epsilon$-global Differential Privacy (DP) has been widely studied. Unlike
bandits without DP, there is a significant gap between the best-known regret
lower and upper bound in this setting, though they "match" in order. Thus, we
revisit the regret lower and upper bounds of $\epsilon$-global DP algorithms
for Bernoulli bandits and improve both. First, we prove a tighter regret lower
bound involving a novel information-theoretic quantity characterising the
hardness of $\epsilon$-global DP in stochastic bandits. Our lower bound
strictly improves on the existing ones across all $\epsilon$ values. Then, we
choose two asymptotically optimal bandit algorithms, i.e. DP-KLUCB and DP-IMED,
and propose their DP versions using a unified blueprint, i.e., (a) running in
arm-dependent phases, and (b) adding Laplace noise to achieve privacy. For
Bernoulli bandits, we analyse the regrets of these algorithms and show that
their regrets asymptotically match our lower bound up to a constant arbitrary
close to 1. This refutes the conjecture that forgetting past rewards is
necessary to design optimal bandit algorithms under global DP. At the core of
our algorithms lies a new concentration inequality for sums of Bernoulli
variables under Laplace mechanism, which is a new DP version of the Chernoff
bound. This result is universally useful as the DP literature commonly treats
the concentrations of Laplace noise and random variables separately, while we
couple them to yield a tighter bound.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.05214v1">Overcoming the hurdle of legal expertise: A reusable model for
  smartwatch privacy policies</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Software Engineering-D91E36">
  <p><b>Published on:</b> 2025-05-08T13:09:12Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Constantin Buschhaus, Arvid Butting, Judith Michael, Verena Nitsch, Sebastian Pütz, Bernhard Rumpe, Carolin Stellmacher, Sabine Theis</p>
    <p><b>Summary:</b> Regulations for privacy protection aim to protect individuals from the
unauthorized storage, processing, and transfer of their personal data but
oftentimes fail in providing helpful support for understanding these
regulations. To better communicate privacy policies for smartwatches, we need
an in-depth understanding of their concepts and provide better ways to enable
developers to integrate them when engineering systems. Up to now, no conceptual
model exists covering privacy statements from different smartwatch
manufacturers that is reusable for developers. This paper introduces such a
conceptual model for privacy policies of smartwatches and shows its use in a
model-driven software engineering approach to create a platform for data
visualization of wearable privacy policies from different smartwatch
manufacturers. We have analyzed the privacy policies of various manufacturers
and extracted the relevant concepts. Moreover, we have checked the model with
lawyers for its correctness, instantiated it with concrete data, and used it in
a model-driven software engineering approach to create a platform for data
visualization. This reusable privacy policy model can enable developers to
easily represent privacy policies in their systems. This provides a foundation
for more structured and understandable privacy policies which, in the long run,
can increase the data sovereignty of application users.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.05155v1">FedTDP: A Privacy-Preserving and Unified Framework for Trajectory Data
  Preparation via Federated Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-08T11:51:23Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhihao Zeng, Ziquan Fang, Wei Shao, Lu Chen, Yunjun Gao</p>
    <p><b>Summary:</b> Trajectory data, which capture the movement patterns of people and vehicles
over time and space, are crucial for applications like traffic optimization and
urban planning. However, issues such as noise and incompleteness often
compromise data quality, leading to inaccurate trajectory analyses and limiting
the potential of these applications. While Trajectory Data Preparation (TDP)
can enhance data quality, existing methods suffer from two key limitations: (i)
they do not address data privacy concerns, particularly in federated settings
where trajectory data sharing is prohibited, and (ii) they typically design
task-specific models that lack generalizability across diverse TDP scenarios.
To overcome these challenges, we propose FedTDP, a privacy-preserving and
unified framework that leverages the capabilities of Large Language Models
(LLMs) for TDP in federated environments. Specifically, we: (i) design a
trajectory privacy autoencoder to secure data transmission and protect privacy,
(ii) introduce a trajectory knowledge enhancer to improve model learning of
TDP-related knowledge, enabling the development of TDP-oriented LLMs, and (iii)
propose federated parallel optimization to enhance training efficiency by
reducing data transmission and enabling parallel model training. Experiments on
6 real datasets and 10 mainstream TDP tasks demonstrate that FedTDP
consistently outperforms 13 state-of-the-art baselines.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.05031v1">LSRP: A Leader-Subordinate Retrieval Framework for Privacy-Preserving
  Cloud-Device Collaboration</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Retrieval-5BC0EB">
  <p><b>Published on:</b> 2025-05-08T08:06:34Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yingyi Zhang, Pengyue Jia, Xianneng Li, Derong Xu, Maolin Wang, Yichao Wang, Zhaocheng Du, Huifeng Guo, Yong Liu, Ruiming Tang, Xiangyu Zhao</p>
    <p><b>Summary:</b> Cloud-device collaboration leverages on-cloud Large Language Models (LLMs)
for handling public user queries and on-device Small Language Models (SLMs) for
processing private user data, collectively forming a powerful and
privacy-preserving solution. However, existing approaches often fail to fully
leverage the scalable problem-solving capabilities of on-cloud LLMs while
underutilizing the advantage of on-device SLMs in accessing and processing
personalized data. This leads to two interconnected issues: 1) Limited
utilization of the problem-solving capabilities of on-cloud LLMs, which fail to
align with personalized user-task needs, and 2) Inadequate integration of user
data into on-device SLM responses, resulting in mismatches in contextual user
information.
  In this paper, we propose a Leader-Subordinate Retrieval framework for
Privacy-preserving cloud-device collaboration (LSRP), a novel solution that
bridges these gaps by: 1) enhancing on-cloud LLM guidance to on-device SLM
through a dynamic selection of task-specific leader strategies named as
user-to-user retrieval-augmented generation (U-U-RAG), and 2) integrating the
data advantages of on-device SLMs through small model feedback Direct
Preference Optimization (SMFB-DPO) for aligning the on-cloud LLM with the
on-device SLM. Experiments on two datasets demonstrate that LSRP consistently
outperforms state-of-the-art baselines, significantly improving question-answer
relevance and personalization, while preserving user privacy through efficient
on-device retrieval. Our code is available at:
https://github.com/Zhang-Yingyi/LSRP.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.06305v1">User Behavior Analysis in Privacy Protection with Large Language Models:
  A Study on Privacy Preferences with Limited Data</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-05-08T04:42:17Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Haowei Yang, Qingyi Lu, Yang Wang, Sibei Liu, Jiayun Zheng, Ao Xiang</p>
    <p><b>Summary:</b> With the widespread application of large language models (LLMs), user privacy
protection has become a significant research topic. Existing privacy preference
modeling methods often rely on large-scale user data, making effective privacy
preference analysis challenging in data-limited environments. This study
explores how LLMs can analyze user behavior related to privacy protection in
scenarios with limited data and proposes a method that integrates Few-shot
Learning and Privacy Computing to model user privacy preferences. The research
utilizes anonymized user privacy settings data, survey responses, and simulated
data, comparing the performance of traditional modeling approaches with
LLM-based methods. Experimental results demonstrate that, even with limited
data, LLMs significantly improve the accuracy of privacy preference modeling.
Additionally, incorporating Differential Privacy and Federated Learning further
reduces the risk of user data exposure. The findings provide new insights into
the application of LLMs in privacy protection and offer theoretical support for
advancing privacy computing and user behavior analysis.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.05519v1">Real-Time Privacy Preservation for Robot Visual Perception</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-05-08T03:27:12Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Minkyu Choi, Yunhao Yang, Neel P. Bhatt, Kushagra Gupta, Sahil Shah, Aditya Rai, David Fridovich-Keil, Ufuk Topcu, Sandeep P. Chinchali</p>
    <p><b>Summary:</b> Many robots (e.g., iRobot's Roomba) operate based on visual observations from
live video streams, and such observations may inadvertently include
privacy-sensitive objects, such as personal identifiers. Existing approaches
for preserving privacy rely on deep learning models, differential privacy, or
cryptography. They lack guarantees for the complete concealment of all
sensitive objects. Guaranteeing concealment requires post-processing techniques
and thus is inadequate for real-time video streams. We develop a method for
privacy-constrained video streaming, PCVS, that conceals sensitive objects
within real-time video streams. PCVS takes a logical specification constraining
the existence of privacy-sensitive objects, e.g., never show faces when a
person exists. It uses a detection model to evaluate the existence of these
objects in each incoming frame. Then, it blurs out a subset of objects such
that the existence of the remaining objects satisfies the specification. We
then propose a conformal prediction approach to (i) establish a theoretical
lower bound on the probability of the existence of these objects in a sequence
of frames satisfying the specification and (ii) update the bound with the
arrival of each subsequent frame. Quantitative evaluations show that PCVS
achieves over 95 percent specification satisfaction rate in multiple datasets,
significantly outperforming other methods. The satisfaction rate is
consistently above the theoretical bounds across all datasets, indicating that
the established bounds hold. Additionally, we deploy PCVS on robots in
real-time operation and show that the robots operate normally without being
compromised when PCVS conceals objects.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.04889v1">FedRE: Robust and Effective Federated Learning with Privacy Preference</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-08T01:50:27Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tianzhe Xiao, Yichen Li, Yu Zhou, Yining Qi, Yi Liu, Wei Wang, Haozhao Wang, Yi Wang, Ruixuan Li</p>
    <p><b>Summary:</b> Despite Federated Learning (FL) employing gradient aggregation at the server
for distributed training to prevent the privacy leakage of raw data, private
information can still be divulged through the analysis of uploaded gradients
from clients. Substantial efforts have been made to integrate local
differential privacy (LDP) into the system to achieve a strict privacy
guarantee. However, existing methods fail to take practical issues into account
by merely perturbing each sample with the same mechanism while each client may
have their own privacy preferences on privacy-sensitive information (PSI),
which is not uniformly distributed across the raw data. In such a case,
excessive privacy protection from private-insensitive information can
additionally introduce unnecessary noise, which may degrade the model
performance. In this work, we study the PSI within data and develop FedRE, that
can simultaneously achieve robustness and effectiveness benefits with LDP
protection. More specifically, we first define PSI with regard to the privacy
preferences of each client. Then, we optimize the LDP by allocating less
privacy budget to gradients with higher PSI in a layer-wise manner, thus
providing a stricter privacy guarantee for PSI. Furthermore, to mitigate the
performance degradation caused by LDP, we design a parameter aggregation
mechanism based on the distribution of the perturbed information. We conducted
experiments with text tamper detection on T-SROIE and DocTamper datasets, and
FedRE achieves competitive performance compared to state-of-the-art methods.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.04799v1">Safeguard-by-Development: A Privacy-Enhanced Development Paradigm for
  Multi-Agent Collaboration Systems</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-07T20:54:43Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jian Cui, Zichuan Li, Luyi Xing, Xiaojing Liao</p>
    <p><b>Summary:</b> Multi-agent collaboration systems (MACS), powered by large language models
(LLMs), solve complex problems efficiently by leveraging each agent's
specialization and communication between agents. However, the inherent exchange
of information between agents and their interaction with external environments,
such as LLM, tools, and users, inevitably introduces significant risks of
sensitive data leakage, including vulnerabilities to attacks like prompt
injection and reconnaissance. Existing MACS fail to enable privacy controls,
making it challenging to manage sensitive information securely. In this paper,
we take the first step to address the MACS's data leakage threat at the system
development level through a privacy-enhanced development paradigm, Maris. Maris
enables rigorous message flow control within MACS by embedding reference
monitors into key multi-agent conversation components. We implemented Maris as
an integral part of AutoGen, a widely adopted open-source multi-agent
development framework. Then, we evaluate Maris for its effectiveness and
performance overhead on privacy-critical MACS use cases, including healthcare,
supply chain optimization, and personalized recommendation system. The result
shows that Maris achieves satisfactory effectiveness, performance overhead and
practicability for adoption.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.04570v1">Privacy-preserving neutral atom-based quantum classifier towards real
  healthcare applications</a></h3>
    
  <p><b>Published on:</b> 2025-05-07T17:03:35Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ettore Canonici, Filippo Caruso</p>
    <p><b>Summary:</b> Technological advances in Artificial Intelligence (AI) and Machine Learning
(ML) for the healthcare domain are rapidly arising, with a growing discussion
regarding the ethical management of their development. In general, ML
healthcare applications crucially require performance, interpretability of
data, and respect for data privacy. The latter is an increasingly debated topic
as commercial cloud computing services become more and more widespread.
Recently, dedicated methods are starting to be developed aiming to protect data
privacy. However, these generally result in a trade-off forcing one to balance
the level of data privacy and the algorithm performance. Here, a Support Vector
Machine (SVM) classifier model is proposed whose training is reformulated into
a Quadratic Unconstrained Binary Optimization (QUBO) problem, and adapted to a
neutral atom-based Quantum Processing Unit (QPU). Our final model does not
require anonymization techniques to protect data privacy since the sensitive
data are not needed to be transferred to the cloud-available QPU. Indeed, the
latter is used only during the training phase, hence allowing a future concrete
application in a real-world scenario. Finally, performance and scaling analyses
on a publicly available breast cancer dataset are discussed, both using ideal
and noisy simulations for the training process, and also successfully tested on
a currently available real neutral-atom QPU.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.04361v1">RDPP-TD: Reputation and Data Privacy-Preserving based Truth Discovery
  Scheme in Mobile Crowdsensing</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computational Engineering, Finance, and Science-5BC0EB">
  <p><b>Published on:</b> 2025-05-07T12:20:55Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Lijian Wu, Weikun Xie, Wei Tan, Tian Wang, Houbing Herbert Song, Anfeng Liu</p>
    <p><b>Summary:</b> Truth discovery (TD) plays an important role in Mobile Crowdsensing (MCS).
However, existing TD methods, including privacy-preserving TD approaches,
estimate the truth by weighting only the data submitted in the current round,
which often results in low data quality. Moreover, there is a lack of effective
TD methods that preserve both reputation and data privacy. To address these
issues, a Reputation and Data Privacy-Preserving based Truth Discovery
(RDPP-TD) scheme is proposed to obtain high-quality data for MCS. The RDPP-TD
scheme consists of two key approaches: a Reputation-based Truth Discovery (RTD)
approach, which integrates the weight of current-round data with workers'
reputation values to estimate the truth, thereby achieving more accurate
results, and a Reputation and Data Privacy-Preserving (RDPP) approach, which
ensures privacy preservation for sensing data and reputation values. First, the
RDPP approach, when seamlessly integrated with RTD, can effectively evaluate
the reliability of workers and their sensing data in a privacy-preserving
manner. Second, the RDPP scheme supports reputation-based worker recruitment
and rewards, ensuring high-quality data collection while incentivizing workers
to provide accurate information. Comprehensive theoretical analysis and
extensive experiments based on real-world datasets demonstrate that the
proposed RDPP-TD scheme provides strong privacy protection and improves data
quality by up to 33.3%.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.04181v1">Privacy Challenges In Image Processing Applications</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-07T07:28:03Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b>  Maneesha, Bharat Gupta, Rishabh Sethi, Charvi Adita Das</p>
    <p><b>Summary:</b> As image processing systems proliferate, privacy concerns intensify given the
sensitive personal information contained in images. This paper examines privacy
challenges in image processing and surveys emerging privacy-preserving
techniques including differential privacy, secure multiparty computation,
homomorphic encryption, and anonymization. Key applications with heightened
privacy risks include healthcare, where medical images contain patient health
data, and surveillance systems that can enable unwarranted tracking.
Differential privacy offers rigorous privacy guarantees by injecting controlled
noise, while MPC facilitates collaborative analytics without exposing raw data
inputs. Homomorphic encryption enables computations on encrypted data and
anonymization directly removes identifying elements. However, balancing privacy
protections and utility remains an open challenge. Promising future directions
identified include quantum-resilient cryptography, federated learning,
dedicated hardware, and conceptual innovations like privacy by design.
Ultimately, a holistic effort combining technological innovations, ethical
considerations, and policy frameworks is necessary to uphold the fundamental
right to privacy as image processing capabilities continue advancing rapidly.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.04034v1">Izhikevich-Inspired Temporal Dynamics for Enhancing Privacy, Efficiency,
  and Transferability in Spiking Neural Networks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Neural and Evolutionary Computing-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-05-07T00:27:00Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ayana Moshruba, Hamed Poursiami, Maryam Parsa</p>
    <p><b>Summary:</b> Biological neurons exhibit diverse temporal spike patterns, which are
believed to support efficient, robust, and adaptive neural information
processing. While models such as Izhikevich can replicate a wide range of these
firing dynamics, their complexity poses challenges for directly integrating
them into scalable spiking neural networks (SNN) training pipelines. In this
work, we propose two probabilistically driven, input-level temporal spike
transformations: Poisson-Burst and Delayed-Burst that introduce biologically
inspired temporal variability directly into standard Leaky Integrate-and-Fire
(LIF) neurons. This enables scalable training and systematic evaluation of how
spike timing dynamics affect privacy, generalization, and learning performance.
Poisson-Burst modulates burst occurrence based on input intensity, while
Delayed-Burst encodes input strength through burst onset timing. Through
extensive experiments across multiple benchmarks, we demonstrate that
Poisson-Burst maintains competitive accuracy and lower resource overhead while
exhibiting enhanced privacy robustness against membership inference attacks,
whereas Delayed-Burst provides stronger privacy protection at a modest accuracy
trade-off. These findings highlight the potential of biologically grounded
temporal spike dynamics in improving the privacy, generalization and biological
plausibility of neuromorphic learning systems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.03639v1">Differential Privacy for Network Assortativity</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-06T15:40:47Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Fei Ma, Jinzhi Ouyang, Xincheng Hu</p>
    <p><b>Summary:</b> The analysis of network assortativity is of great importance for
understanding the structural characteristics of and dynamics upon networks.
Often, network assortativity is quantified using the assortativity coefficient
that is defined based on the Pearson correlation coefficient between vertex
degrees. It is well known that a network may contain sensitive information,
such as the number of friends of an individual in a social network (which is
abstracted as the degree of vertex.). So, the computation of the assortativity
coefficient leads to privacy leakage, which increases the urgent need for
privacy-preserving protocol. However, there has been no scheme addressing the
concern above.
  To bridge this gap, in this work, we are the first to propose approaches
based on differential privacy (DP for short). Specifically, we design three
DP-based algorithms: $Local_{ru}$, $Shuffle_{ru}$, and $Decentral_{ru}$. The
first two algorithms, based on Local DP (LDP) and Shuffle DP respectively, are
designed for settings where each individual only knows his/her direct friends.
In contrast, the third algorithm, based on Decentralized DP (DDP), targets
scenarios where each individual has a broader view, i.e., also knowing his/her
friends' friends. Theoretically, we prove that each algorithm enables an
unbiased estimation of the assortativity coefficient of the network. We further
evaluate the performance of the proposed algorithms using mean squared error
(MSE), showing that $Shuffle_{ru}$ achieves the best performance, followed by
$Decentral_{ru}$, with $Local_{ru}$ performing the worst. Note that these three
algorithms have different assumptions, so each has its applicability scenario.
Lastly, we conduct extensive numerical simulations, which demonstrate that the
presented approaches are adequate to achieve the estimation of network
assortativity under the demand for privacy protection.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.02975v1">Navigating Privacy and Trust: AI Assistants as Social Support for Older
  Adults</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-05-05T19:00:14Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Karina LaRubbio, Malcolm Grba, Diana Freed</p>
    <p><b>Summary:</b> AI assistants are increasingly integrated into older adults' daily lives,
offering new opportunities for social support and accessibility while raising
important questions about privacy, autonomy, and trust. As these systems become
embedded in caregiving and social networks, older adults must navigate
trade-offs between usability, data privacy, and personal agency across
different interaction contexts. Although prior work has explored AI assistants'
potential benefits, further research is needed to understand how perceived
usefulness and risk shape adoption and engagement. This paper examines these
dynamics and advocates for participatory design approaches that position older
adults as active decision makers in shaping AI assistant functionality. By
advancing a framework for privacy-aware, user-centered AI design, this work
contributes to ongoing discussions on developing ethical and transparent AI
systems that enhance well-being without compromising user control.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.02828v1">Privacy Risks and Preservation Methods in Explainable Artificial
  Intelligence: A Scoping Review</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Emerging Technologies-F9C80E">
  <p><b>Published on:</b> 2025-05-05T17:53:28Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sonal Allana, Mohan Kankanhalli, Rozita Dara</p>
    <p><b>Summary:</b> Explainable Artificial Intelligence (XAI) has emerged as a pillar of
Trustworthy AI and aims to bring transparency in complex models that are opaque
by nature. Despite the benefits of incorporating explanations in models, an
urgent need is found in addressing the privacy concerns of providing this
additional information to end users. In this article, we conduct a scoping
review of existing literature to elicit details on the conflict between privacy
and explainability. Using the standard methodology for scoping review, we
extracted 57 articles from 1,943 studies published from January 2019 to
December 2024. The review addresses 3 research questions to present readers
with more understanding of the topic: (1) what are the privacy risks of
releasing explanations in AI systems? (2) what current methods have researchers
employed to achieve privacy preservation in XAI systems? (3) what constitutes a
privacy preserving explanation? Based on the knowledge synthesized from the
selected studies, we categorize the privacy risks and preservation methods in
XAI and propose the characteristics of privacy preserving explanations to aid
researchers and practitioners in understanding the requirements of XAI that is
privacy compliant. Lastly, we identify the challenges in balancing privacy with
other system desiderata and provide recommendations for achieving privacy
preserving XAI. We expect that this review will shed light on the complex
relationship of privacy and explainability, both being the fundamental
principles of Trustworthy AI.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.02798v1">Unifying Laplace Mechanism with Instance Optimality in Differential
  Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B">
  <p><b>Published on:</b> 2025-05-05T17:20:28Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> David Durfee</p>
    <p><b>Summary:</b> We adapt the canonical Laplace mechanism, widely used in differentially
private data analysis, to achieve near instance optimality with respect to the
hardness of the underlying dataset. In particular, we construct a piecewise
Laplace distribution whereby we defy traditional assumptions and show that
Laplace noise can in fact be drawn proportional to the local sensitivity when
done in a piecewise manner. While it may initially seem counterintuitive that
this satisfies (pure) differential privacy and can be sampled, we provide both
through a simple connection to the exponential mechanism and inverse
sensitivity along with the fact that the Laplace distribution is a two-sided
exponential distribution. As a result, we prove that in the continuous setting
our \textit{piecewise Laplace mechanism} strictly dominates the inverse
sensitivity mechanism, which was previously shown to both be nearly instance
optimal and uniformly outperform the smooth sensitivity framework. Furthermore,
in the worst-case where all local sensitivities equal the global sensitivity,
our method simply reduces to a Laplace mechanism. We also complement this with
an approximate local sensitivity variant to potentially ease the computational
cost, which can also extend to higher dimensions.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.02513v1">Trustworthy Inter-Provider Agreements in 6G Using a Privacy-Enabled
  Hybrid Blockchain Framework</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Networking and Internet Architecture-04E762">
  <p><b>Published on:</b> 2025-05-05T09:46:30Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Farhana Javed, Josep Mangues-Bafalluy</p>
    <p><b>Summary:</b> Inter-provider agreements are central to 6G networks, where administrative
domains must securely and dynamically share services. To address the dual need
for transparency and confidentiality, we propose a privacy-enabled hybrid
blockchain setup using Hyperledger Besu, integrating both public and private
transaction workflows. The system enables decentralized service registration,
selection, and SLA breach reporting through role-based smart contracts and
privacy groups. We design and deploy a proof-of-concept implementation,
evaluating performance using end-to-end latency as a key metric within privacy
groups. Results show that public interactions maintain stable latency, while
private transactions incur additional overhead due to off-chain coordination.
The block production rate governed by IBFT 2.0 had limited impact on private
transaction latency, due to encryption and peer synchronization. Lessons
learned highlight design considerations for smart contract structure, validator
management, and scalability patterns suitable for dynamic inter-domain
collaboration. Our findings offer practical insights for deploying trustworthy
agreement systems in 6G networks using privacy-enabled hybrid blockchains.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.02392v2">Moneros Decentralized P2P Exchanges: Functionality, Adoption, and
  Privacy Risks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-05T06:27:37Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yannik Kopyciok, Friedhelm Victor, Stefan Schmid</p>
    <p><b>Summary:</b> Privacy-focused cryptocurrencies like Monero remain popular, despite
increasing regulatory scrutiny that has led to their delisting from major
centralized exchanges. The latter also explains the recent popularity of
decentralized exchanges (DEXs) with no centralized ownership structures. These
platforms typically leverage peer-to-peer (P2P) networks, promising secure and
anonymous asset trading. However, questions of liability remain, and the
academic literature lacks comprehensive insights into the functionality,
trading activity, and privacy claims of these P2P platforms. In this paper, we
provide an early systematization of the current landscape of decentralized
peer-to-peer exchanges within the Monero ecosystem. We examine several recently
developed DEX platforms, analyzing their popularity, functionality,
architectural choices, and potential weaknesses. We further identify and report
on a privacy vulnerability in the recently popularized Haveno exchange,
demonstrating that certain Haveno trades could be detected, allowing
transactions to be linked across the Monero and Bitcoin blockchains. We hope
that our findings can nourish the discussion in the research community about
more secure designs, and provide insights for regulators.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.02383v1">Connecting Thompson Sampling and UCB: Towards More Efficient Trade-offs
  Between Privacy and Regret</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-05-05T05:48:52Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Bingshan Hu, Zhiming Huang, Tianyue H. Zhang, Mathias Lécuyer, Nidhi Hegde</p>
    <p><b>Summary:</b> We address differentially private stochastic bandit problems from the angles
of exploring the deep connections among Thompson Sampling with Gaussian priors,
Gaussian mechanisms, and Gaussian differential privacy (GDP). We propose
DP-TS-UCB, a novel parametrized private bandit algorithm that enables to trade
off privacy and regret. DP-TS-UCB satisfies $ \tilde{O}
\left(T^{0.25(1-\alpha)}\right)$-GDP and enjoys an $O
\left(K\ln^{\alpha+1}(T)/\Delta \right)$ regret bound, where $\alpha \in [0,1]$
controls the trade-off between privacy and regret. Theoretically, our DP-TS-UCB
relies on anti-concentration bounds of Gaussian distributions and links
exploration mechanisms in Thompson Sampling-based algorithms and Upper
Confidence Bound-based algorithms, which may be of independent interest.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.01976v1">A Survey on Privacy Risks and Protection in Large Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-04T03:04:07Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kang Chen, Xiuze Zhou, Yuanguo Lin, Shibo Feng, Li Shen, Pengcheng Wu</p>
    <p><b>Summary:</b> Although Large Language Models (LLMs) have become increasingly integral to
diverse applications, their capabilities raise significant privacy concerns.
This survey offers a comprehensive overview of privacy risks associated with
LLMs and examines current solutions to mitigate these challenges. First, we
analyze privacy leakage and attacks in LLMs, focusing on how these models
unintentionally expose sensitive information through techniques such as model
inversion, training data extraction, and membership inference. We investigate
the mechanisms of privacy leakage, including the unauthorized extraction of
training data and the potential exploitation of these vulnerabilities by
malicious actors. Next, we review existing privacy protection against such
risks, such as inference detection, federated learning, backdoor mitigation,
and confidential computing, and assess their effectiveness in preventing
privacy leakage. Furthermore, we highlight key practical challenges and propose
future research directions to develop secure and privacy-preserving LLMs,
emphasizing privacy risk assessment, secure knowledge transfer between models,
and interdisciplinary frameworks for privacy governance. Ultimately, this
survey aims to establish a roadmap for addressing escalating privacy challenges
in the LLMs domain.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.01879v1">What to Do When Privacy Is Gone</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2025-05-03T17:51:36Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> James Brusseau</p>
    <p><b>Summary:</b> Today's ethics of privacy is largely dedicated to defending personal
information from big data technologies. This essay goes in the other direction.
It considers the struggle to be lost, and explores two strategies for living
after privacy is gone. First, total exposure embraces privacy's decline, and
then contributes to the process with transparency. All personal information is
shared without reservation. The resulting ethics is explored through a big data
version of Robert Nozick's Experience Machine thought experiment. Second,
transient existence responds to privacy's loss by ceaselessly generating new
personal identities, which translates into constantly producing temporarily
unviolated private information. The ethics is explored through Gilles Deleuze's
metaphysics of difference applied in linguistic terms to the formation of the
self. Comparing the exposure and transience alternatives leads to the
conclusion that today's big data reality splits the traditional ethical link
between authenticity and freedom. Exposure provides authenticity, but negates
human freedom. Transience provides freedom, but disdains authenticity.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.01788v1">Privacy Preserving Machine Learning Model Personalization through
  Federated Personalized Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2025-05-03T11:31:38Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Md. Tanzib Hosain, Asif Zaman, Md. Shahriar Sajid, Shadman Sakeeb Khan, Shanjida Akter</p>
    <p><b>Summary:</b> The widespread adoption of Artificial Intelligence (AI) has been driven by
significant advances in intelligent system research. However, this progress has
raised concerns about data privacy, leading to a growing awareness of the need
for privacy-preserving AI. In response, there has been a seismic shift in
interest towards the leading paradigm for training Machine Learning (ML) models
on decentralized data silos while maintaining data privacy, Federated Learning
(FL). This research paper presents a comprehensive performance analysis of a
cutting-edge approach to personalize ML model while preserving privacy achieved
through Privacy Preserving Machine Learning with the innovative framework of
Federated Personalized Learning (PPMLFPL). Regarding the increasing concerns
about data privacy, this study evaluates the effectiveness of PPMLFPL
addressing the critical balance between personalized model refinement and
maintaining the confidentiality of individual user data. According to our
analysis, Adaptive Personalized Cross-Silo Federated Learning with Differential
Privacy (APPLE+DP) offering efficient execution whereas overall, the use of the
Adaptive Personalized Cross-Silo Federated Learning with Homomorphic Encryption
(APPLE+HE) algorithm for privacy-preserving machine learning tasks in federated
personalized learning settings is strongly suggested. The results offer
valuable insights creating it a promising scope for future advancements in the
field of privacy-conscious data-driven technologies.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.01524v1">The DCR Delusion: Measuring the Privacy Risk of Synthetic Data</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-05-02T18:21:14Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zexi Yao, Nataša Krčo, Georgi Ganev, Yves-Alexandre de Montjoye</p>
    <p><b>Summary:</b> Synthetic data has become an increasingly popular way to share data without
revealing sensitive information. Though Membership Inference Attacks (MIAs) are
widely considered the gold standard for empirically assessing the privacy of a
synthetic dataset, practitioners and researchers often rely on simpler proxy
metrics such as Distance to Closest Record (DCR). These metrics estimate
privacy by measuring the similarity between the training data and generated
synthetic data. This similarity is also compared against that between the
training data and a disjoint holdout set of real records to construct a binary
privacy test. If the synthetic data is not more similar to the training data
than the holdout set is, it passes the test and is considered private. In this
work we show that, while computationally inexpensive, DCR and other
distance-based metrics fail to identify privacy leakage. Across multiple
datasets and both classical models such as Baynet and CTGAN and more recent
diffusion models, we show that datasets deemed private by proxy metrics are
highly vulnerable to MIAs. We similarly find both the binary privacy test and
the continuous measure based on these metrics to be uninformative of actual
membership inference risk. We further show that these failures are consistent
across different metric hyperparameter settings and record selection methods.
Finally, we argue DCR and other distance-based metrics to be flawed by design
and show a example of a simple leakage they miss in practice. With this work,
we hope to motivate practitioners to move away from proxy metrics to MIAs as
the rigorous, comprehensive standard of evaluating privacy of synthetic data,
in particular to make claims of datasets being legally anonymous.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.01292v1">Fine-grained Manipulation Attacks to Local Differential Privacy
  Protocols for Data Streams</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-02T14:09:56Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xinyu Li, Xuebin Ren, Shusen Yang, Liang Shi, Chia-Mu Yu</p>
    <p><b>Summary:</b> Local Differential Privacy (LDP) enables massive data collection and analysis
while protecting end users' privacy against untrusted aggregators. It has been
applied to various data types (e.g., categorical, numerical, and graph data)
and application settings (e.g., static and streaming). Recent findings indicate
that LDP protocols can be easily disrupted by poisoning or manipulation
attacks, which leverage injected/corrupted fake users to send crafted data
conforming to the LDP reports. However, current attacks primarily target static
protocols, neglecting the security of LDP protocols in the streaming settings.
Our research fills the gap by developing novel fine-grained manipulation
attacks to LDP protocols for data streams. By reviewing the attack surfaces in
existing algorithms, We introduce a unified attack framework with composable
modules, which can manipulate the LDP estimated stream toward a target stream.
Our attack framework can adapt to state-of-the-art streaming LDP algorithms
with different analytic tasks (e.g., frequency and mean) and LDP models
(event-level, user-level, w-event level). We validate our attacks theoretically
and through extensive experiments on real-world datasets, and finally explore a
possible defense mechanism for mitigating these attacks.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.00951v1">Preserving Privacy and Utility in LLM-Based Product Recommendations</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Retrieval-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-05-02T01:54:08Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tina Khezresmaeilzadeh, Jiang Zhang, Dimitrios Andreadis, Konstantinos Psounis</p>
    <p><b>Summary:</b> Large Language Model (LLM)-based recommendation systems leverage powerful
language models to generate personalized suggestions by processing user
interactions and preferences. Unlike traditional recommendation systems that
rely on structured data and collaborative filtering, LLM-based models process
textual and contextual information, often using cloud-based infrastructure.
This raises privacy concerns, as user data is transmitted to remote servers,
increasing the risk of exposure and reducing control over personal information.
To address this, we propose a hybrid privacy-preserving recommendation
framework which separates sensitive from nonsensitive data and only shares the
latter with the cloud to harness LLM-powered recommendations. To restore lost
recommendations related to obfuscated sensitive data, we design a
de-obfuscation module that reconstructs sensitive recommendations locally.
Experiments on real-world e-commerce datasets show that our framework achieves
almost the same recommendation utility with a system which shares all data with
an LLM, while preserving privacy to a large extend. Compared to
obfuscation-only techniques, our approach improves HR@10 scores and category
distribution alignment, offering a better balance between privacy and
recommendation quality. Furthermore, our method runs efficiently on
consumer-grade hardware, making privacy-aware LLM-based recommendation systems
practical for real-world use.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.00593v1">A Novel Feature-Aware Chaotic Image Encryption Scheme For Data Security
  and Privacy in IoT and Edge Networks</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-01T15:26:48Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Muhammad Shahbaz Khan, Ahmed Al-Dubai, Jawad Ahmad, Nikolaos Pitropakis, Baraq Ghaleb</p>
    <p><b>Summary:</b> The security of image data in the Internet of Things (IoT) and edge networks
is crucial due to the increasing deployment of intelligent systems for
real-time decision-making. Traditional encryption algorithms such as AES and
RSA are computationally expensive for resource-constrained IoT devices and
ineffective for large-volume image data, leading to inefficiencies in
privacy-preserving distributed learning applications. To address these
concerns, this paper proposes a novel Feature-Aware Chaotic Image Encryption
scheme that integrates Feature-Aware Pixel Segmentation (FAPS) with Chaotic
Chain Permutation and Confusion mechanisms to enhance security while
maintaining efficiency. The proposed scheme consists of three stages: (1) FAPS,
which extracts and reorganizes pixels based on high and low edge intensity
features for correlation disruption; (2) Chaotic Chain Permutation, which
employs a logistic chaotic map with SHA-256-based dynamically updated keys for
block-wise permutation; and (3) Chaotic chain Confusion, which utilises
dynamically generated chaotic seed matrices for bitwise XOR operations.
Extensive security and performance evaluations demonstrate that the proposed
scheme significantly reduces pixel correlation -- almost zero, achieves high
entropy values close to 8, and resists differential cryptographic attacks. The
optimum design of the proposed scheme makes it suitable for real-time
deployment in resource-constrained environments.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2505.00257v1">Graph Privacy: A Heterogeneous Federated GNN for Trans-Border Financial
  Data Circulation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-05-01T02:47:43Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhizhong Tan, Jiexin Zheng, Kevin Qi Zhang, Wenyong Wang</p>
    <p><b>Summary:</b> The sharing of external data has become a strong demand of financial
institutions, but the privacy issue has led to the difficulty of
interconnecting different platforms and the low degree of data openness. To
effectively solve the privacy problem of financial data in trans-border flow
and sharing, to ensure that the data is available but not visible, to realize
the joint portrait of all kinds of heterogeneous data of business organizations
in different industries, we propose a Heterogeneous Federated Graph Neural
Network (HFGNN) approach. In this method, the distribution of heterogeneous
business data of trans-border organizations is taken as subgraphs, and the
sharing and circulation process among subgraphs is constructed as a
statistically heterogeneous global graph through a central server. Each
subgraph learns the corresponding personalized service model through local
training to select and update the relevant subset of subgraphs with aggregated
parameters, and effectively separates and combines topological and feature
information among subgraphs. Finally, our simulation experimental results show
that the proposed method has higher accuracy performance and faster convergence
speed than existing methods.</p>
  </details>
</div>

