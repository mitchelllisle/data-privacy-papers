
<h2>2025-02</h2>

<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.20629v1">Towards Privacy-Preserving Split Learning: Destabilizing Adversarial
  Inference and Reconstruction Attacks in the Cloud</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-02-28T01:24:33Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Griffin Higgins, Roozbeh Razavi-Far, Xichen Zhang, Amir David, Ali Ghorbani, Tongyu Ge</p>
    <p><b>Summary:</b> This work aims to provide both privacy and utility within a split learning
framework while considering both forward attribute inference and backward
reconstruction attacks. To address this, a novel approach has been proposed,
which makes use of class activation maps and autoencoders as a plug-in strategy
aiming to increase the user's privacy and destabilize an adversary. The
proposed approach is compared with a dimensionality-reduction-based plug-in
strategy, which makes use of principal component analysis to transform the
feature map onto a lower-dimensional feature space. Our work shows that our
proposed autoencoder-based approach is preferred as it can provide protection
at an earlier split position over the tested architectures in our setting, and,
hence, better utility for resource-constrained devices in edge-cloud
collaborative inference (EC) systems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.20621v1">EPhishCADE: A Privacy-Aware Multi-Dimensional Framework for Email
  Phishing Campaign Detection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-02-28T00:58:42Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Wei Kang, Nan Wang, Jang Seung, Shuo Wang, Alsharif Abuadbba</p>
    <p><b>Summary:</b> Phishing attacks, typically carried out by email, remain a significant
cybersecurity threat with attackers creating legitimate-looking websites to
deceive recipients into revealing sensitive information or executing harmful
actions. In this paper, we propose {\bf EPhishCADE}, the first {\em
privacy-aware}, {\em multi-dimensional} framework for {\bf E}mail {\bf
Phish}ing {\bf CA}mpaign {\bf DE}tection to automatically identify email
phishing campaigns by clustering seemingly unrelated attacks. Our framework
employs a hierarchical architecture combining a structural layer and a
contextual layer, offering a comprehensive analysis of phishing attacks by
thoroughly examining both structural and contextual elements. Specifically, we
implement a graph-based contextual layer to reveal hidden similarities across
multiple dimensions, including textual, numeric, temporal, and spatial
features, among attacks that may initially appear unrelated. Our framework
streamlines the handling of security threat reports, reducing analysts' fatigue
and workload while enhancing protection against these threats. Another key
feature of our framework lies in its sole reliance on phishing URLs in emails
without the need for private information, including senders, recipients,
content, etc. This feature enables a collaborative identification of phishing
campaigns and attacks among multiple organizations without compromising
privacy. Finally, we benchmark our framework against an established
structure-based study (WWW \textquotesingle 17) to demonstrate its
effectiveness.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.20477v1">HELENE: An Open-Source High-Security Privacy-Preserving Blockchain Based
  System for Automating and Managing Laboratory Health Tests</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2025-02-27T19:28:29Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Gabriel Fernández-Blanco, Pedro García-Cereijo, David Lema-Núñez, Diego Ramil-López, Paula Fraga-Lamas, Leire Egia-Mendikute, Asís Palazón, Tiago M. Fernández-Caramés</p>
    <p><b>Summary:</b> In the last years, especially since the COVID-19 pandemic, precision medicine
platforms emerged as useful tools for supporting new tests like the ones that
detect the presence of antibodies and antigens with better sensitivity and
specificity than traditional methods. In addition, the pandemic has also
influenced the way people interact (decentralization), behave (digital world)
and purchase health services (online). Moreover, there is a growing concern in
the way health data are managed, especially in terms of privacy. To tackle such
issues, this article presents a sustainable direct-to-consumer health-service
open-source platform called HELENE that is supported by blockchain and by a
novel decentralized oracle that protects patient data privacy. Specifically,
HELENE enables health test providers to compete through auctions, allowing
patients to bid for their services and to keep the control over their health
test results. Moreover, data exchanges among the involved stakeholders can be
performed in a trustworthy, transparent and standardized way to ease software
integration and to avoid incompatibilities. After providing a thorough
description of the platform, the proposed health platform is assessed in terms
of smart contract performance. In addition, the response time of the developed
oracle is evaluated and NIST SP 800-22 tests are executed to demonstrate the
adequacy of the devised random number generator. Thus, this article shows the
capabilities and novel propositions of HELENE for delivering health services
providing an open-source platform for future researchers, who can enhance it
and adapt it to their needs.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.19912v1">Model-Free Privacy Preserving Power Flow Analysis in Distribution
  Networks</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36">
  <p><b>Published on:</b> 2025-02-27T09:31:53Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Dong Liu, Juan S. Giraldo, Peter Palensky, Pedro P. Vergara</p>
    <p><b>Summary:</b> Model-free power flow calculation, driven by the rise of smart meter (SM)
data and the lack of network topology, often relies on artificial intelligence
neural networks (ANNs). However, training ANNs require vast amounts of SM data,
posing privacy risks for households in distribution networks. To ensure
customers' privacy during the SM data gathering and online sharing, we
introduce a privacy preserving PF calculation framework, composed of two local
strategies: a local randomisation strategy (LRS) and a local zero-knowledge
proof (ZKP)-based data collection strategy. First, the LRS is used to achieve
irreversible transformation and robust privacy protection for active and
reactive power data, thereby ensuring that personal data remains confidential.
Subsequently, the ZKP-based data collecting strategy is adopted to securely
gather the training dataset for the ANN, enabling SMs to interact with the
distribution system operator without revealing the actual voltage magnitude.
Moreover, to mitigate the accuracy loss induced by the seasonal variations in
load profiles, an incremental learning strategy is incorporated into the online
application. The results across three datasets with varying measurement errors
demonstrate that the proposed framework efficiently collects one month of SM
data within one hour. Furthermore, it robustly maintains mean errors of 0.005
p.u. and 0.014 p.u. under multiple measurement errors and seasonal variations
in load profiles, respectively.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.05786v1">FedMentalCare: Towards Privacy-Preserving Fine-Tuned LLMs to Analyze
  Mental Health Status Using Federated Learning Framework</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-02-27T07:04:19Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> S M Sarwar</p>
    <p><b>Summary:</b> With the increasing prevalence of mental health conditions worldwide,
AI-powered chatbots and conversational agents have emerged as accessible tools
to support mental health. However, deploying Large Language Models (LLMs) in
mental healthcare applications raises significant privacy concerns, especially
regarding regulations like HIPAA and GDPR. In this work, we propose
FedMentalCare, a privacy-preserving framework that leverages Federated Learning
(FL) combined with Low-Rank Adaptation (LoRA) to fine-tune LLMs for mental
health analysis. We investigate the performance impact of varying client data
volumes and model architectures (e.g., MobileBERT and MiniLM) in FL
environments. Our framework demonstrates a scalable, privacy-aware approach for
deploying LLMs in real-world mental healthcare scenarios, addressing data
security and computational efficiency challenges.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.00062v1">CRFU: Compressive Representation Forgetting Against Privacy Leakage on
  Machine Unlearning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-02-27T05:59:02Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Weiqi Wang, Chenhan Zhang, Zhiyi Tian, Shushu Liu, Shui Yu</p>
    <p><b>Summary:</b> Machine unlearning allows data owners to erase the impact of their specified
data from trained models. Unfortunately, recent studies have shown that
adversaries can recover the erased data, posing serious threats to user
privacy. An effective unlearning method removes the information of the
specified data from the trained model, resulting in different outputs for the
same input before and after unlearning. Adversaries can exploit these output
differences to conduct privacy leakage attacks, such as reconstruction and
membership inference attacks. However, directly applying traditional defenses
to unlearning leads to significant model utility degradation. In this paper, we
introduce a Compressive Representation Forgetting Unlearning scheme (CRFU),
designed to safeguard against privacy leakage on unlearning. CRFU achieves data
erasure by minimizing the mutual information between the trained compressive
representation (learned through information bottleneck theory) and the erased
data, thereby maximizing the distortion of data. This ensures that the model's
output contains less information that adversaries can exploit. Furthermore, we
introduce a remembering constraint and an unlearning rate to balance the
forgetting of erased data with the preservation of previously learned
knowledge, thereby reducing accuracy degradation. Theoretical analysis
demonstrates that CRFU can effectively defend against privacy leakage attacks.
Our experimental results show that CRFU significantly increases the
reconstruction mean square error (MSE), achieving a defense effect improvement
of approximately $200\%$ against privacy reconstruction attacks with only
$1.5\%$ accuracy degradation on MNIST.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.19154v1">Towards Privacy-Preserving Anomaly-Based Intrusion Detection in Energy
  Communities</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-02-26T14:13:02Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zeeshan Afzal, Giovanni Gaggero, Mikael Asplund</p>
    <p><b>Summary:</b> Energy communities consist of decentralized energy production, storage,
consumption, and distribution and are gaining traction in modern power systems.
However, these communities may increase the vulnerability of the grid to cyber
threats. We propose an anomaly-based intrusion detection system to enhance the
security of energy communities. The system leverages deep autoencoders to
detect deviations from normal operational patterns in order to identify
anomalies induced by malicious activities and attacks. Operational data for
training and evaluation are derived from a Simulink model of an energy
community. The results show that the autoencoder-based intrusion detection
system achieves good detection performance across multiple attack scenarios. We
also demonstrate potential for real-world application of the system by training
a federated model that enables distributed intrusion detection while preserving
data privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.19119v1">Chemical knowledge-informed framework for privacy-aware retrosynthesis
  learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-02-26T13:13:24Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Guikun Chen, Xu Zhang, Yi Yang, Wenguan Wang</p>
    <p><b>Summary:</b> Chemical reaction data is a pivotal asset, driving advances in competitive
fields such as pharmaceuticals, materials science, and industrial chemistry.
Its proprietary nature renders it sensitive, as it often includes confidential
insights and competitive advantages organizations strive to protect. However,
in contrast to this need for confidentiality, the current standard training
paradigm for machine learning-based retrosynthesis gathers reaction data from
multiple sources into one single edge to train prediction models. This paradigm
poses considerable privacy risks as it necessitates broad data availability
across organizational boundaries and frequent data transmission between
entities, potentially exposing proprietary information to unauthorized access
or interception during storage and transfer. In the present study, we introduce
the chemical knowledge-informed framework (CKIF), a privacy-preserving approach
for learning retrosynthesis models. CKIF enables distributed training across
multiple chemical organizations without compromising the confidentiality of
proprietary reaction data. Instead of gathering raw reaction data, CKIF learns
retrosynthesis models through iterative, chemical knowledge-informed
aggregation of model parameters. In particular, the chemical properties of
predicted reactants are leveraged to quantitatively assess the observable
behaviors of individual models, which in turn determines the adaptive weights
used for model aggregation. On a variety of reaction datasets, CKIF outperforms
several strong baselines by a clear margin (e.g., ~20% performance improvement
over FedAvg on USPTO-50K), showing its feasibility and superiority to stimulate
further research on privacy-preserving retrosynthesis.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.19082v2">Trust-Enabled Privacy: Social Media Designs to Support Adolescent User
  Boundary Regulation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-02-26T12:19:53Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> JaeWon Kim, Robert Wolfe, Ramya Bhagirathi Subramanian, Mei-Hsuan Lee, Jessica Colnago, Alexis Hiniker</p>
    <p><b>Summary:</b> Through a three-part co-design study involving 19 teens aged 13-18, we
identify key barriers to effective boundary regulation on social media,
including ambiguous audience expectations, social risks associated with
oversharing, and the lack of design affordances that facilitate trust-building.
Our findings reveal that while adolescents seek casual, frequent sharing to
strengthen relationships, existing platform norms and designs often discourage
such interactions, leading to withdrawal. To address these challenges, we
introduce trust-enabled privacy as a design framework that recognizes trust,
whether building or eroding, as central to boundary regulation. When trust is
supported, boundary regulation becomes more adaptive and empowering; when it
erodes, users default to self-censorship or withdrawal. We propose concrete
design affordances, including guided disclosure, contextual audience
segmentation, intentional engagement signaling, and trust-centered norms, to
help platforms foster a more dynamic and nuanced privacy experience for teen
social media users. By reframing privacy as a trust-driven process rather than
a rigid control-based trade-off, this work provides empirical insights and
actionable guidelines for designing social media environments that empower
teens to manage their online presence while fostering meaningful social
connections.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.18974v1">Distributed Transition System with Tags and Value-wise Metric, for
  Privacy Analysis</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Logic in Computer Science-662E9B">
  <p><b>Published on:</b> 2025-02-26T09:35:48Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Siva Anantharaman, Sabine Frittella, Benjamin Nguyen</p>
    <p><b>Summary:</b> We introduce a logical framework named Distributed Labeled Tagged Transition
System (DLTTS), using concepts from Probabilistic Automata, Probabilistic
Concurrent Systems, and Probabilistic labelled transition systems. We show that
DLTTS can be used to formally model how a given piece of private information P
(e.g., a set of tuples) stored in a given database D can get captured
progressively by an adversary A repeatedly querying D, enhancing the knowledge
acquired from the answers to these queries with relational deductions using
certain additional non-private data. The database D is assumed protected with
generalization mechanisms. We also show that, on a large class of databases,
metrics can be defined 'value-wise', and more general notions of adjacency
between data bases can be defined, based on these metrics. These notions can
also play a role in differentially private protection mechanisms.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.18706v1">Differentially Private Federated Learning With Time-Adaptive Privacy
  Spending</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2025-02-25T23:56:23Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shahrzad Kiani, Nupur Kulkarni, Adam Dziedzic, Stark Draper, Franziska Boenisch</p>
    <p><b>Summary:</b> Federated learning (FL) with differential privacy (DP) provides a framework
for collaborative machine learning, enabling clients to train a shared model
while adhering to strict privacy constraints. The framework allows each client
to have an individual privacy guarantee, e.g., by adding different amounts of
noise to each client's model updates. One underlying assumption is that all
clients spend their privacy budgets uniformly over time (learning rounds).
However, it has been shown in the literature that learning in early rounds
typically focuses on more coarse-grained features that can be learned at lower
signal-to-noise ratios while later rounds learn fine-grained features that
benefit from higher signal-to-noise ratios. Building on this intuition, we
propose a time-adaptive DP-FL framework that expends the privacy budget
non-uniformly across both time and clients. Our framework enables each client
to save privacy budget in early rounds so as to be able to spend more in later
rounds when additional accuracy is beneficial in learning more fine-grained
features. We theoretically prove utility improvements in the case that clients
with stricter privacy budgets spend budgets unevenly across rounds, compared to
clients with more relaxed budgets, who have sufficient budgets to distribute
their spend more evenly. Our practical experiments on standard benchmark
datasets support our theoretical results and show that, in practice, our
algorithms improve the privacy-utility trade-offs compared to baseline schemes.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.18697v1">H-FLTN: A Privacy-Preserving Hierarchical Framework for Electric Vehicle
  Spatio-Temporal Charge Prediction</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2025-02-25T23:20:53Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Robert Marlin, Raja Jurdak, Alsharif Abuadbba</p>
    <p><b>Summary:</b> The widespread adoption of Electric Vehicles (EVs) poses critical challenges
for energy providers, particularly in predicting charging time (temporal
prediction), ensuring user privacy, and managing resources efficiently in
mobility-driven networks. This paper introduces the Hierarchical Federated
Learning Transformer Network (H-FLTN) framework to address these challenges.
H-FLTN employs a three-tier hierarchical architecture comprising EVs, community
Distributed Energy Resource Management Systems (DERMS), and the Energy Provider
Data Centre (EPDC) to enable accurate spatio-temporal predictions of EV
charging needs while preserving privacy. Temporal prediction is enhanced using
Transformer-based learning, capturing complex dependencies in charging
behavior. Privacy is ensured through Secure Aggregation, Additive Secret
Sharing, and Peer-to-Peer (P2P) Sharing with Augmentation, which allow only
secret shares of model weights to be exchanged while securing all
transmissions. To improve training efficiency and resource management, H-FLTN
integrates Dynamic Client Capping Mechanism (DCCM) and Client Rotation
Management (CRM), ensuring that training remains both computationally and
temporally efficient as the number of participating EVs increases. DCCM
optimises client participation by limiting excessive computational loads, while
CRM balances training contributions across epochs, preventing imbalanced
participation. Our simulation results based on large-scale empirical vehicle
mobility data reveal that DCCM and CRM reduce the training time complexity with
increasing EVs from linear to constant. Its integration into real-world smart
city infrastructure enhances energy demand forecasting, resource allocation,
and grid stability, ensuring reliability and sustainability in future mobility
ecosystems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.18623v1">On the Privacy-Preserving Properties of Spiking Neural Networks with
  Unique Surrogate Gradients and Quantization Levels</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Neural and Evolutionary Computing-5BC0EB">
  <p><b>Published on:</b> 2025-02-25T20:14:14Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ayana Moshruba, Shay Snyder, Hamed Poursiami, Maryam Parsa</p>
    <p><b>Summary:</b> As machine learning models increasingly process sensitive data, understanding
their vulnerability to privacy attacks is vital. Membership inference attacks
(MIAs) exploit model responses to infer whether specific data points were used
during training, posing a significant privacy risk. Prior research suggests
that spiking neural networks (SNNs), which rely on event-driven computation and
discrete spike-based encoding, exhibit greater resilience to MIAs than
artificial neural networks (ANNs). This resilience stems from their
non-differentiable activations and inherent stochasticity, which obscure the
correlation between model responses and individual training samples. To enhance
privacy in SNNs, we explore two techniques: quantization and surrogate
gradients. Quantization, which reduces precision to limit information leakage,
has improved privacy in ANNs. Given SNNs' sparse and irregular activations,
quantization may further disrupt the activation patterns exploited by MIAs. We
assess the vulnerability of SNNs and ANNs under weight and activation
quantization across multiple datasets, using the attack model's receiver
operating characteristic (ROC) curve area under the curve (AUC) metric, where
lower values indicate stronger privacy, and evaluate the privacy-accuracy
trade-off. Our findings show that quantization enhances privacy in both
architectures with minimal performance loss, though full-precision SNNs remain
more resilient than quantized ANNs. Additionally, we examine the impact of
surrogate gradients on privacy in SNNs. Among five evaluated gradients, spike
rate escape provides the best privacy-accuracy trade-off, while arctangent
increases vulnerability to MIAs. These results reinforce SNNs' inherent privacy
advantages and demonstrate that quantization and surrogate gradient selection
significantly influence privacy-accuracy trade-offs in SNNs.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.18545v1">PII-Bench: Evaluating Query-Aware Privacy Protection Systems</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Retrieval-5BC0EB">
  <p><b>Published on:</b> 2025-02-25T14:49:08Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Hao Shen, Zhouhong Gu, Haokai Hong, Weili Han</p>
    <p><b>Summary:</b> The widespread adoption of Large Language Models (LLMs) has raised
significant privacy concerns regarding the exposure of personally identifiable
information (PII) in user prompts. To address this challenge, we propose a
query-unrelated PII masking strategy and introduce PII-Bench, the first
comprehensive evaluation framework for assessing privacy protection systems.
PII-Bench comprises 2,842 test samples across 55 fine-grained PII categories,
featuring diverse scenarios from single-subject descriptions to complex
multi-party interactions. Each sample is carefully crafted with a user query,
context description, and standard answer indicating query-relevant PII. Our
empirical evaluation reveals that while current models perform adequately in
basic PII detection, they show significant limitations in determining PII query
relevance. Even state-of-the-art LLMs struggle with this task, particularly in
handling complex multi-subject scenarios, indicating substantial room for
improvement in achieving intelligent PII masking.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.18227v1">TLDP: An Algorithm of Local Differential Privacy for Tensors</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-02-25T14:11:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yachao Yuan, Xiao Tang, Yu Huang, Jin Wang</p>
    <p><b>Summary:</b> Tensor-valued data, increasingly common in applications like spatiotemporal
modeling and social networks, pose unique challenges for privacy protection due
to their multidimensional structure and the risk of losing critical structural
information. Traditional local differential privacy (LDP) methods, designed for
scalars and matrices, are insufficient for tensors, as they fail to preserve
essential relationships among tensor elements. We introduce TLDP, a novel
\emph{LDP} algorithm for \emph{T}ensors, which employs a randomized response
mechanism to perturb tensor components while maintaining structural integrity.
To strike a better balance between utility and privacy, we incorporate a weight
matrix that selectively protects sensitive regions. Both theoretical analysis
and empirical findings from real-world datasets show that TLDP achieves
superior utility while preserving privacy, making it a robust solution for
high-dimensional data.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.17772v2">An Improved Privacy and Utility Analysis of Differentially Private SGD
  with Bounded Domain and Smooth Losses</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2025-02-25T02:05:41Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Hao Liang, Wanrong Zhang, Xinlei He, Kaishun Wu, Hong Xing</p>
    <p><b>Summary:</b> Differentially Private Stochastic Gradient Descent (DPSGD) is widely used to
protect sensitive data during the training of machine learning models, but its
privacy guarantees often come at the cost of model performance, largely due to
the inherent challenge of accurately quantifying privacy loss. While recent
efforts have strengthened privacy guarantees by focusing solely on the final
output and bounded domain cases, they still impose restrictive assumptions,
such as convexity and other parameter limitations, and often lack a thorough
analysis of utility. In this paper, we provide rigorous privacy and utility
characterization for DPSGD for smooth loss functions in both bounded and
unbounded domains. We track the privacy loss over multiple iterations by
exploiting the noisy smooth-reduction property and establish the utility
analysis by leveraging the projection's non-expansiveness and clipped SGD
properties. In particular, we show that for DPSGD with a bounded domain, (i)
the privacy loss can still converge without the convexity assumption, and (ii)
a smaller bounded diameter can improve both privacy and utility simultaneously
under certain conditions. Numerical results validate our results.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.17748v1">FinP: Fairness-in-Privacy in Federated Learning by Addressing
  Disparities in Privacy Risk</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-02-25T00:56:47Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tianyu Zhao, Mahmoud Srewa, Salma Elmalaki</p>
    <p><b>Summary:</b> Ensuring fairness in machine learning, particularly in human-centric
applications, extends beyond algorithmic bias to encompass fairness in privacy,
specifically the equitable distribution of privacy risk. This is critical in
federated learning (FL), where decentralized data necessitates balanced privacy
preservation across clients. We introduce FinP, a framework designed to achieve
fairness in privacy by mitigating disproportionate exposure to source inference
attacks (SIA). FinP employs a dual approach: (1) server-side adaptive
aggregation to address unfairness in client contributions in global model, and
(2) client-side regularization to reduce client vulnerability. This
comprehensive strategy targets both the symptoms and root causes of privacy
unfairness. Evaluated on the Human Activity Recognition (HAR) and CIFAR-10
datasets, FinP demonstrates ~20% improvement in fairness in privacy on HAR with
minimal impact on model utility, and effectively mitigates SIA risks on
CIFAR-10, showcasing its ability to provide fairness in privacy in FL systems
without compromising performance.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.18527v2">GOD model: Privacy Preserved AI School for Personal Assistant</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-02-24T20:30:17Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b>  PIN AI Team, Bill Sun, Gavin Guo, Regan Peng, Boliang Zhang, Shouqiao Wang, Laura Florescu, Xi Wang, Davide Crapis, Ben Wu</p>
    <p><b>Summary:</b> Personal AI assistants (e.g., Apple Intelligence, Meta AI) offer proactive
recommendations that simplify everyday tasks, but their reliance on sensitive
user data raises concerns about privacy and trust. To address these challenges,
we introduce the Guardian of Data (GOD), a secure, privacy-preserving framework
for training and evaluating AI assistants directly on-device. Unlike
traditional benchmarks, the GOD model measures how well assistants can
anticipate user needs-such as suggesting gifts-while protecting user data and
autonomy. Functioning like an AI school, it addresses the cold start problem by
simulating user queries and employing a curriculum-based approach to refine the
performance of each assistant. Running within a Trusted Execution Environment
(TEE), it safeguards user data while applying reinforcement and imitation
learning to refine AI recommendations. A token-based incentive system
encourages users to share data securely, creating a data flywheel that drives
continuous improvement. Specifically, users mine with their data, and the
mining rate is determined by GOD's evaluation of how well their AI assistant
understands them across categories such as shopping, social interactions,
productivity, trading, and Web3. By integrating privacy, personalization, and
trust, the GOD model provides a scalable, responsible path for advancing
personal AI assistants. For community collaboration, part of the framework is
open-sourced at https://github.com/PIN-AI/God-Model.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.17591v2">Proactive Privacy Amnesia for Large Language Models: Safeguarding PII
  with Negligible Impact on Model Utility</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-02-24T19:16:39Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Martin Kuo, Jingyang Zhang, Jianyi Zhang, Minxue Tang, Louis DiValentin, Aolin Ding, Jingwei Sun, William Chen, Amin Hass, Tianlong Chen, Yiran Chen, Hai Li</p>
    <p><b>Summary:</b> With the rise of large language models (LLMs), increasing research has
recognized their risk of leaking personally identifiable information (PII)
under malicious attacks. Although efforts have been made to protect PII in
LLMs, existing methods struggle to balance privacy protection with maintaining
model utility. In this paper, inspired by studies of amnesia in cognitive
science, we propose a novel approach, Proactive Privacy Amnesia (PPA), to
safeguard PII in LLMs while preserving their utility. This mechanism works by
actively identifying and forgetting key memories most closely associated with
PII in sequences, followed by a memory implanting using suitable substitute
memories to maintain the LLM's functionality. We conduct evaluations across
multiple models to protect common PII, such as phone numbers and physical
addresses, against prevalent PII-targeted attacks, demonstrating the
superiority of our method compared with other existing defensive techniques.
The results show that our PPA method completely eliminates the risk of phone
number exposure by 100% and significantly reduces the risk of physical address
exposure by 9.8% - 87.6%, all while maintaining comparable model utility
performance.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.17384v1">On the Dichotomy Between Privacy and Traceability in $\ell_p$ Stochastic
  Convex Optimization</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-02-24T18:10:06Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sasha Voitovych, Mahdi Haghifam, Idan Attias, Gintare Karolina Dziugaite, Roi Livni, Daniel M. Roy</p>
    <p><b>Summary:</b> In this paper, we investigate the necessity of memorization in stochastic
convex optimization (SCO) under $\ell_p$ geometries. Informally, we say a
learning algorithm memorizes $m$ samples (or is $m$-traceable) if, by analyzing
its output, it is possible to identify at least $m$ of its training samples.
Our main results uncover a fundamental tradeoff between traceability and excess
risk in SCO. For every $p\in [1,\infty)$, we establish the existence of a risk
threshold below which any sample-efficient learner must memorize a \em{constant
fraction} of its sample. For $p\in [1,2]$, this threshold coincides with best
risk of differentially private (DP) algorithms, i.e., above this threshold,
there are algorithms that do not memorize even a single sample. This
establishes a sharp dichotomy between privacy and traceability for $p \in
[1,2]$. For $p \in (2,\infty)$, this threshold instead gives novel lower bounds
for DP learning, partially closing an open problem in this setup. En route of
proving these results, we introduce a complexity notion we term \em{trace
value} of a problem, which unifies privacy lower bounds and traceability
results, and prove a sparse variant of the fingerprinting lemma.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.17150v1">Differential privacy guarantees of Markov chain Monte Carlo algorithms</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> 
  <p><b>Published on:</b> 2025-02-24T13:40:46Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Andrea Bertazzi, Tim Johnston, Gareth O. Roberts, Alain Durmus</p>
    <p><b>Summary:</b> This paper aims to provide differential privacy (DP) guarantees for Markov
chain Monte Carlo (MCMC) algorithms. In a first part, we establish DP
guarantees on samples output by MCMC algorithms as well as Monte Carlo
estimators associated with these methods under assumptions on the convergence
properties of the underlying Markov chain. In particular, our results highlight
the critical condition of ensuring the target distribution is differentially
private itself. In a second part, we specialise our analysis to the unadjusted
Langevin algorithm and stochastic gradient Langevin dynamics and establish
guarantees on their (R\'enyi) DP. To this end, we develop a novel methodology
based on Girsanov's theorem combined with a perturbation trick to obtain bounds
for an unbounded domain and in a non-convex setting. We establish: (i) uniform
in $n$ privacy guarantees when the state of the chain after $n$ iterations is
released, (ii) bounds on the privacy of the entire chain trajectory. These
findings provide concrete guidelines for privacy-preserving MCMC.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.17041v1">PrivaCI-Bench: Evaluating Privacy with Contextual Integrity and Legal
  Compliance</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-02-24T10:49:34Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Haoran Li, Wenbin Hu, Huihao Jing, Yulin Chen, Qi Hu, Sirui Han, Tianshu Chu, Peizhao Hu, Yangqiu Song</p>
    <p><b>Summary:</b> Recent advancements in generative large language models (LLMs) have enabled
wider applicability, accessibility, and flexibility. However, their reliability
and trustworthiness are still in doubt, especially for concerns regarding
individuals' data privacy. Great efforts have been made on privacy by building
various evaluation benchmarks to study LLMs' privacy awareness and robustness
from their generated outputs to their hidden representations. Unfortunately,
most of these works adopt a narrow formulation of privacy and only investigate
personally identifiable information (PII). In this paper, we follow the merit
of the Contextual Integrity (CI) theory, which posits that privacy evaluation
should not only cover the transmitted attributes but also encompass the whole
relevant social context through private information flows. We present
PrivaCI-Bench, a comprehensive contextual privacy evaluation benchmark targeted
at legal compliance to cover well-annotated privacy and safety regulations,
real court cases, privacy policies, and synthetic data built from the official
toolkit to study LLMs' privacy and safety compliance. We evaluate the latest
LLMs, including the recent reasoner models QwQ-32B and Deepseek R1. Our
experimental results suggest that though LLMs can effectively capture key CI
parameters inside a given context, they still require further advancements for
privacy compliance.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.16877v1">APINT: A Full-Stack Framework for Acceleration of Privacy-Preserving
  Inference of Transformers based on Garbled Circuits</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Hardware Architecture-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-02-24T06:26:42Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Hyunjun Cho, Jaeho Jeon, Jaehoon Heo, Joo-Young Kim</p>
    <p><b>Summary:</b> As the importance of Privacy-Preserving Inference of Transformers (PiT)
increases, a hybrid protocol that integrates Garbled Circuits (GC) and
Homomorphic Encryption (HE) is emerging for its implementation. While this
protocol is preferred for its ability to maintain accuracy, it has a severe
drawback of excessive latency. To address this, existing protocols primarily
focused on reducing HE latency, thus making GC the new latency bottleneck.
Furthermore, previous studies only focused on individual computing layers, such
as protocol or hardware accelerator, lacking a comprehensive solution at the
system level. This paper presents APINT, a full-stack framework designed to
reduce PiT's overall latency by addressing the latency problem of GC through
both software and hardware solutions. APINT features a novel protocol that
reallocates possible GC workloads to alternative methods (i.e., HE or standard
matrix operation), substantially decreasing the GC workload. It also suggests
GC-friendly circuit generation that reduces the number of AND gates at the
most, which is the expensive operator in GC. Furthermore, APINT proposes an
innovative netlist scheduling that combines coarse-grained operation mapping
and fine-grained scheduling for maximal data reuse and minimal dependency.
Finally, APINT's hardware accelerator, combined with its compiler speculation,
effectively resolves the memory stall issue. Putting it all together, APINT
achieves a remarkable end-to-end reduction in latency, outperforming the
existing protocol on CPU platform by 12.2x online and 2.2x offline. Meanwhile,
the APINT accelerator not only reduces its latency by 3.3x but also saves
energy consumption by 4.6x while operating PiT compared to the state-of-the-art
GC accelerator.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.16739v1">Investigating the Security & Privacy Risks from Unsanctioned Technology
  Use by Educators</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2025-02-23T22:52:58Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Easton Kelso, Ananta Soneji, Syed Zami-Ul-Haque Navid, Yan Soshitaishvili, Sazzadur Rahaman, Rakibul Hasan</p>
    <p><b>Summary:</b> Educational technologies are revolutionizing how educational institutions
operate. Consequently, it makes them a lucrative target for breach and abuse as
they often serve as centralized hubs for diverse types of sensitive data, from
academic records to health information. Existing studies looked into how
existing stakeholders perceive the security and privacy risks of educational
technologies and how those risks are affecting institutional policies for
acquiring new technologies. However, outside of institutional vetting and
approval, there is a pervasive practice of using applications and devices
acquired personally. It is unclear how these applications and devices affect
the dynamics of the overall institutional ecosystem.
  This study aims to address this gap by understanding why instructors use
unsanctioned applications, how instructors perceive the associated risks, and
how it affects institutional security and privacy postures. We designed and
conducted an online survey-based study targeting instructors and administrators
from K-12 and higher education institutions.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.16519v2">Guarding the Privacy of Label-Only Access to Neural Network Classifiers
  via iDP Verification</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Programming Languages-D91E36">
  <p><b>Published on:</b> 2025-02-23T09:50:26Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Anan Kabaha, Dana Drachsler-Cohen</p>
    <p><b>Summary:</b> Neural networks are susceptible to privacy attacks that can extract private
information of the training set. To cope, several training algorithms guarantee
differential privacy (DP) by adding noise to their computation. However, DP
requires to add noise considering every possible training set. This leads to a
significant decrease in the network's accuracy. Individual DP (iDP) restricts
DP to a given training set. We observe that some inputs deterministically
satisfy iDP without any noise. By identifying them, we can provide iDP
label-only access to the network with a minor decrease to its accuracy.
However, identifying the inputs that satisfy iDP without any noise is highly
challenging. Our key idea is to compute the iDP deterministic bound (iDP-DB),
which overapproximates the set of inputs that do not satisfy iDP, and add noise
only to their predicted labels. To compute the tightest iDP-DB, which enables
to guard the label-only access with minimal accuracy decrease, we propose
LUCID, which leverages several formal verification techniques. First, it
encodes the problem as a mixed-integer linear program, defined over a network
and over every network trained identically but without a unique data point.
Second, it abstracts a set of networks using a hyper-network. Third, it
eliminates the overapproximation error via a novel branch-and-bound technique.
Fourth, it bounds the differences of matching neurons in the network and the
hyper-network and employs linear relaxation if they are small. We show that
LUCID can provide classifiers with a perfect individuals' privacy guarantee
(0-iDP) -- which is infeasible for DP training algorithms -- with an accuracy
decrease of 1.4%. For more relaxed $\varepsilon$-iDP guarantees, LUCID has an
accuracy decrease of 1.2%. In contrast, existing DP training algorithms reduce
the accuracy by 12.7%.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.18517v1">RewardDS: Privacy-Preserving Fine-Tuning for Large Language Models via
  Reward Driven Data Synthesis</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-02-23T02:52:23Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jianwei Wang, Junyao Yang, Haoran Li, Huiping Zhuang, Cen Chen, Ziqian Zeng</p>
    <p><b>Summary:</b> The success of large language models (LLMs) has attracted many individuals to
fine-tune them for domain-specific tasks by uploading their data. However, in
sensitive areas like healthcare and finance, privacy concerns often arise. One
promising solution is to sample synthetic data with Differential Privacy (DP)
guarantees to replace private data. However, these synthetic data contain
significant flawed data, which are considered as noise. Existing solutions
typically rely on naive filtering by comparing ROUGE-L scores or embedding
similarities, which are ineffective in addressing the noise. To address this
issue, we propose RewardDS, a novel privacy-preserving framework that
fine-tunes a reward proxy model and uses reward signals to guide the synthetic
data generation. Our RewardDS introduces two key modules, Reward Guided
Filtering and Self-Optimizing Refinement, to both filter and refine the
synthetic data, effectively mitigating the noise. Extensive experiments across
medical, financial, and code generation domains demonstrate the effectiveness
of our method.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.18509v1">Protecting Users From Themselves: Safeguarding Contextual Privacy in
  Interactions with Conversational Agents</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-02-22T09:05:39Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ivoline Ngong, Swanand Kadhe, Hao Wang, Keerthiram Murugesan, Justin D. Weisz, Amit Dhurandhar, Karthikeyan Natesan Ramamurthy</p>
    <p><b>Summary:</b> Conversational agents are increasingly woven into individuals' personal
lives, yet users often underestimate the privacy risks involved. The moment
users share information with these agents (e.g., LLMs), their private
information becomes vulnerable to exposure. In this paper, we characterize the
notion of contextual privacy for user interactions with LLMs. It aims to
minimize privacy risks by ensuring that users (sender) disclose only
information that is both relevant and necessary for achieving their intended
goals when interacting with LLMs (untrusted receivers). Through a formative
design user study, we observe how even "privacy-conscious" users inadvertently
reveal sensitive information through indirect disclosures. Based on insights
from this study, we propose a locally-deployable framework that operates
between users and LLMs, and identifies and reformulates out-of-context
information in user prompts. Our evaluation using examples from ShareGPT shows
that lightweight models can effectively implement this framework, achieving
strong gains in contextual privacy while preserving the user's intended
interaction goals through different approaches to classify information relevant
to the intended goals.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.16091v2">Privacy-Aware Joint DNN Model Deployment and Partition Optimization for
  Delay-Efficient Collaborative Edge Inference</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Networking and Internet Architecture-04E762">
  <p><b>Published on:</b> 2025-02-22T05:27:24Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhipeng Cheng, Xiaoyu Xia, Hong Wang, Minghui Liwang, Ning Chen, Xuwei Fan, Xianbin Wang</p>
    <p><b>Summary:</b> Edge inference (EI) is a key solution to address the growing challenges of
delayed response times, limited scalability, and privacy concerns in
cloud-based Deep Neural Network (DNN) inference. However, deploying DNN models
on resource-constrained edge devices faces more severe challenges, such as
model storage limitations, dynamic service requests, and privacy risks. This
paper proposes a novel framework for privacy-aware joint DNN model deployment
and partition optimization to minimize long-term average inference delay under
resource and privacy constraints. Specifically, the problem is formulated as a
complex optimization problem considering model deployment, user-server
association, and model partition strategies. To handle the NP-hardness and
future uncertainties, a Lyapunov-based approach is introduced to transform the
long-term optimization into a single-time-slot problem, ensuring system
performance. Additionally, a coalition formation game model is proposed for
edge server association, and a greedy-based algorithm is developed for model
deployment within each coalition to efficiently solve the problem. Extensive
simulations show that the proposed algorithms effectively reduce inference
delay while satisfying privacy constraints, outperforming baseline approaches
in various scenarios.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.15929v1">Approximate Differential Privacy of the $\ell_2$ Mechanism</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-02-21T20:56:34Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Matthew Joseph, Alex Kulesza, Alexander Yu</p>
    <p><b>Summary:</b> We study the $\ell_2$ mechanism for computing a $d$-dimensional statistic
with bounded $\ell_2$ sensitivity under approximate differential privacy.
Across a range of privacy parameters, we find that the $\ell_2$ mechanism
obtains lower error than the Laplace and Gaussian mechanisms, matching the
former at $d=1$ and approaching the latter as $d \to \infty$.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.15680v1">Privacy Ripple Effects from Adding or Removing Personal Information in
  Language Model Training</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-02-21T18:59:14Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jaydeep Borkar, Matthew Jagielski, Katherine Lee, Niloofar Mireshghallah, David A. Smith, Christopher A. Choquette-Choo</p>
    <p><b>Summary:</b> Due to the sensitive nature of personally identifiable information (PII), its
owners may have the authority to control its inclusion or request its removal
from large-language model (LLM) training. Beyond this, PII may be added or
removed from training datasets due to evolving dataset curation techniques,
because they were newly scraped for retraining, or because they were included
in a new downstream fine-tuning stage. We find that the amount and ease of PII
memorization is a dynamic property of a model that evolves throughout training
pipelines and depends on commonly altered design choices. We characterize three
such novel phenomena: (1) similar-appearing PII seen later in training can
elicit memorization of earlier-seen sequences in what we call assisted
memorization, and this is a significant factor (in our settings, up to 1/3);
(2) adding PII can increase memorization of other PII significantly (in our
settings, as much as $\approx\!7.5\times$); and (3) removing PII can lead to
other PII being memorized. Model creators should consider these first- and
second-order privacy risks when training models to avoid the risk of new PII
regurgitation.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.15567v1">Model Privacy: A Unified Framework to Understand Model Stealing Attacks
  and Defenses</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> 
  <p><b>Published on:</b> 2025-02-21T16:29:11Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ganghua Wang, Yuhong Yang, Jie Ding</p>
    <p><b>Summary:</b> The use of machine learning (ML) has become increasingly prevalent in various
domains, highlighting the importance of understanding and ensuring its safety.
One pressing concern is the vulnerability of ML applications to model stealing
attacks. These attacks involve adversaries attempting to recover a learned
model through limited query-response interactions, such as those found in
cloud-based services or on-chip artificial intelligence interfaces. While
existing literature proposes various attack and defense strategies, these often
lack a theoretical foundation and standardized evaluation criteria. In
response, this work presents a framework called ``Model Privacy'', providing a
foundation for comprehensively analyzing model stealing attacks and defenses.
We establish a rigorous formulation for the threat model and objectives,
propose methods to quantify the goodness of attack and defense strategies, and
analyze the fundamental tradeoffs between utility and privacy in ML models. Our
developed theory offers valuable insights into enhancing the security of ML
models, especially highlighting the importance of the attack-specific structure
of perturbations for effective defenses. We demonstrate the application of
model privacy from the defender's perspective through various learning
scenarios. Extensive experiments corroborate the insights and the effectiveness
of defense mechanisms developed under the proposed framework.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.15233v1">A General Pseudonymization Framework for Cloud-Based LLMs: Replacing
  Privacy Information in Controlled Text Generation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-02-21T06:15:53Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shilong Hou, Ruilin Shang, Zi Long, Xianghua Fu, Yin Chen</p>
    <p><b>Summary:</b> An increasing number of companies have begun providing services that leverage
cloud-based large language models (LLMs), such as ChatGPT. However, this
development raises substantial privacy concerns, as users' prompts are
transmitted to and processed by the model providers. Among the various privacy
protection methods for LLMs, those implemented during the pre-training and
fine-tuning phrases fail to mitigate the privacy risks associated with the
remote use of cloud-based LLMs by users. On the other hand, methods applied
during the inference phrase are primarily effective in scenarios where the
LLM's inference does not rely on privacy-sensitive information. In this paper,
we outline the process of remote user interaction with LLMs and, for the first
time, propose a detailed definition of a general pseudonymization framework
applicable to cloud-based LLMs. The experimental results demonstrate that the
proposed framework strikes an optimal balance between privacy protection and
utility. The code for our method is available to the public at
https://github.com/Mebymeby/Pseudonymization-Framework.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.14780v1">ReVision: A Dataset and Baseline VLM for Privacy-Preserving
  Task-Oriented Visual Instruction Rewriting</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-02-20T18:01:41Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Abhijit Mishra, Richard Noh, Hsiang Fu, Mingda Li, Minji Kim</p>
    <p><b>Summary:</b> Efficient and privacy-preserving multimodal interaction is essential as AR,
VR, and modern smartphones with powerful cameras become primary interfaces for
human-computer communication. Existing powerful large vision-language models
(VLMs) enabling multimodal interaction often rely on cloud-based processing,
raising significant concerns about (1) visual privacy by transmitting sensitive
vision data to servers, and (2) their limited real-time, on-device usability.
This paper explores Visual Instruction Rewriting, a novel approach that
transforms multimodal instructions into text-only commands, allowing seamless
integration of lightweight on-device instruction rewriter VLMs (250M
parameters) with existing conversational AI systems, enhancing vision data
privacy. To achieve this, we present a dataset of over 39,000 examples across
14 domains and develop a compact VLM, pretrained on image captioning datasets
and fine-tuned for instruction rewriting. Experimental results, evaluated
through NLG metrics such as BLEU, METEOR, and ROUGE, along with semantic
parsing analysis, demonstrate that even a quantized version of the model
(<500MB storage footprint) can achieve effective instruction rewriting, thus
enabling privacy-focused, multimodal AI applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.14761v1">User Awareness and Perspectives Survey on Privacy, Security and
  Usability of Auditory Prostheses</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-02-20T17:36:19Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sohini Saha, Leslie M. Collins, Sherri L. Smith, Boyla O. Mainsah</p>
    <p><b>Summary:</b> According to the World Health Organization, over 466 million people worldwide
suffer from disabling hearing loss, with approximately 34 million of these
being children. Hearing aids (HA) and cochlear implants (CI) have become
indispensable tools for restoring hearing and enhancing the quality of life for
individuals with hearing impairments. Clinical research and consumer studies
indicate that users of HAs and CIs report significant improvements in their
daily lives, including enhanced communication abilities and social engagement
and reduced psychological stress. Modern auditory prosthetic devices are more
advanced and interconnected with digital networks to add functionality, such as
streaming audio directly from smartphones and other devices, remote adjustments
by audiologists, integration with smart home systems, and access to artificial
intelligence-driven sound enhancement features. With this interconnectivity,
issues surrounding data privacy and security have become increasingly
pertinent. There is limited research on the usability perceptions of current HA
and CI models from the perspective of end-users. In addition, no studies have
investigated consumer mental models during the purchasing process, particularly
which factors they prioritize when selecting a device. In this study, we
assessed participants' satisfaction levels with various features of their
auditory prostheses. This work contributes to the field by addressing gaps in
user perceptions of HA and CI usability, identifying key factors in consumer
purchasing decisions, and highlighting the need for improved privacy and
security awareness and education among users.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.14309v2">On Theoretical Limits of Learning with Label Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2025-02-20T06:51:42Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Puning Zhao, Chuan Ma, Li Shen, Shaowei Wang, Rongfei Fan</p>
    <p><b>Summary:</b> Label differential privacy (DP) is designed for learning problems involving
private labels and public features. While various methods have been proposed
for learning under label DP, the theoretical limits remain largely unexplored.
In this paper, we investigate the fundamental limits of learning with label DP
in both local and central models for both classification and regression tasks,
characterized by minimax convergence rates. We establish lower bounds by
converting each task into a multiple hypothesis testing problem and bounding
the test error. Additionally, we develop algorithms that yield matching upper
bounds. Our results demonstrate that under label local DP (LDP), the risk has a
significantly faster convergence rate than that under full LDP, i.e. protecting
both features and labels, indicating the advantages of relaxing the DP
definition to focus solely on labels. In contrast, under the label central DP
(CDP), the risk is only reduced by a constant factor compared to full DP,
indicating that the relaxation of CDP only has limited benefits on the
performance.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.14291v1">A Note on Efficient Privacy-Preserving Similarity Search for Encrypted
  Vectors</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-02-20T06:07:04Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Dongfang Zhao</p>
    <p><b>Summary:</b> Traditional approaches to vector similarity search over encrypted data rely
on fully homomorphic encryption (FHE) to enable computation without decryption.
However, the substantial computational overhead of FHE makes it impractical for
large-scale real-time applications. This work explores a more efficient
alternative: using additively homomorphic encryption (AHE) for
privacy-preserving similarity search. We consider scenarios where either the
query vector or the database vectors remain encrypted, a setting that
frequently arises in applications such as confidential recommender systems and
secure federated learning. While AHE only supports addition and scalar
multiplication, we show that it is sufficient to compute inner product
similarity--one of the most widely used similarity measures in vector
retrieval. Compared to FHE-based solutions, our approach significantly reduces
computational overhead by avoiding ciphertext-ciphertext multiplications and
bootstrapping, while still preserving correctness and privacy. We present an
efficient algorithm for encrypted similarity search under AHE and analyze its
error growth and security implications. Our method provides a scalable and
practical solution for privacy-preserving vector search in real-world machine
learning applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.14087v1">Learning from End User Data with Shuffled Differential Privacy over
  Kernel Densities</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B">
  <p><b>Published on:</b> 2025-02-19T20:27:01Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tal Wagner</p>
    <p><b>Summary:</b> We study a setting of collecting and learning from private data distributed
across end users. In the shuffled model of differential privacy, the end users
partially protect their data locally before sharing it, and their data is also
anonymized during its collection to enhance privacy. This model has recently
become a prominent alternative to central DP, which requires full trust in a
central data curator, and local DP, where fully local data protection takes a
steep toll on downstream accuracy.
  Our main technical result is a shuffled DP protocol for privately estimating
the kernel density function of a distributed dataset, with accuracy essentially
matching central DP. We use it to privately learn a classifier from the end
user data, by learning a private density function per class. Moreover, we show
that the density function itself can recover the semantic content of its class,
despite having been learned in the absence of any unprotected data. Our
experiments show the favorable downstream performance of our approach, and
highlight key downstream considerations and trade-offs in a practical ML
deployment of shuffled DP.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.13833v2">Contrastive Learning-Based privacy metrics in Tabular Synthetic Datasets</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-02-19T15:52:23Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Milton Nicolás Plasencia Palacios, Sebastiano Saccani, Gabriele Sgroi, Alexander Boudewijn, Luca Bortolussi</p>
    <p><b>Summary:</b> Synthetic data has garnered attention as a Privacy Enhancing Technology (PET)
in sectors such as healthcare and finance. When using synthetic data in
practical applications, it is important to provide protection guarantees. In
the literature, two family of approaches are proposed for tabular data: on the
one hand, Similarity-based methods aim at finding the level of similarity
between training and synthetic data. Indeed, a privacy breach can occur if the
generated data is consistently too similar or even identical to the train data.
On the other hand, Attack-based methods conduce deliberate attacks on synthetic
datasets. The success rates of these attacks reveal how secure the synthetic
datasets are.
  In this paper, we introduce a contrastive method that improves privacy
assessment of synthetic datasets by embedding the data in a more representative
space. This overcomes obstacles surrounding the multitude of data types and
attributes. It also makes the use of intuitive distance metrics possible for
similarity measurements and as an attack vector. In a series of experiments
with publicly available datasets, we compare the performances of
similarity-based and attack-based methods, both with and without use of the
contrastive learning-based embeddings. Our results show that relatively
efficient, easy to implement privacy metrics can perform equally well as more
advanced metrics explicitly modeling conditions for privacy referred to by the
GDPR.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.14921v1">The Canary's Echo: Auditing Privacy Risks of LLM-Generated Synthetic
  Text</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-02-19T15:30:30Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Matthieu Meeus, Lukas Wutschitz, Santiago Zanella-Béguelin, Shruti Tople, Reza Shokri</p>
    <p><b>Summary:</b> How much information about training samples can be gleaned from synthetic
data generated by Large Language Models (LLMs)? Overlooking the subtleties of
information flow in synthetic data generation pipelines can lead to a false
sense of privacy. In this paper, we design membership inference attacks (MIAs)
that target data used to fine-tune pre-trained LLMs that are then used to
synthesize data, particularly when the adversary does not have access to the
fine-tuned model but only to the synthetic data. We show that such data-based
MIAs do significantly better than a random guess, meaning that synthetic data
leaks information about the training data. Further, we find that canaries
crafted to maximize vulnerability to model-based MIAs are sub-optimal for
privacy auditing when only synthetic data is released. Such out-of-distribution
canaries have limited influence on the model's output when prompted to generate
useful, in-distribution synthetic data, which drastically reduces their
vulnerability. To tackle this problem, we leverage the mechanics of
auto-regressive models to design canaries with an in-distribution prefix and a
high-perplexity suffix that leave detectable traces in synthetic data. This
enhances the power of data-based MIAs and provides a better assessment of the
privacy risks of releasing synthetic data generated by LLMs.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.13564v1">PRIV-QA: Privacy-Preserving Question Answering for Cloud Large Language
  Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-02-19T09:17:07Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Guangwei Li, Yuansen Zhang, Yinggui Wang, Shoumeng Yan, Lei Wang, Tao Wei</p>
    <p><b>Summary:</b> The rapid development of large language models (LLMs) is redefining the
landscape of human-computer interaction, and their integration into various
user-service applications is becoming increasingly prevalent. However,
transmitting user data to cloud-based LLMs presents significant risks of data
breaches and unauthorized access to personal identification information. In
this paper, we propose a privacy preservation pipeline for protecting privacy
and sensitive information during interactions between users and LLMs in
practical LLM usage scenarios. We construct SensitiveQA, the first privacy
open-ended question-answering dataset. It comprises 57k interactions in Chinese
and English, encompassing a diverse range of user-sensitive information within
the conversations. Our proposed solution employs a multi-stage strategy aimed
at preemptively securing user information while simultaneously preserving the
response quality of cloud-based LLMs. Experimental validation underscores our
method's efficacy in balancing privacy protection with maintaining robust
interaction quality. The code and dataset are available at
https://github.com/ligw1998/PRIV-QA.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.13415v1">Indifferential Privacy: A New Paradigm and Its Applications to Optimal
  Matching in Dark Pool Auctions</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-02-19T04:19:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Antigoni Polychroniadou, T. -H. Hubert Chan, Adya Agrawal</p>
    <p><b>Summary:</b> Public exchanges like the New York Stock Exchange and NASDAQ act as
auctioneers in a public double auction system, where buyers submit their
highest bids and sellers offer their lowest asking prices, along with the
number of shares (volume) they wish to trade. The auctioneer matches compatible
orders and executes the trades when a match is found. However, auctioneers
involved in high-volume exchanges, such as dark pools, may not always be
reliable. They could exploit their position by engaging in practices like
front-running or face significant conflicts of interest, i.e., ethical breaches
that have frequently resulted in hefty fines and regulatory scrutiny within the
financial industry.
  Previous solutions, based on the use of fully homomorphic encryption (Asharov
et al., AAMAS 2020), encrypt orders ensuring that information is revealed only
when a match occurs. However, this approach introduces significant
computational overhead, making it impractical for high-frequency trading
environments such as dark pools.
  In this work, we propose a new system based on differential privacy combined
with lightweight encryption, offering an efficient and practical solution that
mitigates the risks of an untrustworthy auctioneer. Specifically, we introduce
a new concept called Indifferential Privacy, which can be of independent
interest, where a user is indifferent to whether certain information is
revealed after some special event, unlike standard differential privacy. For
example, in an auction, it's reasonable to disclose the true volume of a trade
once all of it has been matched. Moreover, our new concept of Indifferential
Privacy allows for maximum matching, which is impossible with conventional
differential privacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.13313v1">Revisiting Privacy, Utility, and Efficiency Trade-offs when Fine-Tuning
  Large Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-02-18T22:16:03Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Soumi Das, Camila Kolling, Mohammad Aflah Khan, Mahsa Amani, Bishwamittra Ghosh, Qinyuan Wu, Till Speicher, Krishna P. Gummadi</p>
    <p><b>Summary:</b> We study the inherent trade-offs in minimizing privacy risks and maximizing
utility, while maintaining high computational efficiency, when fine-tuning
large language models (LLMs). A number of recent works in privacy research have
attempted to mitigate privacy risks posed by memorizing fine-tuning data by
using differentially private training methods (e.g., DP), albeit at a
significantly higher computational cost (inefficiency). In parallel, several
works in systems research have focussed on developing (parameter) efficient
fine-tuning methods (e.g., LoRA), but few works, if any, investigated whether
such efficient methods enhance or diminish privacy risks. In this paper, we
investigate this gap and arrive at a surprising conclusion: efficient
fine-tuning methods like LoRA mitigate privacy risks similar to private
fine-tuning methods like DP. Our empirical finding directly contradicts
prevailing wisdom that privacy and efficiency objectives are at odds during
fine-tuning. Our finding is established by (a) carefully defining measures of
privacy and utility that distinguish between memorizing sensitive and
non-sensitive tokens in training and test datasets used in fine-tuning and (b)
extensive evaluations using multiple open-source language models from Pythia,
Gemma, and Llama families and different domain-specific datasets.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.12976v1">Does Training with Synthetic Data Truly Protect Privacy?</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-02-18T15:56:52Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yunpeng Zhao, Jie Zhang</p>
    <p><b>Summary:</b> As synthetic data becomes increasingly popular in machine learning tasks,
numerous methods--without formal differential privacy guarantees--use synthetic
data for training. These methods often claim, either explicitly or implicitly,
to protect the privacy of the original training data. In this work, we explore
four different training paradigms: coreset selection, dataset distillation,
data-free knowledge distillation, and synthetic data generated from diffusion
models. While all these methods utilize synthetic data for training, they lead
to vastly different conclusions regarding privacy preservation. We caution that
empirical approaches to preserving data privacy require careful and rigorous
evaluation; otherwise, they risk providing a false sense of privacy.</p>
  </details>
</div>



<h2>2025-03</h2>

<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.09448v1">Optimizing QoE-Privacy Tradeoff for Proactive VR Streaming</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Multimedia-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Multiagent Systems-662E9B">
  <p><b>Published on:</b> 2025-03-12T14:50:06Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xing Wei, Shengqian Han, Chenyang Yang, Chengjian Sun</p>
    <p><b>Summary:</b> Proactive virtual reality (VR) streaming requires users to upload
viewpoint-related information, raising significant privacy concerns. Existing
strategies preserve privacy by introducing errors to viewpoints, which,
however, compromises the quality of experience (QoE) of users. In this paper,
we first delve into the analysis of the viewpoint leakage probability achieved
by existing privacy-preserving approaches. We determine the optimal
distribution of viewpoint errors that minimizes the viewpoint leakage
probability. Our analyses show that existing approaches cannot fully eliminate
viewpoint leakage. Then, we propose a novel privacy-preserving approach that
introduces noise to uploaded viewpoint prediction errors, which can ensure zero
viewpoint leakage probability. Given the proposed approach, the tradeoff
between privacy preservation and QoE is optimized to minimize the QoE loss
while satisfying the privacy requirement. Simulation results validate our
analysis results and demonstrate that the proposed approach offers a promising
solution for balancing privacy and QoE.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.09381v1">Faithful and Privacy-Preserving Implementation of Average Consensus</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36">
  <p><b>Published on:</b> 2025-03-12T13:28:22Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kaoru Teranishi, Kiminao Kogiso, Takashi Tanaka</p>
    <p><b>Summary:</b> We propose a protocol based on mechanism design theory and encrypted control
to solve average consensus problems among rational and strategic agents while
preserving their privacy. The proposed protocol provides a mechanism that
incentivizes the agents to faithfully implement the intended behavior specified
in the protocol. Furthermore, the protocol runs over encrypted data using
homomorphic encryption and secret sharing to protect the privacy of agents. We
also analyze the security of the proposed protocol using a simulation paradigm
in secure multi-party computation. The proposed protocol demonstrates that
mechanism design and encrypted control can complement each other to achieve
security under rational adversaries.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.09365v1">Membership Inference Attacks fueled by Few-Short Learning to detect
  privacy leakage tackling data integrity</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-03-12T13:09:43Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Daniel Jiménez-López, Nuria Rodríguez-Barroso, M. Victoria Luzón, Francisco Herrera</p>
    <p><b>Summary:</b> Deep learning models have an intrinsic privacy issue as they memorize parts
of their training data, creating a privacy leakage. Membership Inference
Attacks (MIA) exploit it to obtain confidential information about the data used
for training, aiming to steal information. They can be repurposed as a
measurement of data integrity by inferring whether it was used to train a
machine learning model. While state-of-the-art attacks achieve a significant
privacy leakage, their requirements are not feasible enough, hindering their
role as practical tools to assess the magnitude of the privacy risk. Moreover,
the most appropriate evaluation metric of MIA, the True Positive Rate at low
False Positive Rate lacks interpretability. We claim that the incorporation of
Few-Shot Learning techniques to the MIA field and a proper qualitative and
quantitative privacy evaluation measure should deal with these issues. In this
context, our proposal is twofold. We propose a Few-Shot learning based MIA,
coined as the FeS-MIA model, which eases the evaluation of the privacy breach
of a deep learning model by significantly reducing the number of resources
required for the purpose. Furthermore, we propose an interpretable quantitative
and qualitative measure of privacy, referred to as Log-MIA measure. Jointly,
these proposals provide new tools to assess the privacy leakage and to ease the
evaluation of the training data integrity of deep learning models, that is, to
analyze the privacy breach of a deep learning model. Experiments carried out
with MIA over image classification and language modeling tasks and its
comparison to the state-of-the-art show that our proposals excel at reporting
the privacy leakage of a deep learning model with little extra information.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.09331v1">Large-Scale FPGA-Based Privacy Amplification Exceeding $10^8$ Bits for
  Quantum Key Distribution</a></h3>
  
  <p><b>Published on:</b> 2025-03-12T12:25:13Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xi Cheng, Hao-kun Mao, Hong-wei Xu, Qiong Li</p>
    <p><b>Summary:</b> Privacy Amplification (PA) is indispensable in Quantum Key Distribution (QKD)
post-processing, as it eliminates information leakage to eavesdroppers.
Field-programmable gate arrays (FPGAs) are highly attractive for QKD systems
due to their flexibility and high integration. However, due to limited
resources, input and output sizes remain the primary bottleneck in FPGA-based
PA schemes for Discrete Variable (DV)-QKD systems. In this paper, we present a
large-scale FPGA-based PA scheme that supports both input block sizes and
output key sizes exceeding $10^8$ bits, effectively addressing the challenges
posed by the finite-size effect. To accommodate the large input and output
sizes, we propose a novel PA algorithm and prove its security. We implement and
evaluate this scheme on a Xilinx XCKU095 FPGA platform. Experimental results
demonstrate that our PA implementation can handle an input block size of $10^8$
bits with flexible output sizes up to the input size. For DV-QKD systems, our
PA scheme supports an input block size nearly two orders of magnitude larger
than current FPGA-based PA schemes, significantly mitigating the impact of the
finite-size effect on the final secure key rate.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.09317v1">RaceTEE: A Practical Privacy-Preserving Off-Chain Smart Contract
  Execution Architecture</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-12T12:10:02Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Keyu Zhang, Andrew Martin</p>
    <p><b>Summary:</b> Decentralized on-chain smart contracts enable trustless collaboration, yet
their inherent data transparency and execution overhead hinder widespread
adoption. Existing cryptographic approaches incur high computational costs and
lack generality. Meanwhile, prior TEE-based solutions suffer from practical
limitations, such as the inability to support inter-contract interactions,
reliance on unbreakable TEEs, and compromised usability. We introduce RaceTEE,
a practical and privacy-preserving off-chain execution architecture for smart
contracts that leverages Trusted Execution Environments (TEEs). RaceTEE
decouples transaction ordering (on-chain) from execution (off-chain), with
computations performed competitively in TEEs, ensuring confidentiality and
minimizing overhead. It further enhances practicality through three key
improvements: supporting secure inter-contract interactions, providing a key
rotation scheme that enforces forward and backward secrecy even in the event of
TEE breaches, and enabling full compatibility with existing blockchains without
altering the user interaction model. To validate its feasibility, we prototype
RaceTEE using Intel SGX and Ethereum, demonstrating its applicability across
various use cases and evaluating its performance.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.09192v1">Differential Privacy Personalized Federated Learning Based on
  Dynamically Sparsified Client Updates</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-12T09:34:05Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chuanyin Wang, Yifei Zhang, Neng Gao, Qiang Luo</p>
    <p><b>Summary:</b> Personalized federated learning is extensively utilized in scenarios
characterized by data heterogeneity, facilitating more efficient and automated
local training on data-owning terminals. This includes the automated selection
of high-performance model parameters for upload, thereby enhancing the overall
training process. However, it entails significant risks of privacy leakage.
Existing studies have attempted to mitigate these risks by utilizing
differential privacy. Nevertheless, these studies present two major
limitations: (1) The integration of differential privacy into personalized
federated learning lacks sufficient personalization, leading to the
introduction of excessive noise into the model. (2) It fails to adequately
control the spatial scope of model update information, resulting in a
suboptimal balance between data privacy and model effectiveness in differential
privacy federated learning. In this paper, we propose a differentially private
personalized federated learning approach that employs dynamically sparsified
client updates through reparameterization and adaptive norm(DP-pFedDSU).
Reparameterization training effectively selects personalized client update
information, thereby reducing the quantity of updates. This approach minimizes
the introduction of noise to the greatest extent possible. Additionally,
dynamic adaptive norm refers to controlling the norm space of model updates
during the training process, mitigating the negative impact of clipping on the
update information. These strategies substantially enhance the effective
integration of differential privacy and personalized federated learning.
Experimental results on EMNIST, CIFAR-10, and CIFAR-100 demonstrate that our
proposed scheme achieves superior performance and is well-suited for more
complex personalized federated learning scenarios.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.08568v1">Privacy Law Enforcement Under Centralized Governance: A Qualitative
  Analysis of Four Years' Special Privacy Rectification Campaigns</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-11T15:56:09Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tao Jing, Yao Li, Jingzhou Ye, Jie Wang, Xueqiang Wang</p>
    <p><b>Summary:</b> In recent years, major privacy laws like the GDPR have brought about positive
changes. However, challenges remain in enforcing the laws, particularly due to
under-resourced regulators facing a large number of potential privacy-violating
software applications (apps) and the high costs of investigating them. Since
2019, China has launched a series of privacy enforcement campaigns known as
Special Privacy Rectification Campaigns (SPRCs) to address widespread privacy
violations in its mobile application (app) ecosystem. Unlike the enforcement of
the GDPR, SPRCs are characterized by large-scale privacy reviews and strict
sanctions, under the strong control of central authorities. In SPRCs, central
government authorities issue administrative orders to mobilize various
resources for market-wide privacy reviews of mobile apps. They enforce strict
sanctions by requiring privacy-violating apps to rectify issues within a short
timeframe or face removal from app stores. While there are a few reports on
SPRCs, the effectiveness and potential problems of this campaign-style privacy
enforcement approach remain unclear to the community. In this study, we
conducted 18 semi-structured interviews with app-related engineers involved in
SPRCs to better understand the campaign-style privacy enforcement. Based on the
interviews, we reported our findings on a variety of aspects of SPRCs, such as
the processes that app engineers regularly follow to achieve privacy compliance
in SPRCs, the challenges they encounter, the solutions they adopt to address
these challenges, and the impacts of SPRCs, etc. We found that app engineers
face a series of challenges in achieving privacy compliance in their apps...</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.08297v1">Privacy for Free: Leveraging Local Differential Privacy Perturbed Data
  from Multiple Services</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-11T11:10:03Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Rong Du, Qingqing Ye, Yue Fu, Haibo Hu</p>
    <p><b>Summary:</b> Local Differential Privacy (LDP) has emerged as a widely adopted
privacy-preserving technique in modern data analytics, enabling users to share
statistical insights while maintaining robust privacy guarantees. However,
current LDP applications assume a single service gathering perturbed
information from users. In reality, multiple services may be interested in
collecting users' data, which poses privacy burdens to users as more such
services emerge. To address this issue, this paper proposes a framework for
collecting and aggregating data based on perturbed information from multiple
services, regardless of their estimated statistics (e.g., mean or distribution)
and perturbation mechanisms.
  Then for mean estimation, we introduce the Unbiased Averaging (UA) method and
its optimized version, User-level Weighted Averaging (UWA). The former utilizes
biased perturbed data, while the latter assigns weights to different perturbed
results based on perturbation information, thereby achieving minimal variance.
For distribution estimation, we propose the User-level Likelihood Estimation
(ULE), which treats all perturbed results from a user as a whole for maximum
likelihood estimation. Experimental results demonstrate that our framework and
constituting methods significantly improve the accuracy of both mean and
distribution estimation.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.08175v1">Privacy-Enhancing Paradigms within Federated Multi-Agent Systems</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-03-11T08:38:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zitong Shi, Guancheng Wan, Wenke Huang, Guibin Zhang, Jiawei Shao, Mang Ye, Carl Yang</p>
    <p><b>Summary:</b> LLM-based Multi-Agent Systems (MAS) have proven highly effective in solving
complex problems by integrating multiple agents, each performing different
roles. However, in sensitive domains, they face emerging privacy protection
challenges. In this paper, we introduce the concept of Federated MAS,
highlighting the fundamental differences between Federated MAS and traditional
FL. We then identify key challenges in developing Federated MAS, including: 1)
heterogeneous privacy protocols among agents, 2) structural differences in
multi-party conversations, and 3) dynamic conversational network structures. To
address these challenges, we propose Embedded Privacy-Enhancing Agents
(EPEAgent), an innovative solution that integrates seamlessly into the
Retrieval-Augmented Generation (RAG) phase and the context retrieval stage.
This solution minimizes data flows, ensuring that only task-relevant,
agent-specific information is shared. Additionally, we design and generate a
comprehensive dataset to evaluate the proposed paradigm. Extensive experiments
demonstrate that EPEAgent effectively enhances privacy protection while
maintaining strong system performance. The code will be availiable at
https://github.com/ZitongShi/EPEAgent</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.08085v2">PRISM: Privacy-Preserving Improved Stochastic Masking for Federated
  Generative Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-03-11T06:37:54Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kyeongkook Seo, Dong-Jun Han, Jaejun Yoo</p>
    <p><b>Summary:</b> Despite recent advancements in federated learning (FL), the integration of
generative models into FL has been limited due to challenges such as high
communication costs and unstable training in heterogeneous data environments.
To address these issues, we propose PRISM, a FL framework tailored for
generative models that ensures (i) stable performance in heterogeneous data
distributions and (ii) resource efficiency in terms of communication cost and
final model size. The key of our method is to search for an optimal stochastic
binary mask for a random network rather than updating the model weights,
identifying a sparse subnetwork with high generative performance; i.e., a
``strong lottery ticket''. By communicating binary masks in a stochastic
manner, PRISM minimizes communication overhead. This approach, combined with
the utilization of maximum mean discrepancy (MMD) loss and a mask-aware dynamic
moving average aggregation method (MADA) on the server side, facilitates stable
and strong generative capabilities by mitigating local divergence in FL
scenarios. Moreover, thanks to its sparsifying characteristic, PRISM yields a
lightweight model without extra pruning or quantization, making it ideal for
environments such as edge devices. Experiments on MNIST, FMNIST, CelebA, and
CIFAR10 demonstrate that PRISM outperforms existing methods, while maintaining
privacy with minimal communication costs. PRISM is the first to successfully
generate images under challenging non-IID and privacy-preserving FL
environments on complex datasets, where previous methods have struggled.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.07775v1">Sublinear Algorithms for Wasserstein and Total Variation Distances:
  Applications to Fairness and Privacy Auditing</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B"> 
  <p><b>Published on:</b> 2025-03-10T18:57:48Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Debabrota Basu, Debarshi Chanda</p>
    <p><b>Summary:</b> Resource-efficiently computing representations of probability distributions
and the distances between them while only having access to the samples is a
fundamental and useful problem across mathematical sciences. In this paper, we
propose a generic algorithmic framework to estimate the PDF and CDF of any
sub-Gaussian distribution while the samples from them arrive in a stream. We
compute mergeable summaries of distributions from the stream of samples that
require sublinear space w.r.t. the number of observed samples. This allows us
to estimate Wasserstein and Total Variation (TV) distances between any two
sub-Gaussian distributions while samples arrive in streams and from multiple
sources (e.g. federated learning). Our algorithms significantly improves on the
existing methods for distance estimation incurring super-linear time and linear
space complexities. In addition, we use the proposed estimators of Wasserstein
and TV distances to audit the fairness and privacy of the ML algorithms. We
empirically demonstrate the efficiency of the algorithms for estimating these
distances and auditing using both synthetic and real-world datasets.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.07570v1">Split-n-Chain: Privacy-Preserving Multi-Node Split Learning with
  Blockchain-Based Auditability</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-03-10T17:40:05Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mukesh Sahani, Binanda Sengupta</p>
    <p><b>Summary:</b> Deep learning, when integrated with a large amount of training data, has the
potential to outperform machine learning in terms of high accuracy. Recently,
privacy-preserving deep learning has drawn significant attention of the
research community. Different privacy notions in deep learning include privacy
of data provided by data-owners and privacy of parameters and/or
hyperparameters of the underlying neural network. Federated learning is a
popular privacy-preserving execution environment where data-owners participate
in learning the parameters collectively without leaking their respective data
to other participants. However, federated learning suffers from certain
security/privacy issues. In this paper, we propose Split-n-Chain, a variant of
split learning where the layers of the network are split among several
distributed nodes. Split-n-Chain achieves several privacy properties:
data-owners need not share their training data with other nodes, and no nodes
have access to the parameters and hyperparameters of the neural network (except
that of the respective layers they hold). Moreover, Split-n-Chain uses
blockchain to audit the computation done by different nodes. Our experimental
results show that: Split-n-Chain is efficient, in terms of time required to
execute different phases, and the training loss trend is similar to that for
the same neural network when implemented in a monolithic fashion.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.07505v1">From Centralized to Decentralized Federated Learning: Theoretical
  Insights, Privacy Preservation, and Robustness Challenges</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2025-03-10T16:27:40Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Qiongxiu Li, Wenrui Yu, Yufei Xia, Jun Pang</p>
    <p><b>Summary:</b> Federated Learning (FL) enables collaborative learning without directly
sharing individual's raw data. FL can be implemented in either a centralized
(server-based) or decentralized (peer-to-peer) manner. In this survey, we
present a novel perspective: the fundamental difference between centralized FL
(CFL) and decentralized FL (DFL) is not merely the network topology, but the
underlying training protocol: separate aggregation vs. joint optimization. We
argue that this distinction in protocol leads to significant differences in
model utility, privacy preservation, and robustness to attacks. We
systematically review and categorize existing works in both CFL and DFL
according to the type of protocol they employ. This taxonomy provides deeper
insights into prior research and clarifies how various approaches relate or
differ. Through our analysis, we identify key gaps in the literature. In
particular, we observe a surprising lack of exploration of DFL approaches based
on distributed optimization methods, despite their potential advantages. We
highlight this under-explored direction and call for more research on
leveraging distributed optimization for federated learning. Overall, this work
offers a comprehensive overview from centralized to decentralized FL, sheds new
light on the core distinctions between approaches, and outlines open challenges
and future directions for the field.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.07427v2">Creating and Evaluating Privacy and Security Micro-Lessons for
  Elementary School Children</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2025-03-10T15:12:11Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Lan Gao, Elana B Blinder, Abigail Barnes, Kevin Song, Tamara Clegg, Jessica Vitak, Marshini Chetty</p>
    <p><b>Summary:</b> The growing use of technology in K--8 classrooms highlights a parallel need
for formal learning opportunities aimed at helping children use technology
safely and protect their personal information. Even the youngest students are
now using tablets, laptops, and apps to support their learning; however, there
are limited curricular materials available for elementary and middle school
children on digital privacy and security topics. To bridge this gap, we
developed a series of micro-lessons to help K--8 children learn about digital
privacy and security at school. We first conducted a formative study by
interviewing elementary school teachers to identify the design needs for
digital privacy and security lessons. We then developed micro-lessons --
multiple 15-20 minute activities designed to be easily inserted into the
existing curriculum -- using a co-design approach with multiple rounds of
developing and revising the micro-lessons in collaboration with teachers.
Throughout the process, we conducted evaluation sessions where teachers
implemented or reviewed the micro-lessons. Our study identifies strengths,
challenges, and teachers' tailoring strategies when incorporating micro-lessons
for K--8 digital privacy and security topics, providing design implications for
facilitating learning about these topics in school classrooms.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.07216v2">FedRand: Enhancing Privacy in Federated Learning with Randomized LoRA
  Subparameter Updates</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-03-10T11:55:50Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sangwoo Park, Seanie Lee, Byungjoo Kim, Sung Ju Hwang</p>
    <p><b>Summary:</b> Federated Learning (FL) is a widely used framework for training models in a
decentralized manner, ensuring that the central server does not have direct
access to data from local clients. However, this approach may still fail to
fully preserve data privacy, as models from local clients are exposed to the
central server during the aggregation process. This issue becomes even more
critical when training vision-language models (VLMs) with FL, as VLMs can
easily memorize training data instances, making them vulnerable to membership
inference attacks (MIAs). To address this challenge, we propose the FedRand
framework, which avoids disclosing the full set of client parameters. In this
framework, each client randomly selects subparameters of Low-Rank Adaptation
(LoRA) from the server and keeps the remaining counterparts of the LoRA weights
as private parameters. After training both parameters on the client's private
dataset, only the non-private client parameters are sent back to the server for
aggregation. This approach mitigates the risk of exposing client-side VLM
parameters, thereby enhancing data privacy. We empirically validate that
FedRand improves robustness against MIAs compared to relevant baselines while
achieving accuracy comparable to methods that communicate full LoRA parameters
across several benchmark datasets.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.07199v1">How Well Can Differential Privacy Be Audited in One Run?</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-10T11:32:30Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Amit Keinan, Moshe Shenfeld, Katrina Ligett</p>
    <p><b>Summary:</b> Recent methods for auditing the privacy of machine learning algorithms have
improved computational efficiency by simultaneously intervening on multiple
training examples in a single training run. Steinke et al. (2024) prove that
one-run auditing indeed lower bounds the true privacy parameter of the audited
algorithm, and give impressive empirical results. Their work leaves open the
question of how precisely one-run auditing can uncover the true privacy
parameter of an algorithm, and how that precision depends on the audited
algorithm. In this work, we characterize the maximum achievable efficacy of
one-run auditing and show that one-run auditing can only perfectly uncover the
true privacy parameters of algorithms whose structure allows the effects of
individual data elements to be isolated. Our characterization helps reveal how
and when one-run auditing is still a promising technique for auditing real
machine learning algorithms, despite these fundamental gaps.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.07048v1">A Failure-Free and Efficient Discrete Laplace Distribution for
  Differential Privacy in MPC</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-10T08:35:16Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ivan Tjuawinata, Jiabo Wang, Mengmeng Yang, Shanxiang Lyu, Huaxiong Wang, Kwok-Yan Lam</p>
    <p><b>Summary:</b> In an MPC-protected distributed computation, although the use of MPC assures
data privacy during computation, sensitive information may still be inferred by
curious MPC participants from the computation output. This can be observed, for
instance, in the inference attacks on either federated learning or a more
standard statistical computation with distributed inputs. In this work, we
address this output privacy issue by proposing a discrete and bounded
Laplace-inspired perturbation mechanism along with a secure realization of this
mechanism using MPC. The proposed mechanism strictly adheres to a zero failure
probability, overcoming the limitation encountered on other existing bounded
and discrete variants of Laplace perturbation. We provide analyses of the
proposed differential privacy (DP) perturbation in terms of its privacy and
utility. Additionally, we designed MPC protocols to implement this mechanism
and presented performance benchmarks based on our experimental setup. The MPC
realization of the proposed mechanism exhibits a complexity similar to the
state-of-the-art discrete Gaussian mechanism, which can be considered an
alternative with comparable efficiency while providing stronger differential
privacy guarantee. Moreover, efficiency of the proposed scheme can be further
enhanced by performing the noise generation offline while leaving the
perturbation phase online.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.06808v1">Privacy Auditing of Large Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-03-09T23:32:15Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ashwinee Panda, Xinyu Tang, Milad Nasr, Christopher A. Choquette-Choo, Prateek Mittal</p>
    <p><b>Summary:</b> Current techniques for privacy auditing of large language models (LLMs) have
limited efficacy -- they rely on basic approaches to generate canaries which
leads to weak membership inference attacks that in turn give loose lower bounds
on the empirical privacy leakage. We develop canaries that are far more
effective than those used in prior work under threat models that cover a range
of realistic settings. We demonstrate through extensive experiments on multiple
families of fine-tuned LLMs that our approach sets a new standard for detection
of privacy leakage. For measuring the memorization rate of non-privately
trained LLMs, our designed canaries surpass prior approaches. For example, on
the Qwen2.5-0.5B model, our designed canaries achieve $49.6\%$ TPR at $1\%$
FPR, vastly surpassing the prior approach's $4.2\%$ TPR at $1\%$ FPR. Our
method can be used to provide a privacy audit of $\varepsilon \approx 1$ for a
model trained with theoretical $\varepsilon$ of 4. To the best of our
knowledge, this is the first time that a privacy audit of LLM training has
achieved nontrivial auditing success in the setting where the attacker cannot
train shadow models, insert gradient canaries, or access the model at every
iteration.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.06732v1">Data Efficient Subset Training with Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-03-09T19:05:10Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ninad Jayesh Gandhi, Moparthy Venkata Subrahmanya Sri Harsha</p>
    <p><b>Summary:</b> Private machine learning introduces a trade-off between the privacy budget
and training performance. Training convergence is substantially slower and
extensive hyper parameter tuning is required. Consequently, efficient methods
to conduct private training of models is thoroughly investigated in the
literature. To this end, we investigate the strength of the data efficient
model training methods in the private training setting. We adapt GLISTER
(Killamsetty et al., 2021b) to the private setting and extensively assess its
performance. We empirically find that practical choices of privacy budgets are
too restrictive for data efficient training in the private setting.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.06455v1">Privacy Protection in Prosumer Energy Management Based on Federated
  Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-03-09T05:29:29Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yunfeng Li, Xiaolin Li Zhitao Li, Gangqiang Li</p>
    <p><b>Summary:</b> With the booming development of prosumers, there is an urgent need for a
prosumer energy management system to take full advantage of the flexibility of
prosumers and take into account the interests of other parties. However,
building such a system will undoubtedly reveal users' privacy. In this paper,
by solving the non-independent and identical distribution of data (Non-IID)
problem in federated learning with federated cluster average(FedClusAvg)
algorithm, prosumers' information can efficiently participate in the
intelligent decision making of the system without revealing privacy. In the
proposed FedClusAvg algorithm, each client performs cluster stratified sampling
and multiple iterations. Then, the average weight of the parameters of the
sub-server is determined according to the degree of deviation of the parameter
from the average parameter. Finally, the sub-server multiple local iterations
and updates, and then upload to the main server. The advantages of FedClusAvg
algorithm are the following two parts. First, the accuracy of the model in the
case of Non-IID is improved through the method of clustering and parameter
weighted average. Second, local multiple iterations and three-tier framework
can effectively reduce communication rounds.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.06150v2">Do Fairness Interventions Come at the Cost of Privacy: Evaluations for
  Binary Classifiers</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-03-08T10:21:21Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Huan Tian, Guangsheng Zhang, Bo Liu, Tianqing Zhu, Ming Ding, Wanlei Zhou</p>
    <p><b>Summary:</b> While in-processing fairness approaches show promise in mitigating biased
predictions, their potential impact on privacy leakage remains under-explored.
We aim to address this gap by assessing the privacy risks of fairness-enhanced
binary classifiers via membership inference attacks (MIAs) and attribute
inference attacks (AIAs). Surprisingly, our results reveal that enhancing
fairness does not necessarily lead to privacy compromises. For example, these
fairness interventions exhibit increased resilience against MIAs and AIAs. This
is because fairness interventions tend to remove sensitive information among
extracted features and reduce confidence scores for the majority of training
data for fairer predictions. However, during the evaluations, we uncover a
potential threat mechanism that exploits prediction discrepancies between fair
and biased models, leading to advanced attack results for both MIAs and AIAs.
This mechanism reveals potent vulnerabilities of fair models and poses
significant privacy risks of current fairness methods. Extensive experiments
across multiple datasets, attack methods, and representative fairness
approaches confirm our findings and demonstrate the efficacy of the uncovered
mechanism. Our study exposes the under-explored privacy threats in fairness
studies, advocating for thorough evaluations of potential security
vulnerabilities before model deployments.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.06021v1">FedEM: A Privacy-Preserving Framework for Concurrent Utility
  Preservation in Federated Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-03-08T02:48:00Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mingcong Xu, Xiaojin Zhang, Wei Chen, Hai Jin</p>
    <p><b>Summary:</b> Federated Learning (FL) enables collaborative training of models across
distributed clients without sharing local data, addressing privacy concerns in
decentralized systems. However, the gradient-sharing process exposes private
data to potential leakage, compromising FL's privacy guarantees in real-world
applications. To address this issue, we propose Federated Error Minimization
(FedEM), a novel algorithm that incorporates controlled perturbations through
adaptive noise injection. This mechanism effectively mitigates gradient leakage
attacks while maintaining model performance. Experimental results on benchmark
datasets demonstrate that FedEM significantly reduces privacy risks and
preserves model accuracy, achieving a robust balance between privacy protection
and utility preservation.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.05954v1">A Survey on Tabular Data Generation: Utility, Alignment, Fidelity,
  Privacy, and Beyond</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-03-07T21:47:11Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mihaela Cătălina Stoian, Eleonora Giunchiglia, Thomas Lukasiewicz</p>
    <p><b>Summary:</b> Generative modelling has become the standard approach for synthesising
tabular data. However, different use cases demand synthetic data to comply with
different requirements to be useful in practice. In this survey, we review deep
generative modelling approaches for tabular data from the perspective of four
types of requirements: utility of the synthetic data, alignment of the
synthetic data with domain-specific knowledge, statistical fidelity of the
synthetic data distribution compared to the real data distribution, and
privacy-preserving capabilities. We group the approaches along two levels of
granularity: (i) based on the primary type of requirements they address and
(ii) according to the underlying model they utilise. Additionally, we summarise
the appropriate evaluation methods for each requirement and the specific
characteristics of each model type. Finally, we discuss future directions for
the field, along with opportunities to improve the current evaluation methods.
Overall, this survey can be seen as a user guide to tabular data generation:
helping readers navigate available models and evaluation methods to find those
best suited to their needs.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.05684v1">Fairness-Aware Low-Rank Adaptation Under Demographic Privacy Constraints</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-03-07T18:49:57Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Parameswaran Kamalaruban, Mark Anderson, Stuart Burrell, Maeve Madigan, Piotr Skalski, David Sutton</p>
    <p><b>Summary:</b> Pre-trained foundation models can be adapted for specific tasks using
Low-Rank Adaptation (LoRA). However, the fairness properties of these adapted
classifiers remain underexplored. Existing fairness-aware fine-tuning methods
rely on direct access to sensitive attributes or their predictors, but in
practice, these sensitive attributes are often held under strict consumer
privacy controls, and neither the attributes nor their predictors are available
to model developers, hampering the development of fair models. To address this
issue, we introduce a set of LoRA-based fine-tuning methods that can be trained
in a distributed fashion, where model developers and fairness auditors
collaborate without sharing sensitive attributes or predictors. In this paper,
we evaluate three such methods - sensitive unlearning, adversarial training,
and orthogonality loss - against a fairness-unaware baseline, using experiments
on the CelebA and UTK-Face datasets with an ImageNet pre-trained ViT-Base
model. We find that orthogonality loss consistently reduces bias while
maintaining or improving utility, whereas adversarial training improves False
Positive Rate Parity and Demographic Parity in some cases, and sensitive
unlearning provides no clear benefit. In tasks where significant biases are
present, distributed fairness-aware fine-tuning methods can effectively
eliminate bias without compromising consumer privacy and, in most cases,
improve model utility.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.04980v1">A Consensus Privacy Metrics Framework for Synthetic Data</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-03-06T21:19:02Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Lisa Pilgram, Fida K. Dankar, Jorg Drechsler, Mark Elliot, Josep Domingo-Ferrer, Paul Francis, Murat Kantarcioglu, Linglong Kong, Bradley Malin, Krishnamurty Muralidhar, Puja Myles, Fabian Prasser, Jean Louis Raisaro, Chao Yan, Khaled El Emam</p>
    <p><b>Summary:</b> Synthetic data generation is one approach for sharing individual-level data.
However, to meet legislative requirements, it is necessary to demonstrate that
the individuals' privacy is adequately protected. There is no consolidated
standard for measuring privacy in synthetic data. Through an expert panel and
consensus process, we developed a framework for evaluating privacy in synthetic
data. Our findings indicate that current similarity metrics fail to measure
identity disclosure, and their use is discouraged. For differentially private
synthetic data, a privacy budget other than close to zero was not considered
interpretable. There was consensus on the importance of membership and
attribute disclosure, both of which involve inferring personal information
about an individual without necessarily revealing their identity. The resultant
framework provides precise recommendations for metrics that address these types
of disclosures effectively. Our findings further present specific opportunities
for future research that can help with widespread adoption of synthetic data.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.04707v1">Iris Style Transfer: Enhancing Iris Recognition with Style Features and
  Privacy Preservation through Neural Style Transfer</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-03-06T18:55:21Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mengdi Wang, Efe Bozkir, Enkelejda Kasneci</p>
    <p><b>Summary:</b> Iris texture is widely regarded as a gold standard biometric modality for
authentication and identification. The demand for robust iris recognition
methods, coupled with growing security and privacy concerns regarding iris
attacks, has escalated recently. Inspired by neural style transfer, an advanced
technique that leverages neural networks to separate content and style
features, we hypothesize that iris texture's style features provide a reliable
foundation for recognition and are more resilient to variations like rotation
and perspective shifts than traditional approaches. Our experimental results
support this hypothesis, showing a significantly higher classification accuracy
compared to conventional features. Further, we propose using neural style
transfer to mask identifiable iris style features, ensuring the protection of
sensitive biometric information while maintaining the utility of eye images for
tasks like eye segmentation and gaze estimation. This work opens new avenues
for iris-oriented, secure, and privacy-aware biometric systems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.04652v1">Evaluation of Privacy-aware Support Vector Machine (SVM) Learning using
  Homomorphic Encryption</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-06T17:42:23Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> William J Buchanan, Hisham Ali</p>
    <p><b>Summary:</b> The requirement for privacy-aware machine learning increases as we continue
to use PII (Personally Identifiable Information) within machine training. To
overcome these privacy issues, we can apply Fully Homomorphic Encryption (FHE)
to encrypt data before it is fed into a machine learning model. This involves
creating a homomorphic encryption key pair, and where the associated public key
will be used to encrypt the input data, and the private key will decrypt the
output. But, there is often a performance hit when we use homomorphic
encryption, and so this paper evaluates the performance overhead of using the
SVM machine learning technique with the OpenFHE homomorphic encryption library.
This uses Python and the scikit-learn library for its implementation. The
experiments include a range of variables such as multiplication depth, scale
size, first modulus size, security level, batch size, and ring dimension, along
with two different SVM models, SVM-Poly and SVM-Linear. Overall, the results
show that the two main parameters which affect performance are the ring
dimension and the modulus size, and that SVM-Poly and SVM-Linear show similar
performance levels.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.04451v1">Privacy Preserving and Robust Aggregation for Cross-Silo Federated
  Learning in Non-IID Settings</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-06T14:06:20Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Marco Arazzi, Mert Cihangiroglu, Antonino Nocera</p>
    <p><b>Summary:</b> Federated Averaging remains the most widely used aggregation strategy in
federated learning due to its simplicity and scalability. However, its
performance degrades significantly in non-IID data settings, where client
distributions are highly imbalanced or skewed. Additionally, it relies on
clients transmitting metadata, specifically the number of training samples,
which introduces privacy risks and may conflict with regulatory frameworks like
the European GDPR. In this paper, we propose a novel aggregation strategy that
addresses these challenges by introducing class-aware gradient masking. Unlike
traditional approaches, our method relies solely on gradient updates,
eliminating the need for any additional client metadata, thereby enhancing
privacy protection. Furthermore, our approach validates and dynamically weights
client contributions based on class-specific importance, ensuring robustness
against non-IID distributions, convergence prevention, and backdoor attacks.
Extensive experiments on benchmark datasets demonstrate that our method not
only outperforms FedAvg and other widely accepted aggregation strategies in
non-IID settings but also preserves model integrity in adversarial scenarios.
Our results establish the effectiveness of gradient masking as a practical and
secure solution for federated learning.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.04866v1">Privacy in Responsible AI: Approaches to Facial Recognition from Cloud
  Providers</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-03-06T12:04:12Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Anna Elivanova</p>
    <p><b>Summary:</b> As the use of facial recognition technology is expanding in different
domains, ensuring its responsible use is gaining more importance. This paper
conducts a comprehensive literature review of existing studies on facial
recognition technology from the perspective of privacy, which is one of the key
Responsible AI principles.
  Cloud providers, such as Microsoft, AWS, and Google, are at the forefront of
delivering facial-related technology services, but their approaches to
responsible use of these technologies vary significantly. This paper compares
how these cloud giants implement the privacy principle into their facial
recognition and detection services. By analysing their approaches, it
identifies both common practices and notable differences. The results of this
research will be valuable for developers and businesses by providing them
insights into best practices of three major companies for integration
responsible AI, particularly privacy, into their cloud-based facial recognition
technologies.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.04054v1">Controlled privacy leakage propagation throughout overlapping grouped
  learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2025-03-06T03:14:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shahrzad Kiani, Franziska Boenisch, Stark C. Draper</p>
    <p><b>Summary:</b> Federated Learning (FL) is the standard protocol for collaborative learning.
In FL, multiple workers jointly train a shared model. They exchange model
updates calculated on their data, while keeping the raw data itself local.
Since workers naturally form groups based on common interests and privacy
policies, we are motivated to extend standard FL to reflect a setting with
multiple, potentially overlapping groups. In this setup where workers can
belong and contribute to more than one group at a time, complexities arise in
understanding privacy leakage and in adhering to privacy policies. To address
the challenges, we propose differential private overlapping grouped learning
(DPOGL), a novel method to implement privacy guarantees within overlapping
groups. Under the honest-but-curious threat model, we derive novel privacy
guarantees between arbitrary pairs of workers. These privacy guarantees
describe and quantify two key effects of privacy leakage in DP-OGL: propagation
delay, i.e., the fact that information from one group will leak to other groups
only with temporal offset through the common workers and information
degradation, i.e., the fact that noise addition over model updates limits
information leakage between workers. Our experiments show that applying DP-OGL
enhances utility while maintaining strong privacy compared to standard FL
setups.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.07483v1">Poisoning Attacks to Local Differential Privacy Protocols for Trajectory
  Data</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-03-06T02:31:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> I-Jung Hsu, Chih-Hsun Lin, Chia-Mu Yu, Sy-Yen Kuo, Chun-Ying Huang</p>
    <p><b>Summary:</b> Trajectory data, which tracks movements through geographic locations, is
crucial for improving real-world applications. However, collecting such
sensitive data raises considerable privacy concerns. Local differential privacy
(LDP) offers a solution by allowing individuals to locally perturb their
trajectory data before sharing it. Despite its privacy benefits, LDP protocols
are vulnerable to data poisoning attacks, where attackers inject fake data to
manipulate aggregated results. In this work, we make the first attempt to
analyze vulnerabilities in several representative LDP trajectory protocols. We
propose \textsc{TraP}, a heuristic algorithm for data \underline{P}oisoning
attacks using a prefix-suffix method to optimize fake \underline{Tra}jectory
selection, significantly reducing computational complexity. Our experimental
results demonstrate that our attack can substantially increase target pattern
occurrences in the perturbed trajectory dataset with few fake users. This study
underscores the urgent need for robust defenses and better protocol designs to
safeguard LDP trajectory data against malicious manipulation.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.03988v1">AI-based Programming Assistants for Privacy-related Code Generation: The
  Developers' Experience</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Software Engineering-D91E36">
  <p><b>Published on:</b> 2025-03-06T00:34:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kashumi Madampe, John Grundy, Nalin Arachchilage</p>
    <p><b>Summary:</b> With the popularising of generative AI, the existence of AI-based programming
assistants for developers is no surprise. Developers increasingly use them for
their work, including generating code to fulfil the data protection
requirements (privacy) of the apps they build. We wanted to know if the reality
is the same as expectations of AI-based programming assistants when trying to
fulfil software privacy requirements, and the challenges developers face when
using AI-based programming assistants and how these can be improved. To this
end, we conducted a survey with 51 developers worldwide. We found that AI-based
programming assistants need to be improved in order for developers to better
trust them with generating code that ensures privacy. In this paper, we provide
some practical recommendations for developers to consider following when using
AI-based programming assistants for privacy-related code development, and some
key further research directions.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.03652v1">Token-Level Privacy in Large Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-05T16:27:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Re'em Harel, Niv Gilboa, Yuval Pinter</p>
    <p><b>Summary:</b> The use of language models as remote services requires transmitting private
information to external providers, raising significant privacy concerns. This
process not only risks exposing sensitive data to untrusted service providers
but also leaves it vulnerable to interception by eavesdroppers. Existing
privacy-preserving methods for natural language processing (NLP) interactions
primarily rely on semantic similarity, overlooking the role of contextual
information. In this work, we introduce dchi-stencil, a novel token-level
privacy-preserving mechanism that integrates contextual and semantic
information while ensuring strong privacy guarantees under the dchi
differential privacy framework, achieving 2epsilon-dchi-privacy. By
incorporating both semantic and contextual nuances, dchi-stencil achieves a
robust balance between privacy and utility. We evaluate dchi-stencil using
state-of-the-art language models and diverse datasets, achieving comparable and
even better trade-off between utility and privacy compared to existing methods.
This work highlights the potential of dchi-stencil to set a new standard for
privacy-preserving NLP in modern, high-risk applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.03587v1">"You don't need a university degree to comprehend data protection this
  way": LLM-Powered Interactive Privacy Policy Assessment</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-03-05T15:22:35Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Vincent Freiberger, Arthur Fleig, Erik Buchmann</p>
    <p><b>Summary:</b> Protecting online privacy requires users to engage with and comprehend
website privacy policies, but many policies are difficult and tedious to read.
We present the first qualitative user study on Large Language Model
(LLM)-driven privacy policy assessment. To this end, we build and evaluate an
LLM-based privacy policy assessment browser extension, which helps users
understand the essence of a lengthy, complex privacy policy while browsing. The
tool integrates a dashboard and an LLM chat. In our qualitative user study
(N=22), we evaluate usability, understandability of the information our tool
provides, and its impacts on awareness. While providing a comprehensible quick
overview and a chat for in-depth discussion improves privacy awareness, users
note issues with building trust in the tool. From our insights, we derive
important design implications to guide future policy analysis tools.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.03539v1">Data Sharing, Privacy and Security Considerations in the Energy Sector:
  A Review from Technical Landscape to Regulatory Specifications</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-05T14:23:56Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shiliang Zhang, Sabita Maharjan, Lee Andrew Bygrave, Shui Yu</p>
    <p><b>Summary:</b> Decarbonization, decentralization and digitalization are the three key
elements driving the twin energy transition. The energy system is evolving to a
more data driven ecosystem, leading to the need of communication and storage of
large amount of data of different resolution from the prosumers and other
stakeholders in the energy ecosystem. While the energy system is certainly
advancing, this paradigm shift is bringing in new privacy and security issues
related to collection, processing and storage of data - not only from the
technical dimension, but also from the regulatory perspective. Understanding
data privacy and security in the evolving energy system, regarding regulatory
compliance, is an immature field of research. Contextualized knowledge of how
related issues are regulated is still in its infancy, and the practical and
technical basis for the regulatory framework for data privacy and security is
not clear. To fill this gap, this paper conducts a comprehensive review of the
data-related issues for the energy system by integrating both technical and
regulatory dimensions. We start by reviewing open-access data, data
communication and data-processing techniques for the energy system, and use it
as the basis to connect the analysis of data-related issues from the integrated
perspective. We classify the issues into three categories: (i) data-sharing
among energy end users and stakeholders (ii) privacy of end users, and (iii)
cyber security, and then explore these issues from a regulatory perspective. We
analyze the evolution of related regulations, and introduce the relevant
regulatory initiatives for the categorized issues in terms of regulatory
definitions, concepts, principles, rights and obligations in the context of
energy systems. Finally, we provide reflections on the gaps that still exist,
and guidelines for regulatory frameworks for a truly participatory energy
system.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.03506v1">Rethinking Synthetic Data definitions: A privacy driven approach</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-03-05T13:54:13Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Vibeke Binz Vallevik, Serena Elizabeth Marshall, Aleksandar Babic, Jan Franz Nygaard</p>
    <p><b>Summary:</b> Synthetic data is gaining traction as a cost-effective solution for the
increasing data demands of AI development and can be generated either from
existing knowledge or derived data captured from real-world events. The source
of the synthetic data generation and the technique used significantly impacts
its residual privacy risk and therefore its opportunity for sharing.
Traditional classification of synthetic data types no longer fit the newer
generation techniques and there is a need to better align the classification
with practical needs. We suggest a new way of grouping synthetic data types
that better supports privacy evaluations to aid regulatory policymaking. Our
novel classification provides flexibility to new advancements like deep
generative methods and offers a more practical framework for future
applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.03428v1">Privacy is All You Need: Revolutionizing Wearable Health Data with
  Advanced PETs</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Emerging Technologies-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-03-05T12:01:22Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Karthik Barma, Seshu Babu Barma</p>
    <p><b>Summary:</b> In a world where data is the new currency, wearable health devices offer
unprecedented insights into daily life, continuously monitoring vital signs and
metrics. However, this convenience raises privacy concerns, as these devices
collect sensitive data that can be misused or breached. Traditional measures
often fail due to real-time data processing needs and limited device power.
Users also lack awareness and control over data sharing and usage. We propose a
Privacy-Enhancing Technology (PET) framework for wearable devices, integrating
federated learning, lightweight cryptographic methods, and selectively deployed
blockchain technology. The blockchain acts as a secure ledger triggered only
upon data transfer requests, granting users real-time notifications and
control. By dismantling data monopolies, this approach returns data sovereignty
to individuals. Through real-world applications like secure medical data
sharing, privacy-preserving fitness tracking, and continuous health monitoring,
our framework reduces privacy risks by up to 70 percent while preserving data
utility and performance. This innovation sets a new benchmark for wearable
privacy and can scale to broader IoT ecosystems, including smart homes and
industry. As data continues to shape our digital landscape, our research
underscores the critical need to maintain privacy and user control at the
forefront of technological progress.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.03267v1">Quantum-Inspired Privacy-Preserving Federated Learning Framework for
  Secure Dementia Classification</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2025-03-05T08:49:31Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Gazi Tanbhir, Md. Farhan Shahriyar</p>
    <p><b>Summary:</b> Dementia, a neurological disorder impacting millions globally, presents
significant challenges in diagnosis and patient care. With the rise of privacy
concerns and security threats in healthcare, federated learning (FL) has
emerged as a promising approach to enable collaborative model training across
decentralized datasets without exposing sensitive patient information. However,
FL remains vulnerable to advanced security breaches such as gradient inversion
and eavesdropping attacks. This paper introduces a novel framework that
integrates federated learning with quantum-inspired encryption techniques for
dementia classification, emphasizing privacy preservation and security.
Leveraging quantum key distribution (QKD), the framework ensures secure
transmission of model weights, protecting against unauthorized access and
interception during training. The methodology utilizes a convolutional neural
network (CNN) for dementia classification, with federated training conducted
across distributed healthcare nodes, incorporating QKD-encrypted weight sharing
to secure the aggregation process. Experimental evaluations conducted on MRI
data from the OASIS dataset demonstrate that the proposed framework achieves
identical accuracy levels to a baseline model while enhancing data security and
reducing loss by almost 1% compared to the classical baseline model. The
framework offers significant implications for democratizing access to AI-driven
dementia diagnostics in low- and middle-income countries, addressing critical
resource and privacy constraints. This work contributes a robust, scalable, and
secure federated learning solution for healthcare applications, paving the way
for broader adoption of quantum-inspired techniques in AI-driven medical
research.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.03146v1">PriFFT: Privacy-preserving Federated Fine-tuning of Large Language
  Models via Function Secret Sharing</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-05T03:41:57Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhichao You, Xuewen Dong, Ke Cheng, Xutong Mu, Jiaxuan Fu, Shiyang Ma, Qiang Qu, Yulong Shen</p>
    <p><b>Summary:</b> Fine-tuning large language models (LLMs) raises privacy concerns due to the
risk of exposing sensitive training data. Federated learning (FL) mitigates
this risk by keeping training samples on local devices, but recent studies show
that adversaries can still infer private information from model updates in FL.
Additionally, LLM parameters are typically shared publicly during federated
fine-tuning, while developers are often reluctant to disclose these parameters,
posing further security challenges. Inspired by the above problems, we propose
PriFFT, a privacy-preserving federated fine-tuning mechanism, to protect both
the model updates and parameters. In PriFFT, clients and the server share model
inputs and parameters by secret sharing, performing secure fine-tuning on
shared values without accessing plaintext data. Due to considerable LLM
parameters, privacy-preserving federated fine-tuning invokes complex secure
calculations and requires substantial communication and computation resources.
To optimize the efficiency of privacy-preserving federated fine-tuning of LLMs,
we introduce function secret-sharing protocols for various operations,
including reciprocal calculation, tensor products, natural exponentiation,
softmax, hyperbolic tangent, and dropout. The proposed protocols achieve up to
4.02X speed improvement and reduce 7.19X communication overhead compared to the
implementation based on existing secret sharing methods. Besides, PriFFT
achieves a 2.23X speed improvement and reduces 4.08X communication overhead in
privacy-preserving fine-tuning without accuracy drop compared to the existing
secret sharing methods.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.03087v1">"Watch My Health, Not My Data": Understanding Perceptions, Barriers,
  Emotional Impact, & Coping Strategies Pertaining to IoT Privacy and Security
  in Health Monitoring for Older Adults</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-03-05T01:04:13Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Suleiman Saka, Sanchari Das</p>
    <p><b>Summary:</b> The proliferation of "Internet of Things (IoT)" provides older adults with
critical support for "health monitoring" and independent living, yet
significant concerns about security and privacy persist. In this paper, we
report on these issues through a two-phase user study, including a survey (N =
22) and semi-structured interviews (n = 9) with adults aged 65+. We found that
while 81.82% of our participants are aware of security features like
"two-factor authentication (2FA)" and encryption, 63.64% express serious
concerns about unauthorized access to sensitive health data. Only 13.64% feel
confident in existing protections, citing confusion over "data sharing
policies" and frustration with "complex security settings" which lead to
distrust and anxiety. To cope, our participants adopt various strategies, such
as relying on family or professional support and limiting feature usage leading
to disengagement. Thus, we recommend "adaptive security mechanisms," simplified
interfaces, and real-time transparency notifications to foster trust and ensure
"privacy and security by design" in IoT health systems for older adults.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.03043v1">Leveraging Randomness in Model and Data Partitioning for Privacy
  Amplification</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-04T22:49:59Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Andy Dong, Wei-Ning Chen, Ayfer Ozgur</p>
    <p><b>Summary:</b> We study how inherent randomness in the training process -- where each sample
(or client in federated learning) contributes only to a randomly selected
portion of training -- can be leveraged for privacy amplification. This
includes (1) data partitioning, where a sample participates in only a subset of
training iterations, and (2) model partitioning, where a sample updates only a
subset of the model parameters. We apply our framework to model parallelism in
federated learning, where each client updates a randomly selected subnetwork to
reduce memory and computational overhead, and show that existing methods, e.g.
model splitting or dropout, provide a significant privacy amplification gain
not captured by previous privacy analysis techniques. Additionally, we
introduce Balanced Iteration Subsampling, a new data partitioning method where
each sample (or client) participates in a fixed number of training iterations.
We show that this method yields stronger privacy amplification than Poisson
(i.i.d.) sampling of data (or clients). Our results demonstrate that randomness
in the training process, which is structured rather than i.i.d. and interacts
with data in complex ways, can be systematically leveraged for significant
privacy amplification.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.02968v1">Privacy-Preserving Fair Synthetic Tabular Data</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-04T19:51:00Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Fatima J. Sarmin, Atiquer R. Rahman, Christopher J. Henry, Noman Mohammed</p>
    <p><b>Summary:</b> Sharing of tabular data containing valuable but private information is
limited due to legal and ethical issues. Synthetic data could be an alternative
solution to this sharing problem, as it is artificially generated by machine
learning algorithms and tries to capture the underlying data distribution.
However, machine learning models are not free from memorization and may
introduce biases, as they rely on training data. Producing synthetic data that
preserves privacy and fairness while maintaining utility close to the real data
is a challenging task. This research simultaneously addresses both the privacy
and fairness aspects of synthetic data, an area not explored by other studies.
In this work, we present PF-WGAN, a privacy-preserving, fair synthetic tabular
data generator based on the WGAN-GP model. We have modified the original
WGAN-GP by adding privacy and fairness constraints forcing it to produce
privacy-preserving fair data. This approach will enable the publication of
datasets that protect individual's privacy and remain unbiased toward any
particular group. We compared the results with three state-of-the-art synthetic
data generator models in terms of utility, privacy, and fairness across four
different datasets. We found that the proposed model exhibits a more balanced
trade-off among utility, privacy, and fairness.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.02862v1">Privacy and Accuracy-Aware AI/ML Model Deduplication</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB">
  <p><b>Published on:</b> 2025-03-04T18:40:38Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Hong Guan, Lei Yu, Lixi Zhou, Li Xiong, Kanchan Chowdhury, Lulu Xie, Xusheng Xiao, Jia Zou</p>
    <p><b>Summary:</b> With the growing adoption of privacy-preserving machine learning algorithms,
such as Differentially Private Stochastic Gradient Descent (DP-SGD), training
or fine-tuning models on private datasets has become increasingly prevalent.
This shift has led to the need for models offering varying privacy guarantees
and utility levels to satisfy diverse user requirements. However, managing
numerous versions of large models introduces significant operational
challenges, including increased inference latency, higher resource consumption,
and elevated costs. Model deduplication is a technique widely used by many
model serving and database systems to support high-performance and low-cost
inference queries and model diagnosis queries. However, none of the existing
model deduplication works has considered privacy, leading to unbounded
aggregation of privacy costs for certain deduplicated models and inefficiencies
when applied to deduplicate DP-trained models. We formalize the problems of
deduplicating DP-trained models for the first time and propose a novel privacy-
and accuracy-aware deduplication mechanism to address the problems. We
developed a greedy strategy to select and assign base models to target models
to minimize storage and privacy costs. When deduplicating a target model, we
dynamically schedule accuracy validations and apply the Sparse Vector Technique
to reduce the privacy costs associated with private validation data. Compared
to baselines that do not provide privacy guarantees, our approach improved the
compression ratio by up to $35\times$ for individual models (including large
language models and vision transformers). We also observed up to $43\times$
inference speedup due to the reduction of I/O operations.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.02693v1">Federated Learning for Privacy-Preserving Feedforward Control in
  Multi-Agent Systems</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Multiagent Systems-662E9B">
  <p><b>Published on:</b> 2025-03-04T15:07:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jakob Weber, Markus Gurtner, Benedikt Alt, Adrian Trachte, Andreas Kugi</p>
    <p><b>Summary:</b> Feedforward control (FF) is often combined with feedback control (FB) in many
control systems, improving tracking performance, efficiency, and stability.
However, designing effective data-driven FF controllers in multi-agent systems
requires significant data collection, including transferring private or
proprietary data, which raises privacy concerns and incurs high communication
costs. Therefore, we propose a novel approach integrating Federated Learning
(FL) into FF control to address these challenges. This approach enables
privacy-preserving, communication-efficient, and decentralized continuous
improvement of FF controllers across multiple agents without sharing personal
or proprietary data. By leveraging FL, each agent learns a local, neural FF
controller using its data and contributes only model updates to a global
aggregation process, ensuring data privacy and scalability. We demonstrate the
effectiveness of our method in an autonomous driving use case. Therein,
vehicles equipped with a trajectory-tracking feedback controller are enhanced
by FL-based neural FF control. Simulations highlight significant improvements
in tracking performance compared to pure FB control, analogous to model-based
FF control. We achieve comparable tracking performance without exchanging
private vehicle-specific data compared to a centralized neural FF control. Our
results underscore the potential of FL-based neural FF control to enable
privacy-preserving learning in multi-agent control systems, paving the way for
scalable and efficient autonomous systems applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.02549v1">Federated nnU-Net for Privacy-Preserving Medical Image Segmentation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-03-04T12:20:06Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Grzegorz Skorupko, Fotios Avgoustidis, Carlos Martín-Isla, Lidia Garrucho, Dimitri A. Kessler, Esmeralda Ruiz Pujadas, Oliver Díaz, Maciej Bobowicz, Katarzyna Gwoździewicz, Xavier Bargalló, Paulius Jaruševičius, Kaisar Kushibar, Karim Lekadir</p>
    <p><b>Summary:</b> The nnU-Net framework has played a crucial role in medical image segmentation
and has become the gold standard in multitudes of applications targeting
different diseases, organs, and modalities. However, so far it has been used
primarily in a centralized approach where the data collected from hospitals are
stored in one center and used to train the nnU-Net. This centralized approach
has various limitations, such as leakage of sensitive patient information and
violation of patient privacy. Federated learning is one of the approaches to
train a segmentation model in a decentralized manner that helps preserve
patient privacy. In this paper, we propose FednnU-Net, a federated learning
extension of nnU-Net. We introduce two novel federated learning methods to the
nnU-Net framework - Federated Fingerprint Extraction (FFE) and Asymmetric
Federated Averaging (AsymFedAvg) - and experimentally show their consistent
performance for breast, cardiac and fetal segmentation using 6 datasets
representing samples from 18 institutions. Additionally, to further promote
research and deployment of decentralized training in privacy constrained
institutions, we make our plug-n-play framework public. The source-code is
available at https://github.com/faildeny/FednnUNet .</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.02455v1">Privacy Preservation Techniques (PPTs) in IoT Systems: A Scoping Review
  and Future Directions</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-04T10:03:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Emmanuel Alalade, Ashraf Matrawy</p>
    <p><b>Summary:</b> Privacy preservation in Internet of Things (IoT) systems requires the use of
privacy-enhancing technologies (PETs) built from innovative technologies such
as cryptography and artificial intelligence (AI) to create techniques called
privacy preservation techniques (PPTs). These PPTs achieve various privacy
goals and address different privacy concerns by mitigating potential privacy
threats within IoT systems. This study carried out a scoping review of
different types of PPTs used in previous research works on IoT systems between
2010 and early 2023 to further explore the advantages of privacy preservation
in these systems. This scoping review looks at privacy goals, possible
technologies used for building PET, the integration of PPTs into the computing
layer of the IoT architecture, different IoT applications in which PPTs are
deployed, and the different privacy types addressed by these techniques within
IoT systems. Key findings, such as the prominent privacy goal and privacy type
in IoT, are discussed in this survey, along with identified research gaps that
could inform future endeavors in privacy research and benefit the privacy
research community and other stakeholders in IoT systems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.02132v1">Video-DPRP: A Differentially Private Approach for Visual
  Privacy-Preserving Video Human Activity Recognition</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-03-03T23:43:12Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Allassan Tchangmena A Nken, Susan Mckeever, Peter Corcoran, Ihsan Ullah</p>
    <p><b>Summary:</b> Considerable effort has been made in privacy-preserving video human activity
recognition (HAR). Two primary approaches to ensure privacy preservation in
Video HAR are differential privacy (DP) and visual privacy. Techniques
enforcing DP during training provide strong theoretical privacy guarantees but
offer limited capabilities for visual privacy assessment. Conversely methods,
such as low-resolution transformations, data obfuscation and adversarial
networks, emphasize visual privacy but lack clear theoretical privacy
assurances. In this work, we focus on two main objectives: (1) leveraging DP
properties to develop a model-free approach for visual privacy in videos and
(2) evaluating our proposed technique using both differential privacy and
visual privacy assessments on HAR tasks. To achieve goal (1), we introduce
Video-DPRP: a Video-sample-wise Differentially Private Random Projection
framework for privacy-preserved video reconstruction for HAR. By using random
projections, noise matrices and right singular vectors derived from the
singular value decomposition of videos, Video-DPRP reconstructs DP videos using
privacy parameters ($\epsilon,\delta$) while enabling visual privacy
assessment. For goal (2), using UCF101 and HMDB51 datasets, we compare
Video-DPRP's performance on activity recognition with traditional DP methods,
and state-of-the-art (SOTA) visual privacy-preserving techniques. Additionally,
we assess its effectiveness in preserving privacy-related attributes such as
facial features, gender, and skin color, using the PA-HMDB and VISPR datasets.
Video-DPRP combines privacy-preservation from both a DP and visual privacy
perspective unlike SOTA methods that typically address only one of these
aspects.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.02114v1">Fairness and/or Privacy on Social Graphs</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Social and Information Networks-662E9B">
  <p><b>Published on:</b> 2025-03-03T22:56:32Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Bartlomiej Surma, Michael Backes, Yang Zhang</p>
    <p><b>Summary:</b> Graph Neural Networks (GNNs) have shown remarkable success in various
graph-based learning tasks. However, recent studies have raised concerns about
fairness and privacy issues in GNNs, highlighting the potential for biased or
discriminatory outcomes and the vulnerability of sensitive information. This
paper presents a comprehensive investigation of fairness and privacy in GNNs,
exploring the impact of various fairness-preserving measures on model
performance. We conduct experiments across diverse datasets and evaluate the
effectiveness of different fairness interventions. Our analysis considers the
trade-offs between fairness, privacy, and accuracy, providing insights into the
challenges and opportunities in achieving both fair and private graph learning.
The results highlight the importance of carefully selecting and combining
fairness-preserving measures based on the specific characteristics of the data
and the desired fairness objectives. This study contributes to a deeper
understanding of the complex interplay between fairness, privacy, and accuracy
in GNNs, paving the way for the development of more robust and ethical graph
learning models.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.02091v1">Which Code Statements Implement Privacy Behaviors in Android
  Applications?</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Software Engineering-D91E36">
  <p><b>Published on:</b> 2025-03-03T22:20:01Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chia-Yi Su, Aakash Bansal, Vijayanta Jain, Sepideh Ghanavati, Sai Teja Peddinti, Collin McMillan</p>
    <p><b>Summary:</b> A "privacy behavior" in software is an action where the software uses
personal information for a service or a feature, such as a website using
location to provide content relevant to a user. Programmers are required by
regulations or application stores to provide privacy notices and labels
describing these privacy behaviors. Although many tools and research prototypes
have been developed to help programmers generate these notices by analyzing the
source code, these approaches are often fairly coarse-grained (i.e., at the
level of whole methods or files, rather than at the statement level). But this
is not necessarily how privacy behaviors exist in code. Privacy behaviors are
embedded in specific statements in code. Current literature does not examine
what statements programmers see as most important, how consistent these views
are, or how to detect them. In this paper, we conduct an empirical study to
examine which statements programmers view as most-related to privacy behaviors.
We find that expression statements that make function calls are most associated
with privacy behaviors, while the type of privacy label has little effect on
the attributes of the selected statements. We then propose an approach to
automatically detect these privacy-relevant statements by fine-tuning three
large language models with the data from the study. We observe that the
agreement between our approach and participants is comparable to or higher than
an agreement between two participants. Our study and detection approach can
help programmers understand which statements in code affect privacy in mobile
applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.02019v1">SLAP: Secure Location-proof and Anonymous Privacy-preserving Spectrum
  Access</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-03T19:52:56Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Saleh Darzi, Attila A. Yavuz</p>
    <p><b>Summary:</b> The rapid advancements in wireless technology have significantly increased
the demand for communication resources, leading to the development of Spectrum
Access Systems (SAS). However, network regulations require disclosing sensitive
user information, such as location coordinates and transmission details,
raising critical privacy concerns. Moreover, as a database-driven architecture
reliant on user-provided data, SAS necessitates robust location verification to
counter identity and location spoofing attacks and remains a primary target for
denial-of-service (DoS) attacks. Addressing these security challenges while
adhering to regulatory requirements is essential. In this paper, we propose
SLAP, a novel framework that ensures location privacy and anonymity during
spectrum queries, usage notifications, and location-proof acquisition. Our
solution includes an adaptive dual-scenario location verification mechanism
with architectural flexibility and a fallback option, along with a counter-DoS
approach using time-lock puzzles. We prove the security of SLAP and demonstrate
its advantages over existing solutions through comprehensive performance
evaluations.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.02017v1">A Lightweight and Secure Deep Learning Model for Privacy-Preserving
  Federated Learning in Intelligent Enterprises</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-03-03T19:51:13Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Reza Fotohi, Fereidoon Shams Aliee, Bahar Farahani</p>
    <p><b>Summary:</b> The ever growing Internet of Things (IoT) connections drive a new type of
organization, the Intelligent Enterprise. In intelligent enterprises, machine
learning based models are adopted to extract insights from data. Due to the
efficiency and privacy challenges of these traditional models, a new federated
learning (FL) paradigm has emerged. In FL, multiple enterprises can jointly
train a model to update a final model. However, firstly, FL trained models
usually perform worse than centralized models, especially when enterprises
training data is non-IID (Independent and Identically Distributed). Second, due
to the centrality of FL and the untrustworthiness of local enterprises,
traditional FL solutions are vulnerable to poisoning and inference attacks and
violate privacy. Thirdly, the continuous transfer of parameters between
enterprises and servers increases communication costs. To this end, the
FedAnil+ model is proposed, a novel, lightweight, and secure Federated Deep
Learning Model that includes three main phases. In the first phase, the goal is
to solve the data type distribution skew challenge. Addressing privacy concerns
against poisoning and inference attacks is covered in the second phase.
Finally, to alleviate the communication overhead, a novel compression approach
is proposed that significantly reduces the size of the updates. The experiment
results validate that FedAnil+ is secure against inference and poisoning
attacks with better accuracy. In addition, it shows improvements over existing
approaches in terms of model accuracy (13%, 16%, and 26%), communication cost
(17%, 21%, and 25%), and computation cost (7%, 9%, and 11%).</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.01482v1">Revisiting Locally Differentially Private Protocols: Towards Better
  Trade-offs in Privacy, Utility, and Attack Resistance</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-03T12:41:01Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Héber H. Arcolezi, Sébastien Gambs</p>
    <p><b>Summary:</b> Local Differential Privacy (LDP) offers strong privacy protection, especially
in settings in which the server collecting the data is untrusted. However,
designing LDP mechanisms that achieve an optimal trade-off between privacy,
utility, and robustness to adversarial inference attacks remains challenging.
In this work, we introduce a general multi-objective optimization framework for
refining LDP protocols, enabling the joint optimization of privacy and utility
under various adversarial settings. While our framework is flexible enough to
accommodate multiple privacy and security attacks as well as utility metrics,
in this paper we specifically optimize for Attacker Success Rate (ASR) under
distinguishability attack as a measure of privacy and Mean Squared Error (MSE)
as a measure of utility. We systematically revisit these trade-offs by
analyzing eight state-of-the-art LDP protocols and proposing refined
counterparts that leverage tailored optimization techniques. Experimental
results demonstrate that our proposed adaptive mechanisms consistently
outperform their non-adaptive counterparts, reducing ASR by up to five orders
of magnitude while maintaining competitive utility. Analytical derivations also
confirm the effectiveness of our mechanisms, moving them closer to the ASR-MSE
Pareto frontier.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.01470v1">Position: Ensuring mutual privacy is necessary for effective external
  evaluation of proprietary AI systems</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-03T12:24:59Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ben Bucknall, Robert F. Trager, Michael A. Osborne</p>
    <p><b>Summary:</b> The external evaluation of AI systems is increasingly recognised as a crucial
approach for understanding their potential risks. However, facilitating
external evaluation in practice faces significant challenges in balancing
evaluators' need for system access with AI developers' privacy and security
concerns. Additionally, evaluators have reason to protect their own privacy -
for example, in order to maintain the integrity of held-out test sets. We refer
to the challenge of ensuring both developers' and evaluators' privacy as one of
providing mutual privacy. In this position paper, we argue that (i) addressing
this mutual privacy challenge is essential for effective external evaluation of
AI systems, and (ii) current methods for facilitating external evaluation
inadequately address this challenge, particularly when it comes to preserving
evaluators' privacy. In making these arguments, we formalise the mutual privacy
problem; examine the privacy and access requirements of both model owners and
evaluators; and explore potential solutions to this challenge, including
through the application of cryptographic and hardware-based approaches.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.01208v1">Watch Out Your Album! On the Inadvertent Privacy Memorization in
  Multi-Modal Large Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-03-03T06:10:27Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tianjie Ju, Yi Hua, Hao Fei, Zhenyu Shao, Yubin Zheng, Haodong Zhao, Mong-Li Lee, Wynne Hsu, Zhuosheng Zhang, Gongshen Liu</p>
    <p><b>Summary:</b> Multi-Modal Large Language Models (MLLMs) have exhibited remarkable
performance on various vision-language tasks such as Visual Question Answering
(VQA). Despite accumulating evidence of privacy concerns associated with
task-relevant content, it remains unclear whether MLLMs inadvertently memorize
private content that is entirely irrelevant to the training tasks. In this
paper, we investigate how randomly generated task-irrelevant private content
can become spuriously correlated with downstream objectives due to partial
mini-batch training dynamics, thus causing inadvertent memorization.
Concretely, we randomly generate task-irrelevant watermarks into VQA
fine-tuning images at varying probabilities and propose a novel probing
framework to determine whether MLLMs have inadvertently encoded such content.
Our experiments reveal that MLLMs exhibit notably different training behaviors
in partial mini-batch settings with task-irrelevant watermarks embedded.
Furthermore, through layer-wise probing, we demonstrate that MLLMs trigger
distinct representational patterns when encountering previously seen
task-irrelevant knowledge, even if this knowledge does not influence their
output during prompting. Our code is available at
https://github.com/illusionhi/ProbingPrivacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.01089v1">Privacy-preserving Machine Learning in Internet of Vehicle Applications:
  Fundamentals, Recent Advances, and Future Direction</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-03T01:24:04Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Nazmul Islam, Mohammad Zulkernine</p>
    <p><b>Summary:</b> Machine learning (ML) has revolutionized Internet of Vehicles (IoV)
applications by enhancing intelligent transportation, autonomous driving
capabilities, and various connected services within a large, heterogeneous
network. However, the increased connectivity and massive data exchange for ML
applications introduce significant privacy challenges. Privacy-preserving
machine learning (PPML) offers potential solutions to address these challenges
by preserving privacy at various stages of the ML pipeline. Despite the rapid
development of ML-based IoV applications and the growing data privacy concerns,
there are limited comprehensive studies on the adoption of PPML within this
domain. Therefore, this study provides a comprehensive review of the
fundamentals, recent advancements, and the challenges of integrating PPML into
IoV applications. To conduct an extensive study, we first review existing
surveys of various PPML techniques and their integration into IoV across
different scopes. We then discuss the fundamentals of IoV and propose a
four-layer IoV architecture. Additionally, we categorize IoV applications into
three key domains and analyze the privacy challenges in leveraging ML for these
application domains. Next, we provide an overview of various PPML techniques,
highlighting their applicability and performance to address the privacy
challenges. Building on these fundamentals, we thoroughly review recent
advancements in integrating various PPML techniques within IoV applications,
discussing their frameworks, key features, and performance evaluation in terms
of privacy, utility, and efficiency. Finally, we identify current challenges
and propose future research directions to enhance privacy and reliability in
IoV applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.01000v1">Privacy vs. Profit: The Impact of Google's Manifest Version 3 (MV3)
  Update on Ad Blocker Effectiveness</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">    
  <p><b>Published on:</b> 2025-03-02T19:41:34Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Karlo Lukic, Lazaros Papadopoulos</p>
    <p><b>Summary:</b> Google's recent update to the manifest file for Chrome browser
extensions-transitioning from manifest version 2 (MV2) to manifest version 3
(MV3)-has raised concerns among users and ad blocker providers, who worry that
the new restrictions, notably the shift from the powerful WebRequest API to the
more restrictive DeclarativeNetRequest API, might reduce ad blocker
effectiveness. Because ad blockers play a vital role for millions of users
seeking a more private and ad-free browsing experience, this study empirically
investigates how the MV3 update affects their ability to block ads and
trackers. Through a browser-based experiment conducted across multiple samples
of ad-supported websites, we compare the MV3 to MV2 instances of four widely
used ad blockers. Our results reveal no statistically significant reduction in
ad-blocking or anti-tracking effectiveness for MV3 ad blockers compared to
their MV2 counterparts, and in some cases, MV3 instances even exhibit slight
improvements in blocking trackers. These findings are reassuring for users,
indicating that the MV3 instances of popular ad blockers continue to provide
effective protection against intrusive ads and privacy-infringing trackers.
While some uncertainties remain, ad blocker providers appear to have
successfully navigated the MV3 update, finding solutions that maintain the core
functionality of their ad blockers.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.00703v1">Towards hyperparameter-free optimization with differential privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-03-02T02:59:52Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhiqi Bu, Ruixuan Liu</p>
    <p><b>Summary:</b> Differential privacy (DP) is a privacy-preserving paradigm that protects the
training data when training deep learning models. Critically, the performance
of models is determined by the training hyperparameters, especially those of
the learning rate schedule, thus requiring fine-grained hyperparameter tuning
on the data. In practice, it is common to tune the learning rate
hyperparameters through the grid search that (1) is computationally expensive
as multiple runs are needed, and (2) increases the risk of data leakage as the
selection of hyperparameters is data-dependent. In this work, we adapt the
automatic learning rate schedule to DP optimization for any models and
optimizers, so as to significantly mitigate or even eliminate the cost of
hyperparameter tuning when applied together with automatic per-sample gradient
clipping. Our hyperparameter-free DP optimization is almost as computationally
efficient as the standard non-DP optimization, and achieves state-of-the-art DP
performance on various language and vision tasks.</p>
  </details>
</div>

