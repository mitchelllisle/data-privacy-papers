
<h2>2025-02</h2>

<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.20629v1">Towards Privacy-Preserving Split Learning: Destabilizing Adversarial
  Inference and Reconstruction Attacks in the Cloud</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-02-28T01:24:33Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Griffin Higgins, Roozbeh Razavi-Far, Xichen Zhang, Amir David, Ali Ghorbani, Tongyu Ge</p>
    <p><b>Summary:</b> This work aims to provide both privacy and utility within a split learning
framework while considering both forward attribute inference and backward
reconstruction attacks. To address this, a novel approach has been proposed,
which makes use of class activation maps and autoencoders as a plug-in strategy
aiming to increase the user's privacy and destabilize an adversary. The
proposed approach is compared with a dimensionality-reduction-based plug-in
strategy, which makes use of principal component analysis to transform the
feature map onto a lower-dimensional feature space. Our work shows that our
proposed autoencoder-based approach is preferred as it can provide protection
at an earlier split position over the tested architectures in our setting, and,
hence, better utility for resource-constrained devices in edge-cloud
collaborative inference (EC) systems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.20621v1">EPhishCADE: A Privacy-Aware Multi-Dimensional Framework for Email
  Phishing Campaign Detection</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-02-28T00:58:42Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Wei Kang, Nan Wang, Jang Seung, Shuo Wang, Alsharif Abuadbba</p>
    <p><b>Summary:</b> Phishing attacks, typically carried out by email, remain a significant
cybersecurity threat with attackers creating legitimate-looking websites to
deceive recipients into revealing sensitive information or executing harmful
actions. In this paper, we propose {\bf EPhishCADE}, the first {\em
privacy-aware}, {\em multi-dimensional} framework for {\bf E}mail {\bf
Phish}ing {\bf CA}mpaign {\bf DE}tection to automatically identify email
phishing campaigns by clustering seemingly unrelated attacks. Our framework
employs a hierarchical architecture combining a structural layer and a
contextual layer, offering a comprehensive analysis of phishing attacks by
thoroughly examining both structural and contextual elements. Specifically, we
implement a graph-based contextual layer to reveal hidden similarities across
multiple dimensions, including textual, numeric, temporal, and spatial
features, among attacks that may initially appear unrelated. Our framework
streamlines the handling of security threat reports, reducing analysts' fatigue
and workload while enhancing protection against these threats. Another key
feature of our framework lies in its sole reliance on phishing URLs in emails
without the need for private information, including senders, recipients,
content, etc. This feature enables a collaborative identification of phishing
campaigns and attacks among multiple organizations without compromising
privacy. Finally, we benchmark our framework against an established
structure-based study (WWW \textquotesingle 17) to demonstrate its
effectiveness.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.20477v1">HELENE: An Open-Source High-Security Privacy-Preserving Blockchain Based
  System for Automating and Managing Laboratory Health Tests</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2025-02-27T19:28:29Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Gabriel Fernández-Blanco, Pedro García-Cereijo, David Lema-Núñez, Diego Ramil-López, Paula Fraga-Lamas, Leire Egia-Mendikute, Asís Palazón, Tiago M. Fernández-Caramés</p>
    <p><b>Summary:</b> In the last years, especially since the COVID-19 pandemic, precision medicine
platforms emerged as useful tools for supporting new tests like the ones that
detect the presence of antibodies and antigens with better sensitivity and
specificity than traditional methods. In addition, the pandemic has also
influenced the way people interact (decentralization), behave (digital world)
and purchase health services (online). Moreover, there is a growing concern in
the way health data are managed, especially in terms of privacy. To tackle such
issues, this article presents a sustainable direct-to-consumer health-service
open-source platform called HELENE that is supported by blockchain and by a
novel decentralized oracle that protects patient data privacy. Specifically,
HELENE enables health test providers to compete through auctions, allowing
patients to bid for their services and to keep the control over their health
test results. Moreover, data exchanges among the involved stakeholders can be
performed in a trustworthy, transparent and standardized way to ease software
integration and to avoid incompatibilities. After providing a thorough
description of the platform, the proposed health platform is assessed in terms
of smart contract performance. In addition, the response time of the developed
oracle is evaluated and NIST SP 800-22 tests are executed to demonstrate the
adequacy of the devised random number generator. Thus, this article shows the
capabilities and novel propositions of HELENE for delivering health services
providing an open-source platform for future researchers, who can enhance it
and adapt it to their needs.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2502.19912v1">Model-Free Privacy Preserving Power Flow Analysis in Distribution
  Networks</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36">
  <p><b>Published on:</b> 2025-02-27T09:31:53Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Dong Liu, Juan S. Giraldo, Peter Palensky, Pedro P. Vergara</p>
    <p><b>Summary:</b> Model-free power flow calculation, driven by the rise of smart meter (SM)
data and the lack of network topology, often relies on artificial intelligence
neural networks (ANNs). However, training ANNs require vast amounts of SM data,
posing privacy risks for households in distribution networks. To ensure
customers' privacy during the SM data gathering and online sharing, we
introduce a privacy preserving PF calculation framework, composed of two local
strategies: a local randomisation strategy (LRS) and a local zero-knowledge
proof (ZKP)-based data collection strategy. First, the LRS is used to achieve
irreversible transformation and robust privacy protection for active and
reactive power data, thereby ensuring that personal data remains confidential.
Subsequently, the ZKP-based data collecting strategy is adopted to securely
gather the training dataset for the ANN, enabling SMs to interact with the
distribution system operator without revealing the actual voltage magnitude.
Moreover, to mitigate the accuracy loss induced by the seasonal variations in
load profiles, an incremental learning strategy is incorporated into the online
application. The results across three datasets with varying measurement errors
demonstrate that the proposed framework efficiently collects one month of SM
data within one hour. Furthermore, it robustly maintains mean errors of 0.005
p.u. and 0.014 p.u. under multiple measurement errors and seasonal variations
in load profiles, respectively.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.05786v2">FedMentalCare: Towards Privacy-Preserving Fine-Tuned LLMs to Analyze
  Mental Health Status Using Federated Learning Framework</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> 
  <p><b>Published on:</b> 2025-02-27T07:04:19Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> S M Sarwar</p>
    <p><b>Summary:</b> With the increasing prevalence of mental health conditions worldwide,
AI-powered chatbots and conversational agents have emerged as accessible tools
to support mental health. However, deploying Large Language Models (LLMs) in
mental healthcare applications raises significant privacy concerns, especially
regarding regulations like HIPAA and GDPR. In this work, we propose
FedMentalCare, a privacy-preserving framework that leverages Federated Learning
(FL) combined with Low-Rank Adaptation (LoRA) to fine-tune LLMs for mental
health analysis. We investigate the performance impact of varying client data
volumes and model architectures (e.g., MobileBERT and MiniLM) in FL
environments. Our framework demonstrates a scalable, privacy-aware approach for
deploying LLMs in real-world mental healthcare scenarios, addressing data
security and computational efficiency challenges.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.00062v1">CRFU: Compressive Representation Forgetting Against Privacy Leakage on
  Machine Unlearning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-02-27T05:59:02Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Weiqi Wang, Chenhan Zhang, Zhiyi Tian, Shushu Liu, Shui Yu</p>
    <p><b>Summary:</b> Machine unlearning allows data owners to erase the impact of their specified
data from trained models. Unfortunately, recent studies have shown that
adversaries can recover the erased data, posing serious threats to user
privacy. An effective unlearning method removes the information of the
specified data from the trained model, resulting in different outputs for the
same input before and after unlearning. Adversaries can exploit these output
differences to conduct privacy leakage attacks, such as reconstruction and
membership inference attacks. However, directly applying traditional defenses
to unlearning leads to significant model utility degradation. In this paper, we
introduce a Compressive Representation Forgetting Unlearning scheme (CRFU),
designed to safeguard against privacy leakage on unlearning. CRFU achieves data
erasure by minimizing the mutual information between the trained compressive
representation (learned through information bottleneck theory) and the erased
data, thereby maximizing the distortion of data. This ensures that the model's
output contains less information that adversaries can exploit. Furthermore, we
introduce a remembering constraint and an unlearning rate to balance the
forgetting of erased data with the preservation of previously learned
knowledge, thereby reducing accuracy degradation. Theoretical analysis
demonstrates that CRFU can effectively defend against privacy leakage attacks.
Our experimental results show that CRFU significantly increases the
reconstruction mean square error (MSE), achieving a defense effect improvement
of approximately $200\%$ against privacy reconstruction attacks with only
$1.5\%$ accuracy degradation on MNIST.</p>
  </details>
</div>



<h2>2025-03</h2>

<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.15287v1">Distributed Generalized Linear Models: A Privacy-Preserving Approach</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">  
  <p><b>Published on:</b> 2025-03-19T15:07:41Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Daniel Tinoco, Raquel Menezes, Carlos Baquero</p>
    <p><b>Summary:</b> This paper presents a novel approach to classical linear regression, enabling
model computation from data streams or in a distributed setting while
preserving data privacy in federated environments. We extend this framework to
generalized linear models (GLMs), ensuring scalability and adaptability to
diverse data distributions while maintaining privacy-preserving properties. To
assess the effectiveness of our approach, we conduct numerical studies on both
simulated and real datasets, comparing our method with conventional maximum
likelihood estimation for GLMs using iteratively reweighted least squares. Our
results demonstrate the advantages of the proposed method in distributed and
federated settings.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.15238v1">Your Signal, Their Data: An Empirical Privacy Analysis of
  Wireless-scanning SDKs in Android</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-19T14:15:02Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Aniketh Girish, Joel Reardon, Juan Tapiador, Srdjan Matic, Narseo Vallina-Rodriguez</p>
    <p><b>Summary:</b> Mobile apps frequently use Bluetooth Low Energy (BLE) and WiFi scanning
permissions to discover nearby devices like peripherals and connect to WiFi
Access Points (APs). However, wireless interfaces also serve as a covert proxy
for geolocation data, enabling continuous user tracking and profiling. This
includes technologies like BLE beacons, which are BLE devices broadcasting
unique identifiers to determine devices' indoor physical locations; such
beacons are easily found in shopping centres. Despite the widespread use of
wireless scanning APIs and their potential for privacy abuse, the interplay
between commercial mobile SDKs with wireless sensing and beaconing technologies
remains largely unexplored. In this work, we conduct the first systematic
analysis of 52 wireless-scanning SDKs, revealing their data collection
practices and privacy risks. We develop a comprehensive analysis pipeline that
enables us to detect beacon scanning capabilities, inject wireless events to
trigger app behaviors, and monitor runtime execution on instrumented devices.
Our findings show that 86% of apps integrating these SDKs collect at least one
sensitive data type, including device and user identifiers such as AAID, email,
along with GPS coordinates, WiFi and Bluetooth scan results. We uncover
widespread SDK-to-SDK data sharing and evidence of ID bridging, where
persistent and resettable identifiers are shared and synchronized within SDKs
embedded in applications to potentially construct detailed mobility profiles,
compromising user anonymity and enabling long-term tracking. We provide
evidence of key actors engaging in these practices and conclude by proposing
mitigation strategies such as stronger SDK sandboxing, stricter enforcement of
platform policies, and improved transparency mechanisms to limit unauthorized
tracking.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.15015v1">OFL: Opportunistic Federated Learning for Resource-Heterogeneous and
  Privacy-Aware Devices</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-19T09:12:47Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yunlong Mao, Mingyang Niu, Ziqin Dang, Chengxi Li, Hanning Xia, Yuejuan Zhu, Haoyu Bian, Yuan Zhang, Jingyu Hua, Sheng Zhong</p>
    <p><b>Summary:</b> Efficient and secure federated learning (FL) is a critical challenge for
resource-limited devices, especially mobile devices. Existing secure FL
solutions commonly incur significant overhead, leading to a contradiction
between efficiency and security. As a result, these two concerns are typically
addressed separately. This paper proposes Opportunistic Federated Learning
(OFL), a novel FL framework designed explicitly for resource-heterogenous and
privacy-aware FL devices, solving efficiency and security problems jointly. OFL
optimizes resource utilization and adaptability across diverse devices by
adopting a novel hierarchical and asynchronous aggregation strategy. OFL
provides strong security by introducing a differentially private and
opportunistic model updating mechanism for intra-cluster model aggregation and
an advanced threshold homomorphic encryption scheme for inter-cluster
aggregation. Moreover, OFL secures global model aggregation by implementing
poisoning attack detection using frequency analysis while keeping models
encrypted. We have implemented OFL in a real-world testbed and evaluated OFL
comprehensively. The evaluation results demonstrate that OFL achieves
satisfying model performance and improves efficiency and security,
outperforming existing solutions.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.14877v1">Synthesizing Grid Data with Cyber Resilience and Privacy Guarantees</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36">
  <p><b>Published on:</b> 2025-03-19T04:11:33Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shengyang Wu, Vladimir Dvorkin</p>
    <p><b>Summary:</b> Differential privacy (DP) provides a principled approach to synthesizing data
(e.g., loads) from real-world power systems while limiting the exposure of
sensitive information. However, adversaries may exploit synthetic data to
calibrate cyberattacks on the source grids. To control these risks, we propose
new DP algorithms for synthesizing data that provide the source grids with both
cyber resilience and privacy guarantees. The algorithms incorporate both normal
operation and attack optimization models to balance the fidelity of synthesized
data and cyber resilience. The resulting post-processing optimization is
reformulated as a robust optimization problem, which is compatible with the
exponential mechanism of DP to moderate its computational burden.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.13872v1">Empirical Calibration and Metric Differential Privacy in Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-18T03:52:12Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Pedro Faustini, Natasha Fernandes, Annabelle McIver, Mark Dras</p>
    <p><b>Summary:</b> NLP models trained with differential privacy (DP) usually adopt the DP-SGD
framework, and privacy guarantees are often reported in terms of the privacy
budget $\epsilon$. However, $\epsilon$ does not have any intrinsic meaning, and
it is generally not possible to compare across variants of the framework. Work
in image processing has therefore explored how to empirically calibrate noise
across frameworks using Membership Inference Attacks (MIAs). However, this kind
of calibration has not been established for NLP. In this paper, we show that
MIAs offer little help in calibrating privacy, whereas reconstruction attacks
are more useful. As a use case, we define a novel kind of directional privacy
based on the von Mises-Fisher (VMF) distribution, a metric DP mechanism that
perturbs angular distance rather than adding (isotropic) Gaussian noise, and
apply this to NLP architectures. We show that, even though formal guarantees
are incomparable, empirical privacy calibration reveals that each mechanism has
different areas of strength with respect to utility-privacy trade-offs.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.13816v1">MOSAIC: Generating Consistent, Privacy-Preserving Scenes from Multiple
  Depth Views in Multi-Room Environments</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-03-18T01:50:57Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhixuan Liu, Haokun Zhu, Rui Chen, Jonathan Francis, Soonmin Hwang, Ji Zhang, Jean Oh</p>
    <p><b>Summary:</b> We introduce a novel diffusion-based approach for generating
privacy-preserving digital twins of multi-room indoor environments from depth
images only. Central to our approach is a novel Multi-view Overlapped Scene
Alignment with Implicit Consistency (MOSAIC) model that explicitly considers
cross-view dependencies within the same scene in the probabilistic sense.
MOSAIC operates through a novel inference-time optimization that avoids error
accumulation common in sequential or single-room constraint in panorama-based
approaches. MOSAIC scales to complex scenes with zero extra training and
provably reduces the variance during denoising processes when more overlapping
views are added, leading to improved generation quality. Experiments show that
MOSAIC outperforms state-of-the-art baselines on image fidelity metrics in
reconstructing complex multi-room environments. Project page is available at:
https://mosaic-cmubig.github.io</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.14539v1">Ethical Implications of AI in Data Collection: Balancing Innovation with
  Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-03-17T14:15:59Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shahmar Mirishli</p>
    <p><b>Summary:</b> This article examines the ethical and legal implications of artificial
intelligence (AI) driven data collection, focusing on developments from 2023 to
2024. It analyzes recent advancements in AI technologies and their impact on
data collection practices across various sectors. The study compares regulatory
approaches in the European Union, the United States, and China, highlighting
the challenges in creating a globally harmonized framework for AI governance.
Key ethical issues, including informed consent, algorithmic bias, and privacy
protection, are critically assessed in the context of increasingly
sophisticated AI systems. The research explores case studies in healthcare,
finance, and smart cities to illustrate the practical challenges of AI
implementation. It evaluates the effectiveness of current legal frameworks and
proposes solutions encompassing legal and policy recommendations, technical
safeguards, and ethical frameworks. The article emphasizes the need for
adaptive governance and international cooperation to address the global nature
of AI development while balancing innovation with the protection of individual
rights and societal values.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.13173v1">PAUSE: Low-Latency and Privacy-Aware Active User Selection for Federated
  Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> 
  <p><b>Published on:</b> 2025-03-17T13:50:35Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ori Peleg, Natalie Lang, Stefano Rini, Nir Shlezinger, Kobi Cohen</p>
    <p><b>Summary:</b> Federated learning (FL) enables multiple edge devices to collaboratively
train a machine learning model without the need to share potentially private
data. Federated learning proceeds through iterative exchanges of model updates,
which pose two key challenges: First, the accumulation of privacy leakage over
time, and second, communication latency. These two limitations are typically
addressed separately: The former via perturbed updates to enhance privacy and
the latter using user selection to mitigate latency - both at the expense of
accuracy. In this work, we propose a method that jointly addresses the
accumulation of privacy leakage and communication latency via active user
selection, aiming to improve the trade-off among privacy, latency, and model
performance. To achieve this, we construct a reward function that accounts for
these three objectives. Building on this reward, we propose a multi-armed
bandit (MAB)-based algorithm, termed Privacy-aware Active User SElection
(PAUSE) which dynamically selects a subset of users each round while ensuring
bounded overall privacy leakage. We establish a theoretical analysis,
systematically showing that the reward growth rate of PAUSE follows that of the
best-known rate in MAB literature. To address the complexity overhead of active
user selection, we propose a simulated annealing-based relaxation of PAUSE and
analyze its ability to approximate the reward-maximizing policy under reduced
complexity. We numerically validate the privacy leakage, associated improved
latency, and accuracy gains of our methods for the federated training in
various scenarios.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.12958v1">FedSDP: Explainable Differential Privacy in Federated Learning via
  Shapley Values</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-17T09:14:19Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yunbo Li, Jiaping Gui, Yue Wu</p>
    <p><b>Summary:</b> Federated learning (FL) enables participants to store data locally while
collaborating in training, yet it remains vulnerable to privacy attacks, such
as data reconstruction. Existing differential privacy (DP) technologies inject
noise dynamically into the training process to mitigate the impact of excessive
noise. However, this dynamic scheduling is often grounded in factors indirectly
related to privacy, making it difficult to clearly explain the intricate
relationship between dynamic noise adjustments and privacy requirements. To
address this issue, we propose FedSDP, a novel and explainable DP-based privacy
protection mechanism that guides noise injection based on privacy contribution.
Specifically, FedSDP leverages Shapley values to assess the contribution of
private attributes to local model training and dynamically adjusts the amount
of noise injected accordingly. By providing theoretical insights into the
injection of varying scales of noise into local training, FedSDP enhances
interpretability. Extensive experiments demonstrate that FedSDP can achieve a
superior balance between privacy preservation and model performance, surpassing
state-of-the-art (SOTA) solutions.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.12786v1">Privacy-Preserving Biometric Verification with Handwritten Random Digit
  String</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-03-17T03:47:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Peirong Zhang, Yuliang Liu, Songxuan Lai, Hongliang Li, Lianwen Jin</p>
    <p><b>Summary:</b> Handwriting verification has stood as a steadfast identity authentication
method for decades. However, this technique risks potential privacy breaches
due to the inclusion of personal information in handwritten biometrics such as
signatures. To address this concern, we propose using the Random Digit String
(RDS) for privacy-preserving handwriting verification. This approach allows
users to authenticate themselves by writing an arbitrary digit sequence,
effectively ensuring privacy protection. To evaluate the effectiveness of RDS,
we construct a new HRDS4BV dataset composed of online naturally handwritten
RDS. Unlike conventional handwriting, RDS encompasses unconstrained and
variable content, posing significant challenges for modeling consistent
personal writing style. To surmount this, we propose the Pattern Attentive
VErification Network (PAVENet), along with a Discriminative Pattern Mining
(DPM) module. DPM adaptively enhances the recognition of consistent and
discriminative writing patterns, thus refining handwriting style
representation. Through comprehensive evaluations, we scrutinize the
applicability of online RDS verification and showcase a pronounced
outperformance of our model over existing methods. Furthermore, we discover a
noteworthy forgery phenomenon that deviates from prior findings and discuss its
positive impact in countering malicious impostor attacks. Substantially, our
work underscores the feasibility of privacy-preserving biometric verification
and propels the prospects of its broader acceptance and application.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.13550v1">Towards Privacy-Preserving Data-Driven Education: The Potential of
  Federated Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-16T14:37:32Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mohammad Khalil, Ronas Shakya, Qinyi Liu</p>
    <p><b>Summary:</b> The increasing adoption of data-driven applications in education such as in
learning analytics and AI in education has raised significant privacy and data
protection concerns. While these challenges have been widely discussed in
previous works, there are still limited practical solutions. Federated learning
has recently been discoursed as a promising privacy-preserving technique, yet
its application in education remains scarce. This paper presents an
experimental evaluation of federated learning for educational data prediction,
comparing its performance to traditional non-federated approaches. Our findings
indicate that federated learning achieves comparable predictive accuracy.
Furthermore, under adversarial attacks, federated learning demonstrates greater
resilience compared to non-federated settings. We summarise that our results
reinforce the value of federated learning as a potential approach for balancing
predictive performance and privacy in educational contexts.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.12464v1">Learning Privacy from Visual Entities</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-03-16T11:39:08Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Alessio Xompero, Andrea Cavallaro</p>
    <p><b>Summary:</b> Subjective interpretation and content diversity make predicting whether an
image is private or public a challenging task. Graph neural networks combined
with convolutional neural networks (CNNs), which consist of 14,000 to 500
millions parameters, generate features for visual entities (e.g., scene and
object types) and identify the entities that contribute to the decision. In
this paper, we show that using a simpler combination of transfer learning and a
CNN to relate privacy with scene types optimises only 732 parameters while
achieving comparable performance to that of graph-based methods. On the
contrary, end-to-end training of graph-based methods can mask the contribution
of individual components to the classification performance. Furthermore, we
show that a high-dimensional feature vector, extracted with CNNs for each
visual entity, is unnecessary and complexifies the model. The graph component
has also negligible impact on performance, which is driven by fine-tuning the
CNN to optimise image features for privacy nodes.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.12347v1">Synthesizing Privacy-Preserving Text Data via Finetuning without
  Finetuning Billion-Scale LLMs</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-03-16T04:00:32Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Bowen Tan, Zheng Xu, Eric Xing, Zhiting Hu, Shanshan Wu</p>
    <p><b>Summary:</b> Synthetic data offers a promising path to train models while preserving data
privacy. Differentially private (DP) finetuning of large language models (LLMs)
as data generator is effective, but is impractical when computation resources
are limited. Meanwhile, prompt-based methods such as private evolution, depend
heavily on the manual prompts, and ineffectively use private information in
their iterative data selection process. To overcome these limitations, we
propose CTCL (Data Synthesis with ConTrollability and CLustering), a novel
framework for generating privacy-preserving synthetic data without extensive
prompt engineering or billion-scale LLM finetuning. CTCL pretrains a
lightweight 140M conditional generator and a clustering-based topic model on
large-scale public data. To further adapt to the private domain, the generator
is DP finetuned on private data for fine-grained textual information, while the
topic model extracts a DP histogram representing distributional information.
The DP generator then samples according to the DP histogram to synthesize a
desired number of data examples. Evaluation across five diverse domains
demonstrates the effectiveness of our framework, particularly in the strong
privacy regime. Systematic ablation validates the design of each framework
component and highlights the scalability of our approach.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.12314v1">Empirical Privacy Variance</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-16T01:43:49Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yuzheng Hu, Fan Wu, Ruicheng Xian, Yuhang Liu, Lydia Zakynthinou, Pritish Kamath, Chiyuan Zhang, David Forsyth</p>
    <p><b>Summary:</b> We propose the notion of empirical privacy variance and study it in the
context of differentially private fine-tuning of language models. Specifically,
we show that models calibrated to the same $(\varepsilon, \delta)$-DP guarantee
using DP-SGD with different hyperparameter configurations can exhibit
significant variations in empirical privacy, which we quantify through the lens
of memorization. We investigate the generality of this phenomenon across
multiple dimensions and discuss why it is surprising and relevant. Through
regression analysis, we examine how individual and composite hyperparameters
influence empirical privacy. The results reveal a no-free-lunch trade-off:
existing practices of hyperparameter tuning in DP-SGD, which focus on
optimizing utility under a fixed privacy budget, often come at the expense of
empirical privacy. To address this, we propose refined heuristics for
hyperparameter selection that explicitly account for empirical privacy, showing
that they are both precise and practically useful. Finally, we take preliminary
steps to understand empirical privacy variance. We propose two hypotheses,
identify limitations in existing techniques like privacy auditing, and outline
open questions for future research.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.12232v1">From Laboratory to Real World: A New Benchmark Towards Privacy-Preserved
  Visible-Infrared Person Re-Identification</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-03-15T18:56:29Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yan Jiang, Hao Yu, Xu Cheng, Haoyu Chen, Zhaodong Sun, Guoying Zhao</p>
    <p><b>Summary:</b> Aiming to match pedestrian images captured under varying lighting conditions,
visible-infrared person re-identification (VI-ReID) has drawn intensive
research attention and achieved promising results. However, in real-world
surveillance contexts, data is distributed across multiple devices/entities,
raising privacy and ownership concerns that make existing centralized training
impractical for VI-ReID. To tackle these challenges, we propose L2RW, a
benchmark that brings VI-ReID closer to real-world applications. The rationale
of L2RW is that integrating decentralized training into VI-ReID can address
privacy concerns in scenarios with limited data-sharing regulation.
Specifically, we design protocols and corresponding algorithms for different
privacy sensitivity levels. In our new benchmark, we ensure the model training
is done in the conditions that: 1) data from each camera remains completely
isolated, or 2) different data entities (e.g., data controllers of a certain
region) can selectively share the data. In this way, we simulate scenarios with
strict privacy constraints which is closer to real-world conditions. Intensive
experiments with various server-side federated algorithms are conducted,
showing the feasibility of decentralized VI-ReID training. Notably, when
evaluated in unseen domains (i.e., new data entities), our L2RW, trained with
isolated data (privacy-preserved), achieves performance comparable to SOTAs
trained with shared data (privacy-unrestricted). We hope this work offers a
novel research entry for deploying VI-ReID that fits real-world scenarios and
can benefit the community.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.12226v1">Research on Large Language Model Cross-Cloud Privacy Protection and
  Collaborative Training based on Federated Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-03-15T18:44:50Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ze Yang, Yihong Jin, Yihan Zhang, Juntian Liu, Xinhe Xu</p>
    <p><b>Summary:</b> The fast development of large language models (LLMs) and popularization of
cloud computing have led to increasing concerns on privacy safeguarding and
data security of cross-cloud model deployment and training as the key
challenges. We present a new framework for addressing these issues along with
enabling privacy preserving collaboration on training between distributed
clouds based on federated learning. Our mechanism encompasses cutting-edge
cryptographic primitives, dynamic model aggregation techniques, and cross-cloud
data harmonization solutions to enhance security, efficiency, and scalability
to the traditional federated learning paradigm. Furthermore, we proposed a
hybrid aggregation scheme to mitigate the threat of Data Leakage and to
optimize the aggregation of model updates, thus achieving substantial
enhancement on the model effectiveness and stability. Experimental results
demonstrate that the training efficiency, privacy protection, and model
accuracy of the proposed model compare favorably to those of the traditional
federated learning method.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.12225v1">Interpretation Gaps in LLM-Assisted Comprehension of Privacy Documents</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Information Retrieval-5BC0EB">
  <p><b>Published on:</b> 2025-03-15T18:43:13Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Rinku Dewri</p>
    <p><b>Summary:</b> This article explores the gaps that can manifest when using a large language
model (LLM) to obtain simplified interpretations of data practices from a
complex privacy policy. We exemplify these gaps to showcase issues in accuracy,
completeness, clarity and representation, while advocating for continued
research to realize an LLM's true potential in revolutionizing privacy
management through personal assistants and automated compliance checking.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.12220v1">A Bubble-Cluster Federated Learning Framework for Privacy-Preserving
  Demand Forecasting on Heterogeneous Retail Data</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-15T18:07:54Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yunbo Long, Liming Xu, Ge Zheng, Alexandra Brintrup</p>
    <p><b>Summary:</b> Federated learning (FL) enables retailers to share model parameters for
demand forecasting while maintaining privacy. However, heterogeneous data
across diverse regions, driven by factors such as varying consumer behavior,
poses challenges to the effectiveness of federated learning. To tackle this
challenge, we propose Bubble-Cluster Federated Learning (BFL), a novel
clustering-based federated learning framework tailored for sales prediction. By
leveraging differential privacy and feature importance distribution, BFL groups
retailers into distinct "bubbles", each forming its own federated learning (FL)
system to effectively isolate data heterogeneity. Within each bubble,
Transformer models are designed to predict local sales for each client. Our
experiments demonstrate that BFL significantly surpasses FedAvg and outperforms
local learning in demand forecasting performance across all participating
clients. Compared to local learning, BFL can achieve a 5.4\% improvement in
R\textsuperscript{2}, a 69\% reduction in RMSE, and a 45\% decrease in MAE. Our
study highlights BFL's adaptability in enabling effective federated learning
through dynamic adjustments to noise levels and the range of clients
participating in each bubble. This approach strategically groups participants
into distinct "bubbles" while proactively identifying and filtering out risky
clients that could compromise the FL system. The findings demonstrate BFL's
ability to enhance collaborative learning in regression tasks on heterogeneous
data, achieving a balance between forecasting accuracy and privacy preservation
in retail applications. Additionally, BFL's capability to detect and neutralize
poisoned data from clients enhances the system's robustness and reliability,
ensuring more secure and effective federated learning.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.12156v1">Efficient and Privacy-Preserved Link Prediction via Condensed Graphs</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Social and Information Networks-662E9B">
  <p><b>Published on:</b> 2025-03-15T14:54:04Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yunbo Long, Liming Xu, Alexandra Brintrup</p>
    <p><b>Summary:</b> Link prediction is crucial for uncovering hidden connections within complex
networks, enabling applications such as identifying potential customers and
products. However, this research faces significant challenges, including
concerns about data privacy, as well as high computational and storage costs,
especially when dealing with large-scale networks. Condensed graphs, which are
much smaller than the original graphs while retaining essential information,
has become an effective solution to both maintain data utility and preserve
privacy. Existing methods, however, initialize synthetic graphs through random
node selection without considering node connectivity, and are mainly designed
for node classification tasks. As a result, their potential for
privacy-preserving link prediction remains largely unexplored. We introduce
HyDRO\textsuperscript{+}, a graph condensation method guided by algebraic
Jaccard similarity, which leverages local connectivity information to optimize
condensed graph structures. Extensive experiments on four real-world networks
show that our method outperforms state-of-the-art methods and even the original
networks in balancing link prediction accuracy and privacy preservation.
Moreover, our method achieves nearly 20* faster training and reduces storage
requirements by 452*, as demonstrated on the Computers dataset, compared to
link prediction on the original networks. This work represents the first
attempt to leverage condensed graphs for privacy-preserving link prediction
information sharing in real-world complex networks. It offers a promising
pathway for preserving link prediction information while safeguarding privacy,
advancing the use of graph condensation in large-scale networks with privacy
concerns.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.12045v1">Auditing Differential Privacy in the Black-Box Setting</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-03-15T08:34:40Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kaining Shi, Cong Ma</p>
    <p><b>Summary:</b> This paper introduces a novel theoretical framework for auditing differential
privacy (DP) in a black-box setting. Leveraging the concept of $f$-differential
privacy, we explicitly define type I and type II errors and propose an auditing
mechanism based on conformal inference. Our approach robustly controls the type
I error rate under minimal assumptions. Furthermore, we establish a fundamental
impossibility result, demonstrating the inherent difficulty of simultaneously
controlling both type I and type II errors without additional assumptions.
Nevertheless, under a monotone likelihood ratio (MLR) assumption, our auditing
mechanism effectively controls both errors. We also extend our method to
construct valid confidence bands for the trade-off function in the
finite-sample regime.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.11963v1">Effective and Efficient Cross-City Traffic Knowledge Transfer A
  Privacy-Preserving Perspective</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-15T02:26:24Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhihao Zeng, Ziquan Fang, Yuting Huang, Lu Chen, Yunjun Gao</p>
    <p><b>Summary:</b> Traffic prediction targets forecasting future traffic conditions using
historical traffic data, serving a critical role in urban computing and
transportation management. To mitigate the scarcity of traffic data while
maintaining data privacy, numerous Federated Traffic Knowledge Transfer (FTT)
approaches have been developed, which use transfer learning and federated
learning to transfer traffic knowledge from data-rich cities to data-scarce
cities, enhancing traffic prediction capabilities for the latter. However,
current FTT approaches face challenges such as privacy leakage, cross-city data
distribution discrepancies, low data quality, and inefficient knowledge
transfer, limiting their privacy protection, effectiveness, robustness, and
efficiency in real-world applications.
  To this end, we propose FedTT, an effective, efficient, and privacy-aware
cross-city traffic knowledge transfer framework that transforms the traffic
data domain from the data-rich cities and trains traffic models using the
transformed data for the data-scarce cities. First, to safeguard data privacy,
we propose a traffic secret transmission method that securely transmits and
aggregates traffic domain-transformed data from source cities using a
lightweight secret aggregation approach. Second, to mitigate the impact of
traffic data distribution discrepancies on model performance, we introduce a
traffic domain adapter to uniformly transform traffic data from the source
cities' domains to that of the target city. Third, to improve traffic data
quality, we design a traffic view imputation method to fill in and predict
missing traffic data. Finally, to enhance transfer efficiency, FedTT is
equipped with a federated parallel training method that enables the
simultaneous training of multiple modules. Extensive experiments using 4
real-life datasets demonstrate that FedTT outperforms the 14 state-of-the-art
baselines.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.11950v1">Privacy Ethics Alignment in AI (PEA-AI): A Stakeholder-Centric Based
  Framework for Ethcial AI</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-03-15T01:42:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ankur Barthwal, Molly Campbell, Ajay Kumar Shrestha</p>
    <p><b>Summary:</b> The increasing integration of Artificial Intelligence (AI) in digital
ecosystems has reshaped privacy dynamics, particularly for young digital
citizens navigating data-driven environments. This study explores evolving
privacy concerns across three key stakeholder groups, digital citizens (ages
16-19), parents, educators, and AI professionals, and assesses differences in
data ownership, trust, transparency, parental mediation, education, and
risk-benefit perceptions. Employing a grounded theory methodology, this
research synthesizes insights from 482 participants through structured surveys,
qualitative interviews, and focus groups. The findings reveal distinct privacy
expectations- Young users emphasize autonomy and digital freedom, while parents
and educators advocate for regulatory oversight and AI literacy programs. AI
professionals, in contrast, prioritize the balance between ethical system
design and technological efficiency. The data further highlights gaps in AI
literacy and transparency, emphasizing the need for comprehensive,
stakeholder-driven privacy frameworks that accommodate diverse user needs.
Using comparative thematic analysis, this study identifies key tensions in
privacy governance and develops the novel Privacy-Ethics Alignment in AI
(PEA-AI) model, which structures privacy decision-making as a dynamic
negotiation between stakeholders. By systematically analyzing themes such as
transparency, user control, risk perception, and parental mediation, this
research provides a scalable, adaptive foundation for AI governance, ensuring
that privacy protections evolve alongside emerging AI technologies and
youth-centric digital interactions.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.11947v1">Ethical AI for Young Digital Citizens: A Call to Action on Privacy
  Governance</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-03-15T01:35:56Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Austin Shouli, Ankur Barthwal, Molly Campbell, Ajay Kumar Shrestha</p>
    <p><b>Summary:</b> The rapid expansion of Artificial Intelligence (AI) in digital platforms used
by youth has created significant challenges related to privacy, autonomy, and
data protection. While AI-driven personalization offers enhanced user
experiences, it often operates without clear ethical boundaries, leaving young
users vulnerable to data exploitation and algorithmic biases. This paper
presents a call to action for ethical AI governance, advocating for a
structured framework that ensures youth-centred privacy protections,
transparent data practices, and regulatory oversight. We outline key areas
requiring urgent intervention, including algorithmic transparency, privacy
education, parental data-sharing ethics, and accountability measures. Through
this approach, we seek to empower youth with greater control over their digital
identities and propose actionable strategies for policymakers, AI developers,
and educators to build a fairer and more accountable AI ecosystem.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.11920v1">Practical Implications of Implementing Local Differential Privacy for
  Smart grids</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-14T23:11:46Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Khadija Hafeez, Mubashir Husain Rehmani, Sumita Mishra, Donna OShea</p>
    <p><b>Summary:</b> Recent smart grid advancements enable near-realtime reporting of electricity
consumption, raising concerns about consumer privacy. Differential privacy (DP)
has emerged as a viable privacy solution, where a calculated amount of noise is
added to the data by a trusted third party, or individual users perturb their
information locally, and only send the randomized data to an aggregator for
analysis safeguarding users and aggregators privacy. However, the practical
implementation of a Local DP-based (LDP) privacy model for smart grids has its
own challenges. In this paper, we discuss the challenges of implementing an
LDP-based model for smart grids. We compare existing LDP mechanisms in smart
grids for privacy preservation of numerical data and discuss different methods
for selecting privacy parameters in the existing literature, their limitations
and the non-existence of an optimal method for selecting the privacy
parameters. We also discuss the challenges of translating theoretical models of
LDP into a practical setting for smart grids for different utility functions,
the impact of the size of data set on privacy and accuracy, and vulnerability
of LDP-based smart grids to manipulation attacks. Finally, we discuss future
directions in research for better practical applications in LDP based models
for smart grids.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.11850v1">Local Pan-Privacy for Federated Analytics</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-03-14T20:18:33Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Vitaly Feldman, Audra McMillan, Guy N. Rothblum, Kunal Talwar</p>
    <p><b>Summary:</b> Pan-privacy was proposed by Dwork et al. as an approach to designing a
private analytics system that retains its privacy properties in the face of
intrusions that expose the system's internal state. Motivated by federated
telemetry applications, we study local pan-privacy, where privacy should be
retained under repeated unannounced intrusions on the local state. We consider
the problem of monitoring the count of an event in a federated system, where
event occurrences on a local device should be hidden even from an intruder on
that device. We show that under reasonable constraints, the goal of providing
information-theoretic differential privacy under intrusion is incompatible with
collecting telemetry information. We then show that this problem can be solved
in a scalable way using standard cryptographic primitives.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.11232v1">PrivacyScalpel: Enhancing LLM Privacy via Interpretable Feature
  Intervention with Sparse Autoencoders</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-03-14T09:31:01Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ahmed Frikha, Muhammad Reza Ar Razi, Krishna Kanth Nakka, Ricardo Mendes, Xue Jiang, Xuebing Zhou</p>
    <p><b>Summary:</b> Large Language Models (LLMs) have demonstrated remarkable capabilities in
natural language processing but also pose significant privacy risks by
memorizing and leaking Personally Identifiable Information (PII). Existing
mitigation strategies, such as differential privacy and neuron-level
interventions, often degrade model utility or fail to effectively prevent
leakage. To address this challenge, we introduce PrivacyScalpel, a novel
privacy-preserving framework that leverages LLM interpretability techniques to
identify and mitigate PII leakage while maintaining performance. PrivacyScalpel
comprises three key steps: (1) Feature Probing, which identifies layers in the
model that encode PII-rich representations, (2) Sparse Autoencoding, where a
k-Sparse Autoencoder (k-SAE) disentangles and isolates privacy-sensitive
features,
  and (3) Feature-Level Interventions, which employ targeted ablation and
vector steering to suppress PII leakage.
  Our empirical evaluation on Gemma2-2b and Llama2-7b, fine-tuned on the Enron
dataset, shows that PrivacyScalpel significantly reduces email leakage from
5.15\% to as low as 0.0\%, while maintaining over 99.4\% of the original
model's utility. Notably, our method outperforms neuron-level interventions in
privacy-utility trade-offs, demonstrating that acting on sparse, monosemantic
features is more effective than manipulating polysemantic neurons. Beyond
improving LLM privacy, our approach offers insights into the mechanisms
underlying PII memorization, contributing to the broader field of model
interpretability and secure AI deployment.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.11169v1">Security and Privacy: Key Requirements for Molecular Communication in
  Medicine and Healthcare</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Information Theory-D91E36"> 
  <p><b>Published on:</b> 2025-03-14T08:14:14Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Vida Gholamiyan, Yaning Zhao, Wafa Labidi, Holger Boche, Christian Deppe</p>
    <p><b>Summary:</b> Molecular communication (MC) is an emerging paradigm that enables data
transmission through biochemical signals rather than traditional
electromagnetic waves. This approach is particularly promising for environments
where conventional wireless communication is impractical, such as within the
human body. However, security and privacy pose significant challenges that must
be addressed to ensure reliable communication. Moreover, MC is often
event-triggered, making it logical to adopt goal-oriented communication
strategies, similar to those used in message identification. This work explores
secure identification strategies for MC, with a focus on the
information-theoretic security of message identification over Poisson wiretap
channels (DT-PWC).</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.11051v1">Towards Privacy-preserved Pre-training of Remote Sensing Foundation
  Models with Federated Mutual-guidance Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-03-14T03:38:49Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jieyi Tan, Chengwei Zhang, Bo Dang, Yansheng Li</p>
    <p><b>Summary:</b> Traditional Remote Sensing Foundation models (RSFMs) are pre-trained with a
data-centralized paradigm, through self-supervision on large-scale curated
remote sensing data. For each institution, however, pre-training RSFMs with
limited data in a standalone manner may lead to suboptimal performance, while
aggregating remote sensing data from multiple institutions for centralized
pre-training raises privacy concerns. Seeking for collaboration is a promising
solution to resolve this dilemma, where multiple institutions can
collaboratively train RSFMs without sharing private data. In this paper, we
propose a novel privacy-preserved pre-training framework (FedSense), which
enables multiple institutions to collaboratively train RSFMs without sharing
private data. However, it is a non-trivial task hindered by a vicious cycle,
which results from model drift by remote sensing data heterogeneity and high
communication overhead. To break this vicious cycle, we introduce Federated
Mutual-guidance Learning. Specifically, we propose a Server-to-Clients Guidance
(SCG) mechanism to guide clients updates towards global-flatness optimal
solutions. Additionally, we propose a Clients-to-Server Guidance (CSG)
mechanism to inject local knowledge into the server by low-bit communication.
Extensive experiments on four downstream tasks demonstrate the effectiveness of
our FedSense in both full-precision and communication-reduced scenarios,
showcasing remarkable communication efficiency and performance gains.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.10945v1">$(\varepsilon, δ)$ Considered Harmful: Best Practices for Reporting
  Differential Privacy Guarantees</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> 
  <p><b>Published on:</b> 2025-03-13T23:06:30Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Juan Felipe Gomez, Bogdan Kulynych, Georgios Kaissis, Jamie Hayes, Borja Balle, Antti Honkela</p>
    <p><b>Summary:</b> Current practices for reporting the level of differential privacy (DP)
guarantees for machine learning (ML) algorithms provide an incomplete and
potentially misleading picture of the guarantees and make it difficult to
compare privacy levels across different settings. We argue for using Gaussian
differential privacy (GDP) as the primary means of communicating DP guarantees
in ML, with the full privacy profile as a secondary option in case GDP is too
inaccurate. Unlike other widely used alternatives, GDP has only one parameter,
which ensures easy comparability of guarantees, and it can accurately capture
the full privacy profile of many important ML applications. To support our
claims, we investigate the privacy profiles of state-of-the-art DP large-scale
image classification, and the TopDown algorithm for the U.S. Decennial Census,
observing that GDP fits the profiles remarkably well in all three cases.
Although GDP is ideal for reporting the final guarantees, other formalisms
(e.g., privacy loss random variables) are needed for accurate privacy
accounting. We show that such intermediate representations can be efficiently
converted to GDP with minimal loss in tightness.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.10915v1">Usable Privacy in Virtual Worlds: Design Implications for Data
  Collection Awareness and Control Interfaces in Virtual Reality</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-03-13T22:02:54Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Viktorija Paneva, Verena Winterhalter, Naga Sai Surya Vamsy Malladi, Marvin Strauss, Stefan Schneegass, Florian Alt</p>
    <p><b>Summary:</b> Extended reality (XR) devices have become ubiquitous. They are equipped with
arrays of sensors, collecting extensive user and environmental data, allowing
inferences about sensitive user information users may not realize they are
sharing. Current VR privacy notices largely replicate mechanisms from 2D
interfaces, failing to leverage the unique affordances of virtual 3D
environments. To address this, we conducted brainstorming and sketching
sessions with novice game developers and designers, followed by privacy expert
evaluations, to explore and refine privacy interfaces tailored for VR. Key
challenges include balancing user engagement with privacy awareness, managing
complex privacy information with user comprehension, and maintaining compliance
and trust. We identify design implications such as thoughtful gamification,
explicit and purpose-tied consent mechanisms, and granular, modifiable privacy
control options. Our findings provide actionable guidance to researchers and
practitioners for developing privacy-aware and user-friendly VR experiences.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.10350v1">Enhancing Facial Privacy Protection via Weakening Diffusion Purification</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-03-13T13:27:53Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ali Salar, Qing Liu, Yingli Tian, Guoying Zhao</p>
    <p><b>Summary:</b> The rapid growth of social media has led to the widespread sharing of
individual portrait images, which pose serious privacy risks due to the
capabilities of automatic face recognition (AFR) systems for mass surveillance.
Hence, protecting facial privacy against unauthorized AFR systems is essential.
Inspired by the generation capability of the emerging diffusion models, recent
methods employ diffusion models to generate adversarial face images for privacy
protection. However, they suffer from the diffusion purification effect,
leading to a low protection success rate (PSR). In this paper, we first propose
learning unconditional embeddings to increase the learning capacity for
adversarial modifications and then use them to guide the modification of the
adversarial latent code to weaken the diffusion purification effect. Moreover,
we integrate an identity-preserving structure to maintain structural
consistency between the original and generated images, allowing human observers
to recognize the generated image as having the same identity as the original.
Extensive experiments conducted on two public datasets, i.e., CelebA-HQ and
LADN, demonstrate the superiority of our approach. The protected faces
generated by our method outperform those produced by existing facial privacy
protection approaches in terms of transferability and natural appearance.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.10727v1">Word-level Annotation of GDPR Transparency Compliance in Privacy
  Policies using Large Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-03-13T11:41:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Thomas Cory, Wolf Rieder, Julia Krämer, Philip Raschke, Patrick Herbke, Axel Küpper</p>
    <p><b>Summary:</b> Ensuring transparency of data practices related to personal information is a
fundamental requirement under the General Data Protection Regulation (GDPR),
particularly as mandated by Articles 13 and 14. However, assessing compliance
at scale remains a challenge due to the complexity and variability of privacy
policy language. Manual audits are resource-intensive and inconsistent, while
existing automated approaches lack the granularity needed to capture nuanced
transparency disclosures.
  In this paper, we introduce a large language model (LLM)-based framework for
word-level GDPR transparency compliance annotation. Our approach comprises a
two-stage annotation pipeline that combines initial LLM-based annotation with a
self-correction mechanism for iterative refinement. This annotation pipeline
enables the systematic identification and fine-grained annotation of
transparency-related content in privacy policies, aligning with 21 GDPR-derived
transparency requirements. To enable large-scale analysis, we compile a dataset
of 703,791 English-language policies, from which we generate a sample of 200
manually annotated privacy policies.
  To evaluate our approach, we introduce a two-tiered methodology assessing
both label- and span-level annotation performance. We conduct a comparative
analysis of eight high-profile LLMs, providing insights into their
effectiveness in identifying GDPR transparency disclosures. Our findings
contribute to advancing the automation of GDPR compliance assessments and
provide valuable resources for future research in privacy policy analysis.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.10239v1">I Can Tell Your Secrets: Inferring Privacy Attributes from Mini-app
  Interaction History in Super-apps</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-13T10:29:40Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yifeng Cai, Ziqi Zhang, Mengyu Yao, Junlin Liu, Xiaoke Zhao, Xinyi Fu, Ruoyu Li, Zhe Li, Xiangqun Chen, Yao Guo, Ding Li</p>
    <p><b>Summary:</b> Super-apps have emerged as comprehensive platforms integrating various
mini-apps to provide diverse services. While super-apps offer convenience and
enriched functionality, they can introduce new privacy risks. This paper
reveals a new privacy leakage source in super-apps: mini-app interaction
history, including mini-app usage history (Mini-H) and operation history
(Op-H). Mini-H refers to the history of mini-apps accessed by users, such as
their frequency and categories. Op-H captures user interactions within
mini-apps, including button clicks, bar drags, and image views. Super-apps can
naturally collect these data without instrumentation due to the web-based
feature of mini-apps. We identify these data types as novel and unexplored
privacy risks through a literature review of 30 papers and an empirical
analysis of 31 super-apps. We design a mini-app interaction history-oriented
inference attack (THEFT), to exploit this new vulnerability. Using THEFT, the
insider threats within the low-privilege business department of the super-app
vendor acting as the adversary can achieve more than 95.5% accuracy in
inferring privacy attributes of over 16.1% of users. THEFT only requires a
small training dataset of 200 users from public breached databases on the
Internet. We also engage with super-app vendors and a standards association to
increase industry awareness and commitment to protect this data. Our
contributions are significant in identifying overlooked privacy risks,
demonstrating the effectiveness of a new attack, and influencing industry
practices toward better privacy protection in the super-app ecosystem.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.10147v1">Optimal Privacy-Preserving Distributed Median Consensus</a></h3>
  
  <p><b>Published on:</b> 2025-03-13T08:19:12Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Wenrui Yu, Qiongxiu Li, Richard Heusdens, Sokol Kosta</p>
    <p><b>Summary:</b> Distributed median consensus has emerged as a critical paradigm in
multi-agent systems due to the inherent robustness of the median against
outliers and anomalies in measurement. Despite the sensitivity of the data
involved, the development of privacy-preserving mechanisms for median consensus
remains underexplored. In this work, we present the first rigorous analysis of
privacy in distributed median consensus, focusing on an $L_1$-norm minimization
framework. We establish necessary and sufficient conditions under which exact
consensus and perfect privacy-defined as zero information leakage-can be
achieved simultaneously. Our information-theoretic analysis provides provable
guarantees against passive and eavesdropping adversaries, ensuring that private
data remain concealed. Extensive numerical experiments validate our theoretical
results, demonstrating the practical feasibility of achieving both accuracy and
privacy in distributed median consensus.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.09823v1">Data Traceability for Privacy Alignment</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2025-03-12T20:42:23Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kevin Liao, Shreya Thipireddy, Daniel Weitzner</p>
    <p><b>Summary:</b> This paper offers a new privacy approach for the growing ecosystem of
services--ranging from open banking to healthcare--dependent on sensitive
personal data sharing between individuals and third-parties. While these
services offer significant benefits, individuals want control over their data,
transparency regarding how their data is used, and accountability from
third-parties for misuse. However, existing legal and technical mechanisms are
inadequate for supporting these needs. A comprehensive approach to the modern
privacy challenges of accountable third-party data sharing requires a closer
alignment of technical system architecture and legal institutional design. In
order to achieve this privacy alignment, we extend traditional security threat
modeling and analysis to encompass a broader range of privacy notions than has
been typically considered. In particular, we introduce the concept of
covert-accountability, which addresses adversaries that may act dishonestly but
face potential identification and legal consequences. As a concrete instance of
this design approach, we present the OTrace protocol, designed to provide
traceable, accountable, consumer-control in third-party data sharing
ecosystems. OTrace empowers consumers with the knowledge of where their data
is, who has it, what it is being used for, and whom it is being shared with. By
applying our alignment framework to OTrace, we demonstrate that OTrace's
technical affordances can provide more confident, scalable regulatory oversight
when combined with complementary legal mechanisms.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.09780v1">AgentDAM: Privacy Leakage Evaluation for Autonomous Web Agents</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-03-12T19:30:31Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Arman Zharmagambetov, Chuan Guo, Ivan Evtimov, Maya Pavlova, Ruslan Salakhutdinov, Kamalika Chaudhuri</p>
    <p><b>Summary:</b> LLM-powered AI agents are an emerging frontier with tremendous potential to
increase human productivity. However, empowering AI agents to take action on
their user's behalf in day-to-day tasks involves giving them access to
potentially sensitive and private information, which leads to a possible risk
of inadvertent privacy leakage when the agent malfunctions. In this work, we
propose one way to address that potential risk, by training AI agents to better
satisfy the privacy principle of data minimization. For the purposes of this
benchmark, by "data minimization" we mean instances where private information
is shared only when it is necessary to fulfill a specific task-relevant
purpose. We develop a benchmark called AgentDAM to evaluate how well existing
and future AI agents can limit processing of potentially private information
that we designate "necessary" to fulfill the task. Our benchmark simulates
realistic web interaction scenarios and is adaptable to all existing web
navigation agents. We use AgentDAM to evaluate how well AI agents built on top
of GPT-4, Llama-3 and Claude can limit processing of potentially private
information when unnecessary, and show that these agents are often prone to
inadvertent use of unnecessary sensitive information. We finally propose a
prompting-based approach that reduces this.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.11711v1">Privacy-Preserved Automated Scoring using Federated Learning for
  Educational Research</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-03-12T19:06:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ehsan Latif, Xiaoming Zhai</p>
    <p><b>Summary:</b> Data privacy remains a critical concern in educational research,
necessitating Institutional Review Board (IRB) certification and stringent data
handling protocols to ensure compliance with ethical standards. Traditional
approaches rely on anonymization and controlled data-sharing mechanisms to
facilitate research while mitigating privacy risks. However, these methods
still involve direct access to raw student data, posing potential
vulnerabilities and being time-consuming. This study proposes a federated
learning (FL) framework for automatic scoring in educational assessments,
eliminating the need to share raw data. Our approach leverages client-side
model training, where student responses are processed locally on edge devices,
and only optimized model parameters are shared with a central aggregation
server. To effectively aggregate heterogeneous model updates, we introduce an
adaptive weighted averaging strategy, which dynamically adjusts weight
contributions based on client-specific learning characteristics. This method
ensures robust model convergence while preserving privacy. We evaluate our
framework using assessment data from nine middle schools, comparing the
accuracy of federated learning-based scoring models with traditionally trained
centralized models. A statistical significance test (paired t-test, $t(8) =
2.29, p = 0.051$) confirms that the accuracy difference between the two
approaches is not statistically significant, demonstrating that federated
learning achieves comparable performance while safeguarding student data.
Furthermore, our method significantly reduces data collection, processing, and
deployment overhead, accelerating the adoption of AI-driven educational
assessments in a privacy-compliant manner.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.09448v1">Optimizing QoE-Privacy Tradeoff for Proactive VR Streaming</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Multimedia-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Multiagent Systems-662E9B">
  <p><b>Published on:</b> 2025-03-12T14:50:06Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xing Wei, Shengqian Han, Chenyang Yang, Chengjian Sun</p>
    <p><b>Summary:</b> Proactive virtual reality (VR) streaming requires users to upload
viewpoint-related information, raising significant privacy concerns. Existing
strategies preserve privacy by introducing errors to viewpoints, which,
however, compromises the quality of experience (QoE) of users. In this paper,
we first delve into the analysis of the viewpoint leakage probability achieved
by existing privacy-preserving approaches. We determine the optimal
distribution of viewpoint errors that minimizes the viewpoint leakage
probability. Our analyses show that existing approaches cannot fully eliminate
viewpoint leakage. Then, we propose a novel privacy-preserving approach that
introduces noise to uploaded viewpoint prediction errors, which can ensure zero
viewpoint leakage probability. Given the proposed approach, the tradeoff
between privacy preservation and QoE is optimized to minimize the QoE loss
while satisfying the privacy requirement. Simulation results validate our
analysis results and demonstrate that the proposed approach offers a promising
solution for balancing privacy and QoE.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.09381v2">Faithful and Privacy-Preserving Implementation of Average Consensus</a></h3>
   <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Systems and Control-D91E36">
  <p><b>Published on:</b> 2025-03-12T13:28:22Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kaoru Teranishi, Kiminao Kogiso, Takashi Tanaka</p>
    <p><b>Summary:</b> We propose a protocol based on mechanism design theory and encrypted control
to solve average consensus problems among rational and strategic agents while
preserving their privacy. The proposed protocol provides a mechanism that
incentivizes the agents to faithfully implement the intended behavior specified
in the protocol. Furthermore, the protocol runs over encrypted data using
homomorphic encryption and secret sharing to protect the privacy of agents. We
also analyze the security of the proposed protocol using a simulation paradigm
in secure multi-party computation. The proposed protocol demonstrates that
mechanism design and encrypted control can complement each other to achieve
security under rational adversaries.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.09365v1">Membership Inference Attacks fueled by Few-Short Learning to detect
  privacy leakage tackling data integrity</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-03-12T13:09:43Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Daniel Jiménez-López, Nuria Rodríguez-Barroso, M. Victoria Luzón, Francisco Herrera</p>
    <p><b>Summary:</b> Deep learning models have an intrinsic privacy issue as they memorize parts
of their training data, creating a privacy leakage. Membership Inference
Attacks (MIA) exploit it to obtain confidential information about the data used
for training, aiming to steal information. They can be repurposed as a
measurement of data integrity by inferring whether it was used to train a
machine learning model. While state-of-the-art attacks achieve a significant
privacy leakage, their requirements are not feasible enough, hindering their
role as practical tools to assess the magnitude of the privacy risk. Moreover,
the most appropriate evaluation metric of MIA, the True Positive Rate at low
False Positive Rate lacks interpretability. We claim that the incorporation of
Few-Shot Learning techniques to the MIA field and a proper qualitative and
quantitative privacy evaluation measure should deal with these issues. In this
context, our proposal is twofold. We propose a Few-Shot learning based MIA,
coined as the FeS-MIA model, which eases the evaluation of the privacy breach
of a deep learning model by significantly reducing the number of resources
required for the purpose. Furthermore, we propose an interpretable quantitative
and qualitative measure of privacy, referred to as Log-MIA measure. Jointly,
these proposals provide new tools to assess the privacy leakage and to ease the
evaluation of the training data integrity of deep learning models, that is, to
analyze the privacy breach of a deep learning model. Experiments carried out
with MIA over image classification and language modeling tasks and its
comparison to the state-of-the-art show that our proposals excel at reporting
the privacy leakage of a deep learning model with little extra information.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.09331v1">Large-Scale FPGA-Based Privacy Amplification Exceeding $10^8$ Bits for
  Quantum Key Distribution</a></h3>
  
  <p><b>Published on:</b> 2025-03-12T12:25:13Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Xi Cheng, Hao-kun Mao, Hong-wei Xu, Qiong Li</p>
    <p><b>Summary:</b> Privacy Amplification (PA) is indispensable in Quantum Key Distribution (QKD)
post-processing, as it eliminates information leakage to eavesdroppers.
Field-programmable gate arrays (FPGAs) are highly attractive for QKD systems
due to their flexibility and high integration. However, due to limited
resources, input and output sizes remain the primary bottleneck in FPGA-based
PA schemes for Discrete Variable (DV)-QKD systems. In this paper, we present a
large-scale FPGA-based PA scheme that supports both input block sizes and
output key sizes exceeding $10^8$ bits, effectively addressing the challenges
posed by the finite-size effect. To accommodate the large input and output
sizes, we propose a novel PA algorithm and prove its security. We implement and
evaluate this scheme on a Xilinx XCKU095 FPGA platform. Experimental results
demonstrate that our PA implementation can handle an input block size of $10^8$
bits with flexible output sizes up to the input size. For DV-QKD systems, our
PA scheme supports an input block size nearly two orders of magnitude larger
than current FPGA-based PA schemes, significantly mitigating the impact of the
finite-size effect on the final secure key rate.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.09317v1">RaceTEE: A Practical Privacy-Preserving Off-Chain Smart Contract
  Execution Architecture</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-12T12:10:02Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Keyu Zhang, Andrew Martin</p>
    <p><b>Summary:</b> Decentralized on-chain smart contracts enable trustless collaboration, yet
their inherent data transparency and execution overhead hinder widespread
adoption. Existing cryptographic approaches incur high computational costs and
lack generality. Meanwhile, prior TEE-based solutions suffer from practical
limitations, such as the inability to support inter-contract interactions,
reliance on unbreakable TEEs, and compromised usability. We introduce RaceTEE,
a practical and privacy-preserving off-chain execution architecture for smart
contracts that leverages Trusted Execution Environments (TEEs). RaceTEE
decouples transaction ordering (on-chain) from execution (off-chain), with
computations performed competitively in TEEs, ensuring confidentiality and
minimizing overhead. It further enhances practicality through three key
improvements: supporting secure inter-contract interactions, providing a key
rotation scheme that enforces forward and backward secrecy even in the event of
TEE breaches, and enabling full compatibility with existing blockchains without
altering the user interaction model. To validate its feasibility, we prototype
RaceTEE using Intel SGX and Ethereum, demonstrating its applicability across
various use cases and evaluating its performance.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.09192v1">Differential Privacy Personalized Federated Learning Based on
  Dynamically Sparsified Client Updates</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-12T09:34:05Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chuanyin Wang, Yifei Zhang, Neng Gao, Qiang Luo</p>
    <p><b>Summary:</b> Personalized federated learning is extensively utilized in scenarios
characterized by data heterogeneity, facilitating more efficient and automated
local training on data-owning terminals. This includes the automated selection
of high-performance model parameters for upload, thereby enhancing the overall
training process. However, it entails significant risks of privacy leakage.
Existing studies have attempted to mitigate these risks by utilizing
differential privacy. Nevertheless, these studies present two major
limitations: (1) The integration of differential privacy into personalized
federated learning lacks sufficient personalization, leading to the
introduction of excessive noise into the model. (2) It fails to adequately
control the spatial scope of model update information, resulting in a
suboptimal balance between data privacy and model effectiveness in differential
privacy federated learning. In this paper, we propose a differentially private
personalized federated learning approach that employs dynamically sparsified
client updates through reparameterization and adaptive norm(DP-pFedDSU).
Reparameterization training effectively selects personalized client update
information, thereby reducing the quantity of updates. This approach minimizes
the introduction of noise to the greatest extent possible. Additionally,
dynamic adaptive norm refers to controlling the norm space of model updates
during the training process, mitigating the negative impact of clipping on the
update information. These strategies substantially enhance the effective
integration of differential privacy and personalized federated learning.
Experimental results on EMNIST, CIFAR-10, and CIFAR-100 demonstrate that our
proposed scheme achieves superior performance and is well-suited for more
complex personalized federated learning scenarios.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.08568v1">Privacy Law Enforcement Under Centralized Governance: A Qualitative
  Analysis of Four Years' Special Privacy Rectification Campaigns</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-11T15:56:09Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tao Jing, Yao Li, Jingzhou Ye, Jie Wang, Xueqiang Wang</p>
    <p><b>Summary:</b> In recent years, major privacy laws like the GDPR have brought about positive
changes. However, challenges remain in enforcing the laws, particularly due to
under-resourced regulators facing a large number of potential privacy-violating
software applications (apps) and the high costs of investigating them. Since
2019, China has launched a series of privacy enforcement campaigns known as
Special Privacy Rectification Campaigns (SPRCs) to address widespread privacy
violations in its mobile application (app) ecosystem. Unlike the enforcement of
the GDPR, SPRCs are characterized by large-scale privacy reviews and strict
sanctions, under the strong control of central authorities. In SPRCs, central
government authorities issue administrative orders to mobilize various
resources for market-wide privacy reviews of mobile apps. They enforce strict
sanctions by requiring privacy-violating apps to rectify issues within a short
timeframe or face removal from app stores. While there are a few reports on
SPRCs, the effectiveness and potential problems of this campaign-style privacy
enforcement approach remain unclear to the community. In this study, we
conducted 18 semi-structured interviews with app-related engineers involved in
SPRCs to better understand the campaign-style privacy enforcement. Based on the
interviews, we reported our findings on a variety of aspects of SPRCs, such as
the processes that app engineers regularly follow to achieve privacy compliance
in SPRCs, the challenges they encounter, the solutions they adopt to address
these challenges, and the impacts of SPRCs, etc. We found that app engineers
face a series of challenges in achieving privacy compliance in their apps...</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.08297v1">Privacy for Free: Leveraging Local Differential Privacy Perturbed Data
  from Multiple Services</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-11T11:10:03Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Rong Du, Qingqing Ye, Yue Fu, Haibo Hu</p>
    <p><b>Summary:</b> Local Differential Privacy (LDP) has emerged as a widely adopted
privacy-preserving technique in modern data analytics, enabling users to share
statistical insights while maintaining robust privacy guarantees. However,
current LDP applications assume a single service gathering perturbed
information from users. In reality, multiple services may be interested in
collecting users' data, which poses privacy burdens to users as more such
services emerge. To address this issue, this paper proposes a framework for
collecting and aggregating data based on perturbed information from multiple
services, regardless of their estimated statistics (e.g., mean or distribution)
and perturbation mechanisms.
  Then for mean estimation, we introduce the Unbiased Averaging (UA) method and
its optimized version, User-level Weighted Averaging (UWA). The former utilizes
biased perturbed data, while the latter assigns weights to different perturbed
results based on perturbation information, thereby achieving minimal variance.
For distribution estimation, we propose the User-level Likelihood Estimation
(ULE), which treats all perturbed results from a user as a whole for maximum
likelihood estimation. Experimental results demonstrate that our framework and
constituting methods significantly improve the accuracy of both mean and
distribution estimation.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.08175v1">Privacy-Enhancing Paradigms within Federated Multi-Agent Systems</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-03-11T08:38:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zitong Shi, Guancheng Wan, Wenke Huang, Guibin Zhang, Jiawei Shao, Mang Ye, Carl Yang</p>
    <p><b>Summary:</b> LLM-based Multi-Agent Systems (MAS) have proven highly effective in solving
complex problems by integrating multiple agents, each performing different
roles. However, in sensitive domains, they face emerging privacy protection
challenges. In this paper, we introduce the concept of Federated MAS,
highlighting the fundamental differences between Federated MAS and traditional
FL. We then identify key challenges in developing Federated MAS, including: 1)
heterogeneous privacy protocols among agents, 2) structural differences in
multi-party conversations, and 3) dynamic conversational network structures. To
address these challenges, we propose Embedded Privacy-Enhancing Agents
(EPEAgent), an innovative solution that integrates seamlessly into the
Retrieval-Augmented Generation (RAG) phase and the context retrieval stage.
This solution minimizes data flows, ensuring that only task-relevant,
agent-specific information is shared. Additionally, we design and generate a
comprehensive dataset to evaluate the proposed paradigm. Extensive experiments
demonstrate that EPEAgent effectively enhances privacy protection while
maintaining strong system performance. The code will be availiable at
https://github.com/ZitongShi/EPEAgent</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.08085v2">PRISM: Privacy-Preserving Improved Stochastic Masking for Federated
  Generative Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-03-11T06:37:54Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kyeongkook Seo, Dong-Jun Han, Jaejun Yoo</p>
    <p><b>Summary:</b> Despite recent advancements in federated learning (FL), the integration of
generative models into FL has been limited due to challenges such as high
communication costs and unstable training in heterogeneous data environments.
To address these issues, we propose PRISM, a FL framework tailored for
generative models that ensures (i) stable performance in heterogeneous data
distributions and (ii) resource efficiency in terms of communication cost and
final model size. The key of our method is to search for an optimal stochastic
binary mask for a random network rather than updating the model weights,
identifying a sparse subnetwork with high generative performance; i.e., a
``strong lottery ticket''. By communicating binary masks in a stochastic
manner, PRISM minimizes communication overhead. This approach, combined with
the utilization of maximum mean discrepancy (MMD) loss and a mask-aware dynamic
moving average aggregation method (MADA) on the server side, facilitates stable
and strong generative capabilities by mitigating local divergence in FL
scenarios. Moreover, thanks to its sparsifying characteristic, PRISM yields a
lightweight model without extra pruning or quantization, making it ideal for
environments such as edge devices. Experiments on MNIST, FMNIST, CelebA, and
CIFAR10 demonstrate that PRISM outperforms existing methods, while maintaining
privacy with minimal communication costs. PRISM is the first to successfully
generate images under challenging non-IID and privacy-preserving FL
environments on complex datasets, where previous methods have struggled.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.07775v1">Sublinear Algorithms for Wasserstein and Total Variation Distances:
  Applications to Fairness and Privacy Auditing</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Data Structures and Algorithms-662E9B"> 
  <p><b>Published on:</b> 2025-03-10T18:57:48Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Debabrota Basu, Debarshi Chanda</p>
    <p><b>Summary:</b> Resource-efficiently computing representations of probability distributions
and the distances between them while only having access to the samples is a
fundamental and useful problem across mathematical sciences. In this paper, we
propose a generic algorithmic framework to estimate the PDF and CDF of any
sub-Gaussian distribution while the samples from them arrive in a stream. We
compute mergeable summaries of distributions from the stream of samples that
require sublinear space w.r.t. the number of observed samples. This allows us
to estimate Wasserstein and Total Variation (TV) distances between any two
sub-Gaussian distributions while samples arrive in streams and from multiple
sources (e.g. federated learning). Our algorithms significantly improves on the
existing methods for distance estimation incurring super-linear time and linear
space complexities. In addition, we use the proposed estimators of Wasserstein
and TV distances to audit the fairness and privacy of the ML algorithms. We
empirically demonstrate the efficiency of the algorithms for estimating these
distances and auditing using both synthetic and real-world datasets.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.07570v1">Split-n-Chain: Privacy-Preserving Multi-Node Split Learning with
  Blockchain-Based Auditability</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-03-10T17:40:05Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mukesh Sahani, Binanda Sengupta</p>
    <p><b>Summary:</b> Deep learning, when integrated with a large amount of training data, has the
potential to outperform machine learning in terms of high accuracy. Recently,
privacy-preserving deep learning has drawn significant attention of the
research community. Different privacy notions in deep learning include privacy
of data provided by data-owners and privacy of parameters and/or
hyperparameters of the underlying neural network. Federated learning is a
popular privacy-preserving execution environment where data-owners participate
in learning the parameters collectively without leaking their respective data
to other participants. However, federated learning suffers from certain
security/privacy issues. In this paper, we propose Split-n-Chain, a variant of
split learning where the layers of the network are split among several
distributed nodes. Split-n-Chain achieves several privacy properties:
data-owners need not share their training data with other nodes, and no nodes
have access to the parameters and hyperparameters of the neural network (except
that of the respective layers they hold). Moreover, Split-n-Chain uses
blockchain to audit the computation done by different nodes. Our experimental
results show that: Split-n-Chain is efficient, in terms of time required to
execute different phases, and the training loss trend is similar to that for
the same neural network when implemented in a monolithic fashion.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.07505v1">From Centralized to Decentralized Federated Learning: Theoretical
  Insights, Privacy Preservation, and Robustness Challenges</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2025-03-10T16:27:40Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Qiongxiu Li, Wenrui Yu, Yufei Xia, Jun Pang</p>
    <p><b>Summary:</b> Federated Learning (FL) enables collaborative learning without directly
sharing individual's raw data. FL can be implemented in either a centralized
(server-based) or decentralized (peer-to-peer) manner. In this survey, we
present a novel perspective: the fundamental difference between centralized FL
(CFL) and decentralized FL (DFL) is not merely the network topology, but the
underlying training protocol: separate aggregation vs. joint optimization. We
argue that this distinction in protocol leads to significant differences in
model utility, privacy preservation, and robustness to attacks. We
systematically review and categorize existing works in both CFL and DFL
according to the type of protocol they employ. This taxonomy provides deeper
insights into prior research and clarifies how various approaches relate or
differ. Through our analysis, we identify key gaps in the literature. In
particular, we observe a surprising lack of exploration of DFL approaches based
on distributed optimization methods, despite their potential advantages. We
highlight this under-explored direction and call for more research on
leveraging distributed optimization for federated learning. Overall, this work
offers a comprehensive overview from centralized to decentralized FL, sheds new
light on the core distinctions between approaches, and outlines open challenges
and future directions for the field.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.07427v2">Creating and Evaluating Privacy and Security Micro-Lessons for
  Elementary School Children</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">
  <p><b>Published on:</b> 2025-03-10T15:12:11Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Lan Gao, Elana B Blinder, Abigail Barnes, Kevin Song, Tamara Clegg, Jessica Vitak, Marshini Chetty</p>
    <p><b>Summary:</b> The growing use of technology in K--8 classrooms highlights a parallel need
for formal learning opportunities aimed at helping children use technology
safely and protect their personal information. Even the youngest students are
now using tablets, laptops, and apps to support their learning; however, there
are limited curricular materials available for elementary and middle school
children on digital privacy and security topics. To bridge this gap, we
developed a series of micro-lessons to help K--8 children learn about digital
privacy and security at school. We first conducted a formative study by
interviewing elementary school teachers to identify the design needs for
digital privacy and security lessons. We then developed micro-lessons --
multiple 15-20 minute activities designed to be easily inserted into the
existing curriculum -- using a co-design approach with multiple rounds of
developing and revising the micro-lessons in collaboration with teachers.
Throughout the process, we conducted evaluation sessions where teachers
implemented or reviewed the micro-lessons. Our study identifies strengths,
challenges, and teachers' tailoring strategies when incorporating micro-lessons
for K--8 digital privacy and security topics, providing design implications for
facilitating learning about these topics in school classrooms.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.07216v2">FedRand: Enhancing Privacy in Federated Learning with Randomized LoRA
  Subparameter Updates</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-03-10T11:55:50Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Sangwoo Park, Seanie Lee, Byungjoo Kim, Sung Ju Hwang</p>
    <p><b>Summary:</b> Federated Learning (FL) is a widely used framework for training models in a
decentralized manner, ensuring that the central server does not have direct
access to data from local clients. However, this approach may still fail to
fully preserve data privacy, as models from local clients are exposed to the
central server during the aggregation process. This issue becomes even more
critical when training vision-language models (VLMs) with FL, as VLMs can
easily memorize training data instances, making them vulnerable to membership
inference attacks (MIAs). To address this challenge, we propose the FedRand
framework, which avoids disclosing the full set of client parameters. In this
framework, each client randomly selects subparameters of Low-Rank Adaptation
(LoRA) from the server and keeps the remaining counterparts of the LoRA weights
as private parameters. After training both parameters on the client's private
dataset, only the non-private client parameters are sent back to the server for
aggregation. This approach mitigates the risk of exposing client-side VLM
parameters, thereby enhancing data privacy. We empirically validate that
FedRand improves robustness against MIAs compared to relevant baselines while
achieving accuracy comparable to methods that communicate full LoRA parameters
across several benchmark datasets.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.07199v1">How Well Can Differential Privacy Be Audited in One Run?</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-10T11:32:30Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Amit Keinan, Moshe Shenfeld, Katrina Ligett</p>
    <p><b>Summary:</b> Recent methods for auditing the privacy of machine learning algorithms have
improved computational efficiency by simultaneously intervening on multiple
training examples in a single training run. Steinke et al. (2024) prove that
one-run auditing indeed lower bounds the true privacy parameter of the audited
algorithm, and give impressive empirical results. Their work leaves open the
question of how precisely one-run auditing can uncover the true privacy
parameter of an algorithm, and how that precision depends on the audited
algorithm. In this work, we characterize the maximum achievable efficacy of
one-run auditing and show that one-run auditing can only perfectly uncover the
true privacy parameters of algorithms whose structure allows the effects of
individual data elements to be isolated. Our characterization helps reveal how
and when one-run auditing is still a promising technique for auditing real
machine learning algorithms, despite these fundamental gaps.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.07048v1">A Failure-Free and Efficient Discrete Laplace Distribution for
  Differential Privacy in MPC</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-10T08:35:16Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ivan Tjuawinata, Jiabo Wang, Mengmeng Yang, Shanxiang Lyu, Huaxiong Wang, Kwok-Yan Lam</p>
    <p><b>Summary:</b> In an MPC-protected distributed computation, although the use of MPC assures
data privacy during computation, sensitive information may still be inferred by
curious MPC participants from the computation output. This can be observed, for
instance, in the inference attacks on either federated learning or a more
standard statistical computation with distributed inputs. In this work, we
address this output privacy issue by proposing a discrete and bounded
Laplace-inspired perturbation mechanism along with a secure realization of this
mechanism using MPC. The proposed mechanism strictly adheres to a zero failure
probability, overcoming the limitation encountered on other existing bounded
and discrete variants of Laplace perturbation. We provide analyses of the
proposed differential privacy (DP) perturbation in terms of its privacy and
utility. Additionally, we designed MPC protocols to implement this mechanism
and presented performance benchmarks based on our experimental setup. The MPC
realization of the proposed mechanism exhibits a complexity similar to the
state-of-the-art discrete Gaussian mechanism, which can be considered an
alternative with comparable efficiency while providing stronger differential
privacy guarantee. Moreover, efficiency of the proposed scheme can be further
enhanced by performing the noise generation offline while leaving the
perturbation phase online.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.06808v1">Privacy Auditing of Large Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-03-09T23:32:15Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ashwinee Panda, Xinyu Tang, Milad Nasr, Christopher A. Choquette-Choo, Prateek Mittal</p>
    <p><b>Summary:</b> Current techniques for privacy auditing of large language models (LLMs) have
limited efficacy -- they rely on basic approaches to generate canaries which
leads to weak membership inference attacks that in turn give loose lower bounds
on the empirical privacy leakage. We develop canaries that are far more
effective than those used in prior work under threat models that cover a range
of realistic settings. We demonstrate through extensive experiments on multiple
families of fine-tuned LLMs that our approach sets a new standard for detection
of privacy leakage. For measuring the memorization rate of non-privately
trained LLMs, our designed canaries surpass prior approaches. For example, on
the Qwen2.5-0.5B model, our designed canaries achieve $49.6\%$ TPR at $1\%$
FPR, vastly surpassing the prior approach's $4.2\%$ TPR at $1\%$ FPR. Our
method can be used to provide a privacy audit of $\varepsilon \approx 1$ for a
model trained with theoretical $\varepsilon$ of 4. To the best of our
knowledge, this is the first time that a privacy audit of LLM training has
achieved nontrivial auditing success in the setting where the attacker cannot
train shadow models, insert gradient canaries, or access the model at every
iteration.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.06732v1">Data Efficient Subset Training with Differential Privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-03-09T19:05:10Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ninad Jayesh Gandhi, Moparthy Venkata Subrahmanya Sri Harsha</p>
    <p><b>Summary:</b> Private machine learning introduces a trade-off between the privacy budget
and training performance. Training convergence is substantially slower and
extensive hyper parameter tuning is required. Consequently, efficient methods
to conduct private training of models is thoroughly investigated in the
literature. To this end, we investigate the strength of the data efficient
model training methods in the private training setting. We adapt GLISTER
(Killamsetty et al., 2021b) to the private setting and extensively assess its
performance. We empirically find that practical choices of privacy budgets are
too restrictive for data efficient training in the private setting.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.06455v1">Privacy Protection in Prosumer Energy Management Based on Federated
  Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-03-09T05:29:29Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Yunfeng Li, Xiaolin Li Zhitao Li, Gangqiang Li</p>
    <p><b>Summary:</b> With the booming development of prosumers, there is an urgent need for a
prosumer energy management system to take full advantage of the flexibility of
prosumers and take into account the interests of other parties. However,
building such a system will undoubtedly reveal users' privacy. In this paper,
by solving the non-independent and identical distribution of data (Non-IID)
problem in federated learning with federated cluster average(FedClusAvg)
algorithm, prosumers' information can efficiently participate in the
intelligent decision making of the system without revealing privacy. In the
proposed FedClusAvg algorithm, each client performs cluster stratified sampling
and multiple iterations. Then, the average weight of the parameters of the
sub-server is determined according to the degree of deviation of the parameter
from the average parameter. Finally, the sub-server multiple local iterations
and updates, and then upload to the main server. The advantages of FedClusAvg
algorithm are the following two parts. First, the accuracy of the model in the
case of Non-IID is improved through the method of clustering and parameter
weighted average. Second, local multiple iterations and three-tier framework
can effectively reduce communication rounds.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.06150v2">Do Fairness Interventions Come at the Cost of Privacy: Evaluations for
  Binary Classifiers</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-03-08T10:21:21Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Huan Tian, Guangsheng Zhang, Bo Liu, Tianqing Zhu, Ming Ding, Wanlei Zhou</p>
    <p><b>Summary:</b> While in-processing fairness approaches show promise in mitigating biased
predictions, their potential impact on privacy leakage remains under-explored.
We aim to address this gap by assessing the privacy risks of fairness-enhanced
binary classifiers via membership inference attacks (MIAs) and attribute
inference attacks (AIAs). Surprisingly, our results reveal that enhancing
fairness does not necessarily lead to privacy compromises. For example, these
fairness interventions exhibit increased resilience against MIAs and AIAs. This
is because fairness interventions tend to remove sensitive information among
extracted features and reduce confidence scores for the majority of training
data for fairer predictions. However, during the evaluations, we uncover a
potential threat mechanism that exploits prediction discrepancies between fair
and biased models, leading to advanced attack results for both MIAs and AIAs.
This mechanism reveals potent vulnerabilities of fair models and poses
significant privacy risks of current fairness methods. Extensive experiments
across multiple datasets, attack methods, and representative fairness
approaches confirm our findings and demonstrate the efficacy of the uncovered
mechanism. Our study exposes the under-explored privacy threats in fairness
studies, advocating for thorough evaluations of potential security
vulnerabilities before model deployments.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.06021v1">FedEM: A Privacy-Preserving Framework for Concurrent Utility
  Preservation in Federated Learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-03-08T02:48:00Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mingcong Xu, Xiaojin Zhang, Wei Chen, Hai Jin</p>
    <p><b>Summary:</b> Federated Learning (FL) enables collaborative training of models across
distributed clients without sharing local data, addressing privacy concerns in
decentralized systems. However, the gradient-sharing process exposes private
data to potential leakage, compromising FL's privacy guarantees in real-world
applications. To address this issue, we propose Federated Error Minimization
(FedEM), a novel algorithm that incorporates controlled perturbations through
adaptive noise injection. This mechanism effectively mitigates gradient leakage
attacks while maintaining model performance. Experimental results on benchmark
datasets demonstrate that FedEM significantly reduces privacy risks and
preserves model accuracy, achieving a robust balance between privacy protection
and utility preservation.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.05954v1">A Survey on Tabular Data Generation: Utility, Alignment, Fidelity,
  Privacy, and Beyond</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-03-07T21:47:11Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mihaela Cătălina Stoian, Eleonora Giunchiglia, Thomas Lukasiewicz</p>
    <p><b>Summary:</b> Generative modelling has become the standard approach for synthesising
tabular data. However, different use cases demand synthetic data to comply with
different requirements to be useful in practice. In this survey, we review deep
generative modelling approaches for tabular data from the perspective of four
types of requirements: utility of the synthetic data, alignment of the
synthetic data with domain-specific knowledge, statistical fidelity of the
synthetic data distribution compared to the real data distribution, and
privacy-preserving capabilities. We group the approaches along two levels of
granularity: (i) based on the primary type of requirements they address and
(ii) according to the underlying model they utilise. Additionally, we summarise
the appropriate evaluation methods for each requirement and the specific
characteristics of each model type. Finally, we discuss future directions for
the field, along with opportunities to improve the current evaluation methods.
Overall, this survey can be seen as a user guide to tabular data generation:
helping readers navigate available models and evaluation methods to find those
best suited to their needs.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.05684v1">Fairness-Aware Low-Rank Adaptation Under Demographic Privacy Constraints</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-03-07T18:49:57Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Parameswaran Kamalaruban, Mark Anderson, Stuart Burrell, Maeve Madigan, Piotr Skalski, David Sutton</p>
    <p><b>Summary:</b> Pre-trained foundation models can be adapted for specific tasks using
Low-Rank Adaptation (LoRA). However, the fairness properties of these adapted
classifiers remain underexplored. Existing fairness-aware fine-tuning methods
rely on direct access to sensitive attributes or their predictors, but in
practice, these sensitive attributes are often held under strict consumer
privacy controls, and neither the attributes nor their predictors are available
to model developers, hampering the development of fair models. To address this
issue, we introduce a set of LoRA-based fine-tuning methods that can be trained
in a distributed fashion, where model developers and fairness auditors
collaborate without sharing sensitive attributes or predictors. In this paper,
we evaluate three such methods - sensitive unlearning, adversarial training,
and orthogonality loss - against a fairness-unaware baseline, using experiments
on the CelebA and UTK-Face datasets with an ImageNet pre-trained ViT-Base
model. We find that orthogonality loss consistently reduces bias while
maintaining or improving utility, whereas adversarial training improves False
Positive Rate Parity and Demographic Parity in some cases, and sensitive
unlearning provides no clear benefit. In tasks where significant biases are
present, distributed fairness-aware fine-tuning methods can effectively
eliminate bias without compromising consumer privacy and, in most cases,
improve model utility.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.04980v1">A Consensus Privacy Metrics Framework for Synthetic Data</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-03-06T21:19:02Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Lisa Pilgram, Fida K. Dankar, Jorg Drechsler, Mark Elliot, Josep Domingo-Ferrer, Paul Francis, Murat Kantarcioglu, Linglong Kong, Bradley Malin, Krishnamurty Muralidhar, Puja Myles, Fabian Prasser, Jean Louis Raisaro, Chao Yan, Khaled El Emam</p>
    <p><b>Summary:</b> Synthetic data generation is one approach for sharing individual-level data.
However, to meet legislative requirements, it is necessary to demonstrate that
the individuals' privacy is adequately protected. There is no consolidated
standard for measuring privacy in synthetic data. Through an expert panel and
consensus process, we developed a framework for evaluating privacy in synthetic
data. Our findings indicate that current similarity metrics fail to measure
identity disclosure, and their use is discouraged. For differentially private
synthetic data, a privacy budget other than close to zero was not considered
interpretable. There was consensus on the importance of membership and
attribute disclosure, both of which involve inferring personal information
about an individual without necessarily revealing their identity. The resultant
framework provides precise recommendations for metrics that address these types
of disclosures effectively. Our findings further present specific opportunities
for future research that can help with widespread adoption of synthetic data.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.04707v1">Iris Style Transfer: Enhancing Iris Recognition with Style Features and
  Privacy Preservation through Neural Style Transfer</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-03-06T18:55:21Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Mengdi Wang, Efe Bozkir, Enkelejda Kasneci</p>
    <p><b>Summary:</b> Iris texture is widely regarded as a gold standard biometric modality for
authentication and identification. The demand for robust iris recognition
methods, coupled with growing security and privacy concerns regarding iris
attacks, has escalated recently. Inspired by neural style transfer, an advanced
technique that leverages neural networks to separate content and style
features, we hypothesize that iris texture's style features provide a reliable
foundation for recognition and are more resilient to variations like rotation
and perspective shifts than traditional approaches. Our experimental results
support this hypothesis, showing a significantly higher classification accuracy
compared to conventional features. Further, we propose using neural style
transfer to mask identifiable iris style features, ensuring the protection of
sensitive biometric information while maintaining the utility of eye images for
tasks like eye segmentation and gaze estimation. This work opens new avenues
for iris-oriented, secure, and privacy-aware biometric systems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.04652v1">Evaluation of Privacy-aware Support Vector Machine (SVM) Learning using
  Homomorphic Encryption</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-06T17:42:23Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> William J Buchanan, Hisham Ali</p>
    <p><b>Summary:</b> The requirement for privacy-aware machine learning increases as we continue
to use PII (Personally Identifiable Information) within machine training. To
overcome these privacy issues, we can apply Fully Homomorphic Encryption (FHE)
to encrypt data before it is fed into a machine learning model. This involves
creating a homomorphic encryption key pair, and where the associated public key
will be used to encrypt the input data, and the private key will decrypt the
output. But, there is often a performance hit when we use homomorphic
encryption, and so this paper evaluates the performance overhead of using the
SVM machine learning technique with the OpenFHE homomorphic encryption library.
This uses Python and the scikit-learn library for its implementation. The
experiments include a range of variables such as multiplication depth, scale
size, first modulus size, security level, batch size, and ring dimension, along
with two different SVM models, SVM-Poly and SVM-Linear. Overall, the results
show that the two main parameters which affect performance are the ring
dimension and the modulus size, and that SVM-Poly and SVM-Linear show similar
performance levels.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.04451v1">Privacy Preserving and Robust Aggregation for Cross-Silo Federated
  Learning in Non-IID Settings</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-06T14:06:20Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Marco Arazzi, Mert Cihangiroglu, Antonino Nocera</p>
    <p><b>Summary:</b> Federated Averaging remains the most widely used aggregation strategy in
federated learning due to its simplicity and scalability. However, its
performance degrades significantly in non-IID data settings, where client
distributions are highly imbalanced or skewed. Additionally, it relies on
clients transmitting metadata, specifically the number of training samples,
which introduces privacy risks and may conflict with regulatory frameworks like
the European GDPR. In this paper, we propose a novel aggregation strategy that
addresses these challenges by introducing class-aware gradient masking. Unlike
traditional approaches, our method relies solely on gradient updates,
eliminating the need for any additional client metadata, thereby enhancing
privacy protection. Furthermore, our approach validates and dynamically weights
client contributions based on class-specific importance, ensuring robustness
against non-IID distributions, convergence prevention, and backdoor attacks.
Extensive experiments on benchmark datasets demonstrate that our method not
only outperforms FedAvg and other widely accepted aggregation strategies in
non-IID settings but also preserves model integrity in adversarial scenarios.
Our results establish the effectiveness of gradient masking as a practical and
secure solution for federated learning.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.04866v1">Privacy in Responsible AI: Approaches to Facial Recognition from Cloud
  Providers</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-03-06T12:04:12Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Anna Elivanova</p>
    <p><b>Summary:</b> As the use of facial recognition technology is expanding in different
domains, ensuring its responsible use is gaining more importance. This paper
conducts a comprehensive literature review of existing studies on facial
recognition technology from the perspective of privacy, which is one of the key
Responsible AI principles.
  Cloud providers, such as Microsoft, AWS, and Google, are at the forefront of
delivering facial-related technology services, but their approaches to
responsible use of these technologies vary significantly. This paper compares
how these cloud giants implement the privacy principle into their facial
recognition and detection services. By analysing their approaches, it
identifies both common practices and notable differences. The results of this
research will be valuable for developers and businesses by providing them
insights into best practices of three major companies for integration
responsible AI, particularly privacy, into their cloud-based facial recognition
technologies.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.04054v1">Controlled privacy leakage propagation throughout overlapping grouped
  learning</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2025-03-06T03:14:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shahrzad Kiani, Franziska Boenisch, Stark C. Draper</p>
    <p><b>Summary:</b> Federated Learning (FL) is the standard protocol for collaborative learning.
In FL, multiple workers jointly train a shared model. They exchange model
updates calculated on their data, while keeping the raw data itself local.
Since workers naturally form groups based on common interests and privacy
policies, we are motivated to extend standard FL to reflect a setting with
multiple, potentially overlapping groups. In this setup where workers can
belong and contribute to more than one group at a time, complexities arise in
understanding privacy leakage and in adhering to privacy policies. To address
the challenges, we propose differential private overlapping grouped learning
(DPOGL), a novel method to implement privacy guarantees within overlapping
groups. Under the honest-but-curious threat model, we derive novel privacy
guarantees between arbitrary pairs of workers. These privacy guarantees
describe and quantify two key effects of privacy leakage in DP-OGL: propagation
delay, i.e., the fact that information from one group will leak to other groups
only with temporal offset through the common workers and information
degradation, i.e., the fact that noise addition over model updates limits
information leakage between workers. Our experiments show that applying DP-OGL
enhances utility while maintaining strong privacy compared to standard FL
setups.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.07483v1">Poisoning Attacks to Local Differential Privacy Protocols for Trajectory
  Data</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-03-06T02:31:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> I-Jung Hsu, Chih-Hsun Lin, Chia-Mu Yu, Sy-Yen Kuo, Chun-Ying Huang</p>
    <p><b>Summary:</b> Trajectory data, which tracks movements through geographic locations, is
crucial for improving real-world applications. However, collecting such
sensitive data raises considerable privacy concerns. Local differential privacy
(LDP) offers a solution by allowing individuals to locally perturb their
trajectory data before sharing it. Despite its privacy benefits, LDP protocols
are vulnerable to data poisoning attacks, where attackers inject fake data to
manipulate aggregated results. In this work, we make the first attempt to
analyze vulnerabilities in several representative LDP trajectory protocols. We
propose \textsc{TraP}, a heuristic algorithm for data \underline{P}oisoning
attacks using a prefix-suffix method to optimize fake \underline{Tra}jectory
selection, significantly reducing computational complexity. Our experimental
results demonstrate that our attack can substantially increase target pattern
occurrences in the perturbed trajectory dataset with few fake users. This study
underscores the urgent need for robust defenses and better protocol designs to
safeguard LDP trajectory data against malicious manipulation.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.03988v1">AI-based Programming Assistants for Privacy-related Code Generation: The
  Developers' Experience</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Software Engineering-D91E36">
  <p><b>Published on:</b> 2025-03-06T00:34:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Kashumi Madampe, John Grundy, Nalin Arachchilage</p>
    <p><b>Summary:</b> With the popularising of generative AI, the existence of AI-based programming
assistants for developers is no surprise. Developers increasingly use them for
their work, including generating code to fulfil the data protection
requirements (privacy) of the apps they build. We wanted to know if the reality
is the same as expectations of AI-based programming assistants when trying to
fulfil software privacy requirements, and the challenges developers face when
using AI-based programming assistants and how these can be improved. To this
end, we conducted a survey with 51 developers worldwide. We found that AI-based
programming assistants need to be improved in order for developers to better
trust them with generating code that ensures privacy. In this paper, we provide
some practical recommendations for developers to consider following when using
AI-based programming assistants for privacy-related code development, and some
key further research directions.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.03652v1">Token-Level Privacy in Large Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-05T16:27:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Re'em Harel, Niv Gilboa, Yuval Pinter</p>
    <p><b>Summary:</b> The use of language models as remote services requires transmitting private
information to external providers, raising significant privacy concerns. This
process not only risks exposing sensitive data to untrusted service providers
but also leaves it vulnerable to interception by eavesdroppers. Existing
privacy-preserving methods for natural language processing (NLP) interactions
primarily rely on semantic similarity, overlooking the role of contextual
information. In this work, we introduce dchi-stencil, a novel token-level
privacy-preserving mechanism that integrates contextual and semantic
information while ensuring strong privacy guarantees under the dchi
differential privacy framework, achieving 2epsilon-dchi-privacy. By
incorporating both semantic and contextual nuances, dchi-stencil achieves a
robust balance between privacy and utility. We evaluate dchi-stencil using
state-of-the-art language models and diverse datasets, achieving comparable and
even better trade-off between utility and privacy compared to existing methods.
This work highlights the potential of dchi-stencil to set a new standard for
privacy-preserving NLP in modern, high-risk applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.03587v1">"You don't need a university degree to comprehend data protection this
  way": LLM-Powered Interactive Privacy Policy Assessment</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-03-05T15:22:35Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Vincent Freiberger, Arthur Fleig, Erik Buchmann</p>
    <p><b>Summary:</b> Protecting online privacy requires users to engage with and comprehend
website privacy policies, but many policies are difficult and tedious to read.
We present the first qualitative user study on Large Language Model
(LLM)-driven privacy policy assessment. To this end, we build and evaluate an
LLM-based privacy policy assessment browser extension, which helps users
understand the essence of a lengthy, complex privacy policy while browsing. The
tool integrates a dashboard and an LLM chat. In our qualitative user study
(N=22), we evaluate usability, understandability of the information our tool
provides, and its impacts on awareness. While providing a comprehensible quick
overview and a chat for in-depth discussion improves privacy awareness, users
note issues with building trust in the tool. From our insights, we derive
important design implications to guide future policy analysis tools.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.03539v1">Data Sharing, Privacy and Security Considerations in the Energy Sector:
  A Review from Technical Landscape to Regulatory Specifications</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-05T14:23:56Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Shiliang Zhang, Sabita Maharjan, Lee Andrew Bygrave, Shui Yu</p>
    <p><b>Summary:</b> Decarbonization, decentralization and digitalization are the three key
elements driving the twin energy transition. The energy system is evolving to a
more data driven ecosystem, leading to the need of communication and storage of
large amount of data of different resolution from the prosumers and other
stakeholders in the energy ecosystem. While the energy system is certainly
advancing, this paradigm shift is bringing in new privacy and security issues
related to collection, processing and storage of data - not only from the
technical dimension, but also from the regulatory perspective. Understanding
data privacy and security in the evolving energy system, regarding regulatory
compliance, is an immature field of research. Contextualized knowledge of how
related issues are regulated is still in its infancy, and the practical and
technical basis for the regulatory framework for data privacy and security is
not clear. To fill this gap, this paper conducts a comprehensive review of the
data-related issues for the energy system by integrating both technical and
regulatory dimensions. We start by reviewing open-access data, data
communication and data-processing techniques for the energy system, and use it
as the basis to connect the analysis of data-related issues from the integrated
perspective. We classify the issues into three categories: (i) data-sharing
among energy end users and stakeholders (ii) privacy of end users, and (iii)
cyber security, and then explore these issues from a regulatory perspective. We
analyze the evolution of related regulations, and introduce the relevant
regulatory initiatives for the categorized issues in terms of regulatory
definitions, concepts, principles, rights and obligations in the context of
energy systems. Finally, we provide reflections on the gaps that still exist,
and guidelines for regulatory frameworks for a truly participatory energy
system.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.03506v1">Rethinking Synthetic Data definitions: A privacy driven approach</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-03-05T13:54:13Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Vibeke Binz Vallevik, Serena Elizabeth Marshall, Aleksandar Babic, Jan Franz Nygaard</p>
    <p><b>Summary:</b> Synthetic data is gaining traction as a cost-effective solution for the
increasing data demands of AI development and can be generated either from
existing knowledge or derived data captured from real-world events. The source
of the synthetic data generation and the technique used significantly impacts
its residual privacy risk and therefore its opportunity for sharing.
Traditional classification of synthetic data types no longer fit the newer
generation techniques and there is a need to better align the classification
with practical needs. We suggest a new way of grouping synthetic data types
that better supports privacy evaluations to aid regulatory policymaking. Our
novel classification provides flexibility to new advancements like deep
generative methods and offers a more practical framework for future
applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.03428v1">Privacy is All You Need: Revolutionizing Wearable Health Data with
  Advanced PETs</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Emerging Technologies-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-03-05T12:01:22Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Karthik Barma, Seshu Babu Barma</p>
    <p><b>Summary:</b> In a world where data is the new currency, wearable health devices offer
unprecedented insights into daily life, continuously monitoring vital signs and
metrics. However, this convenience raises privacy concerns, as these devices
collect sensitive data that can be misused or breached. Traditional measures
often fail due to real-time data processing needs and limited device power.
Users also lack awareness and control over data sharing and usage. We propose a
Privacy-Enhancing Technology (PET) framework for wearable devices, integrating
federated learning, lightweight cryptographic methods, and selectively deployed
blockchain technology. The blockchain acts as a secure ledger triggered only
upon data transfer requests, granting users real-time notifications and
control. By dismantling data monopolies, this approach returns data sovereignty
to individuals. Through real-world applications like secure medical data
sharing, privacy-preserving fitness tracking, and continuous health monitoring,
our framework reduces privacy risks by up to 70 percent while preserving data
utility and performance. This innovation sets a new benchmark for wearable
privacy and can scale to broader IoT ecosystems, including smart homes and
industry. As data continues to shape our digital landscape, our research
underscores the critical need to maintain privacy and user control at the
forefront of technological progress.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.03267v1">Quantum-Inspired Privacy-Preserving Federated Learning Framework for
  Secure Dementia Classification</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Distributed, Parallel, and Cluster Computing-5BC0EB">
  <p><b>Published on:</b> 2025-03-05T08:49:31Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Gazi Tanbhir, Md. Farhan Shahriyar</p>
    <p><b>Summary:</b> Dementia, a neurological disorder impacting millions globally, presents
significant challenges in diagnosis and patient care. With the rise of privacy
concerns and security threats in healthcare, federated learning (FL) has
emerged as a promising approach to enable collaborative model training across
decentralized datasets without exposing sensitive patient information. However,
FL remains vulnerable to advanced security breaches such as gradient inversion
and eavesdropping attacks. This paper introduces a novel framework that
integrates federated learning with quantum-inspired encryption techniques for
dementia classification, emphasizing privacy preservation and security.
Leveraging quantum key distribution (QKD), the framework ensures secure
transmission of model weights, protecting against unauthorized access and
interception during training. The methodology utilizes a convolutional neural
network (CNN) for dementia classification, with federated training conducted
across distributed healthcare nodes, incorporating QKD-encrypted weight sharing
to secure the aggregation process. Experimental evaluations conducted on MRI
data from the OASIS dataset demonstrate that the proposed framework achieves
identical accuracy levels to a baseline model while enhancing data security and
reducing loss by almost 1% compared to the classical baseline model. The
framework offers significant implications for democratizing access to AI-driven
dementia diagnostics in low- and middle-income countries, addressing critical
resource and privacy constraints. This work contributes a robust, scalable, and
secure federated learning solution for healthcare applications, paving the way
for broader adoption of quantum-inspired techniques in AI-driven medical
research.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.03146v1">PriFFT: Privacy-preserving Federated Fine-tuning of Large Language
  Models via Function Secret Sharing</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-05T03:41:57Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhichao You, Xuewen Dong, Ke Cheng, Xutong Mu, Jiaxuan Fu, Shiyang Ma, Qiang Qu, Yulong Shen</p>
    <p><b>Summary:</b> Fine-tuning large language models (LLMs) raises privacy concerns due to the
risk of exposing sensitive training data. Federated learning (FL) mitigates
this risk by keeping training samples on local devices, but recent studies show
that adversaries can still infer private information from model updates in FL.
Additionally, LLM parameters are typically shared publicly during federated
fine-tuning, while developers are often reluctant to disclose these parameters,
posing further security challenges. Inspired by the above problems, we propose
PriFFT, a privacy-preserving federated fine-tuning mechanism, to protect both
the model updates and parameters. In PriFFT, clients and the server share model
inputs and parameters by secret sharing, performing secure fine-tuning on
shared values without accessing plaintext data. Due to considerable LLM
parameters, privacy-preserving federated fine-tuning invokes complex secure
calculations and requires substantial communication and computation resources.
To optimize the efficiency of privacy-preserving federated fine-tuning of LLMs,
we introduce function secret-sharing protocols for various operations,
including reciprocal calculation, tensor products, natural exponentiation,
softmax, hyperbolic tangent, and dropout. The proposed protocols achieve up to
4.02X speed improvement and reduce 7.19X communication overhead compared to the
implementation based on existing secret sharing methods. Besides, PriFFT
achieves a 2.23X speed improvement and reduces 4.08X communication overhead in
privacy-preserving fine-tuning without accuracy drop compared to the existing
secret sharing methods.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.03087v1">"Watch My Health, Not My Data": Understanding Perceptions, Barriers,
  Emotional Impact, & Coping Strategies Pertaining to IoT Privacy and Security
  in Health Monitoring for Older Adults</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/HumanComputer Interaction-D91E36">
  <p><b>Published on:</b> 2025-03-05T01:04:13Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Suleiman Saka, Sanchari Das</p>
    <p><b>Summary:</b> The proliferation of "Internet of Things (IoT)" provides older adults with
critical support for "health monitoring" and independent living, yet
significant concerns about security and privacy persist. In this paper, we
report on these issues through a two-phase user study, including a survey (N =
22) and semi-structured interviews (n = 9) with adults aged 65+. We found that
while 81.82% of our participants are aware of security features like
"two-factor authentication (2FA)" and encryption, 63.64% express serious
concerns about unauthorized access to sensitive health data. Only 13.64% feel
confident in existing protections, citing confusion over "data sharing
policies" and frustration with "complex security settings" which lead to
distrust and anxiety. To cope, our participants adopt various strategies, such
as relying on family or professional support and limiting feature usage leading
to disengagement. Thus, we recommend "adaptive security mechanisms," simplified
interfaces, and real-time transparency notifications to foster trust and ensure
"privacy and security by design" in IoT health systems for older adults.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.03043v1">Leveraging Randomness in Model and Data Partitioning for Privacy
  Amplification</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-04T22:49:59Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Andy Dong, Wei-Ning Chen, Ayfer Ozgur</p>
    <p><b>Summary:</b> We study how inherent randomness in the training process -- where each sample
(or client in federated learning) contributes only to a randomly selected
portion of training -- can be leveraged for privacy amplification. This
includes (1) data partitioning, where a sample participates in only a subset of
training iterations, and (2) model partitioning, where a sample updates only a
subset of the model parameters. We apply our framework to model parallelism in
federated learning, where each client updates a randomly selected subnetwork to
reduce memory and computational overhead, and show that existing methods, e.g.
model splitting or dropout, provide a significant privacy amplification gain
not captured by previous privacy analysis techniques. Additionally, we
introduce Balanced Iteration Subsampling, a new data partitioning method where
each sample (or client) participates in a fixed number of training iterations.
We show that this method yields stronger privacy amplification than Poisson
(i.i.d.) sampling of data (or clients). Our results demonstrate that randomness
in the training process, which is structured rather than i.i.d. and interacts
with data in complex ways, can be systematically leveraged for significant
privacy amplification.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.02968v1">Privacy-Preserving Fair Synthetic Tabular Data</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-04T19:51:00Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Fatima J. Sarmin, Atiquer R. Rahman, Christopher J. Henry, Noman Mohammed</p>
    <p><b>Summary:</b> Sharing of tabular data containing valuable but private information is
limited due to legal and ethical issues. Synthetic data could be an alternative
solution to this sharing problem, as it is artificially generated by machine
learning algorithms and tries to capture the underlying data distribution.
However, machine learning models are not free from memorization and may
introduce biases, as they rely on training data. Producing synthetic data that
preserves privacy and fairness while maintaining utility close to the real data
is a challenging task. This research simultaneously addresses both the privacy
and fairness aspects of synthetic data, an area not explored by other studies.
In this work, we present PF-WGAN, a privacy-preserving, fair synthetic tabular
data generator based on the WGAN-GP model. We have modified the original
WGAN-GP by adding privacy and fairness constraints forcing it to produce
privacy-preserving fair data. This approach will enable the publication of
datasets that protect individual's privacy and remain unbiased toward any
particular group. We compared the results with three state-of-the-art synthetic
data generator models in terms of utility, privacy, and fairness across four
different datasets. We found that the proposed model exhibits a more balanced
trade-off among utility, privacy, and fairness.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.02862v1">Privacy and Accuracy-Aware AI/ML Model Deduplication</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Databases-5BC0EB">
  <p><b>Published on:</b> 2025-03-04T18:40:38Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Hong Guan, Lei Yu, Lixi Zhou, Li Xiong, Kanchan Chowdhury, Lulu Xie, Xusheng Xiao, Jia Zou</p>
    <p><b>Summary:</b> With the growing adoption of privacy-preserving machine learning algorithms,
such as Differentially Private Stochastic Gradient Descent (DP-SGD), training
or fine-tuning models on private datasets has become increasingly prevalent.
This shift has led to the need for models offering varying privacy guarantees
and utility levels to satisfy diverse user requirements. However, managing
numerous versions of large models introduces significant operational
challenges, including increased inference latency, higher resource consumption,
and elevated costs. Model deduplication is a technique widely used by many
model serving and database systems to support high-performance and low-cost
inference queries and model diagnosis queries. However, none of the existing
model deduplication works has considered privacy, leading to unbounded
aggregation of privacy costs for certain deduplicated models and inefficiencies
when applied to deduplicate DP-trained models. We formalize the problems of
deduplicating DP-trained models for the first time and propose a novel privacy-
and accuracy-aware deduplication mechanism to address the problems. We
developed a greedy strategy to select and assign base models to target models
to minimize storage and privacy costs. When deduplicating a target model, we
dynamically schedule accuracy validations and apply the Sparse Vector Technique
to reduce the privacy costs associated with private validation data. Compared
to baselines that do not provide privacy guarantees, our approach improved the
compression ratio by up to $35\times$ for individual models (including large
language models and vision transformers). We also observed up to $43\times$
inference speedup due to the reduction of I/O operations.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.02693v1">Federated Learning for Privacy-Preserving Feedforward Control in
  Multi-Agent Systems</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Multiagent Systems-662E9B">
  <p><b>Published on:</b> 2025-03-04T15:07:25Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Jakob Weber, Markus Gurtner, Benedikt Alt, Adrian Trachte, Andreas Kugi</p>
    <p><b>Summary:</b> Feedforward control (FF) is often combined with feedback control (FB) in many
control systems, improving tracking performance, efficiency, and stability.
However, designing effective data-driven FF controllers in multi-agent systems
requires significant data collection, including transferring private or
proprietary data, which raises privacy concerns and incurs high communication
costs. Therefore, we propose a novel approach integrating Federated Learning
(FL) into FF control to address these challenges. This approach enables
privacy-preserving, communication-efficient, and decentralized continuous
improvement of FF controllers across multiple agents without sharing personal
or proprietary data. By leveraging FL, each agent learns a local, neural FF
controller using its data and contributes only model updates to a global
aggregation process, ensuring data privacy and scalability. We demonstrate the
effectiveness of our method in an autonomous driving use case. Therein,
vehicles equipped with a trajectory-tracking feedback controller are enhanced
by FL-based neural FF control. Simulations highlight significant improvements
in tracking performance compared to pure FB control, analogous to model-based
FF control. We achieve comparable tracking performance without exchanging
private vehicle-specific data compared to a centralized neural FF control. Our
results underscore the potential of FL-based neural FF control to enable
privacy-preserving learning in multi-agent control systems, paving the way for
scalable and efficient autonomous systems applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.02549v1">Federated nnU-Net for Privacy-Preserving Medical Image Segmentation</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B">
  <p><b>Published on:</b> 2025-03-04T12:20:06Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Grzegorz Skorupko, Fotios Avgoustidis, Carlos Martín-Isla, Lidia Garrucho, Dimitri A. Kessler, Esmeralda Ruiz Pujadas, Oliver Díaz, Maciej Bobowicz, Katarzyna Gwoździewicz, Xavier Bargalló, Paulius Jaruševičius, Kaisar Kushibar, Karim Lekadir</p>
    <p><b>Summary:</b> The nnU-Net framework has played a crucial role in medical image segmentation
and has become the gold standard in multitudes of applications targeting
different diseases, organs, and modalities. However, so far it has been used
primarily in a centralized approach where the data collected from hospitals are
stored in one center and used to train the nnU-Net. This centralized approach
has various limitations, such as leakage of sensitive patient information and
violation of patient privacy. Federated learning is one of the approaches to
train a segmentation model in a decentralized manner that helps preserve
patient privacy. In this paper, we propose FednnU-Net, a federated learning
extension of nnU-Net. We introduce two novel federated learning methods to the
nnU-Net framework - Federated Fingerprint Extraction (FFE) and Asymmetric
Federated Averaging (AsymFedAvg) - and experimentally show their consistent
performance for breast, cardiac and fetal segmentation using 6 datasets
representing samples from 18 institutions. Additionally, to further promote
research and deployment of decentralized training in privacy constrained
institutions, we make our plug-n-play framework public. The source-code is
available at https://github.com/faildeny/FednnUNet .</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.02455v1">Privacy Preservation Techniques (PPTs) in IoT Systems: A Scoping Review
  and Future Directions</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-04T10:03:45Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Emmanuel Alalade, Ashraf Matrawy</p>
    <p><b>Summary:</b> Privacy preservation in Internet of Things (IoT) systems requires the use of
privacy-enhancing technologies (PETs) built from innovative technologies such
as cryptography and artificial intelligence (AI) to create techniques called
privacy preservation techniques (PPTs). These PPTs achieve various privacy
goals and address different privacy concerns by mitigating potential privacy
threats within IoT systems. This study carried out a scoping review of
different types of PPTs used in previous research works on IoT systems between
2010 and early 2023 to further explore the advantages of privacy preservation
in these systems. This scoping review looks at privacy goals, possible
technologies used for building PET, the integration of PPTs into the computing
layer of the IoT architecture, different IoT applications in which PPTs are
deployed, and the different privacy types addressed by these techniques within
IoT systems. Key findings, such as the prominent privacy goal and privacy type
in IoT, are discussed in this survey, along with identified research gaps that
could inform future endeavors in privacy research and benefit the privacy
research community and other stakeholders in IoT systems.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.02132v1">Video-DPRP: A Differentially Private Approach for Visual
  Privacy-Preserving Video Human Activity Recognition</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-03-03T23:43:12Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Allassan Tchangmena A Nken, Susan Mckeever, Peter Corcoran, Ihsan Ullah</p>
    <p><b>Summary:</b> Considerable effort has been made in privacy-preserving video human activity
recognition (HAR). Two primary approaches to ensure privacy preservation in
Video HAR are differential privacy (DP) and visual privacy. Techniques
enforcing DP during training provide strong theoretical privacy guarantees but
offer limited capabilities for visual privacy assessment. Conversely methods,
such as low-resolution transformations, data obfuscation and adversarial
networks, emphasize visual privacy but lack clear theoretical privacy
assurances. In this work, we focus on two main objectives: (1) leveraging DP
properties to develop a model-free approach for visual privacy in videos and
(2) evaluating our proposed technique using both differential privacy and
visual privacy assessments on HAR tasks. To achieve goal (1), we introduce
Video-DPRP: a Video-sample-wise Differentially Private Random Projection
framework for privacy-preserved video reconstruction for HAR. By using random
projections, noise matrices and right singular vectors derived from the
singular value decomposition of videos, Video-DPRP reconstructs DP videos using
privacy parameters ($\epsilon,\delta$) while enabling visual privacy
assessment. For goal (2), using UCF101 and HMDB51 datasets, we compare
Video-DPRP's performance on activity recognition with traditional DP methods,
and state-of-the-art (SOTA) visual privacy-preserving techniques. Additionally,
we assess its effectiveness in preserving privacy-related attributes such as
facial features, gender, and skin color, using the PA-HMDB and VISPR datasets.
Video-DPRP combines privacy-preservation from both a DP and visual privacy
perspective unlike SOTA methods that typically address only one of these
aspects.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.02114v1">Fairness and/or Privacy on Social Graphs</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Social and Information Networks-662E9B">
  <p><b>Published on:</b> 2025-03-03T22:56:32Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Bartlomiej Surma, Michael Backes, Yang Zhang</p>
    <p><b>Summary:</b> Graph Neural Networks (GNNs) have shown remarkable success in various
graph-based learning tasks. However, recent studies have raised concerns about
fairness and privacy issues in GNNs, highlighting the potential for biased or
discriminatory outcomes and the vulnerability of sensitive information. This
paper presents a comprehensive investigation of fairness and privacy in GNNs,
exploring the impact of various fairness-preserving measures on model
performance. We conduct experiments across diverse datasets and evaluate the
effectiveness of different fairness interventions. Our analysis considers the
trade-offs between fairness, privacy, and accuracy, providing insights into the
challenges and opportunities in achieving both fair and private graph learning.
The results highlight the importance of carefully selecting and combining
fairness-preserving measures based on the specific characteristics of the data
and the desired fairness objectives. This study contributes to a deeper
understanding of the complex interplay between fairness, privacy, and accuracy
in GNNs, paving the way for the development of more robust and ethical graph
learning models.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.02091v1">Which Code Statements Implement Privacy Behaviors in Android
  Applications?</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Software Engineering-D91E36">
  <p><b>Published on:</b> 2025-03-03T22:20:01Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Chia-Yi Su, Aakash Bansal, Vijayanta Jain, Sepideh Ghanavati, Sai Teja Peddinti, Collin McMillan</p>
    <p><b>Summary:</b> A "privacy behavior" in software is an action where the software uses
personal information for a service or a feature, such as a website using
location to provide content relevant to a user. Programmers are required by
regulations or application stores to provide privacy notices and labels
describing these privacy behaviors. Although many tools and research prototypes
have been developed to help programmers generate these notices by analyzing the
source code, these approaches are often fairly coarse-grained (i.e., at the
level of whole methods or files, rather than at the statement level). But this
is not necessarily how privacy behaviors exist in code. Privacy behaviors are
embedded in specific statements in code. Current literature does not examine
what statements programmers see as most important, how consistent these views
are, or how to detect them. In this paper, we conduct an empirical study to
examine which statements programmers view as most-related to privacy behaviors.
We find that expression statements that make function calls are most associated
with privacy behaviors, while the type of privacy label has little effect on
the attributes of the selected statements. We then propose an approach to
automatically detect these privacy-relevant statements by fine-tuning three
large language models with the data from the study. We observe that the
agreement between our approach and participants is comparable to or higher than
an agreement between two participants. Our study and detection approach can
help programmers understand which statements in code affect privacy in mobile
applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.02019v1">SLAP: Secure Location-proof and Anonymous Privacy-preserving Spectrum
  Access</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-03T19:52:56Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Saleh Darzi, Attila A. Yavuz</p>
    <p><b>Summary:</b> The rapid advancements in wireless technology have significantly increased
the demand for communication resources, leading to the development of Spectrum
Access Systems (SAS). However, network regulations require disclosing sensitive
user information, such as location coordinates and transmission details,
raising critical privacy concerns. Moreover, as a database-driven architecture
reliant on user-provided data, SAS necessitates robust location verification to
counter identity and location spoofing attacks and remains a primary target for
denial-of-service (DoS) attacks. Addressing these security challenges while
adhering to regulatory requirements is essential. In this paper, we propose
SLAP, a novel framework that ensures location privacy and anonymity during
spectrum queries, usage notifications, and location-proof acquisition. Our
solution includes an adaptive dual-scenario location verification mechanism
with architectural flexibility and a fallback option, along with a counter-DoS
approach using time-lock puzzles. We prove the security of SLAP and demonstrate
its advantages over existing solutions through comprehensive performance
evaluations.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.02017v1">A Lightweight and Secure Deep Learning Model for Privacy-Preserving
  Federated Learning in Intelligent Enterprises</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B">
  <p><b>Published on:</b> 2025-03-03T19:51:13Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Reza Fotohi, Fereidoon Shams Aliee, Bahar Farahani</p>
    <p><b>Summary:</b> The ever growing Internet of Things (IoT) connections drive a new type of
organization, the Intelligent Enterprise. In intelligent enterprises, machine
learning based models are adopted to extract insights from data. Due to the
efficiency and privacy challenges of these traditional models, a new federated
learning (FL) paradigm has emerged. In FL, multiple enterprises can jointly
train a model to update a final model. However, firstly, FL trained models
usually perform worse than centralized models, especially when enterprises
training data is non-IID (Independent and Identically Distributed). Second, due
to the centrality of FL and the untrustworthiness of local enterprises,
traditional FL solutions are vulnerable to poisoning and inference attacks and
violate privacy. Thirdly, the continuous transfer of parameters between
enterprises and servers increases communication costs. To this end, the
FedAnil+ model is proposed, a novel, lightweight, and secure Federated Deep
Learning Model that includes three main phases. In the first phase, the goal is
to solve the data type distribution skew challenge. Addressing privacy concerns
against poisoning and inference attacks is covered in the second phase.
Finally, to alleviate the communication overhead, a novel compression approach
is proposed that significantly reduces the size of the updates. The experiment
results validate that FedAnil+ is secure against inference and poisoning
attacks with better accuracy. In addition, it shows improvements over existing
approaches in terms of model accuracy (13%, 16%, and 26%), communication cost
(17%, 21%, and 25%), and computation cost (7%, 9%, and 11%).</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.01482v1">Revisiting Locally Differentially Private Protocols: Towards Better
  Trade-offs in Privacy, Utility, and Attack Resistance</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-03T12:41:01Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Héber H. Arcolezi, Sébastien Gambs</p>
    <p><b>Summary:</b> Local Differential Privacy (LDP) offers strong privacy protection, especially
in settings in which the server collecting the data is untrusted. However,
designing LDP mechanisms that achieve an optimal trade-off between privacy,
utility, and robustness to adversarial inference attacks remains challenging.
In this work, we introduce a general multi-objective optimization framework for
refining LDP protocols, enabling the joint optimization of privacy and utility
under various adversarial settings. While our framework is flexible enough to
accommodate multiple privacy and security attacks as well as utility metrics,
in this paper we specifically optimize for Attacker Success Rate (ASR) under
distinguishability attack as a measure of privacy and Mean Squared Error (MSE)
as a measure of utility. We systematically revisit these trade-offs by
analyzing eight state-of-the-art LDP protocols and proposing refined
counterparts that leverage tailored optimization techniques. Experimental
results demonstrate that our proposed adaptive mechanisms consistently
outperform their non-adaptive counterparts, reducing ASR by up to five orders
of magnitude while maintaining competitive utility. Analytical derivations also
confirm the effectiveness of our mechanisms, moving them closer to the ASR-MSE
Pareto frontier.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.01470v1">Position: Ensuring mutual privacy is necessary for effective external
  evaluation of proprietary AI systems</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB"> <img alt="Category Badge" src="https://img.shields.io/badge/Artificial Intelligence-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-03T12:24:59Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Ben Bucknall, Robert F. Trager, Michael A. Osborne</p>
    <p><b>Summary:</b> The external evaluation of AI systems is increasingly recognised as a crucial
approach for understanding their potential risks. However, facilitating
external evaluation in practice faces significant challenges in balancing
evaluators' need for system access with AI developers' privacy and security
concerns. Additionally, evaluators have reason to protect their own privacy -
for example, in order to maintain the integrity of held-out test sets. We refer
to the challenge of ensuring both developers' and evaluators' privacy as one of
providing mutual privacy. In this position paper, we argue that (i) addressing
this mutual privacy challenge is essential for effective external evaluation of
AI systems, and (ii) current methods for facilitating external evaluation
inadequately address this challenge, particularly when it comes to preserving
evaluators' privacy. In making these arguments, we formalise the mutual privacy
problem; examine the privacy and access requirements of both model owners and
evaluators; and explore potential solutions to this challenge, including
through the application of cryptographic and hardware-based approaches.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.01208v1">Watch Out Your Album! On the Inadvertent Privacy Memorization in
  Multi-Modal Large Language Models</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762">
  <p><b>Published on:</b> 2025-03-03T06:10:27Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Tianjie Ju, Yi Hua, Hao Fei, Zhenyu Shao, Yubin Zheng, Haodong Zhao, Mong-Li Lee, Wynne Hsu, Zhuosheng Zhang, Gongshen Liu</p>
    <p><b>Summary:</b> Multi-Modal Large Language Models (MLLMs) have exhibited remarkable
performance on various vision-language tasks such as Visual Question Answering
(VQA). Despite accumulating evidence of privacy concerns associated with
task-relevant content, it remains unclear whether MLLMs inadvertently memorize
private content that is entirely irrelevant to the training tasks. In this
paper, we investigate how randomly generated task-irrelevant private content
can become spuriously correlated with downstream objectives due to partial
mini-batch training dynamics, thus causing inadvertent memorization.
Concretely, we randomly generate task-irrelevant watermarks into VQA
fine-tuning images at varying probabilities and propose a novel probing
framework to determine whether MLLMs have inadvertently encoded such content.
Our experiments reveal that MLLMs exhibit notably different training behaviors
in partial mini-batch settings with task-irrelevant watermarks embedded.
Furthermore, through layer-wise probing, we demonstrate that MLLMs trigger
distinct representational patterns when encountering previously seen
task-irrelevant knowledge, even if this knowledge does not influence their
output during prompting. Our code is available at
https://github.com/illusionhi/ProbingPrivacy.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.01089v1">Privacy-preserving Machine Learning in Internet of Vehicle Applications:
  Fundamentals, Recent Advances, and Future Direction</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36">
  <p><b>Published on:</b> 2025-03-03T01:24:04Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Nazmul Islam, Mohammad Zulkernine</p>
    <p><b>Summary:</b> Machine learning (ML) has revolutionized Internet of Vehicles (IoV)
applications by enhancing intelligent transportation, autonomous driving
capabilities, and various connected services within a large, heterogeneous
network. However, the increased connectivity and massive data exchange for ML
applications introduce significant privacy challenges. Privacy-preserving
machine learning (PPML) offers potential solutions to address these challenges
by preserving privacy at various stages of the ML pipeline. Despite the rapid
development of ML-based IoV applications and the growing data privacy concerns,
there are limited comprehensive studies on the adoption of PPML within this
domain. Therefore, this study provides a comprehensive review of the
fundamentals, recent advancements, and the challenges of integrating PPML into
IoV applications. To conduct an extensive study, we first review existing
surveys of various PPML techniques and their integration into IoV across
different scopes. We then discuss the fundamentals of IoV and propose a
four-layer IoV architecture. Additionally, we categorize IoV applications into
three key domains and analyze the privacy challenges in leveraging ML for these
application domains. Next, we provide an overview of various PPML techniques,
highlighting their applicability and performance to address the privacy
challenges. Building on these fundamentals, we thoroughly review recent
advancements in integrating various PPML techniques within IoV applications,
discussing their frameworks, key features, and performance evaluation in terms
of privacy, utility, and efficiency. Finally, we identify current challenges
and propose future research directions to enhance privacy and reliability in
IoV applications.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.01000v1">Privacy vs. Profit: The Impact of Google's Manifest Version 3 (MV3)
  Update on Ad Blocker Effectiveness</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Computers and Society-5BC0EB">    
  <p><b>Published on:</b> 2025-03-02T19:41:34Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Karlo Lukic, Lazaros Papadopoulos</p>
    <p><b>Summary:</b> Google's recent update to the manifest file for Chrome browser
extensions-transitioning from manifest version 2 (MV2) to manifest version 3
(MV3)-has raised concerns among users and ad blocker providers, who worry that
the new restrictions, notably the shift from the powerful WebRequest API to the
more restrictive DeclarativeNetRequest API, might reduce ad blocker
effectiveness. Because ad blockers play a vital role for millions of users
seeking a more private and ad-free browsing experience, this study empirically
investigates how the MV3 update affects their ability to block ads and
trackers. Through a browser-based experiment conducted across multiple samples
of ad-supported websites, we compare the MV3 to MV2 instances of four widely
used ad blockers. Our results reveal no statistically significant reduction in
ad-blocking or anti-tracking effectiveness for MV3 ad blockers compared to
their MV2 counterparts, and in some cases, MV3 instances even exhibit slight
improvements in blocking trackers. These findings are reassuring for users,
indicating that the MV3 instances of popular ad blockers continue to provide
effective protection against intrusive ads and privacy-infringing trackers.
While some uncertainties remain, ad blocker providers appear to have
successfully navigated the MV3 update, finding solutions that maintain the core
functionality of their ad blockers.</p>
  </details>
</div>


<div class="arxiv-entry">
  <h3><a href="http://arxiv.org/abs/2503.00703v1">Towards hyperparameter-free optimization with differential privacy</a></h3>
  <img alt="Category Badge" src="https://img.shields.io/badge/Machine Learning-662E9B"> <img alt="Category Badge" src="https://img.shields.io/badge/Computation and Language-04E762"> <img alt="Category Badge" src="https://img.shields.io/badge/Cryptography and Security-D91E36"> <img alt="Category Badge" src="https://img.shields.io/badge/Computer Vision and Pattern Recognition-F9C80E">
  <p><b>Published on:</b> 2025-03-02T02:59:52Z</p>
  <details>
    <summary>More Details</summary>
    <p><b>Authors:</b> Zhiqi Bu, Ruixuan Liu</p>
    <p><b>Summary:</b> Differential privacy (DP) is a privacy-preserving paradigm that protects the
training data when training deep learning models. Critically, the performance
of models is determined by the training hyperparameters, especially those of
the learning rate schedule, thus requiring fine-grained hyperparameter tuning
on the data. In practice, it is common to tune the learning rate
hyperparameters through the grid search that (1) is computationally expensive
as multiple runs are needed, and (2) increases the risk of data leakage as the
selection of hyperparameters is data-dependent. In this work, we adapt the
automatic learning rate schedule to DP optimization for any models and
optimizers, so as to significantly mitigate or even eliminate the cost of
hyperparameter tuning when applied together with automatic per-sample gradient
clipping. Our hyperparameter-free DP optimization is almost as computationally
efficient as the standard non-DP optimization, and achieves state-of-the-art DP
performance on various language and vision tasks.</p>
  </details>
</div>

